{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To save file size, I cleared some of the output. You could redo these inputs to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time \n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_svmlight_file(\"housing_scale.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data[0],data[1]\n",
    "X = X.toarray()\n",
    "y = y.reshape((len(y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 127\n"
     ]
    }
   ],
   "source": [
    "print(str(len(X_train))+' '+str(len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement a base class of Linear Regression Model\n",
    "class LinearRegression(object):\n",
    "    def __init__(self):\n",
    "        self.W = 0\n",
    "        self.lamda = 0.0\n",
    "        \n",
    "    \n",
    "    def initialPar(self,shape,method='default'):\n",
    "        '''\n",
    "        shape: the shape of par W\n",
    "        mathod: the initial method of par W\n",
    "        '''\n",
    "        if method == 'default':\n",
    "            #add one dimension of b\n",
    "            self.W = np.zeros(shape+1)\n",
    "        elif method == 'random':\n",
    "            self.W = np.random.rand(shape+1)\n",
    "        elif method == 'norm':\n",
    "            self.W = np.random.randn(shape+1)\n",
    "        self.W = self.W.reshape((len(self.W),1))\n",
    "        \n",
    "    def getLoss(self,y_pre,y_ground,method='squared'):\n",
    "        '''\n",
    "        method: the method of get Loss\n",
    "        '''\n",
    "        #self.lamda = lamda\n",
    "        if method == 'absolute':\n",
    "            loss = np.sum(np.fabs(y_pre-y_ground)) / y_pre.shape[0]\n",
    "        elif method == 'squared':\n",
    "            loss = np.sum(np.square(y_pre-y_ground)) /(2* y_pre.shape[0])\n",
    "        sloss = loss + np.sum(np.square(self.W))*self.lamda/2\n",
    "        print('Pure loss: '+str(loss)+'.....Total loss: '+str(sloss))\n",
    "        return sloss\n",
    "        \n",
    "    def train(self,X_train,y_train):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        X_test = np.insert(X_test,0,1,axis=1)\n",
    "        y_pre = np.dot(X_test,self.W)\n",
    "        return y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a subclass of Linear Regression Model: Closed-Form Solution\n",
    "class ClosedFormLinearRegression(LinearRegression):\n",
    "    def __init__(self):\n",
    "        super(ClosedFormLinearRegression,self).__init__()\n",
    "        \n",
    "    def train(self,X_train,y_train):\n",
    "        start = time.time()\n",
    "        print('Training...')\n",
    "        X_train = np.insert(X_train,0,1,axis=1)\n",
    "        print(X_train.shape)\n",
    "        w_temp = self.lamda * np.identity(X_train.shape[1]) + np.dot(np.transpose(X_train),X_train)\n",
    "        #w_temp = np.dot(np.transpose(X_train),X_train)\n",
    "        try:\n",
    "            w_temp = np.linalg.inv(w_temp)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print('Singular matrix!!!!!Process interprutted!!!!!')\n",
    "        else:\n",
    "            w_temp = np.dot(w_temp,np.transpose(X_train))\n",
    "            w_temp = np.dot(w_temp,y_train)\n",
    "        self.W = w_temp\n",
    "        print('Training...'+str(time.time()-start)+'s...Successful!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1 = ClosedFormLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc1.initialPar(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(379, 14)\n",
      "Training...0.4996984004974365s...Successful!!\n"
     ]
    }
   ],
   "source": [
    "cc1.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.29036552],\n",
       "       [ -4.94528571],\n",
       "       [  2.37003287],\n",
       "       [  0.45394093],\n",
       "       [  1.33044433],\n",
       "       [ -3.73390086],\n",
       "       [ 10.26552349],\n",
       "       [ -0.04712845],\n",
       "       [ -7.9074083 ],\n",
       "       [  3.61482155],\n",
       "       [ -3.6777979 ],\n",
       "       [ -3.8785513 ],\n",
       "       [  1.91193063],\n",
       "       [-10.14223224]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 10.678908835709004.....Total loss: 10.678908835709004\n"
     ]
    }
   ],
   "source": [
    "loss1 = cc1.getLoss(cc1.predict(X_val),y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_losses.append(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(379, 14)\n",
      "Training...0.0s...Successful!!\n",
      "Pure loss: 10.678908835709004.....Total loss: 10.678908835709004\n"
     ]
    }
   ],
   "source": [
    "cc2 = ClosedFormLinearRegression()\n",
    "cc2.initialPar(X_train.shape[1])\n",
    "cc2.train(X_train,y_train)\n",
    "loss2 = cc2.getLoss(cc2.predict(X_val),y_val)\n",
    "closed_losses.append(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(379, 14)\n",
      "Training...0.0s...Successful!!\n",
      "Pure loss: 10.678908835709004.....Total loss: 10.678908835709004\n"
     ]
    }
   ],
   "source": [
    "cc3 = ClosedFormLinearRegression()\n",
    "cc3.initialPar(X_train.shape[1])\n",
    "cc3.train(X_train,y_train)\n",
    "loss3 = cc3.getLoss(cc3.predict(X_val),y_val)\n",
    "closed_losses.append(loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(379, 14)\n",
      "Training...0.0s...Successful!!\n",
      "Pure loss: 10.678908835709004.....Total loss: 10.678908835709004\n"
     ]
    }
   ],
   "source": [
    "cc4 = ClosedFormLinearRegression()\n",
    "cc4.initialPar(X_train.shape[1])\n",
    "cc4.train(X_train,y_train)\n",
    "loss4 = cc4.getLoss(cc4.predict(X_val),y_val)\n",
    "closed_losses.append(loss4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(379, 14)\n",
      "Training...0.0s...Successful!!\n",
      "Pure loss: 10.678908835709004.....Total loss: 10.678908835709004\n"
     ]
    }
   ],
   "source": [
    "cc5 = ClosedFormLinearRegression()\n",
    "cc5.initialPar(X_train.shape[1])\n",
    "cc5.train(X_train,y_train)\n",
    "loss5 = cc5.getLoss(cc5.predict(X_val),y_val)\n",
    "closed_losses.append(loss5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,6),closed_losses)\n",
    "plt.xticks(range(1,6), ('cc1', 'cc2', 'cc3', 'cc4', 'cc5'))\n",
    "plt.xlabel('closed-form models')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('cf1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement a subclass of Linear Regression Model: Gradient Descent Solution\n",
    "class GradientDescentLinearRegression(LinearRegression):\n",
    "    def __init__(self):\n",
    "        super(GradientDescentLinearRegression,self).__init__()\n",
    "    \n",
    "    def train(self,X_train,y_train,X_val,y_val,maxLoop=5000,epsilon=0.01,rate0 = 0.01):\n",
    "        #f = open(filename,'w')\n",
    "        start = time.time()\n",
    "        print('Training...')\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        X_train = np.insert(X_train,0,1,axis=1)\n",
    "        for i in range(maxLoop):\n",
    "            rate = 1/float(i+1)+ rate0\n",
    "            count = np.random.choice(X_train.shape[0])\n",
    "            #f.write('\\nrandom choice: '+str(count))\n",
    "            x_sample = X_train[count]\n",
    "            y_sample = y_train[count]\n",
    "            x_sample = x_sample.reshape((1,len(x_sample)))\n",
    "            y_sample = y_sample.reshape((1,len(y_sample)))\n",
    "            #f.write('\\nX_sample:'+str(x_sample))\n",
    "            #f.write('\\ny_sample:'+str(y_sample))\n",
    "            #g_temp1 = self.lamda*self.W - np.dot(x_sample.T,y_sample)\n",
    "            #g_temp2 = np.dot(np.dot(x_sample.T,x_sample),self.W)\n",
    "            g_temp0 = np.dot(x_sample,self.W) - y_sample\n",
    "            g_temp1 = np.dot(x_sample.T,g_temp0)\n",
    "            gradient = g_temp1 + self.lamda*self.W\n",
    "            self.W = self.W - rate * gradient\n",
    "            #f.write('gradient: '+str(gradient)+'\\nlearning rate:  '+str(rate))\n",
    "            \n",
    "            \n",
    "            y_train_pre = np.dot(X_train,self.W)\n",
    "            y_val_pre = self.predict(X_val)\n",
    "            y_train_loss = self.getLoss(y_train_pre,y_train)\n",
    "            y_val_loss = self.getLoss(y_val_pre,y_val)\n",
    "            #f.write('\\nepoch '+str(i+1)+'   Training loss:   '+str(y_train_loss)+'  Valing loss:   '+str(y_val_loss)+'\\n\\n')\n",
    "            print('epoch '+str(i+1)+' learning rate:  '+str(rate)+'   Training loss:   '+str(y_train_loss)+'  Valing loss:   '+str(y_val_loss))\n",
    "            train_losses.append(y_train_loss)\n",
    "            val_losses.append(y_val_loss)\n",
    "            #if abs(loss - y_train_loss) > epsilon:\n",
    "            #    loss = y_train_loss\n",
    "            #else:\n",
    "            #    print(str(loss-y_train_loss))\n",
    "            #    print(\"Convergencing...\")\n",
    "            #    break\n",
    "        print('Training...'+str(time.time()-start)+'s...Successful!!')\n",
    "        plt.plot(range(1,len(train_losses)+1),train_losses)\n",
    "        plt.plot(range(1,len(val_losses)+1),val_losses)\n",
    "        return train_losses, val_losses\n",
    "        #f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg1 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg1.initialPar(X_train.shape[1],'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Pure loss: 8126.183129804695.....Total loss: 8126.183129804695\n",
      "Pure loss: 8769.0727041503.....Total loss: 8769.0727041503\n",
      "epoch 1 learning rate:  1.01   Training loss:   8126.183129804695  Valing loss:   8769.0727041503\n",
      "Pure loss: 99969.47582488031.....Total loss: 99969.47582488031\n",
      "Pure loss: 107721.00782283106.....Total loss: 107721.00782283106\n",
      "epoch 2 learning rate:  0.51   Training loss:   99969.47582488031  Valing loss:   107721.00782283106\n",
      "Pure loss: 75778.96707988664.....Total loss: 75778.96707988664\n",
      "Pure loss: 82906.92543137642.....Total loss: 82906.92543137642\n",
      "epoch 3 learning rate:  0.3433333333333333   Training loss:   75778.96707988664  Valing loss:   82906.92543137642\n",
      "Pure loss: 54795.22232046891.....Total loss: 54795.22232046891\n",
      "Pure loss: 57086.575836046126.....Total loss: 57086.575836046126\n",
      "epoch 4 learning rate:  0.26   Training loss:   54795.22232046891  Valing loss:   57086.575836046126\n",
      "Pure loss: 69479.25548930185.....Total loss: 69479.25548930185\n",
      "Pure loss: 75976.79877448286.....Total loss: 75976.79877448286\n",
      "epoch 5 learning rate:  0.21000000000000002   Training loss:   69479.25548930185  Valing loss:   75976.79877448286\n",
      "Pure loss: 56048.85613672233.....Total loss: 56048.85613672233\n",
      "Pure loss: 60701.48474063922.....Total loss: 60701.48474063922\n",
      "epoch 6 learning rate:  0.17666666666666667   Training loss:   56048.85613672233  Valing loss:   60701.48474063922\n",
      "Pure loss: 49008.42674197339.....Total loss: 49008.42674197339\n",
      "Pure loss: 51853.23419691711.....Total loss: 51853.23419691711\n",
      "epoch 7 learning rate:  0.15285714285714286   Training loss:   49008.42674197339  Valing loss:   51853.23419691711\n",
      "Pure loss: 47003.33047871673.....Total loss: 47003.33047871673\n",
      "Pure loss: 48662.1955855278.....Total loss: 48662.1955855278\n",
      "epoch 8 learning rate:  0.135   Training loss:   47003.33047871673  Valing loss:   48662.1955855278\n",
      "Pure loss: 42800.61348899709.....Total loss: 42800.61348899709\n",
      "Pure loss: 40410.13426172667.....Total loss: 40410.13426172667\n",
      "epoch 9 learning rate:  0.1211111111111111   Training loss:   42800.61348899709  Valing loss:   40410.13426172667\n",
      "Pure loss: 3415.084901227304.....Total loss: 3415.084901227304\n",
      "Pure loss: 3588.9108931801543.....Total loss: 3588.9108931801543\n",
      "epoch 10 learning rate:  0.11   Training loss:   3415.084901227304  Valing loss:   3588.9108931801543\n",
      "Pure loss: 3733.98533504791.....Total loss: 3733.98533504791\n",
      "Pure loss: 3850.068264298164.....Total loss: 3850.068264298164\n",
      "epoch 11 learning rate:  0.1009090909090909   Training loss:   3733.98533504791  Valing loss:   3850.068264298164\n",
      "Pure loss: 2446.859923482531.....Total loss: 2446.859923482531\n",
      "Pure loss: 2660.620637202707.....Total loss: 2660.620637202707\n",
      "epoch 12 learning rate:  0.09333333333333332   Training loss:   2446.859923482531  Valing loss:   2660.620637202707\n",
      "Pure loss: 2429.219662066875.....Total loss: 2429.219662066875\n",
      "Pure loss: 2609.1296837980067.....Total loss: 2609.1296837980067\n",
      "epoch 13 learning rate:  0.08692307692307692   Training loss:   2429.219662066875  Valing loss:   2609.1296837980067\n",
      "Pure loss: 1800.3917659363362.....Total loss: 1800.3917659363362\n",
      "Pure loss: 2002.433098625128.....Total loss: 2002.433098625128\n",
      "epoch 14 learning rate:  0.08142857142857142   Training loss:   1800.3917659363362  Valing loss:   2002.433098625128\n",
      "Pure loss: 1802.8554193774744.....Total loss: 1802.8554193774744\n",
      "Pure loss: 2004.2903406628177.....Total loss: 2004.2903406628177\n",
      "epoch 15 learning rate:  0.07666666666666666   Training loss:   1802.8554193774744  Valing loss:   2004.2903406628177\n",
      "Pure loss: 1794.12044864023.....Total loss: 1794.12044864023\n",
      "Pure loss: 1995.4103867814988.....Total loss: 1995.4103867814988\n",
      "epoch 16 learning rate:  0.0725   Training loss:   1794.12044864023  Valing loss:   1995.4103867814988\n",
      "Pure loss: 769.316156848837.....Total loss: 769.316156848837\n",
      "Pure loss: 884.5018578381819.....Total loss: 884.5018578381819\n",
      "epoch 17 learning rate:  0.0688235294117647   Training loss:   769.316156848837  Valing loss:   884.5018578381819\n",
      "Pure loss: 687.4778708933761.....Total loss: 687.4778708933761\n",
      "Pure loss: 791.0599159975877.....Total loss: 791.0599159975877\n",
      "epoch 18 learning rate:  0.06555555555555555   Training loss:   687.4778708933761  Valing loss:   791.0599159975877\n",
      "Pure loss: 687.5128534239204.....Total loss: 687.5128534239204\n",
      "Pure loss: 791.0952722398301.....Total loss: 791.0952722398301\n",
      "epoch 19 learning rate:  0.06263157894736841   Training loss:   687.5128534239204  Valing loss:   791.0952722398301\n",
      "Pure loss: 618.4313070526098.....Total loss: 618.4313070526098\n",
      "Pure loss: 709.5336709415441.....Total loss: 709.5336709415441\n",
      "epoch 20 learning rate:  0.060000000000000005   Training loss:   618.4313070526098  Valing loss:   709.5336709415441\n",
      "Pure loss: 801.3566805526847.....Total loss: 801.3566805526847\n",
      "Pure loss: 913.1685350838981.....Total loss: 913.1685350838981\n",
      "epoch 21 learning rate:  0.05761904761904762   Training loss:   801.3566805526847  Valing loss:   913.1685350838981\n",
      "Pure loss: 797.5740770361805.....Total loss: 797.5740770361805\n",
      "Pure loss: 891.6613774175994.....Total loss: 891.6613774175994\n",
      "epoch 22 learning rate:  0.05545454545454546   Training loss:   797.5740770361805  Valing loss:   891.6613774175994\n",
      "Pure loss: 579.2387282934875.....Total loss: 579.2387282934875\n",
      "Pure loss: 666.7469068211643.....Total loss: 666.7469068211643\n",
      "epoch 23 learning rate:  0.05347826086956522   Training loss:   579.2387282934875  Valing loss:   666.7469068211643\n",
      "Pure loss: 550.6004534022782.....Total loss: 550.6004534022782\n",
      "Pure loss: 636.5602494166046.....Total loss: 636.5602494166046\n",
      "epoch 24 learning rate:  0.051666666666666666   Training loss:   550.6004534022782  Valing loss:   636.5602494166046\n",
      "Pure loss: 526.0834334731803.....Total loss: 526.0834334731803\n",
      "Pure loss: 610.3177848839626.....Total loss: 610.3177848839626\n",
      "epoch 25 learning rate:  0.05   Training loss:   526.0834334731803  Valing loss:   610.3177848839626\n",
      "Pure loss: 506.94068686574536.....Total loss: 506.94068686574536\n",
      "Pure loss: 583.7620083347713.....Total loss: 583.7620083347713\n",
      "epoch 26 learning rate:  0.048461538461538466   Training loss:   506.94068686574536  Valing loss:   583.7620083347713\n",
      "Pure loss: 509.3856077368588.....Total loss: 509.3856077368588\n",
      "Pure loss: 584.5883331510186.....Total loss: 584.5883331510186\n",
      "epoch 27 learning rate:  0.04703703703703704   Training loss:   509.3856077368588  Valing loss:   584.5883331510186\n",
      "Pure loss: 542.7222493752934.....Total loss: 542.7222493752934\n",
      "Pure loss: 622.1149925661024.....Total loss: 622.1149925661024\n",
      "epoch 28 learning rate:  0.045714285714285714   Training loss:   542.7222493752934  Valing loss:   622.1149925661024\n",
      "Pure loss: 512.7654520661993.....Total loss: 512.7654520661993\n",
      "Pure loss: 591.0081495727588.....Total loss: 591.0081495727588\n",
      "epoch 29 learning rate:  0.04448275862068966   Training loss:   512.7654520661993  Valing loss:   591.0081495727588\n",
      "Pure loss: 540.7875100992886.....Total loss: 540.7875100992886\n",
      "Pure loss: 622.7020380060293.....Total loss: 622.7020380060293\n",
      "epoch 30 learning rate:  0.043333333333333335   Training loss:   540.7875100992886  Valing loss:   622.7020380060293\n",
      "Pure loss: 568.7857947138658.....Total loss: 568.7857947138658\n",
      "Pure loss: 646.2241820245308.....Total loss: 646.2241820245308\n",
      "epoch 31 learning rate:  0.04225806451612903   Training loss:   568.7857947138658  Valing loss:   646.2241820245308\n",
      "Pure loss: 573.8009510330589.....Total loss: 573.8009510330589\n",
      "Pure loss: 649.1348937982008.....Total loss: 649.1348937982008\n",
      "epoch 32 learning rate:  0.04125   Training loss:   573.8009510330589  Valing loss:   649.1348937982008\n",
      "Pure loss: 344.2841892637471.....Total loss: 344.2841892637471\n",
      "Pure loss: 384.4008752200159.....Total loss: 384.4008752200159\n",
      "epoch 33 learning rate:  0.040303030303030306   Training loss:   344.2841892637471  Valing loss:   384.4008752200159\n",
      "Pure loss: 351.771223293667.....Total loss: 351.771223293667\n",
      "Pure loss: 386.1048672068767.....Total loss: 386.1048672068767\n",
      "epoch 34 learning rate:  0.039411764705882354   Training loss:   351.771223293667  Valing loss:   386.1048672068767\n",
      "Pure loss: 371.58094924777623.....Total loss: 371.58094924777623\n",
      "Pure loss: 392.26130468455557.....Total loss: 392.26130468455557\n",
      "epoch 35 learning rate:  0.03857142857142857   Training loss:   371.58094924777623  Valing loss:   392.26130468455557\n",
      "Pure loss: 293.92918298510494.....Total loss: 293.92918298510494\n",
      "Pure loss: 311.43217486214746.....Total loss: 311.43217486214746\n",
      "epoch 36 learning rate:  0.03777777777777778   Training loss:   293.92918298510494  Valing loss:   311.43217486214746\n",
      "Pure loss: 294.71578557127054.....Total loss: 294.71578557127054\n",
      "Pure loss: 310.99963819430144.....Total loss: 310.99963819430144\n",
      "epoch 37 learning rate:  0.03702702702702703   Training loss:   294.71578557127054  Valing loss:   310.99963819430144\n",
      "Pure loss: 312.56228333199783.....Total loss: 312.56228333199783\n",
      "Pure loss: 313.58505662136344.....Total loss: 313.58505662136344\n",
      "epoch 38 learning rate:  0.03631578947368421   Training loss:   312.56228333199783  Valing loss:   313.58505662136344\n",
      "Pure loss: 284.06473955231604.....Total loss: 284.06473955231604\n",
      "Pure loss: 284.42164469695257.....Total loss: 284.42164469695257\n",
      "epoch 39 learning rate:  0.03564102564102564   Training loss:   284.06473955231604  Valing loss:   284.42164469695257\n",
      "Pure loss: 302.76379821391834.....Total loss: 302.76379821391834\n",
      "Pure loss: 294.6133415409301.....Total loss: 294.6133415409301\n",
      "epoch 40 learning rate:  0.035   Training loss:   302.76379821391834  Valing loss:   294.6133415409301\n",
      "Pure loss: 290.919619001747.....Total loss: 290.919619001747\n",
      "Pure loss: 287.5833852983048.....Total loss: 287.5833852983048\n",
      "epoch 41 learning rate:  0.03439024390243903   Training loss:   290.919619001747  Valing loss:   287.5833852983048\n",
      "Pure loss: 293.3025069957906.....Total loss: 293.3025069957906\n",
      "Pure loss: 288.5635868082362.....Total loss: 288.5635868082362\n",
      "epoch 42 learning rate:  0.03380952380952381   Training loss:   293.3025069957906  Valing loss:   288.5635868082362\n",
      "Pure loss: 328.2172573725025.....Total loss: 328.2172573725025\n",
      "Pure loss: 309.48936631426096.....Total loss: 309.48936631426096\n",
      "epoch 43 learning rate:  0.033255813953488374   Training loss:   328.2172573725025  Valing loss:   309.48936631426096\n",
      "Pure loss: 315.2065021426955.....Total loss: 315.2065021426955\n",
      "Pure loss: 298.6942118825048.....Total loss: 298.6942118825048\n",
      "epoch 44 learning rate:  0.03272727272727273   Training loss:   315.2065021426955  Valing loss:   298.6942118825048\n",
      "Pure loss: 335.361020281683.....Total loss: 335.361020281683\n",
      "Pure loss: 312.5364583556644.....Total loss: 312.5364583556644\n",
      "epoch 45 learning rate:  0.03222222222222222   Training loss:   335.361020281683  Valing loss:   312.5364583556644\n",
      "Pure loss: 318.8315813790459.....Total loss: 318.8315813790459\n",
      "Pure loss: 292.3720715730683.....Total loss: 292.3720715730683\n",
      "epoch 46 learning rate:  0.03173913043478261   Training loss:   318.8315813790459  Valing loss:   292.3720715730683\n",
      "Pure loss: 295.66180907846973.....Total loss: 295.66180907846973\n",
      "Pure loss: 276.9710042179632.....Total loss: 276.9710042179632\n",
      "epoch 47 learning rate:  0.03127659574468085   Training loss:   295.66180907846973  Valing loss:   276.9710042179632\n",
      "Pure loss: 298.7780401618632.....Total loss: 298.7780401618632\n",
      "Pure loss: 278.5812682171157.....Total loss: 278.5812682171157\n",
      "epoch 48 learning rate:  0.03083333333333333   Training loss:   298.7780401618632  Valing loss:   278.5812682171157\n",
      "Pure loss: 290.814543024302.....Total loss: 290.814543024302\n",
      "Pure loss: 273.6419816022671.....Total loss: 273.6419816022671\n",
      "epoch 49 learning rate:  0.03040816326530612   Training loss:   290.814543024302  Valing loss:   273.6419816022671\n",
      "Pure loss: 283.36080612480913.....Total loss: 283.36080612480913\n",
      "Pure loss: 268.9749850740296.....Total loss: 268.9749850740296\n",
      "epoch 50 learning rate:  0.03   Training loss:   283.36080612480913  Valing loss:   268.9749850740296\n",
      "Pure loss: 284.09618798107385.....Total loss: 284.09618798107385\n",
      "Pure loss: 269.20875001888004.....Total loss: 269.20875001888004\n",
      "epoch 51 learning rate:  0.029607843137254904   Training loss:   284.09618798107385  Valing loss:   269.20875001888004\n",
      "Pure loss: 277.6998915125461.....Total loss: 277.6998915125461\n",
      "Pure loss: 283.87917691122965.....Total loss: 283.87917691122965\n",
      "epoch 52 learning rate:  0.029230769230769234   Training loss:   277.6998915125461  Valing loss:   283.87917691122965\n",
      "Pure loss: 281.69010523011326.....Total loss: 281.69010523011326\n",
      "Pure loss: 290.6085950747494.....Total loss: 290.6085950747494\n",
      "epoch 53 learning rate:  0.028867924528301885   Training loss:   281.69010523011326  Valing loss:   290.6085950747494\n",
      "Pure loss: 258.17750127574243.....Total loss: 258.17750127574243\n",
      "Pure loss: 261.5861024128689.....Total loss: 261.5861024128689\n",
      "epoch 54 learning rate:  0.02851851851851852   Training loss:   258.17750127574243  Valing loss:   261.5861024128689\n",
      "Pure loss: 261.5488357429127.....Total loss: 261.5488357429127\n",
      "Pure loss: 250.15900008832975.....Total loss: 250.15900008832975\n",
      "epoch 55 learning rate:  0.028181818181818183   Training loss:   261.5488357429127  Valing loss:   250.15900008832975\n",
      "Pure loss: 263.4603019226624.....Total loss: 263.4603019226624\n",
      "Pure loss: 250.77273339965623.....Total loss: 250.77273339965623\n",
      "epoch 56 learning rate:  0.027857142857142858   Training loss:   263.4603019226624  Valing loss:   250.77273339965623\n",
      "Pure loss: 278.809201516798.....Total loss: 278.809201516798\n",
      "Pure loss: 257.26489237003113.....Total loss: 257.26489237003113\n",
      "epoch 57 learning rate:  0.027543859649122808   Training loss:   278.809201516798  Valing loss:   257.26489237003113\n",
      "Pure loss: 263.81675061200457.....Total loss: 263.81675061200457\n",
      "Pure loss: 245.26700424978367.....Total loss: 245.26700424978367\n",
      "epoch 58 learning rate:  0.02724137931034483   Training loss:   263.81675061200457  Valing loss:   245.26700424978367\n",
      "Pure loss: 269.8157623376846.....Total loss: 269.8157623376846\n",
      "Pure loss: 248.2224571585499.....Total loss: 248.2224571585499\n",
      "epoch 59 learning rate:  0.02694915254237288   Training loss:   269.8157623376846  Valing loss:   248.2224571585499\n",
      "Pure loss: 252.0871505845912.....Total loss: 252.0871505845912\n",
      "Pure loss: 238.93738273687697.....Total loss: 238.93738273687697\n",
      "epoch 60 learning rate:  0.026666666666666665   Training loss:   252.0871505845912  Valing loss:   238.93738273687697\n",
      "Pure loss: 242.83553769692116.....Total loss: 242.83553769692116\n",
      "Pure loss: 258.94864281548604.....Total loss: 258.94864281548604\n",
      "epoch 61 learning rate:  0.02639344262295082   Training loss:   242.83553769692116  Valing loss:   258.94864281548604\n",
      "Pure loss: 242.5349086650673.....Total loss: 242.5349086650673\n",
      "Pure loss: 258.5512438333536.....Total loss: 258.5512438333536\n",
      "epoch 62 learning rate:  0.026129032258064518   Training loss:   242.5349086650673  Valing loss:   258.5512438333536\n",
      "Pure loss: 253.93246918669865.....Total loss: 253.93246918669865\n",
      "Pure loss: 274.6092823460429.....Total loss: 274.6092823460429\n",
      "epoch 63 learning rate:  0.025873015873015874   Training loss:   253.93246918669865  Valing loss:   274.6092823460429\n",
      "Pure loss: 246.571591835746.....Total loss: 246.571591835746\n",
      "Pure loss: 264.98120230525114.....Total loss: 264.98120230525114\n",
      "epoch 64 learning rate:  0.025625000000000002   Training loss:   246.571591835746  Valing loss:   264.98120230525114\n",
      "Pure loss: 242.06227544847255.....Total loss: 242.06227544847255\n",
      "Pure loss: 258.0306033734043.....Total loss: 258.0306033734043\n",
      "epoch 65 learning rate:  0.025384615384615387   Training loss:   242.06227544847255  Valing loss:   258.0306033734043\n",
      "Pure loss: 235.00875410346057.....Total loss: 235.00875410346057\n",
      "Pure loss: 245.65212989589043.....Total loss: 245.65212989589043\n",
      "epoch 66 learning rate:  0.025151515151515154   Training loss:   235.00875410346057  Valing loss:   245.65212989589043\n",
      "Pure loss: 236.08667747808036.....Total loss: 236.08667747808036\n",
      "Pure loss: 248.4993660079645.....Total loss: 248.4993660079645\n",
      "epoch 67 learning rate:  0.024925373134328358   Training loss:   236.08667747808036  Valing loss:   248.4993660079645\n",
      "Pure loss: 236.7011270005169.....Total loss: 236.7011270005169\n",
      "Pure loss: 251.121568973607.....Total loss: 251.121568973607\n",
      "epoch 68 learning rate:  0.024705882352941175   Training loss:   236.7011270005169  Valing loss:   251.121568973607\n",
      "Pure loss: 234.1354541266229.....Total loss: 234.1354541266229\n",
      "Pure loss: 238.7705479400508.....Total loss: 238.7705479400508\n",
      "epoch 69 learning rate:  0.024492753623188406   Training loss:   234.1354541266229  Valing loss:   238.7705479400508\n",
      "Pure loss: 236.10590972393285.....Total loss: 236.10590972393285\n",
      "Pure loss: 238.69170144695929.....Total loss: 238.69170144695929\n",
      "epoch 70 learning rate:  0.024285714285714285   Training loss:   236.10590972393285  Valing loss:   238.69170144695929\n",
      "Pure loss: 236.0957097892522.....Total loss: 236.0957097892522\n",
      "Pure loss: 238.68860541988624.....Total loss: 238.68860541988624\n",
      "epoch 71 learning rate:  0.02408450704225352   Training loss:   236.0957097892522  Valing loss:   238.68860541988624\n",
      "Pure loss: 234.1805662717307.....Total loss: 234.1805662717307\n",
      "Pure loss: 239.07533875287695.....Total loss: 239.07533875287695\n",
      "epoch 72 learning rate:  0.02388888888888889   Training loss:   234.1805662717307  Valing loss:   239.07533875287695\n",
      "Pure loss: 230.55902618225173.....Total loss: 230.55902618225173\n",
      "Pure loss: 243.47863567931864.....Total loss: 243.47863567931864\n",
      "epoch 73 learning rate:  0.023698630136986303   Training loss:   230.55902618225173  Valing loss:   243.47863567931864\n",
      "Pure loss: 226.91806611729237.....Total loss: 226.91806611729237\n",
      "Pure loss: 235.09375640764316.....Total loss: 235.09375640764316\n",
      "epoch 74 learning rate:  0.023513513513513516   Training loss:   226.91806611729237  Valing loss:   235.09375640764316\n",
      "Pure loss: 230.03592074661924.....Total loss: 230.03592074661924\n",
      "Pure loss: 236.16239670587683.....Total loss: 236.16239670587683\n",
      "epoch 75 learning rate:  0.023333333333333334   Training loss:   230.03592074661924  Valing loss:   236.16239670587683\n",
      "Pure loss: 236.3964697768563.....Total loss: 236.3964697768563\n",
      "Pure loss: 245.14741663412434.....Total loss: 245.14741663412434\n",
      "epoch 76 learning rate:  0.023157894736842106   Training loss:   236.3964697768563  Valing loss:   245.14741663412434\n",
      "Pure loss: 234.93666767428772.....Total loss: 234.93666767428772\n",
      "Pure loss: 244.44668850607334.....Total loss: 244.44668850607334\n",
      "epoch 77 learning rate:  0.022987012987012986   Training loss:   234.93666767428772  Valing loss:   244.44668850607334\n",
      "Pure loss: 242.30753006045703.....Total loss: 242.30753006045703\n",
      "Pure loss: 244.30768933052923.....Total loss: 244.30768933052923\n",
      "epoch 78 learning rate:  0.02282051282051282   Training loss:   242.30753006045703  Valing loss:   244.30768933052923\n",
      "Pure loss: 221.08987638257506.....Total loss: 221.08987638257506\n",
      "Pure loss: 229.92393625376374.....Total loss: 229.92393625376374\n",
      "epoch 79 learning rate:  0.022658227848101266   Training loss:   221.08987638257506  Valing loss:   229.92393625376374\n",
      "Pure loss: 219.8551868622587.....Total loss: 219.8551868622587\n",
      "Pure loss: 229.8518331216966.....Total loss: 229.8518331216966\n",
      "epoch 80 learning rate:  0.0225   Training loss:   219.8551868622587  Valing loss:   229.8518331216966\n",
      "Pure loss: 230.97163483711188.....Total loss: 230.97163483711188\n",
      "Pure loss: 245.7772144766495.....Total loss: 245.7772144766495\n",
      "epoch 81 learning rate:  0.02234567901234568   Training loss:   230.97163483711188  Valing loss:   245.7772144766495\n",
      "Pure loss: 230.29807646612593.....Total loss: 230.29807646612593\n",
      "Pure loss: 245.48455411706027.....Total loss: 245.48455411706027\n",
      "epoch 82 learning rate:  0.02219512195121951   Training loss:   230.29807646612593  Valing loss:   245.48455411706027\n",
      "Pure loss: 217.26928701644485.....Total loss: 217.26928701644485\n",
      "Pure loss: 231.75187387326247.....Total loss: 231.75187387326247\n",
      "epoch 83 learning rate:  0.022048192771084336   Training loss:   217.26928701644485  Valing loss:   231.75187387326247\n",
      "Pure loss: 217.5875428172268.....Total loss: 217.5875428172268\n",
      "Pure loss: 225.14093808533195.....Total loss: 225.14093808533195\n",
      "epoch 84 learning rate:  0.021904761904761906   Training loss:   217.5875428172268  Valing loss:   225.14093808533195\n",
      "Pure loss: 218.53694878317347.....Total loss: 218.53694878317347\n",
      "Pure loss: 225.04905379850396.....Total loss: 225.04905379850396\n",
      "epoch 85 learning rate:  0.021764705882352943   Training loss:   218.53694878317347  Valing loss:   225.04905379850396\n",
      "Pure loss: 222.39388493852894.....Total loss: 222.39388493852894\n",
      "Pure loss: 226.52045999155425.....Total loss: 226.52045999155425\n",
      "epoch 86 learning rate:  0.021627906976744188   Training loss:   222.39388493852894  Valing loss:   226.52045999155425\n",
      "Pure loss: 210.50467975036523.....Total loss: 210.50467975036523\n",
      "Pure loss: 213.2991431020531.....Total loss: 213.2991431020531\n",
      "epoch 87 learning rate:  0.02149425287356322   Training loss:   210.50467975036523  Valing loss:   213.2991431020531\n",
      "Pure loss: 204.88064841595082.....Total loss: 204.88064841595082\n",
      "Pure loss: 207.8764045834426.....Total loss: 207.8764045834426\n",
      "epoch 88 learning rate:  0.021363636363636362   Training loss:   204.88064841595082  Valing loss:   207.8764045834426\n",
      "Pure loss: 204.15114061868812.....Total loss: 204.15114061868812\n",
      "Pure loss: 206.06859264128764.....Total loss: 206.06859264128764\n",
      "epoch 89 learning rate:  0.021235955056179777   Training loss:   204.15114061868812  Valing loss:   206.06859264128764\n",
      "Pure loss: 204.17852868525995.....Total loss: 204.17852868525995\n",
      "Pure loss: 206.17214855439246.....Total loss: 206.17214855439246\n",
      "epoch 90 learning rate:  0.021111111111111112   Training loss:   204.17852868525995  Valing loss:   206.17214855439246\n",
      "Pure loss: 203.98525513154357.....Total loss: 203.98525513154357\n",
      "Pure loss: 200.06527435567713.....Total loss: 200.06527435567713\n",
      "epoch 91 learning rate:  0.02098901098901099   Training loss:   203.98525513154357  Valing loss:   200.06527435567713\n",
      "Pure loss: 203.28435787606358.....Total loss: 203.28435787606358\n",
      "Pure loss: 201.1287967569071.....Total loss: 201.1287967569071\n",
      "epoch 92 learning rate:  0.020869565217391306   Training loss:   203.28435787606358  Valing loss:   201.1287967569071\n",
      "Pure loss: 204.87405371366.....Total loss: 204.87405371366\n",
      "Pure loss: 210.20118525516412.....Total loss: 210.20118525516412\n",
      "epoch 93 learning rate:  0.02075268817204301   Training loss:   204.87405371366  Valing loss:   210.20118525516412\n",
      "Pure loss: 199.5259985555829.....Total loss: 199.5259985555829\n",
      "Pure loss: 201.41617874271472.....Total loss: 201.41617874271472\n",
      "epoch 94 learning rate:  0.020638297872340425   Training loss:   199.5259985555829  Valing loss:   201.41617874271472\n",
      "Pure loss: 199.94775047477086.....Total loss: 199.94775047477086\n",
      "Pure loss: 203.01620075926866.....Total loss: 203.01620075926866\n",
      "epoch 95 learning rate:  0.020526315789473684   Training loss:   199.94775047477086  Valing loss:   203.01620075926866\n",
      "Pure loss: 199.64208814322126.....Total loss: 199.64208814322126\n",
      "Pure loss: 200.9198824776567.....Total loss: 200.9198824776567\n",
      "epoch 96 learning rate:  0.020416666666666666   Training loss:   199.64208814322126  Valing loss:   200.9198824776567\n",
      "Pure loss: 199.9609818189865.....Total loss: 199.9609818189865\n",
      "Pure loss: 201.96152202709777.....Total loss: 201.96152202709777\n",
      "epoch 97 learning rate:  0.020309278350515464   Training loss:   199.9609818189865  Valing loss:   201.96152202709777\n",
      "Pure loss: 205.3157401775862.....Total loss: 205.3157401775862\n",
      "Pure loss: 213.84462514658227.....Total loss: 213.84462514658227\n",
      "epoch 98 learning rate:  0.02020408163265306   Training loss:   205.3157401775862  Valing loss:   213.84462514658227\n",
      "Pure loss: 198.0852081229542.....Total loss: 198.0852081229542\n",
      "Pure loss: 198.4054279055074.....Total loss: 198.4054279055074\n",
      "epoch 99 learning rate:  0.020101010101010102   Training loss:   198.0852081229542  Valing loss:   198.4054279055074\n",
      "Pure loss: 198.16013471585606.....Total loss: 198.16013471585606\n",
      "Pure loss: 196.74903205658143.....Total loss: 196.74903205658143\n",
      "epoch 100 learning rate:  0.02   Training loss:   198.16013471585606  Valing loss:   196.74903205658143\n",
      "Pure loss: 198.6237235887078.....Total loss: 198.6237235887078\n",
      "Pure loss: 198.66542249384653.....Total loss: 198.66542249384653\n",
      "epoch 101 learning rate:  0.0199009900990099   Training loss:   198.6237235887078  Valing loss:   198.66542249384653\n",
      "Pure loss: 206.9962685312354.....Total loss: 206.9962685312354\n",
      "Pure loss: 208.6427490011819.....Total loss: 208.6427490011819\n",
      "epoch 102 learning rate:  0.019803921568627453   Training loss:   206.9962685312354  Valing loss:   208.6427490011819\n",
      "Pure loss: 203.572872212221.....Total loss: 203.572872212221\n",
      "Pure loss: 205.124835886423.....Total loss: 205.124835886423\n",
      "epoch 103 learning rate:  0.019708737864077668   Training loss:   203.572872212221  Valing loss:   205.124835886423\n",
      "Pure loss: 203.25455788708476.....Total loss: 203.25455788708476\n",
      "Pure loss: 204.73616867014462.....Total loss: 204.73616867014462\n",
      "epoch 104 learning rate:  0.019615384615384618   Training loss:   203.25455788708476  Valing loss:   204.73616867014462\n",
      "Pure loss: 201.9147449204345.....Total loss: 201.9147449204345\n",
      "Pure loss: 202.1663172068639.....Total loss: 202.1663172068639\n",
      "epoch 105 learning rate:  0.019523809523809527   Training loss:   201.9147449204345  Valing loss:   202.1663172068639\n",
      "Pure loss: 205.0602979723731.....Total loss: 205.0602979723731\n",
      "Pure loss: 205.60104592888771.....Total loss: 205.60104592888771\n",
      "epoch 106 learning rate:  0.019433962264150943   Training loss:   205.0602979723731  Valing loss:   205.60104592888771\n",
      "Pure loss: 209.89851099631295.....Total loss: 209.89851099631295\n",
      "Pure loss: 210.33260913117022.....Total loss: 210.33260913117022\n",
      "epoch 107 learning rate:  0.019345794392523364   Training loss:   209.89851099631295  Valing loss:   210.33260913117022\n",
      "Pure loss: 207.4197118630683.....Total loss: 207.4197118630683\n",
      "Pure loss: 205.5787507251137.....Total loss: 205.5787507251137\n",
      "epoch 108 learning rate:  0.01925925925925926   Training loss:   207.4197118630683  Valing loss:   205.5787507251137\n",
      "Pure loss: 203.775735837736.....Total loss: 203.775735837736\n",
      "Pure loss: 199.4513660086243.....Total loss: 199.4513660086243\n",
      "epoch 109 learning rate:  0.019174311926605507   Training loss:   203.775735837736  Valing loss:   199.4513660086243\n",
      "Pure loss: 193.91569654344465.....Total loss: 193.91569654344465\n",
      "Pure loss: 192.54713411496402.....Total loss: 192.54713411496402\n",
      "epoch 110 learning rate:  0.01909090909090909   Training loss:   193.91569654344465  Valing loss:   192.54713411496402\n",
      "Pure loss: 194.84427890070393.....Total loss: 194.84427890070393\n",
      "Pure loss: 188.87454223626798.....Total loss: 188.87454223626798\n",
      "epoch 111 learning rate:  0.01900900900900901   Training loss:   194.84427890070393  Valing loss:   188.87454223626798\n",
      "Pure loss: 196.05044345286748.....Total loss: 196.05044345286748\n",
      "Pure loss: 187.758349552923.....Total loss: 187.758349552923\n",
      "epoch 112 learning rate:  0.018928571428571427   Training loss:   196.05044345286748  Valing loss:   187.758349552923\n",
      "Pure loss: 198.6943690134972.....Total loss: 198.6943690134972\n",
      "Pure loss: 186.1408626451884.....Total loss: 186.1408626451884\n",
      "epoch 113 learning rate:  0.018849557522123896   Training loss:   198.6943690134972  Valing loss:   186.1408626451884\n",
      "Pure loss: 199.45880979187137.....Total loss: 199.45880979187137\n",
      "Pure loss: 186.4237491375235.....Total loss: 186.4237491375235\n",
      "epoch 114 learning rate:  0.018771929824561405   Training loss:   199.45880979187137  Valing loss:   186.4237491375235\n",
      "Pure loss: 196.66306889256157.....Total loss: 196.66306889256157\n",
      "Pure loss: 184.7469123249971.....Total loss: 184.7469123249971\n",
      "epoch 115 learning rate:  0.018695652173913044   Training loss:   196.66306889256157  Valing loss:   184.7469123249971\n",
      "Pure loss: 191.60157968141476.....Total loss: 191.60157968141476\n",
      "Pure loss: 183.90527020592006.....Total loss: 183.90527020592006\n",
      "epoch 116 learning rate:  0.018620689655172412   Training loss:   191.60157968141476  Valing loss:   183.90527020592006\n",
      "Pure loss: 192.10745177452822.....Total loss: 192.10745177452822\n",
      "Pure loss: 183.73887426518922.....Total loss: 183.73887426518922\n",
      "epoch 117 learning rate:  0.018547008547008546   Training loss:   192.10745177452822  Valing loss:   183.73887426518922\n",
      "Pure loss: 192.22434724717118.....Total loss: 192.22434724717118\n",
      "Pure loss: 183.354132408377.....Total loss: 183.354132408377\n",
      "epoch 118 learning rate:  0.01847457627118644   Training loss:   192.22434724717118  Valing loss:   183.354132408377\n",
      "Pure loss: 192.67231180759146.....Total loss: 192.67231180759146\n",
      "Pure loss: 182.32450741774122.....Total loss: 182.32450741774122\n",
      "epoch 119 learning rate:  0.018403361344537815   Training loss:   192.67231180759146  Valing loss:   182.32450741774122\n",
      "Pure loss: 194.45396777106654.....Total loss: 194.45396777106654\n",
      "Pure loss: 182.04443925140694.....Total loss: 182.04443925140694\n",
      "epoch 120 learning rate:  0.018333333333333333   Training loss:   194.45396777106654  Valing loss:   182.04443925140694\n",
      "Pure loss: 192.91743069608225.....Total loss: 192.91743069608225\n",
      "Pure loss: 182.0714754515841.....Total loss: 182.0714754515841\n",
      "epoch 121 learning rate:  0.018264462809917358   Training loss:   192.91743069608225  Valing loss:   182.0714754515841\n",
      "Pure loss: 192.1181060575206.....Total loss: 192.1181060575206\n",
      "Pure loss: 183.1530372573424.....Total loss: 183.1530372573424\n",
      "epoch 122 learning rate:  0.01819672131147541   Training loss:   192.1181060575206  Valing loss:   183.1530372573424\n",
      "Pure loss: 192.45740923102247.....Total loss: 192.45740923102247\n",
      "Pure loss: 182.1329081428057.....Total loss: 182.1329081428057\n",
      "epoch 123 learning rate:  0.01813008130081301   Training loss:   192.45740923102247  Valing loss:   182.1329081428057\n",
      "Pure loss: 193.38856907162798.....Total loss: 193.38856907162798\n",
      "Pure loss: 181.55888806643603.....Total loss: 181.55888806643603\n",
      "epoch 124 learning rate:  0.01806451612903226   Training loss:   193.38856907162798  Valing loss:   181.55888806643603\n",
      "Pure loss: 192.00841367328837.....Total loss: 192.00841367328837\n",
      "Pure loss: 182.16206354879003.....Total loss: 182.16206354879003\n",
      "epoch 125 learning rate:  0.018000000000000002   Training loss:   192.00841367328837  Valing loss:   182.16206354879003\n",
      "Pure loss: 192.99686048413977.....Total loss: 192.99686048413977\n",
      "Pure loss: 182.89017319500115.....Total loss: 182.89017319500115\n",
      "epoch 126 learning rate:  0.017936507936507938   Training loss:   192.99686048413977  Valing loss:   182.89017319500115\n",
      "Pure loss: 193.2197718872364.....Total loss: 193.2197718872364\n",
      "Pure loss: 184.7352438699929.....Total loss: 184.7352438699929\n",
      "epoch 127 learning rate:  0.017874015748031498   Training loss:   193.2197718872364  Valing loss:   184.7352438699929\n",
      "Pure loss: 192.6579502829665.....Total loss: 192.6579502829665\n",
      "Pure loss: 183.90685971616114.....Total loss: 183.90685971616114\n",
      "epoch 128 learning rate:  0.017812500000000002   Training loss:   192.6579502829665  Valing loss:   183.90685971616114\n",
      "Pure loss: 192.55786090912545.....Total loss: 192.55786090912545\n",
      "Pure loss: 182.90539979587777.....Total loss: 182.90539979587777\n",
      "epoch 129 learning rate:  0.017751937984496126   Training loss:   192.55786090912545  Valing loss:   182.90539979587777\n",
      "Pure loss: 193.25642608847855.....Total loss: 193.25642608847855\n",
      "Pure loss: 183.818983071473.....Total loss: 183.818983071473\n",
      "epoch 130 learning rate:  0.017692307692307695   Training loss:   193.25642608847855  Valing loss:   183.818983071473\n",
      "Pure loss: 193.52800443836043.....Total loss: 193.52800443836043\n",
      "Pure loss: 183.47169739760835.....Total loss: 183.47169739760835\n",
      "epoch 131 learning rate:  0.017633587786259543   Training loss:   193.52800443836043  Valing loss:   183.47169739760835\n",
      "Pure loss: 190.6426059595945.....Total loss: 190.6426059595945\n",
      "Pure loss: 180.82500304577104.....Total loss: 180.82500304577104\n",
      "epoch 132 learning rate:  0.017575757575757578   Training loss:   190.6426059595945  Valing loss:   180.82500304577104\n",
      "Pure loss: 189.06813792660856.....Total loss: 189.06813792660856\n",
      "Pure loss: 179.5062497772659.....Total loss: 179.5062497772659\n",
      "epoch 133 learning rate:  0.017518796992481205   Training loss:   189.06813792660856  Valing loss:   179.5062497772659\n",
      "Pure loss: 187.5273438430067.....Total loss: 187.5273438430067\n",
      "Pure loss: 179.09878553270045.....Total loss: 179.09878553270045\n",
      "epoch 134 learning rate:  0.01746268656716418   Training loss:   187.5273438430067  Valing loss:   179.09878553270045\n",
      "Pure loss: 193.54659755294574.....Total loss: 193.54659755294574\n",
      "Pure loss: 182.3542591508866.....Total loss: 182.3542591508866\n",
      "epoch 135 learning rate:  0.017407407407407406   Training loss:   193.54659755294574  Valing loss:   182.3542591508866\n",
      "Pure loss: 186.50626027633223.....Total loss: 186.50626027633223\n",
      "Pure loss: 177.1599076555296.....Total loss: 177.1599076555296\n",
      "epoch 136 learning rate:  0.01735294117647059   Training loss:   186.50626027633223  Valing loss:   177.1599076555296\n",
      "Pure loss: 194.78001567135638.....Total loss: 194.78001567135638\n",
      "Pure loss: 180.78303878504678.....Total loss: 180.78303878504678\n",
      "epoch 137 learning rate:  0.0172992700729927   Training loss:   194.78001567135638  Valing loss:   180.78303878504678\n",
      "Pure loss: 202.04977915918255.....Total loss: 202.04977915918255\n",
      "Pure loss: 183.98370149078372.....Total loss: 183.98370149078372\n",
      "epoch 138 learning rate:  0.017246376811594202   Training loss:   202.04977915918255  Valing loss:   183.98370149078372\n",
      "Pure loss: 194.66571835329833.....Total loss: 194.66571835329833\n",
      "Pure loss: 175.97164432195095.....Total loss: 175.97164432195095\n",
      "epoch 139 learning rate:  0.017194244604316546   Training loss:   194.66571835329833  Valing loss:   175.97164432195095\n",
      "Pure loss: 190.13774109519485.....Total loss: 190.13774109519485\n",
      "Pure loss: 171.8643327350471.....Total loss: 171.8643327350471\n",
      "epoch 140 learning rate:  0.017142857142857144   Training loss:   190.13774109519485  Valing loss:   171.8643327350471\n",
      "Pure loss: 188.58253063131036.....Total loss: 188.58253063131036\n",
      "Pure loss: 170.49925846902536.....Total loss: 170.49925846902536\n",
      "epoch 141 learning rate:  0.017092198581560282   Training loss:   188.58253063131036  Valing loss:   170.49925846902536\n",
      "Pure loss: 185.69772538523796.....Total loss: 185.69772538523796\n",
      "Pure loss: 169.18814826211926.....Total loss: 169.18814826211926\n",
      "epoch 142 learning rate:  0.01704225352112676   Training loss:   185.69772538523796  Valing loss:   169.18814826211926\n",
      "Pure loss: 185.21754274490544.....Total loss: 185.21754274490544\n",
      "Pure loss: 169.04730225218077.....Total loss: 169.04730225218077\n",
      "epoch 143 learning rate:  0.016993006993006994   Training loss:   185.21754274490544  Valing loss:   169.04730225218077\n",
      "Pure loss: 187.91942170418773.....Total loss: 187.91942170418773\n",
      "Pure loss: 169.34827001150384.....Total loss: 169.34827001150384\n",
      "epoch 144 learning rate:  0.016944444444444443   Training loss:   187.91942170418773  Valing loss:   169.34827001150384\n",
      "Pure loss: 186.60287718135618.....Total loss: 186.60287718135618\n",
      "Pure loss: 169.02375849318983.....Total loss: 169.02375849318983\n",
      "epoch 145 learning rate:  0.016896551724137933   Training loss:   186.60287718135618  Valing loss:   169.02375849318983\n",
      "Pure loss: 182.6389102743288.....Total loss: 182.6389102743288\n",
      "Pure loss: 170.99991473567275.....Total loss: 170.99991473567275\n",
      "epoch 146 learning rate:  0.016849315068493152   Training loss:   182.6389102743288  Valing loss:   170.99991473567275\n",
      "Pure loss: 184.58578005801922.....Total loss: 184.58578005801922\n",
      "Pure loss: 173.25902822320018.....Total loss: 173.25902822320018\n",
      "epoch 147 learning rate:  0.016802721088435373   Training loss:   184.58578005801922  Valing loss:   173.25902822320018\n",
      "Pure loss: 188.71744715586655.....Total loss: 188.71744715586655\n",
      "Pure loss: 182.48420455777634.....Total loss: 182.48420455777634\n",
      "epoch 148 learning rate:  0.01675675675675676   Training loss:   188.71744715586655  Valing loss:   182.48420455777634\n",
      "Pure loss: 192.90031436633075.....Total loss: 192.90031436633075\n",
      "Pure loss: 192.25734087048988.....Total loss: 192.25734087048988\n",
      "epoch 149 learning rate:  0.016711409395973154   Training loss:   192.90031436633075  Valing loss:   192.25734087048988\n",
      "Pure loss: 190.69455347104127.....Total loss: 190.69455347104127\n",
      "Pure loss: 186.57850187902278.....Total loss: 186.57850187902278\n",
      "epoch 150 learning rate:  0.016666666666666666   Training loss:   190.69455347104127  Valing loss:   186.57850187902278\n",
      "Pure loss: 180.91884124992262.....Total loss: 180.91884124992262\n",
      "Pure loss: 177.23770615232948.....Total loss: 177.23770615232948\n",
      "epoch 151 learning rate:  0.016622516556291392   Training loss:   180.91884124992262  Valing loss:   177.23770615232948\n",
      "Pure loss: 180.6040504969512.....Total loss: 180.6040504969512\n",
      "Pure loss: 175.92702426738614.....Total loss: 175.92702426738614\n",
      "epoch 152 learning rate:  0.016578947368421054   Training loss:   180.6040504969512  Valing loss:   175.92702426738614\n",
      "Pure loss: 180.46794907877356.....Total loss: 180.46794907877356\n",
      "Pure loss: 175.72374240272185.....Total loss: 175.72374240272185\n",
      "epoch 153 learning rate:  0.0165359477124183   Training loss:   180.46794907877356  Valing loss:   175.72374240272185\n",
      "Pure loss: 178.22639069196362.....Total loss: 178.22639069196362\n",
      "Pure loss: 170.17342887301214.....Total loss: 170.17342887301214\n",
      "epoch 154 learning rate:  0.016493506493506494   Training loss:   178.22639069196362  Valing loss:   170.17342887301214\n",
      "Pure loss: 177.96655568540373.....Total loss: 177.96655568540373\n",
      "Pure loss: 167.95818784546353.....Total loss: 167.95818784546353\n",
      "epoch 155 learning rate:  0.016451612903225808   Training loss:   177.96655568540373  Valing loss:   167.95818784546353\n",
      "Pure loss: 170.65600886136554.....Total loss: 170.65600886136554\n",
      "Pure loss: 162.04339375936436.....Total loss: 162.04339375936436\n",
      "epoch 156 learning rate:  0.01641025641025641   Training loss:   170.65600886136554  Valing loss:   162.04339375936436\n",
      "Pure loss: 170.50840632762834.....Total loss: 170.50840632762834\n",
      "Pure loss: 161.9504297435544.....Total loss: 161.9504297435544\n",
      "epoch 157 learning rate:  0.016369426751592357   Training loss:   170.50840632762834  Valing loss:   161.9504297435544\n",
      "Pure loss: 172.21811216466494.....Total loss: 172.21811216466494\n",
      "Pure loss: 161.3167146472119.....Total loss: 161.3167146472119\n",
      "epoch 158 learning rate:  0.016329113924050634   Training loss:   172.21811216466494  Valing loss:   161.3167146472119\n",
      "Pure loss: 173.96681122097692.....Total loss: 173.96681122097692\n",
      "Pure loss: 161.1263910477824.....Total loss: 161.1263910477824\n",
      "epoch 159 learning rate:  0.01628930817610063   Training loss:   173.96681122097692  Valing loss:   161.1263910477824\n",
      "Pure loss: 175.58365057432724.....Total loss: 175.58365057432724\n",
      "Pure loss: 161.1247174178852.....Total loss: 161.1247174178852\n",
      "epoch 160 learning rate:  0.01625   Training loss:   175.58365057432724  Valing loss:   161.1247174178852\n",
      "Pure loss: 179.03266463511218.....Total loss: 179.03266463511218\n",
      "Pure loss: 162.66775593182786.....Total loss: 162.66775593182786\n",
      "epoch 161 learning rate:  0.016211180124223602   Training loss:   179.03266463511218  Valing loss:   162.66775593182786\n",
      "Pure loss: 165.6872187264979.....Total loss: 165.6872187264979\n",
      "Pure loss: 157.89305461446378.....Total loss: 157.89305461446378\n",
      "epoch 162 learning rate:  0.01617283950617284   Training loss:   165.6872187264979  Valing loss:   157.89305461446378\n",
      "Pure loss: 165.684144268516.....Total loss: 165.684144268516\n",
      "Pure loss: 157.9006224016125.....Total loss: 157.9006224016125\n",
      "epoch 163 learning rate:  0.016134969325153375   Training loss:   165.684144268516  Valing loss:   157.9006224016125\n",
      "Pure loss: 166.4559313185948.....Total loss: 166.4559313185948\n",
      "Pure loss: 158.91625226393796.....Total loss: 158.91625226393796\n",
      "epoch 164 learning rate:  0.016097560975609757   Training loss:   166.4559313185948  Valing loss:   158.91625226393796\n",
      "Pure loss: 166.45507414066606.....Total loss: 166.45507414066606\n",
      "Pure loss: 158.94942096804593.....Total loss: 158.94942096804593\n",
      "epoch 165 learning rate:  0.01606060606060606   Training loss:   166.45507414066606  Valing loss:   158.94942096804593\n",
      "Pure loss: 160.70615244794118.....Total loss: 160.70615244794118\n",
      "Pure loss: 151.0496728769962.....Total loss: 151.0496728769962\n",
      "epoch 166 learning rate:  0.01602409638554217   Training loss:   160.70615244794118  Valing loss:   151.0496728769962\n",
      "Pure loss: 162.97958613594358.....Total loss: 162.97958613594358\n",
      "Pure loss: 151.3404103093012.....Total loss: 151.3404103093012\n",
      "epoch 167 learning rate:  0.01598802395209581   Training loss:   162.97958613594358  Valing loss:   151.3404103093012\n",
      "Pure loss: 168.65611510987992.....Total loss: 168.65611510987992\n",
      "Pure loss: 152.90041852834756.....Total loss: 152.90041852834756\n",
      "epoch 168 learning rate:  0.015952380952380954   Training loss:   168.65611510987992  Valing loss:   152.90041852834756\n",
      "Pure loss: 165.2649738268958.....Total loss: 165.2649738268958\n",
      "Pure loss: 150.1462055085679.....Total loss: 150.1462055085679\n",
      "epoch 169 learning rate:  0.01591715976331361   Training loss:   165.2649738268958  Valing loss:   150.1462055085679\n",
      "Pure loss: 164.16557458015035.....Total loss: 164.16557458015035\n",
      "Pure loss: 149.8562978915845.....Total loss: 149.8562978915845\n",
      "epoch 170 learning rate:  0.015882352941176472   Training loss:   164.16557458015035  Valing loss:   149.8562978915845\n",
      "Pure loss: 166.1882212989777.....Total loss: 166.1882212989777\n",
      "Pure loss: 150.35603509570785.....Total loss: 150.35603509570785\n",
      "epoch 171 learning rate:  0.01584795321637427   Training loss:   166.1882212989777  Valing loss:   150.35603509570785\n",
      "Pure loss: 166.40746141083.....Total loss: 166.40746141083\n",
      "Pure loss: 150.5412050947643.....Total loss: 150.5412050947643\n",
      "epoch 172 learning rate:  0.01581395348837209   Training loss:   166.40746141083  Valing loss:   150.5412050947643\n",
      "Pure loss: 159.45449412154332.....Total loss: 159.45449412154332\n",
      "Pure loss: 148.87573680988072.....Total loss: 148.87573680988072\n",
      "epoch 173 learning rate:  0.015780346820809248   Training loss:   159.45449412154332  Valing loss:   148.87573680988072\n",
      "Pure loss: 158.75879747907467.....Total loss: 158.75879747907467\n",
      "Pure loss: 150.86162851862682.....Total loss: 150.86162851862682\n",
      "epoch 174 learning rate:  0.01574712643678161   Training loss:   158.75879747907467  Valing loss:   150.86162851862682\n",
      "Pure loss: 160.32530211075743.....Total loss: 160.32530211075743\n",
      "Pure loss: 152.735251527993.....Total loss: 152.735251527993\n",
      "epoch 175 learning rate:  0.015714285714285715   Training loss:   160.32530211075743  Valing loss:   152.735251527993\n",
      "Pure loss: 154.7873624352272.....Total loss: 154.7873624352272\n",
      "Pure loss: 148.62398362948974.....Total loss: 148.62398362948974\n",
      "epoch 176 learning rate:  0.015681818181818182   Training loss:   154.7873624352272  Valing loss:   148.62398362948974\n",
      "Pure loss: 155.5494806783432.....Total loss: 155.5494806783432\n",
      "Pure loss: 147.02682514500174.....Total loss: 147.02682514500174\n",
      "epoch 177 learning rate:  0.015649717514124292   Training loss:   155.5494806783432  Valing loss:   147.02682514500174\n",
      "Pure loss: 155.4792878405113.....Total loss: 155.4792878405113\n",
      "Pure loss: 147.8945613055818.....Total loss: 147.8945613055818\n",
      "epoch 178 learning rate:  0.015617977528089888   Training loss:   155.4792878405113  Valing loss:   147.8945613055818\n",
      "Pure loss: 155.58820633230067.....Total loss: 155.58820633230067\n",
      "Pure loss: 149.0607361369269.....Total loss: 149.0607361369269\n",
      "epoch 179 learning rate:  0.01558659217877095   Training loss:   155.58820633230067  Valing loss:   149.0607361369269\n",
      "Pure loss: 156.89009834620222.....Total loss: 156.89009834620222\n",
      "Pure loss: 156.28076447591462.....Total loss: 156.28076447591462\n",
      "epoch 180 learning rate:  0.015555555555555555   Training loss:   156.89009834620222  Valing loss:   156.28076447591462\n",
      "Pure loss: 156.14421980022504.....Total loss: 156.14421980022504\n",
      "Pure loss: 154.86741840134462.....Total loss: 154.86741840134462\n",
      "epoch 181 learning rate:  0.015524861878453039   Training loss:   156.14421980022504  Valing loss:   154.86741840134462\n",
      "Pure loss: 151.0215544880239.....Total loss: 151.0215544880239\n",
      "Pure loss: 150.4980342172181.....Total loss: 150.4980342172181\n",
      "epoch 182 learning rate:  0.015494505494505494   Training loss:   151.0215544880239  Valing loss:   150.4980342172181\n",
      "Pure loss: 150.46982285578682.....Total loss: 150.46982285578682\n",
      "Pure loss: 146.9515992962026.....Total loss: 146.9515992962026\n",
      "epoch 183 learning rate:  0.015464480874316941   Training loss:   150.46982285578682  Valing loss:   146.9515992962026\n",
      "Pure loss: 151.12193324005034.....Total loss: 151.12193324005034\n",
      "Pure loss: 148.1971361026923.....Total loss: 148.1971361026923\n",
      "epoch 184 learning rate:  0.015434782608695652   Training loss:   151.12193324005034  Valing loss:   148.1971361026923\n",
      "Pure loss: 150.88917360623006.....Total loss: 150.88917360623006\n",
      "Pure loss: 145.40662617361085.....Total loss: 145.40662617361085\n",
      "epoch 185 learning rate:  0.015405405405405406   Training loss:   150.88917360623006  Valing loss:   145.40662617361085\n",
      "Pure loss: 151.11101798244243.....Total loss: 151.11101798244243\n",
      "Pure loss: 149.8289639660833.....Total loss: 149.8289639660833\n",
      "epoch 186 learning rate:  0.015376344086021506   Training loss:   151.11101798244243  Valing loss:   149.8289639660833\n",
      "Pure loss: 149.90706353370933.....Total loss: 149.90706353370933\n",
      "Pure loss: 143.90228238439565.....Total loss: 143.90228238439565\n",
      "epoch 187 learning rate:  0.0153475935828877   Training loss:   149.90706353370933  Valing loss:   143.90228238439565\n",
      "Pure loss: 150.035607850852.....Total loss: 150.035607850852\n",
      "Pure loss: 145.4661456363174.....Total loss: 145.4661456363174\n",
      "epoch 188 learning rate:  0.015319148936170212   Training loss:   150.035607850852  Valing loss:   145.4661456363174\n",
      "Pure loss: 149.76519510447403.....Total loss: 149.76519510447403\n",
      "Pure loss: 144.68135772639224.....Total loss: 144.68135772639224\n",
      "epoch 189 learning rate:  0.015291005291005291   Training loss:   149.76519510447403  Valing loss:   144.68135772639224\n",
      "Pure loss: 149.33861585972207.....Total loss: 149.33861585972207\n",
      "Pure loss: 143.3054907542767.....Total loss: 143.3054907542767\n",
      "epoch 190 learning rate:  0.015263157894736843   Training loss:   149.33861585972207  Valing loss:   143.3054907542767\n",
      "Pure loss: 149.0796119138516.....Total loss: 149.0796119138516\n",
      "Pure loss: 145.51808130052757.....Total loss: 145.51808130052757\n",
      "epoch 191 learning rate:  0.015235602094240838   Training loss:   149.0796119138516  Valing loss:   145.51808130052757\n",
      "Pure loss: 146.81689151892058.....Total loss: 146.81689151892058\n",
      "Pure loss: 143.5362858664338.....Total loss: 143.5362858664338\n",
      "epoch 192 learning rate:  0.015208333333333334   Training loss:   146.81689151892058  Valing loss:   143.5362858664338\n",
      "Pure loss: 147.1183783855381.....Total loss: 147.1183783855381\n",
      "Pure loss: 142.9947959396126.....Total loss: 142.9947959396126\n",
      "epoch 193 learning rate:  0.015181347150259067   Training loss:   147.1183783855381  Valing loss:   142.9947959396126\n",
      "Pure loss: 147.5784594829224.....Total loss: 147.5784594829224\n",
      "Pure loss: 142.68809950709243.....Total loss: 142.68809950709243\n",
      "epoch 194 learning rate:  0.015154639175257733   Training loss:   147.5784594829224  Valing loss:   142.68809950709243\n",
      "Pure loss: 145.91202149143263.....Total loss: 145.91202149143263\n",
      "Pure loss: 145.73578928203136.....Total loss: 145.73578928203136\n",
      "epoch 195 learning rate:  0.01512820512820513   Training loss:   145.91202149143263  Valing loss:   145.73578928203136\n",
      "Pure loss: 145.9256864414376.....Total loss: 145.9256864414376\n",
      "Pure loss: 145.8303833084135.....Total loss: 145.8303833084135\n",
      "epoch 196 learning rate:  0.015102040816326531   Training loss:   145.9256864414376  Valing loss:   145.8303833084135\n",
      "Pure loss: 145.75750243221816.....Total loss: 145.75750243221816\n",
      "Pure loss: 144.6879578388077.....Total loss: 144.6879578388077\n",
      "epoch 197 learning rate:  0.015076142131979695   Training loss:   145.75750243221816  Valing loss:   144.6879578388077\n",
      "Pure loss: 145.66359842764624.....Total loss: 145.66359842764624\n",
      "Pure loss: 145.17660086824253.....Total loss: 145.17660086824253\n",
      "epoch 198 learning rate:  0.01505050505050505   Training loss:   145.66359842764624  Valing loss:   145.17660086824253\n",
      "Pure loss: 145.1099644015901.....Total loss: 145.1099644015901\n",
      "Pure loss: 145.18163577956852.....Total loss: 145.18163577956852\n",
      "epoch 199 learning rate:  0.015025125628140703   Training loss:   145.1099644015901  Valing loss:   145.18163577956852\n",
      "Pure loss: 146.08758339495998.....Total loss: 146.08758339495998\n",
      "Pure loss: 145.6966892786086.....Total loss: 145.6966892786086\n",
      "epoch 200 learning rate:  0.015   Training loss:   146.08758339495998  Valing loss:   145.6966892786086\n",
      "Pure loss: 148.4617400593691.....Total loss: 148.4617400593691\n",
      "Pure loss: 144.26670549454695.....Total loss: 144.26670549454695\n",
      "epoch 201 learning rate:  0.014975124378109453   Training loss:   148.4617400593691  Valing loss:   144.26670549454695\n",
      "Pure loss: 146.9807213840198.....Total loss: 146.9807213840198\n",
      "Pure loss: 144.4696932626888.....Total loss: 144.4696932626888\n",
      "epoch 202 learning rate:  0.01495049504950495   Training loss:   146.9807213840198  Valing loss:   144.4696932626888\n",
      "Pure loss: 147.398958422313.....Total loss: 147.398958422313\n",
      "Pure loss: 144.57123941535846.....Total loss: 144.57123941535846\n",
      "epoch 203 learning rate:  0.014926108374384236   Training loss:   147.398958422313  Valing loss:   144.57123941535846\n",
      "Pure loss: 145.172011426037.....Total loss: 145.172011426037\n",
      "Pure loss: 146.26668209459126.....Total loss: 146.26668209459126\n",
      "epoch 204 learning rate:  0.014901960784313726   Training loss:   145.172011426037  Valing loss:   146.26668209459126\n",
      "Pure loss: 145.41358262635296.....Total loss: 145.41358262635296\n",
      "Pure loss: 144.5223592915234.....Total loss: 144.5223592915234\n",
      "epoch 205 learning rate:  0.014878048780487804   Training loss:   145.41358262635296  Valing loss:   144.5223592915234\n",
      "Pure loss: 144.19671577570668.....Total loss: 144.19671577570668\n",
      "Pure loss: 143.88815917787406.....Total loss: 143.88815917787406\n",
      "epoch 206 learning rate:  0.014854368932038835   Training loss:   144.19671577570668  Valing loss:   143.88815917787406\n",
      "Pure loss: 144.23182871995746.....Total loss: 144.23182871995746\n",
      "Pure loss: 142.92927009262795.....Total loss: 142.92927009262795\n",
      "epoch 207 learning rate:  0.014830917874396134   Training loss:   144.23182871995746  Valing loss:   142.92927009262795\n",
      "Pure loss: 145.40877387508644.....Total loss: 145.40877387508644\n",
      "Pure loss: 150.91214026060408.....Total loss: 150.91214026060408\n",
      "epoch 208 learning rate:  0.014807692307692308   Training loss:   145.40877387508644  Valing loss:   150.91214026060408\n",
      "Pure loss: 147.41561896122687.....Total loss: 147.41561896122687\n",
      "Pure loss: 153.11584999209762.....Total loss: 153.11584999209762\n",
      "epoch 209 learning rate:  0.01478468899521531   Training loss:   147.41561896122687  Valing loss:   153.11584999209762\n",
      "Pure loss: 153.5626547402722.....Total loss: 153.5626547402722\n",
      "Pure loss: 162.1654975907821.....Total loss: 162.1654975907821\n",
      "epoch 210 learning rate:  0.014761904761904763   Training loss:   153.5626547402722  Valing loss:   162.1654975907821\n",
      "Pure loss: 145.8786151405773.....Total loss: 145.8786151405773\n",
      "Pure loss: 153.58293677132826.....Total loss: 153.58293677132826\n",
      "epoch 211 learning rate:  0.014739336492890996   Training loss:   145.8786151405773  Valing loss:   153.58293677132826\n",
      "Pure loss: 143.8901537220833.....Total loss: 143.8901537220833\n",
      "Pure loss: 150.21420766109088.....Total loss: 150.21420766109088\n",
      "epoch 212 learning rate:  0.014716981132075473   Training loss:   143.8901537220833  Valing loss:   150.21420766109088\n",
      "Pure loss: 142.73291189848857.....Total loss: 142.73291189848857\n",
      "Pure loss: 148.32132446616288.....Total loss: 148.32132446616288\n",
      "epoch 213 learning rate:  0.014694835680751174   Training loss:   142.73291189848857  Valing loss:   148.32132446616288\n",
      "Pure loss: 140.24596481369824.....Total loss: 140.24596481369824\n",
      "Pure loss: 141.85031651016234.....Total loss: 141.85031651016234\n",
      "epoch 214 learning rate:  0.014672897196261681   Training loss:   140.24596481369824  Valing loss:   141.85031651016234\n",
      "Pure loss: 139.3595559405556.....Total loss: 139.3595559405556\n",
      "Pure loss: 137.9344976225719.....Total loss: 137.9344976225719\n",
      "epoch 215 learning rate:  0.014651162790697675   Training loss:   139.3595559405556  Valing loss:   137.9344976225719\n",
      "Pure loss: 140.78191776323592.....Total loss: 140.78191776323592\n",
      "Pure loss: 134.38018581845074.....Total loss: 134.38018581845074\n",
      "epoch 216 learning rate:  0.01462962962962963   Training loss:   140.78191776323592  Valing loss:   134.38018581845074\n",
      "Pure loss: 139.2819568868336.....Total loss: 139.2819568868336\n",
      "Pure loss: 133.48827648353716.....Total loss: 133.48827648353716\n",
      "epoch 217 learning rate:  0.014608294930875575   Training loss:   139.2819568868336  Valing loss:   133.48827648353716\n",
      "Pure loss: 138.2288403592807.....Total loss: 138.2288403592807\n",
      "Pure loss: 136.81854068712533.....Total loss: 136.81854068712533\n",
      "epoch 218 learning rate:  0.014587155963302753   Training loss:   138.2288403592807  Valing loss:   136.81854068712533\n",
      "Pure loss: 138.06156310265195.....Total loss: 138.06156310265195\n",
      "Pure loss: 136.38441240420332.....Total loss: 136.38441240420332\n",
      "epoch 219 learning rate:  0.0145662100456621   Training loss:   138.06156310265195  Valing loss:   136.38441240420332\n",
      "Pure loss: 138.072303491968.....Total loss: 138.072303491968\n",
      "Pure loss: 131.58214189614347.....Total loss: 131.58214189614347\n",
      "epoch 220 learning rate:  0.014545454545454545   Training loss:   138.072303491968  Valing loss:   131.58214189614347\n",
      "Pure loss: 137.90798659685842.....Total loss: 137.90798659685842\n",
      "Pure loss: 132.22239689120892.....Total loss: 132.22239689120892\n",
      "epoch 221 learning rate:  0.014524886877828054   Training loss:   137.90798659685842  Valing loss:   132.22239689120892\n",
      "Pure loss: 138.7251486800172.....Total loss: 138.7251486800172\n",
      "Pure loss: 139.4676384908598.....Total loss: 139.4676384908598\n",
      "epoch 222 learning rate:  0.014504504504504506   Training loss:   138.7251486800172  Valing loss:   139.4676384908598\n",
      "Pure loss: 136.12065931842938.....Total loss: 136.12065931842938\n",
      "Pure loss: 137.22514276990327.....Total loss: 137.22514276990327\n",
      "epoch 223 learning rate:  0.014484304932735427   Training loss:   136.12065931842938  Valing loss:   137.22514276990327\n",
      "Pure loss: 135.32049946523964.....Total loss: 135.32049946523964\n",
      "Pure loss: 134.58525625404874.....Total loss: 134.58525625404874\n",
      "epoch 224 learning rate:  0.014464285714285714   Training loss:   135.32049946523964  Valing loss:   134.58525625404874\n",
      "Pure loss: 135.39898016335073.....Total loss: 135.39898016335073\n",
      "Pure loss: 134.7650348854955.....Total loss: 134.7650348854955\n",
      "epoch 225 learning rate:  0.014444444444444444   Training loss:   135.39898016335073  Valing loss:   134.7650348854955\n",
      "Pure loss: 135.56720149861422.....Total loss: 135.56720149861422\n",
      "Pure loss: 134.8472329765561.....Total loss: 134.8472329765561\n",
      "epoch 226 learning rate:  0.014424778761061947   Training loss:   135.56720149861422  Valing loss:   134.8472329765561\n",
      "Pure loss: 135.43705370942646.....Total loss: 135.43705370942646\n",
      "Pure loss: 134.46706160238293.....Total loss: 134.46706160238293\n",
      "epoch 227 learning rate:  0.014405286343612336   Training loss:   135.43705370942646  Valing loss:   134.46706160238293\n",
      "Pure loss: 135.47313274792654.....Total loss: 135.47313274792654\n",
      "Pure loss: 134.5374753533369.....Total loss: 134.5374753533369\n",
      "epoch 228 learning rate:  0.014385964912280702   Training loss:   135.47313274792654  Valing loss:   134.5374753533369\n",
      "Pure loss: 135.7135901105189.....Total loss: 135.7135901105189\n",
      "Pure loss: 134.7431699287737.....Total loss: 134.7431699287737\n",
      "epoch 229 learning rate:  0.014366812227074236   Training loss:   135.7135901105189  Valing loss:   134.7431699287737\n",
      "Pure loss: 136.5786247166534.....Total loss: 136.5786247166534\n",
      "Pure loss: 136.93085228995204.....Total loss: 136.93085228995204\n",
      "epoch 230 learning rate:  0.014347826086956523   Training loss:   136.5786247166534  Valing loss:   136.93085228995204\n",
      "Pure loss: 134.4821482850216.....Total loss: 134.4821482850216\n",
      "Pure loss: 133.3795122549155.....Total loss: 133.3795122549155\n",
      "epoch 231 learning rate:  0.01432900432900433   Training loss:   134.4821482850216  Valing loss:   133.3795122549155\n",
      "Pure loss: 131.61445437765397.....Total loss: 131.61445437765397\n",
      "Pure loss: 129.6076370949112.....Total loss: 129.6076370949112\n",
      "epoch 232 learning rate:  0.014310344827586207   Training loss:   131.61445437765397  Valing loss:   129.6076370949112\n",
      "Pure loss: 131.97508600801112.....Total loss: 131.97508600801112\n",
      "Pure loss: 127.73169398091461.....Total loss: 127.73169398091461\n",
      "epoch 233 learning rate:  0.014291845493562232   Training loss:   131.97508600801112  Valing loss:   127.73169398091461\n",
      "Pure loss: 132.71837567867.....Total loss: 132.71837567867\n",
      "Pure loss: 138.82251297356044.....Total loss: 138.82251297356044\n",
      "epoch 234 learning rate:  0.014273504273504274   Training loss:   132.71837567867  Valing loss:   138.82251297356044\n",
      "Pure loss: 130.55548161076737.....Total loss: 130.55548161076737\n",
      "Pure loss: 134.39482738656525.....Total loss: 134.39482738656525\n",
      "epoch 235 learning rate:  0.014255319148936171   Training loss:   130.55548161076737  Valing loss:   134.39482738656525\n",
      "Pure loss: 129.2357110651236.....Total loss: 129.2357110651236\n",
      "Pure loss: 131.06362445684863.....Total loss: 131.06362445684863\n",
      "epoch 236 learning rate:  0.014237288135593221   Training loss:   129.2357110651236  Valing loss:   131.06362445684863\n",
      "Pure loss: 128.75050320452672.....Total loss: 128.75050320452672\n",
      "Pure loss: 130.47689802643538.....Total loss: 130.47689802643538\n",
      "epoch 237 learning rate:  0.014219409282700421   Training loss:   128.75050320452672  Valing loss:   130.47689802643538\n",
      "Pure loss: 128.17414006640277.....Total loss: 128.17414006640277\n",
      "Pure loss: 128.6164590489172.....Total loss: 128.6164590489172\n",
      "epoch 238 learning rate:  0.014201680672268908   Training loss:   128.17414006640277  Valing loss:   128.6164590489172\n",
      "Pure loss: 128.1693766775267.....Total loss: 128.1693766775267\n",
      "Pure loss: 128.5893230883715.....Total loss: 128.5893230883715\n",
      "epoch 239 learning rate:  0.01418410041841004   Training loss:   128.1693766775267  Valing loss:   128.5893230883715\n",
      "Pure loss: 127.93350856173275.....Total loss: 127.93350856173275\n",
      "Pure loss: 127.25007681961048.....Total loss: 127.25007681961048\n",
      "epoch 240 learning rate:  0.014166666666666668   Training loss:   127.93350856173275  Valing loss:   127.25007681961048\n",
      "Pure loss: 127.93486582400324.....Total loss: 127.93486582400324\n",
      "Pure loss: 127.45971954323242.....Total loss: 127.45971954323242\n",
      "epoch 241 learning rate:  0.014149377593360997   Training loss:   127.93486582400324  Valing loss:   127.45971954323242\n",
      "Pure loss: 128.38178733514457.....Total loss: 128.38178733514457\n",
      "Pure loss: 128.62864439175465.....Total loss: 128.62864439175465\n",
      "epoch 242 learning rate:  0.014132231404958678   Training loss:   128.38178733514457  Valing loss:   128.62864439175465\n",
      "Pure loss: 128.10491296370787.....Total loss: 128.10491296370787\n",
      "Pure loss: 127.74002258680295.....Total loss: 127.74002258680295\n",
      "epoch 243 learning rate:  0.01411522633744856   Training loss:   128.10491296370787  Valing loss:   127.74002258680295\n",
      "Pure loss: 127.64851210115111.....Total loss: 127.64851210115111\n",
      "Pure loss: 124.93930102898115.....Total loss: 124.93930102898115\n",
      "epoch 244 learning rate:  0.014098360655737704   Training loss:   127.64851210115111  Valing loss:   124.93930102898115\n",
      "Pure loss: 127.959007551925.....Total loss: 127.959007551925\n",
      "Pure loss: 125.05723744890322.....Total loss: 125.05723744890322\n",
      "epoch 245 learning rate:  0.014081632653061225   Training loss:   127.959007551925  Valing loss:   125.05723744890322\n",
      "Pure loss: 127.79042407752276.....Total loss: 127.79042407752276\n",
      "Pure loss: 124.07030252965197.....Total loss: 124.07030252965197\n",
      "epoch 246 learning rate:  0.014065040650406504   Training loss:   127.79042407752276  Valing loss:   124.07030252965197\n",
      "Pure loss: 129.20384852376597.....Total loss: 129.20384852376597\n",
      "Pure loss: 128.6382668639896.....Total loss: 128.6382668639896\n",
      "epoch 247 learning rate:  0.014048582995951417   Training loss:   129.20384852376597  Valing loss:   128.6382668639896\n",
      "Pure loss: 127.13050053938957.....Total loss: 127.13050053938957\n",
      "Pure loss: 122.40001302946547.....Total loss: 122.40001302946547\n",
      "epoch 248 learning rate:  0.014032258064516129   Training loss:   127.13050053938957  Valing loss:   122.40001302946547\n",
      "Pure loss: 127.12445209994753.....Total loss: 127.12445209994753\n",
      "Pure loss: 122.22000254688547.....Total loss: 122.22000254688547\n",
      "epoch 249 learning rate:  0.014016064257028112   Training loss:   127.12445209994753  Valing loss:   122.22000254688547\n",
      "Pure loss: 127.51322259433036.....Total loss: 127.51322259433036\n",
      "Pure loss: 119.90212973454692.....Total loss: 119.90212973454692\n",
      "epoch 250 learning rate:  0.014   Training loss:   127.51322259433036  Valing loss:   119.90212973454692\n",
      "Pure loss: 127.4711593759487.....Total loss: 127.4711593759487\n",
      "Pure loss: 119.99084420007996.....Total loss: 119.99084420007996\n",
      "epoch 251 learning rate:  0.01398406374501992   Training loss:   127.4711593759487  Valing loss:   119.99084420007996\n",
      "Pure loss: 126.93501614458916.....Total loss: 126.93501614458916\n",
      "Pure loss: 124.82751191761885.....Total loss: 124.82751191761885\n",
      "epoch 252 learning rate:  0.013968253968253968   Training loss:   126.93501614458916  Valing loss:   124.82751191761885\n",
      "Pure loss: 126.31087223050275.....Total loss: 126.31087223050275\n",
      "Pure loss: 123.05069934624181.....Total loss: 123.05069934624181\n",
      "epoch 253 learning rate:  0.013952569169960474   Training loss:   126.31087223050275  Valing loss:   123.05069934624181\n",
      "Pure loss: 125.98666125601598.....Total loss: 125.98666125601598\n",
      "Pure loss: 122.63853291691754.....Total loss: 122.63853291691754\n",
      "epoch 254 learning rate:  0.013937007874015748   Training loss:   125.98666125601598  Valing loss:   122.63853291691754\n",
      "Pure loss: 125.1938744885539.....Total loss: 125.1938744885539\n",
      "Pure loss: 119.09435015729852.....Total loss: 119.09435015729852\n",
      "epoch 255 learning rate:  0.01392156862745098   Training loss:   125.1938744885539  Valing loss:   119.09435015729852\n",
      "Pure loss: 125.41819947304616.....Total loss: 125.41819947304616\n",
      "Pure loss: 118.76345412359721.....Total loss: 118.76345412359721\n",
      "epoch 256 learning rate:  0.01390625   Training loss:   125.41819947304616  Valing loss:   118.76345412359721\n",
      "Pure loss: 124.75225166706267.....Total loss: 124.75225166706267\n",
      "Pure loss: 121.20780401323815.....Total loss: 121.20780401323815\n",
      "epoch 257 learning rate:  0.013891050583657588   Training loss:   124.75225166706267  Valing loss:   121.20780401323815\n",
      "Pure loss: 126.79911035945796.....Total loss: 126.79911035945796\n",
      "Pure loss: 126.37611125144373.....Total loss: 126.37611125144373\n",
      "epoch 258 learning rate:  0.013875968992248062   Training loss:   126.79911035945796  Valing loss:   126.37611125144373\n",
      "Pure loss: 124.96037636730527.....Total loss: 124.96037636730527\n",
      "Pure loss: 123.4405186835067.....Total loss: 123.4405186835067\n",
      "epoch 259 learning rate:  0.013861003861003862   Training loss:   124.96037636730527  Valing loss:   123.4405186835067\n",
      "Pure loss: 121.30332223006033.....Total loss: 121.30332223006033\n",
      "Pure loss: 120.43098049370882.....Total loss: 120.43098049370882\n",
      "epoch 260 learning rate:  0.013846153846153847   Training loss:   121.30332223006033  Valing loss:   120.43098049370882\n",
      "Pure loss: 121.12694611296327.....Total loss: 121.12694611296327\n",
      "Pure loss: 119.86274954956512.....Total loss: 119.86274954956512\n",
      "epoch 261 learning rate:  0.013831417624521072   Training loss:   121.12694611296327  Valing loss:   119.86274954956512\n",
      "Pure loss: 121.18507821607767.....Total loss: 121.18507821607767\n",
      "Pure loss: 119.98794392913089.....Total loss: 119.98794392913089\n",
      "epoch 262 learning rate:  0.01381679389312977   Training loss:   121.18507821607767  Valing loss:   119.98794392913089\n",
      "Pure loss: 120.4877068672149.....Total loss: 120.4877068672149\n",
      "Pure loss: 119.26119441772329.....Total loss: 119.26119441772329\n",
      "epoch 263 learning rate:  0.013802281368821293   Training loss:   120.4877068672149  Valing loss:   119.26119441772329\n",
      "Pure loss: 120.47385012198544.....Total loss: 120.47385012198544\n",
      "Pure loss: 119.25282250730389.....Total loss: 119.25282250730389\n",
      "epoch 264 learning rate:  0.013787878787878788   Training loss:   120.47385012198544  Valing loss:   119.25282250730389\n",
      "Pure loss: 120.91202899593311.....Total loss: 120.91202899593311\n",
      "Pure loss: 120.12256924134884.....Total loss: 120.12256924134884\n",
      "epoch 265 learning rate:  0.013773584905660378   Training loss:   120.91202899593311  Valing loss:   120.12256924134884\n",
      "Pure loss: 120.74484109955262.....Total loss: 120.74484109955262\n",
      "Pure loss: 120.72796552032884.....Total loss: 120.72796552032884\n",
      "epoch 266 learning rate:  0.013759398496240602   Training loss:   120.74484109955262  Valing loss:   120.72796552032884\n",
      "Pure loss: 125.83040703157452.....Total loss: 125.83040703157452\n",
      "Pure loss: 134.39797205899865.....Total loss: 134.39797205899865\n",
      "epoch 267 learning rate:  0.013745318352059926   Training loss:   125.83040703157452  Valing loss:   134.39797205899865\n",
      "Pure loss: 130.32834982875593.....Total loss: 130.32834982875593\n",
      "Pure loss: 139.90828240343993.....Total loss: 139.90828240343993\n",
      "epoch 268 learning rate:  0.01373134328358209   Training loss:   130.32834982875593  Valing loss:   139.90828240343993\n",
      "Pure loss: 127.04401940582636.....Total loss: 127.04401940582636\n",
      "Pure loss: 135.00575861713742.....Total loss: 135.00575861713742\n",
      "epoch 269 learning rate:  0.013717472118959108   Training loss:   127.04401940582636  Valing loss:   135.00575861713742\n",
      "Pure loss: 123.53482742283602.....Total loss: 123.53482742283602\n",
      "Pure loss: 129.52280549453528.....Total loss: 129.52280549453528\n",
      "epoch 270 learning rate:  0.013703703703703704   Training loss:   123.53482742283602  Valing loss:   129.52280549453528\n",
      "Pure loss: 121.76001743445015.....Total loss: 121.76001743445015\n",
      "Pure loss: 126.40308145531066.....Total loss: 126.40308145531066\n",
      "epoch 271 learning rate:  0.013690036900369004   Training loss:   121.76001743445015  Valing loss:   126.40308145531066\n",
      "Pure loss: 118.68505559358421.....Total loss: 118.68505559358421\n",
      "Pure loss: 122.710565183116.....Total loss: 122.710565183116\n",
      "epoch 272 learning rate:  0.013676470588235293   Training loss:   118.68505559358421  Valing loss:   122.710565183116\n",
      "Pure loss: 119.14966598571489.....Total loss: 119.14966598571489\n",
      "Pure loss: 123.6288308218385.....Total loss: 123.6288308218385\n",
      "epoch 273 learning rate:  0.013663003663003662   Training loss:   119.14966598571489  Valing loss:   123.6288308218385\n",
      "Pure loss: 118.54160258902448.....Total loss: 118.54160258902448\n",
      "Pure loss: 122.31149038931493.....Total loss: 122.31149038931493\n",
      "epoch 274 learning rate:  0.013649635036496351   Training loss:   118.54160258902448  Valing loss:   122.31149038931493\n",
      "Pure loss: 120.97771117766499.....Total loss: 120.97771117766499\n",
      "Pure loss: 127.56422760394027.....Total loss: 127.56422760394027\n",
      "epoch 275 learning rate:  0.013636363636363637   Training loss:   120.97771117766499  Valing loss:   127.56422760394027\n",
      "Pure loss: 120.58944702477471.....Total loss: 120.58944702477471\n",
      "Pure loss: 127.16996723604355.....Total loss: 127.16996723604355\n",
      "epoch 276 learning rate:  0.013623188405797102   Training loss:   120.58944702477471  Valing loss:   127.16996723604355\n",
      "Pure loss: 120.97550327651278.....Total loss: 120.97550327651278\n",
      "Pure loss: 127.79352985033579.....Total loss: 127.79352985033579\n",
      "epoch 277 learning rate:  0.013610108303249099   Training loss:   120.97550327651278  Valing loss:   127.79352985033579\n",
      "Pure loss: 116.98608237694222.....Total loss: 116.98608237694222\n",
      "Pure loss: 119.88585411456377.....Total loss: 119.88585411456377\n",
      "epoch 278 learning rate:  0.013597122302158274   Training loss:   116.98608237694222  Valing loss:   119.88585411456377\n",
      "Pure loss: 115.54115009350396.....Total loss: 115.54115009350396\n",
      "Pure loss: 115.34591005479271.....Total loss: 115.34591005479271\n",
      "epoch 279 learning rate:  0.013584229390681005   Training loss:   115.54115009350396  Valing loss:   115.34591005479271\n",
      "Pure loss: 115.58433533538063.....Total loss: 115.58433533538063\n",
      "Pure loss: 116.10527222203467.....Total loss: 116.10527222203467\n",
      "epoch 280 learning rate:  0.013571428571428571   Training loss:   115.58433533538063  Valing loss:   116.10527222203467\n",
      "Pure loss: 115.4303613678629.....Total loss: 115.4303613678629\n",
      "Pure loss: 113.73868632911734.....Total loss: 113.73868632911734\n",
      "epoch 281 learning rate:  0.013558718861209965   Training loss:   115.4303613678629  Valing loss:   113.73868632911734\n",
      "Pure loss: 115.38820422031209.....Total loss: 115.38820422031209\n",
      "Pure loss: 113.85189311924745.....Total loss: 113.85189311924745\n",
      "epoch 282 learning rate:  0.013546099290780142   Training loss:   115.38820422031209  Valing loss:   113.85189311924745\n",
      "Pure loss: 116.2783344218448.....Total loss: 116.2783344218448\n",
      "Pure loss: 114.50031445183716.....Total loss: 114.50031445183716\n",
      "epoch 283 learning rate:  0.013533568904593639   Training loss:   116.2783344218448  Valing loss:   114.50031445183716\n",
      "Pure loss: 115.11226410484431.....Total loss: 115.11226410484431\n",
      "Pure loss: 115.30475357732062.....Total loss: 115.30475357732062\n",
      "epoch 284 learning rate:  0.013521126760563381   Training loss:   115.11226410484431  Valing loss:   115.30475357732062\n",
      "Pure loss: 114.65705239572026.....Total loss: 114.65705239572026\n",
      "Pure loss: 115.10364711307469.....Total loss: 115.10364711307469\n",
      "epoch 285 learning rate:  0.013508771929824562   Training loss:   114.65705239572026  Valing loss:   115.10364711307469\n",
      "Pure loss: 114.41661877901754.....Total loss: 114.41661877901754\n",
      "Pure loss: 114.28942898581361.....Total loss: 114.28942898581361\n",
      "epoch 286 learning rate:  0.013496503496503496   Training loss:   114.41661877901754  Valing loss:   114.28942898581361\n",
      "Pure loss: 114.32391331206021.....Total loss: 114.32391331206021\n",
      "Pure loss: 113.5195912394836.....Total loss: 113.5195912394836\n",
      "epoch 287 learning rate:  0.01348432055749129   Training loss:   114.32391331206021  Valing loss:   113.5195912394836\n",
      "Pure loss: 114.29995676889048.....Total loss: 114.29995676889048\n",
      "Pure loss: 112.18943906351679.....Total loss: 112.18943906351679\n",
      "epoch 288 learning rate:  0.013472222222222222   Training loss:   114.29995676889048  Valing loss:   112.18943906351679\n",
      "Pure loss: 114.11215758883661.....Total loss: 114.11215758883661\n",
      "Pure loss: 114.67181042138006.....Total loss: 114.67181042138006\n",
      "epoch 289 learning rate:  0.013460207612456747   Training loss:   114.11215758883661  Valing loss:   114.67181042138006\n",
      "Pure loss: 114.52204839190763.....Total loss: 114.52204839190763\n",
      "Pure loss: 116.47965872976896.....Total loss: 116.47965872976896\n",
      "epoch 290 learning rate:  0.013448275862068966   Training loss:   114.52204839190763  Valing loss:   116.47965872976896\n",
      "Pure loss: 114.2704470730746.....Total loss: 114.2704470730746\n",
      "Pure loss: 116.01509908899195.....Total loss: 116.01509908899195\n",
      "epoch 291 learning rate:  0.013436426116838488   Training loss:   114.2704470730746  Valing loss:   116.01509908899195\n",
      "Pure loss: 113.44903257589039.....Total loss: 113.44903257589039\n",
      "Pure loss: 113.81625023390117.....Total loss: 113.81625023390117\n",
      "epoch 292 learning rate:  0.013424657534246575   Training loss:   113.44903257589039  Valing loss:   113.81625023390117\n",
      "Pure loss: 113.0675873697237.....Total loss: 113.0675873697237\n",
      "Pure loss: 111.91651553374146.....Total loss: 111.91651553374146\n",
      "epoch 293 learning rate:  0.013412969283276451   Training loss:   113.0675873697237  Valing loss:   111.91651553374146\n",
      "Pure loss: 113.42823287894599.....Total loss: 113.42823287894599\n",
      "Pure loss: 112.39633342515613.....Total loss: 112.39633342515613\n",
      "epoch 294 learning rate:  0.013401360544217688   Training loss:   113.42823287894599  Valing loss:   112.39633342515613\n",
      "Pure loss: 113.02971054330544.....Total loss: 113.02971054330544\n",
      "Pure loss: 110.44962763309786.....Total loss: 110.44962763309786\n",
      "epoch 295 learning rate:  0.013389830508474577   Training loss:   113.02971054330544  Valing loss:   110.44962763309786\n",
      "Pure loss: 113.11090523028362.....Total loss: 113.11090523028362\n",
      "Pure loss: 112.07826036808656.....Total loss: 112.07826036808656\n",
      "epoch 296 learning rate:  0.013378378378378379   Training loss:   113.11090523028362  Valing loss:   112.07826036808656\n",
      "Pure loss: 115.8052814830584.....Total loss: 115.8052814830584\n",
      "Pure loss: 115.8521468048073.....Total loss: 115.8521468048073\n",
      "epoch 297 learning rate:  0.013367003367003367   Training loss:   115.8052814830584  Valing loss:   115.8521468048073\n",
      "Pure loss: 113.67503682209515.....Total loss: 113.67503682209515\n",
      "Pure loss: 112.93721144520566.....Total loss: 112.93721144520566\n",
      "epoch 298 learning rate:  0.013355704697986578   Training loss:   113.67503682209515  Valing loss:   112.93721144520566\n",
      "Pure loss: 113.62226263560109.....Total loss: 113.62226263560109\n",
      "Pure loss: 112.8689687663692.....Total loss: 112.8689687663692\n",
      "epoch 299 learning rate:  0.01334448160535117   Training loss:   113.62226263560109  Valing loss:   112.8689687663692\n",
      "Pure loss: 112.38160298487097.....Total loss: 112.38160298487097\n",
      "Pure loss: 109.0768393178428.....Total loss: 109.0768393178428\n",
      "epoch 300 learning rate:  0.013333333333333334   Training loss:   112.38160298487097  Valing loss:   109.0768393178428\n",
      "Pure loss: 112.62476631235731.....Total loss: 112.62476631235731\n",
      "Pure loss: 106.90888732579525.....Total loss: 106.90888732579525\n",
      "epoch 301 learning rate:  0.013322259136212624   Training loss:   112.62476631235731  Valing loss:   106.90888732579525\n",
      "Pure loss: 113.13028595139.....Total loss: 113.13028595139\n",
      "Pure loss: 105.63291038434834.....Total loss: 105.63291038434834\n",
      "epoch 302 learning rate:  0.013311258278145695   Training loss:   113.13028595139  Valing loss:   105.63291038434834\n",
      "Pure loss: 113.11388279316688.....Total loss: 113.11388279316688\n",
      "Pure loss: 105.83304966896402.....Total loss: 105.83304966896402\n",
      "epoch 303 learning rate:  0.0133003300330033   Training loss:   113.11388279316688  Valing loss:   105.83304966896402\n",
      "Pure loss: 113.04957878721143.....Total loss: 113.04957878721143\n",
      "Pure loss: 105.19095537608423.....Total loss: 105.19095537608423\n",
      "epoch 304 learning rate:  0.013289473684210526   Training loss:   113.04957878721143  Valing loss:   105.19095537608423\n",
      "Pure loss: 113.40730042292671.....Total loss: 113.40730042292671\n",
      "Pure loss: 104.80276752366233.....Total loss: 104.80276752366233\n",
      "epoch 305 learning rate:  0.013278688524590163   Training loss:   113.40730042292671  Valing loss:   104.80276752366233\n",
      "Pure loss: 113.40144832780621.....Total loss: 113.40144832780621\n",
      "Pure loss: 104.81735978153232.....Total loss: 104.81735978153232\n",
      "epoch 306 learning rate:  0.013267973856209151   Training loss:   113.40144832780621  Valing loss:   104.81735978153232\n",
      "Pure loss: 113.38567245661724.....Total loss: 113.38567245661724\n",
      "Pure loss: 104.42817591680418.....Total loss: 104.42817591680418\n",
      "epoch 307 learning rate:  0.013257328990228013   Training loss:   113.38567245661724  Valing loss:   104.42817591680418\n",
      "Pure loss: 114.5142093812173.....Total loss: 114.5142093812173\n",
      "Pure loss: 104.39446076024286.....Total loss: 104.39446076024286\n",
      "epoch 308 learning rate:  0.013246753246753246   Training loss:   114.5142093812173  Valing loss:   104.39446076024286\n",
      "Pure loss: 115.79444380473382.....Total loss: 115.79444380473382\n",
      "Pure loss: 105.86611695722058.....Total loss: 105.86611695722058\n",
      "epoch 309 learning rate:  0.013236245954692557   Training loss:   115.79444380473382  Valing loss:   105.86611695722058\n",
      "Pure loss: 116.63489136293437.....Total loss: 116.63489136293437\n",
      "Pure loss: 106.27328547371486.....Total loss: 106.27328547371486\n",
      "epoch 310 learning rate:  0.013225806451612903   Training loss:   116.63489136293437  Valing loss:   106.27328547371486\n",
      "Pure loss: 113.27500570449239.....Total loss: 113.27500570449239\n",
      "Pure loss: 104.90299796316792.....Total loss: 104.90299796316792\n",
      "epoch 311 learning rate:  0.013215434083601287   Training loss:   113.27500570449239  Valing loss:   104.90299796316792\n",
      "Pure loss: 112.40989644051254.....Total loss: 112.40989644051254\n",
      "Pure loss: 103.93591112583523.....Total loss: 103.93591112583523\n",
      "epoch 312 learning rate:  0.013205128205128206   Training loss:   112.40989644051254  Valing loss:   103.93591112583523\n",
      "Pure loss: 112.3320161851401.....Total loss: 112.3320161851401\n",
      "Pure loss: 103.88272520363738.....Total loss: 103.88272520363738\n",
      "epoch 313 learning rate:  0.013194888178913738   Training loss:   112.3320161851401  Valing loss:   103.88272520363738\n",
      "Pure loss: 108.84370515262846.....Total loss: 108.84370515262846\n",
      "Pure loss: 103.30106923079106.....Total loss: 103.30106923079106\n",
      "epoch 314 learning rate:  0.01318471337579618   Training loss:   108.84370515262846  Valing loss:   103.30106923079106\n",
      "Pure loss: 108.83935318317033.....Total loss: 108.83935318317033\n",
      "Pure loss: 102.37423534376457.....Total loss: 102.37423534376457\n",
      "epoch 315 learning rate:  0.013174603174603176   Training loss:   108.83935318317033  Valing loss:   102.37423534376457\n",
      "Pure loss: 110.16459612093934.....Total loss: 110.16459612093934\n",
      "Pure loss: 102.81292094109872.....Total loss: 102.81292094109872\n",
      "epoch 316 learning rate:  0.013164556962025316   Training loss:   110.16459612093934  Valing loss:   102.81292094109872\n",
      "Pure loss: 112.51265696213314.....Total loss: 112.51265696213314\n",
      "Pure loss: 103.77600890908157.....Total loss: 103.77600890908157\n",
      "epoch 317 learning rate:  0.013154574132492113   Training loss:   112.51265696213314  Valing loss:   103.77600890908157\n",
      "Pure loss: 110.94428780261785.....Total loss: 110.94428780261785\n",
      "Pure loss: 102.54934422301324.....Total loss: 102.54934422301324\n",
      "epoch 318 learning rate:  0.013144654088050316   Training loss:   110.94428780261785  Valing loss:   102.54934422301324\n",
      "Pure loss: 113.41058488926855.....Total loss: 113.41058488926855\n",
      "Pure loss: 103.33494978714978.....Total loss: 103.33494978714978\n",
      "epoch 319 learning rate:  0.013134796238244515   Training loss:   113.41058488926855  Valing loss:   103.33494978714978\n",
      "Pure loss: 111.76253952413079.....Total loss: 111.76253952413079\n",
      "Pure loss: 102.14330882488943.....Total loss: 102.14330882488943\n",
      "epoch 320 learning rate:  0.013125000000000001   Training loss:   111.76253952413079  Valing loss:   102.14330882488943\n",
      "Pure loss: 110.07844617757121.....Total loss: 110.07844617757121\n",
      "Pure loss: 100.70797670380162.....Total loss: 100.70797670380162\n",
      "epoch 321 learning rate:  0.013115264797507789   Training loss:   110.07844617757121  Valing loss:   100.70797670380162\n",
      "Pure loss: 108.48282475420024.....Total loss: 108.48282475420024\n",
      "Pure loss: 99.48727627316275.....Total loss: 99.48727627316275\n",
      "epoch 322 learning rate:  0.013105590062111802   Training loss:   108.48282475420024  Valing loss:   99.48727627316275\n",
      "Pure loss: 108.33016871885064.....Total loss: 108.33016871885064\n",
      "Pure loss: 99.45285721729476.....Total loss: 99.45285721729476\n",
      "epoch 323 learning rate:  0.013095975232198142   Training loss:   108.33016871885064  Valing loss:   99.45285721729476\n",
      "Pure loss: 108.15878278512174.....Total loss: 108.15878278512174\n",
      "Pure loss: 99.42694767474225.....Total loss: 99.42694767474225\n",
      "epoch 324 learning rate:  0.01308641975308642   Training loss:   108.15878278512174  Valing loss:   99.42694767474225\n",
      "Pure loss: 106.71009185560335.....Total loss: 106.71009185560335\n",
      "Pure loss: 99.84606119047994.....Total loss: 99.84606119047994\n",
      "epoch 325 learning rate:  0.013076923076923076   Training loss:   106.71009185560335  Valing loss:   99.84606119047994\n",
      "Pure loss: 106.1063524659753.....Total loss: 106.1063524659753\n",
      "Pure loss: 101.80215453881318.....Total loss: 101.80215453881318\n",
      "epoch 326 learning rate:  0.013067484662576687   Training loss:   106.1063524659753  Valing loss:   101.80215453881318\n",
      "Pure loss: 105.95837077747757.....Total loss: 105.95837077747757\n",
      "Pure loss: 100.23007707557993.....Total loss: 100.23007707557993\n",
      "epoch 327 learning rate:  0.013058103975535168   Training loss:   105.95837077747757  Valing loss:   100.23007707557993\n",
      "Pure loss: 105.54658085874985.....Total loss: 105.54658085874985\n",
      "Pure loss: 99.28097837205138.....Total loss: 99.28097837205138\n",
      "epoch 328 learning rate:  0.01304878048780488   Training loss:   105.54658085874985  Valing loss:   99.28097837205138\n",
      "Pure loss: 106.34482934929109.....Total loss: 106.34482934929109\n",
      "Pure loss: 98.41539775638017.....Total loss: 98.41539775638017\n",
      "epoch 329 learning rate:  0.01303951367781155   Training loss:   106.34482934929109  Valing loss:   98.41539775638017\n",
      "Pure loss: 107.62329439951526.....Total loss: 107.62329439951526\n",
      "Pure loss: 97.80126600184867.....Total loss: 97.80126600184867\n",
      "epoch 330 learning rate:  0.013030303030303031   Training loss:   107.62329439951526  Valing loss:   97.80126600184867\n",
      "Pure loss: 107.08367072543501.....Total loss: 107.08367072543501\n",
      "Pure loss: 97.75449716472976.....Total loss: 97.75449716472976\n",
      "epoch 331 learning rate:  0.013021148036253778   Training loss:   107.08367072543501  Valing loss:   97.75449716472976\n",
      "Pure loss: 103.58915539006428.....Total loss: 103.58915539006428\n",
      "Pure loss: 97.36564146471564.....Total loss: 97.36564146471564\n",
      "epoch 332 learning rate:  0.013012048192771086   Training loss:   103.58915539006428  Valing loss:   97.36564146471564\n",
      "Pure loss: 106.58741637877485.....Total loss: 106.58741637877485\n",
      "Pure loss: 96.46266956085005.....Total loss: 96.46266956085005\n",
      "epoch 333 learning rate:  0.013003003003003003   Training loss:   106.58741637877485  Valing loss:   96.46266956085005\n",
      "Pure loss: 103.07571250589137.....Total loss: 103.07571250589137\n",
      "Pure loss: 96.28729628003813.....Total loss: 96.28729628003813\n",
      "epoch 334 learning rate:  0.012994011976047905   Training loss:   103.07571250589137  Valing loss:   96.28729628003813\n",
      "Pure loss: 103.77743869566315.....Total loss: 103.77743869566315\n",
      "Pure loss: 97.07162158868547.....Total loss: 97.07162158868547\n",
      "epoch 335 learning rate:  0.012985074626865671   Training loss:   103.77743869566315  Valing loss:   97.07162158868547\n",
      "Pure loss: 104.34070834773549.....Total loss: 104.34070834773549\n",
      "Pure loss: 97.32221580059255.....Total loss: 97.32221580059255\n",
      "epoch 336 learning rate:  0.012976190476190476   Training loss:   104.34070834773549  Valing loss:   97.32221580059255\n",
      "Pure loss: 100.27654420233229.....Total loss: 100.27654420233229\n",
      "Pure loss: 97.53253725897525.....Total loss: 97.53253725897525\n",
      "epoch 337 learning rate:  0.012967359050445104   Training loss:   100.27654420233229  Valing loss:   97.53253725897525\n",
      "Pure loss: 100.48590646588472.....Total loss: 100.48590646588472\n",
      "Pure loss: 96.9513898920077.....Total loss: 96.9513898920077\n",
      "epoch 338 learning rate:  0.012958579881656804   Training loss:   100.48590646588472  Valing loss:   96.9513898920077\n",
      "Pure loss: 100.50311379828294.....Total loss: 100.50311379828294\n",
      "Pure loss: 101.39727128493051.....Total loss: 101.39727128493051\n",
      "epoch 339 learning rate:  0.012949852507374631   Training loss:   100.50311379828294  Valing loss:   101.39727128493051\n",
      "Pure loss: 100.35179952809435.....Total loss: 100.35179952809435\n",
      "Pure loss: 100.71395773171913.....Total loss: 100.71395773171913\n",
      "epoch 340 learning rate:  0.012941176470588235   Training loss:   100.35179952809435  Valing loss:   100.71395773171913\n",
      "Pure loss: 100.17362890367339.....Total loss: 100.17362890367339\n",
      "Pure loss: 100.29873624824317.....Total loss: 100.29873624824317\n",
      "epoch 341 learning rate:  0.012932551319648093   Training loss:   100.17362890367339  Valing loss:   100.29873624824317\n",
      "Pure loss: 101.0458551054361.....Total loss: 101.0458551054361\n",
      "Pure loss: 100.90995746895868.....Total loss: 100.90995746895868\n",
      "epoch 342 learning rate:  0.012923976608187135   Training loss:   101.0458551054361  Valing loss:   100.90995746895868\n",
      "Pure loss: 101.17257244050784.....Total loss: 101.17257244050784\n",
      "Pure loss: 103.4356195786033.....Total loss: 103.4356195786033\n",
      "epoch 343 learning rate:  0.012915451895043732   Training loss:   101.17257244050784  Valing loss:   103.4356195786033\n",
      "Pure loss: 101.55461792621188.....Total loss: 101.55461792621188\n",
      "Pure loss: 102.16721909251935.....Total loss: 102.16721909251935\n",
      "epoch 344 learning rate:  0.012906976744186047   Training loss:   101.55461792621188  Valing loss:   102.16721909251935\n",
      "Pure loss: 102.17299242002088.....Total loss: 102.17299242002088\n",
      "Pure loss: 102.62561452871877.....Total loss: 102.62561452871877\n",
      "epoch 345 learning rate:  0.012898550724637681   Training loss:   102.17299242002088  Valing loss:   102.62561452871877\n",
      "Pure loss: 101.50125256170101.....Total loss: 101.50125256170101\n",
      "Pure loss: 103.14284267324594.....Total loss: 103.14284267324594\n",
      "epoch 346 learning rate:  0.012890173410404625   Training loss:   101.50125256170101  Valing loss:   103.14284267324594\n",
      "Pure loss: 101.50019902446438.....Total loss: 101.50019902446438\n",
      "Pure loss: 103.14393526938464.....Total loss: 103.14393526938464\n",
      "epoch 347 learning rate:  0.012881844380403459   Training loss:   101.50019902446438  Valing loss:   103.14393526938464\n",
      "Pure loss: 101.69916747243592.....Total loss: 101.69916747243592\n",
      "Pure loss: 102.41479806359224.....Total loss: 102.41479806359224\n",
      "epoch 348 learning rate:  0.012873563218390805   Training loss:   101.69916747243592  Valing loss:   102.41479806359224\n",
      "Pure loss: 100.88213416058566.....Total loss: 100.88213416058566\n",
      "Pure loss: 102.63486978061351.....Total loss: 102.63486978061351\n",
      "epoch 349 learning rate:  0.012865329512893982   Training loss:   100.88213416058566  Valing loss:   102.63486978061351\n",
      "Pure loss: 101.13916111259684.....Total loss: 101.13916111259684\n",
      "Pure loss: 106.12664769203326.....Total loss: 106.12664769203326\n",
      "epoch 350 learning rate:  0.012857142857142857   Training loss:   101.13916111259684  Valing loss:   106.12664769203326\n",
      "Pure loss: 101.19144271758381.....Total loss: 101.19144271758381\n",
      "Pure loss: 106.52844966953707.....Total loss: 106.52844966953707\n",
      "epoch 351 learning rate:  0.012849002849002849   Training loss:   101.19144271758381  Valing loss:   106.52844966953707\n",
      "Pure loss: 98.10333471078687.....Total loss: 98.10333471078687\n",
      "Pure loss: 100.66357085603185.....Total loss: 100.66357085603185\n",
      "epoch 352 learning rate:  0.01284090909090909   Training loss:   98.10333471078687  Valing loss:   100.66357085603185\n",
      "Pure loss: 98.77042747888132.....Total loss: 98.77042747888132\n",
      "Pure loss: 100.82058444490735.....Total loss: 100.82058444490735\n",
      "epoch 353 learning rate:  0.0128328611898017   Training loss:   98.77042747888132  Valing loss:   100.82058444490735\n",
      "Pure loss: 98.4479211776678.....Total loss: 98.4479211776678\n",
      "Pure loss: 101.3432184826518.....Total loss: 101.3432184826518\n",
      "epoch 354 learning rate:  0.012824858757062147   Training loss:   98.4479211776678  Valing loss:   101.3432184826518\n",
      "Pure loss: 98.26829899786358.....Total loss: 98.26829899786358\n",
      "Pure loss: 100.68466938848432.....Total loss: 100.68466938848432\n",
      "epoch 355 learning rate:  0.012816901408450704   Training loss:   98.26829899786358  Valing loss:   100.68466938848432\n",
      "Pure loss: 97.91249497588275.....Total loss: 97.91249497588275\n",
      "Pure loss: 98.60025024970066.....Total loss: 98.60025024970066\n",
      "epoch 356 learning rate:  0.012808988764044944   Training loss:   97.91249497588275  Valing loss:   98.60025024970066\n",
      "Pure loss: 98.72160157359453.....Total loss: 98.72160157359453\n",
      "Pure loss: 96.32755691068319.....Total loss: 96.32755691068319\n",
      "epoch 357 learning rate:  0.012801120448179272   Training loss:   98.72160157359453  Valing loss:   96.32755691068319\n",
      "Pure loss: 97.70913425036974.....Total loss: 97.70913425036974\n",
      "Pure loss: 96.01808820241887.....Total loss: 96.01808820241887\n",
      "epoch 358 learning rate:  0.012793296089385476   Training loss:   97.70913425036974  Valing loss:   96.01808820241887\n",
      "Pure loss: 98.43670167403094.....Total loss: 98.43670167403094\n",
      "Pure loss: 95.50673336149458.....Total loss: 95.50673336149458\n",
      "epoch 359 learning rate:  0.012785515320334262   Training loss:   98.43670167403094  Valing loss:   95.50673336149458\n",
      "Pure loss: 97.06564716591434.....Total loss: 97.06564716591434\n",
      "Pure loss: 95.53067901144654.....Total loss: 95.53067901144654\n",
      "epoch 360 learning rate:  0.012777777777777779   Training loss:   97.06564716591434  Valing loss:   95.53067901144654\n",
      "Pure loss: 97.05340702603242.....Total loss: 97.05340702603242\n",
      "Pure loss: 95.59015132102839.....Total loss: 95.59015132102839\n",
      "epoch 361 learning rate:  0.012770083102493075   Training loss:   97.05340702603242  Valing loss:   95.59015132102839\n",
      "Pure loss: 95.47913738862651.....Total loss: 95.47913738862651\n",
      "Pure loss: 94.30666159949158.....Total loss: 94.30666159949158\n",
      "epoch 362 learning rate:  0.01276243093922652   Training loss:   95.47913738862651  Valing loss:   94.30666159949158\n",
      "Pure loss: 95.49900366696845.....Total loss: 95.49900366696845\n",
      "Pure loss: 94.21218543202467.....Total loss: 94.21218543202467\n",
      "epoch 363 learning rate:  0.012754820936639119   Training loss:   95.49900366696845  Valing loss:   94.21218543202467\n",
      "Pure loss: 95.95918843634716.....Total loss: 95.95918843634716\n",
      "Pure loss: 91.94473973971786.....Total loss: 91.94473973971786\n",
      "epoch 364 learning rate:  0.012747252747252748   Training loss:   95.95918843634716  Valing loss:   91.94473973971786\n",
      "Pure loss: 95.77317311334244.....Total loss: 95.77317311334244\n",
      "Pure loss: 91.99416570461894.....Total loss: 91.99416570461894\n",
      "epoch 365 learning rate:  0.012739726027397261   Training loss:   95.77317311334244  Valing loss:   91.99416570461894\n",
      "Pure loss: 95.54628488663145.....Total loss: 95.54628488663145\n",
      "Pure loss: 91.91342244856553.....Total loss: 91.91342244856553\n",
      "epoch 366 learning rate:  0.01273224043715847   Training loss:   95.54628488663145  Valing loss:   91.91342244856553\n",
      "Pure loss: 97.70091817520799.....Total loss: 97.70091817520799\n",
      "Pure loss: 90.71751029344347.....Total loss: 90.71751029344347\n",
      "epoch 367 learning rate:  0.012724795640326976   Training loss:   97.70091817520799  Valing loss:   90.71751029344347\n",
      "Pure loss: 99.773542162298.....Total loss: 99.773542162298\n",
      "Pure loss: 90.51753925797276.....Total loss: 90.51753925797276\n",
      "epoch 368 learning rate:  0.012717391304347826   Training loss:   99.773542162298  Valing loss:   90.51753925797276\n",
      "Pure loss: 95.8565340827713.....Total loss: 95.8565340827713\n",
      "Pure loss: 89.63400993972593.....Total loss: 89.63400993972593\n",
      "epoch 369 learning rate:  0.012710027100271002   Training loss:   95.8565340827713  Valing loss:   89.63400993972593\n",
      "Pure loss: 97.98920963992752.....Total loss: 97.98920963992752\n",
      "Pure loss: 89.5797398421384.....Total loss: 89.5797398421384\n",
      "epoch 370 learning rate:  0.012702702702702703   Training loss:   97.98920963992752  Valing loss:   89.5797398421384\n",
      "Pure loss: 98.60984564402348.....Total loss: 98.60984564402348\n",
      "Pure loss: 89.78328813301258.....Total loss: 89.78328813301258\n",
      "epoch 371 learning rate:  0.012695417789757413   Training loss:   98.60984564402348  Valing loss:   89.78328813301258\n",
      "Pure loss: 96.12564727315694.....Total loss: 96.12564727315694\n",
      "Pure loss: 87.62481667966097.....Total loss: 87.62481667966097\n",
      "epoch 372 learning rate:  0.012688172043010752   Training loss:   96.12564727315694  Valing loss:   87.62481667966097\n",
      "Pure loss: 99.92711758638794.....Total loss: 99.92711758638794\n",
      "Pure loss: 89.10545325097056.....Total loss: 89.10545325097056\n",
      "epoch 373 learning rate:  0.012680965147453083   Training loss:   99.92711758638794  Valing loss:   89.10545325097056\n",
      "Pure loss: 99.48773016792475.....Total loss: 99.48773016792475\n",
      "Pure loss: 88.90708122300119.....Total loss: 88.90708122300119\n",
      "epoch 374 learning rate:  0.01267379679144385   Training loss:   99.48773016792475  Valing loss:   88.90708122300119\n",
      "Pure loss: 98.19633587438122.....Total loss: 98.19633587438122\n",
      "Pure loss: 88.05612470321815.....Total loss: 88.05612470321815\n",
      "epoch 375 learning rate:  0.012666666666666666   Training loss:   98.19633587438122  Valing loss:   88.05612470321815\n",
      "Pure loss: 91.79559245081495.....Total loss: 91.79559245081495\n",
      "Pure loss: 85.59435268910643.....Total loss: 85.59435268910643\n",
      "epoch 376 learning rate:  0.012659574468085107   Training loss:   91.79559245081495  Valing loss:   85.59435268910643\n",
      "Pure loss: 91.22516125349608.....Total loss: 91.22516125349608\n",
      "Pure loss: 85.25367246498145.....Total loss: 85.25367246498145\n",
      "epoch 377 learning rate:  0.012652519893899204   Training loss:   91.22516125349608  Valing loss:   85.25367246498145\n",
      "Pure loss: 91.19164831011817.....Total loss: 91.19164831011817\n",
      "Pure loss: 85.26591923530488.....Total loss: 85.26591923530488\n",
      "epoch 378 learning rate:  0.012645502645502646   Training loss:   91.19164831011817  Valing loss:   85.26591923530488\n",
      "Pure loss: 91.38418606875686.....Total loss: 91.38418606875686\n",
      "Pure loss: 84.96513338123926.....Total loss: 84.96513338123926\n",
      "epoch 379 learning rate:  0.012638522427440634   Training loss:   91.38418606875686  Valing loss:   84.96513338123926\n",
      "Pure loss: 91.09472587687395.....Total loss: 91.09472587687395\n",
      "Pure loss: 85.53987014910693.....Total loss: 85.53987014910693\n",
      "epoch 380 learning rate:  0.01263157894736842   Training loss:   91.09472587687395  Valing loss:   85.53987014910693\n",
      "Pure loss: 91.04642236117864.....Total loss: 91.04642236117864\n",
      "Pure loss: 86.22569523054798.....Total loss: 86.22569523054798\n",
      "epoch 381 learning rate:  0.012624671916010499   Training loss:   91.04642236117864  Valing loss:   86.22569523054798\n",
      "Pure loss: 91.17490641744273.....Total loss: 91.17490641744273\n",
      "Pure loss: 86.51976937809545.....Total loss: 86.51976937809545\n",
      "epoch 382 learning rate:  0.01261780104712042   Training loss:   91.17490641744273  Valing loss:   86.51976937809545\n",
      "Pure loss: 90.61444757650624.....Total loss: 90.61444757650624\n",
      "Pure loss: 85.89638173849862.....Total loss: 85.89638173849862\n",
      "epoch 383 learning rate:  0.012610966057441254   Training loss:   90.61444757650624  Valing loss:   85.89638173849862\n",
      "Pure loss: 90.56793199919662.....Total loss: 90.56793199919662\n",
      "Pure loss: 85.47957784735776.....Total loss: 85.47957784735776\n",
      "epoch 384 learning rate:  0.012604166666666666   Training loss:   90.56793199919662  Valing loss:   85.47957784735776\n",
      "Pure loss: 90.47729290291825.....Total loss: 90.47729290291825\n",
      "Pure loss: 84.15085716102293.....Total loss: 84.15085716102293\n",
      "epoch 385 learning rate:  0.012597402597402597   Training loss:   90.47729290291825  Valing loss:   84.15085716102293\n",
      "Pure loss: 90.43345556844504.....Total loss: 90.43345556844504\n",
      "Pure loss: 84.07659325264109.....Total loss: 84.07659325264109\n",
      "epoch 386 learning rate:  0.012590673575129534   Training loss:   90.43345556844504  Valing loss:   84.07659325264109\n",
      "Pure loss: 92.07183174395756.....Total loss: 92.07183174395756\n",
      "Pure loss: 83.03556372800453.....Total loss: 83.03556372800453\n",
      "epoch 387 learning rate:  0.012583979328165375   Training loss:   92.07183174395756  Valing loss:   83.03556372800453\n",
      "Pure loss: 92.33736666759675.....Total loss: 92.33736666759675\n",
      "Pure loss: 83.08844721887647.....Total loss: 83.08844721887647\n",
      "epoch 388 learning rate:  0.012577319587628866   Training loss:   92.33736666759675  Valing loss:   83.08844721887647\n",
      "Pure loss: 91.15218440224047.....Total loss: 91.15218440224047\n",
      "Pure loss: 82.84194402249592.....Total loss: 82.84194402249592\n",
      "epoch 389 learning rate:  0.012570694087403599   Training loss:   91.15218440224047  Valing loss:   82.84194402249592\n",
      "Pure loss: 92.26926155781274.....Total loss: 92.26926155781274\n",
      "Pure loss: 82.83297034835903.....Total loss: 82.83297034835903\n",
      "epoch 390 learning rate:  0.012564102564102564   Training loss:   92.26926155781274  Valing loss:   82.83297034835903\n",
      "Pure loss: 92.66483581331862.....Total loss: 92.66483581331862\n",
      "Pure loss: 82.87348732145465.....Total loss: 82.87348732145465\n",
      "epoch 391 learning rate:  0.012557544757033249   Training loss:   92.66483581331862  Valing loss:   82.87348732145465\n",
      "Pure loss: 87.62537123245815.....Total loss: 87.62537123245815\n",
      "Pure loss: 83.77789846610324.....Total loss: 83.77789846610324\n",
      "epoch 392 learning rate:  0.012551020408163265   Training loss:   87.62537123245815  Valing loss:   83.77789846610324\n",
      "Pure loss: 88.18086746717799.....Total loss: 88.18086746717799\n",
      "Pure loss: 82.62965348690366.....Total loss: 82.62965348690366\n",
      "epoch 393 learning rate:  0.012544529262086514   Training loss:   88.18086746717799  Valing loss:   82.62965348690366\n",
      "Pure loss: 87.37167835353432.....Total loss: 87.37167835353432\n",
      "Pure loss: 83.12977036050867.....Total loss: 83.12977036050867\n",
      "epoch 394 learning rate:  0.012538071065989847   Training loss:   87.37167835353432  Valing loss:   83.12977036050867\n",
      "Pure loss: 87.24911552313887.....Total loss: 87.24911552313887\n",
      "Pure loss: 83.55058337203978.....Total loss: 83.55058337203978\n",
      "epoch 395 learning rate:  0.012531645569620253   Training loss:   87.24911552313887  Valing loss:   83.55058337203978\n",
      "Pure loss: 87.19029036809148.....Total loss: 87.19029036809148\n",
      "Pure loss: 84.42944873747288.....Total loss: 84.42944873747288\n",
      "epoch 396 learning rate:  0.012525252525252526   Training loss:   87.19029036809148  Valing loss:   84.42944873747288\n",
      "Pure loss: 87.31920438115041.....Total loss: 87.31920438115041\n",
      "Pure loss: 82.23765581068253.....Total loss: 82.23765581068253\n",
      "epoch 397 learning rate:  0.012518891687657432   Training loss:   87.31920438115041  Valing loss:   82.23765581068253\n",
      "Pure loss: 86.67694009686412.....Total loss: 86.67694009686412\n",
      "Pure loss: 83.56115044150033.....Total loss: 83.56115044150033\n",
      "epoch 398 learning rate:  0.012512562814070352   Training loss:   86.67694009686412  Valing loss:   83.56115044150033\n",
      "Pure loss: 86.62392710730829.....Total loss: 86.62392710730829\n",
      "Pure loss: 83.4144593444149.....Total loss: 83.4144593444149\n",
      "epoch 399 learning rate:  0.012506265664160401   Training loss:   86.62392710730829  Valing loss:   83.4144593444149\n",
      "Pure loss: 86.61600755056881.....Total loss: 86.61600755056881\n",
      "Pure loss: 83.32972242828878.....Total loss: 83.32972242828878\n",
      "epoch 400 learning rate:  0.0125   Training loss:   86.61600755056881  Valing loss:   83.32972242828878\n",
      "Pure loss: 86.40510682596238.....Total loss: 86.40510682596238\n",
      "Pure loss: 83.34419219854462.....Total loss: 83.34419219854462\n",
      "epoch 401 learning rate:  0.012493765586034912   Training loss:   86.40510682596238  Valing loss:   83.34419219854462\n",
      "Pure loss: 86.41279913544673.....Total loss: 86.41279913544673\n",
      "Pure loss: 83.37446929629597.....Total loss: 83.37446929629597\n",
      "epoch 402 learning rate:  0.012487562189054727   Training loss:   86.41279913544673  Valing loss:   83.37446929629597\n",
      "Pure loss: 86.4002003066138.....Total loss: 86.4002003066138\n",
      "Pure loss: 83.31965906239797.....Total loss: 83.31965906239797\n",
      "epoch 403 learning rate:  0.012481389578163773   Training loss:   86.4002003066138  Valing loss:   83.31965906239797\n",
      "Pure loss: 86.24139252132223.....Total loss: 86.24139252132223\n",
      "Pure loss: 82.67131230745831.....Total loss: 82.67131230745831\n",
      "epoch 404 learning rate:  0.012475247524752476   Training loss:   86.24139252132223  Valing loss:   82.67131230745831\n",
      "Pure loss: 86.43815006027762.....Total loss: 86.43815006027762\n",
      "Pure loss: 80.08335731738407.....Total loss: 80.08335731738407\n",
      "epoch 405 learning rate:  0.012469135802469136   Training loss:   86.43815006027762  Valing loss:   80.08335731738407\n",
      "Pure loss: 85.7329908977905.....Total loss: 85.7329908977905\n",
      "Pure loss: 84.29121955223691.....Total loss: 84.29121955223691\n",
      "epoch 406 learning rate:  0.012463054187192119   Training loss:   85.7329908977905  Valing loss:   84.29121955223691\n",
      "Pure loss: 85.35638189999085.....Total loss: 85.35638189999085\n",
      "Pure loss: 83.08795094603127.....Total loss: 83.08795094603127\n",
      "epoch 407 learning rate:  0.012457002457002457   Training loss:   85.35638189999085  Valing loss:   83.08795094603127\n",
      "Pure loss: 85.93512173026856.....Total loss: 85.93512173026856\n",
      "Pure loss: 84.56912888586182.....Total loss: 84.56912888586182\n",
      "epoch 408 learning rate:  0.012450980392156863   Training loss:   85.93512173026856  Valing loss:   84.56912888586182\n",
      "Pure loss: 87.72839394149612.....Total loss: 87.72839394149612\n",
      "Pure loss: 88.22431149278012.....Total loss: 88.22431149278012\n",
      "epoch 409 learning rate:  0.012444987775061124   Training loss:   87.72839394149612  Valing loss:   88.22431149278012\n",
      "Pure loss: 88.04518927476073.....Total loss: 88.04518927476073\n",
      "Pure loss: 88.64984307689534.....Total loss: 88.64984307689534\n",
      "epoch 410 learning rate:  0.012439024390243903   Training loss:   88.04518927476073  Valing loss:   88.64984307689534\n",
      "Pure loss: 86.59014624880248.....Total loss: 86.59014624880248\n",
      "Pure loss: 87.29321080973351.....Total loss: 87.29321080973351\n",
      "epoch 411 learning rate:  0.012433090024330901   Training loss:   86.59014624880248  Valing loss:   87.29321080973351\n",
      "Pure loss: 85.85783100614864.....Total loss: 85.85783100614864\n",
      "Pure loss: 85.92404780886258.....Total loss: 85.92404780886258\n",
      "epoch 412 learning rate:  0.012427184466019418   Training loss:   85.85783100614864  Valing loss:   85.92404780886258\n",
      "Pure loss: 85.76161536858197.....Total loss: 85.76161536858197\n",
      "Pure loss: 85.71073667722884.....Total loss: 85.71073667722884\n",
      "epoch 413 learning rate:  0.012421307506053268   Training loss:   85.76161536858197  Valing loss:   85.71073667722884\n",
      "Pure loss: 86.20168479605853.....Total loss: 86.20168479605853\n",
      "Pure loss: 86.65638708472916.....Total loss: 86.65638708472916\n",
      "epoch 414 learning rate:  0.012415458937198068   Training loss:   86.20168479605853  Valing loss:   86.65638708472916\n",
      "Pure loss: 84.70500521022359.....Total loss: 84.70500521022359\n",
      "Pure loss: 84.55363842287908.....Total loss: 84.55363842287908\n",
      "epoch 415 learning rate:  0.012409638554216867   Training loss:   84.70500521022359  Valing loss:   84.55363842287908\n",
      "Pure loss: 83.98280888091705.....Total loss: 83.98280888091705\n",
      "Pure loss: 81.96548562264998.....Total loss: 81.96548562264998\n",
      "epoch 416 learning rate:  0.012403846153846154   Training loss:   83.98280888091705  Valing loss:   81.96548562264998\n",
      "Pure loss: 83.98202388016988.....Total loss: 83.98202388016988\n",
      "Pure loss: 81.09070371773275.....Total loss: 81.09070371773275\n",
      "epoch 417 learning rate:  0.012398081534772183   Training loss:   83.98202388016988  Valing loss:   81.09070371773275\n",
      "Pure loss: 84.22664491406175.....Total loss: 84.22664491406175\n",
      "Pure loss: 80.59328793836104.....Total loss: 80.59328793836104\n",
      "epoch 418 learning rate:  0.012392344497607656   Training loss:   84.22664491406175  Valing loss:   80.59328793836104\n",
      "Pure loss: 83.77443258916021.....Total loss: 83.77443258916021\n",
      "Pure loss: 80.12834251334131.....Total loss: 80.12834251334131\n",
      "epoch 419 learning rate:  0.012386634844868735   Training loss:   83.77443258916021  Valing loss:   80.12834251334131\n",
      "Pure loss: 84.90381598515806.....Total loss: 84.90381598515806\n",
      "Pure loss: 78.77338014003949.....Total loss: 78.77338014003949\n",
      "epoch 420 learning rate:  0.012380952380952381   Training loss:   84.90381598515806  Valing loss:   78.77338014003949\n",
      "Pure loss: 85.28336926895166.....Total loss: 85.28336926895166\n",
      "Pure loss: 78.6931316929379.....Total loss: 78.6931316929379\n",
      "epoch 421 learning rate:  0.012375296912114014   Training loss:   85.28336926895166  Valing loss:   78.6931316929379\n",
      "Pure loss: 83.48887296060903.....Total loss: 83.48887296060903\n",
      "Pure loss: 79.37745922609746.....Total loss: 79.37745922609746\n",
      "epoch 422 learning rate:  0.012369668246445498   Training loss:   83.48887296060903  Valing loss:   79.37745922609746\n",
      "Pure loss: 82.89767422541723.....Total loss: 82.89767422541723\n",
      "Pure loss: 80.18831237725826.....Total loss: 80.18831237725826\n",
      "epoch 423 learning rate:  0.012364066193853428   Training loss:   82.89767422541723  Valing loss:   80.18831237725826\n",
      "Pure loss: 83.19992032945297.....Total loss: 83.19992032945297\n",
      "Pure loss: 77.93775205053248.....Total loss: 77.93775205053248\n",
      "epoch 424 learning rate:  0.012358490566037736   Training loss:   83.19992032945297  Valing loss:   77.93775205053248\n",
      "Pure loss: 83.4640998806108.....Total loss: 83.4640998806108\n",
      "Pure loss: 77.67221389766235.....Total loss: 77.67221389766235\n",
      "epoch 425 learning rate:  0.012352941176470587   Training loss:   83.4640998806108  Valing loss:   77.67221389766235\n",
      "Pure loss: 83.71711511457077.....Total loss: 83.71711511457077\n",
      "Pure loss: 77.64757230848642.....Total loss: 77.64757230848642\n",
      "epoch 426 learning rate:  0.012347417840375588   Training loss:   83.71711511457077  Valing loss:   77.64757230848642\n",
      "Pure loss: 82.41532812844954.....Total loss: 82.41532812844954\n",
      "Pure loss: 78.11758699914428.....Total loss: 78.11758699914428\n",
      "epoch 427 learning rate:  0.012341920374707261   Training loss:   82.41532812844954  Valing loss:   78.11758699914428\n",
      "Pure loss: 82.54562055403849.....Total loss: 82.54562055403849\n",
      "Pure loss: 78.02880948599682.....Total loss: 78.02880948599682\n",
      "epoch 428 learning rate:  0.012336448598130842   Training loss:   82.54562055403849  Valing loss:   78.02880948599682\n",
      "Pure loss: 83.45794668194135.....Total loss: 83.45794668194135\n",
      "Pure loss: 76.4719723995451.....Total loss: 76.4719723995451\n",
      "epoch 429 learning rate:  0.01233100233100233   Training loss:   83.45794668194135  Valing loss:   76.4719723995451\n",
      "Pure loss: 82.6988067475198.....Total loss: 82.6988067475198\n",
      "Pure loss: 76.81822687266872.....Total loss: 76.81822687266872\n",
      "epoch 430 learning rate:  0.012325581395348837   Training loss:   82.6988067475198  Valing loss:   76.81822687266872\n",
      "Pure loss: 81.42129767329311.....Total loss: 81.42129767329311\n",
      "Pure loss: 75.6967225687496.....Total loss: 75.6967225687496\n",
      "epoch 431 learning rate:  0.012320185614849188   Training loss:   81.42129767329311  Valing loss:   75.6967225687496\n",
      "Pure loss: 81.52576252582091.....Total loss: 81.52576252582091\n",
      "Pure loss: 75.67801997889298.....Total loss: 75.67801997889298\n",
      "epoch 432 learning rate:  0.012314814814814815   Training loss:   81.52576252582091  Valing loss:   75.67801997889298\n",
      "Pure loss: 81.14914901451856.....Total loss: 81.14914901451856\n",
      "Pure loss: 76.14465326202362.....Total loss: 76.14465326202362\n",
      "epoch 433 learning rate:  0.012309468822170901   Training loss:   81.14914901451856  Valing loss:   76.14465326202362\n",
      "Pure loss: 81.30605255175233.....Total loss: 81.30605255175233\n",
      "Pure loss: 75.95866721665585.....Total loss: 75.95866721665585\n",
      "epoch 434 learning rate:  0.012304147465437789   Training loss:   81.30605255175233  Valing loss:   75.95866721665585\n",
      "Pure loss: 81.34155184823061.....Total loss: 81.34155184823061\n",
      "Pure loss: 75.92096171209218.....Total loss: 75.92096171209218\n",
      "epoch 435 learning rate:  0.012298850574712644   Training loss:   81.34155184823061  Valing loss:   75.92096171209218\n",
      "Pure loss: 81.65214408319643.....Total loss: 81.65214408319643\n",
      "Pure loss: 75.85389569356116.....Total loss: 75.85389569356116\n",
      "epoch 436 learning rate:  0.012293577981651376   Training loss:   81.65214408319643  Valing loss:   75.85389569356116\n",
      "Pure loss: 83.08250830954842.....Total loss: 83.08250830954842\n",
      "Pure loss: 75.97759219760485.....Total loss: 75.97759219760485\n",
      "epoch 437 learning rate:  0.0122883295194508   Training loss:   83.08250830954842  Valing loss:   75.97759219760485\n",
      "Pure loss: 84.32506523230607.....Total loss: 84.32506523230607\n",
      "Pure loss: 77.48562114285627.....Total loss: 77.48562114285627\n",
      "epoch 438 learning rate:  0.01228310502283105   Training loss:   84.32506523230607  Valing loss:   77.48562114285627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 83.82640619867104.....Total loss: 83.82640619867104\n",
      "Pure loss: 77.35376910244742.....Total loss: 77.35376910244742\n",
      "epoch 439 learning rate:  0.012277904328018223   Training loss:   83.82640619867104  Valing loss:   77.35376910244742\n",
      "Pure loss: 82.66191828976586.....Total loss: 82.66191828976586\n",
      "Pure loss: 77.15194069122185.....Total loss: 77.15194069122185\n",
      "epoch 440 learning rate:  0.012272727272727272   Training loss:   82.66191828976586  Valing loss:   77.15194069122185\n",
      "Pure loss: 83.12240281574003.....Total loss: 83.12240281574003\n",
      "Pure loss: 77.29756206663704.....Total loss: 77.29756206663704\n",
      "epoch 441 learning rate:  0.012267573696145125   Training loss:   83.12240281574003  Valing loss:   77.29756206663704\n",
      "Pure loss: 84.2826824629181.....Total loss: 84.2826824629181\n",
      "Pure loss: 77.41859888516538.....Total loss: 77.41859888516538\n",
      "epoch 442 learning rate:  0.012262443438914028   Training loss:   84.2826824629181  Valing loss:   77.41859888516538\n",
      "Pure loss: 83.4468325313341.....Total loss: 83.4468325313341\n",
      "Pure loss: 76.54499415729134.....Total loss: 76.54499415729134\n",
      "epoch 443 learning rate:  0.012257336343115124   Training loss:   83.4468325313341  Valing loss:   76.54499415729134\n",
      "Pure loss: 85.06331177025574.....Total loss: 85.06331177025574\n",
      "Pure loss: 76.97482481904333.....Total loss: 76.97482481904333\n",
      "epoch 444 learning rate:  0.012252252252252252   Training loss:   85.06331177025574  Valing loss:   76.97482481904333\n",
      "Pure loss: 83.44153672872356.....Total loss: 83.44153672872356\n",
      "Pure loss: 75.21989569411053.....Total loss: 75.21989569411053\n",
      "epoch 445 learning rate:  0.012247191011235956   Training loss:   83.44153672872356  Valing loss:   75.21989569411053\n",
      "Pure loss: 80.29209912786321.....Total loss: 80.29209912786321\n",
      "Pure loss: 74.10322595382402.....Total loss: 74.10322595382402\n",
      "epoch 446 learning rate:  0.012242152466367713   Training loss:   80.29209912786321  Valing loss:   74.10322595382402\n",
      "Pure loss: 80.47792589449773.....Total loss: 80.47792589449773\n",
      "Pure loss: 74.2409866745389.....Total loss: 74.2409866745389\n",
      "epoch 447 learning rate:  0.012237136465324386   Training loss:   80.47792589449773  Valing loss:   74.2409866745389\n",
      "Pure loss: 81.51680873591191.....Total loss: 81.51680873591191\n",
      "Pure loss: 74.06445571917395.....Total loss: 74.06445571917395\n",
      "epoch 448 learning rate:  0.012232142857142858   Training loss:   81.51680873591191  Valing loss:   74.06445571917395\n",
      "Pure loss: 79.17594372289518.....Total loss: 79.17594372289518\n",
      "Pure loss: 74.06585669085236.....Total loss: 74.06585669085236\n",
      "epoch 449 learning rate:  0.0122271714922049   Training loss:   79.17594372289518  Valing loss:   74.06585669085236\n",
      "Pure loss: 79.20275516970474.....Total loss: 79.20275516970474\n",
      "Pure loss: 73.97982342012783.....Total loss: 73.97982342012783\n",
      "epoch 450 learning rate:  0.012222222222222223   Training loss:   79.20275516970474  Valing loss:   73.97982342012783\n",
      "Pure loss: 79.13448372224713.....Total loss: 79.13448372224713\n",
      "Pure loss: 76.98170974033958.....Total loss: 76.98170974033958\n",
      "epoch 451 learning rate:  0.01221729490022173   Training loss:   79.13448372224713  Valing loss:   76.98170974033958\n",
      "Pure loss: 80.09379395217158.....Total loss: 80.09379395217158\n",
      "Pure loss: 79.00536600568094.....Total loss: 79.00536600568094\n",
      "epoch 452 learning rate:  0.012212389380530974   Training loss:   80.09379395217158  Valing loss:   79.00536600568094\n",
      "Pure loss: 79.12872979365243.....Total loss: 79.12872979365243\n",
      "Pure loss: 78.05234717824054.....Total loss: 78.05234717824054\n",
      "epoch 453 learning rate:  0.012207505518763796   Training loss:   79.12872979365243  Valing loss:   78.05234717824054\n",
      "Pure loss: 79.10637360614807.....Total loss: 79.10637360614807\n",
      "Pure loss: 78.00571378257396.....Total loss: 78.00571378257396\n",
      "epoch 454 learning rate:  0.012202643171806167   Training loss:   79.10637360614807  Valing loss:   78.00571378257396\n",
      "Pure loss: 80.03668461080181.....Total loss: 80.03668461080181\n",
      "Pure loss: 80.06347914283613.....Total loss: 80.06347914283613\n",
      "epoch 455 learning rate:  0.012197802197802199   Training loss:   80.03668461080181  Valing loss:   80.06347914283613\n",
      "Pure loss: 78.65176204881325.....Total loss: 78.65176204881325\n",
      "Pure loss: 77.09795351386295.....Total loss: 77.09795351386295\n",
      "epoch 456 learning rate:  0.012192982456140351   Training loss:   78.65176204881325  Valing loss:   77.09795351386295\n",
      "Pure loss: 79.98159263734115.....Total loss: 79.98159263734115\n",
      "Pure loss: 80.20302642291071.....Total loss: 80.20302642291071\n",
      "epoch 457 learning rate:  0.012188183807439825   Training loss:   79.98159263734115  Valing loss:   80.20302642291071\n",
      "Pure loss: 80.83975458961.....Total loss: 80.83975458961\n",
      "Pure loss: 81.67836703541607.....Total loss: 81.67836703541607\n",
      "epoch 458 learning rate:  0.012183406113537118   Training loss:   80.83975458961  Valing loss:   81.67836703541607\n",
      "Pure loss: 80.74897071403892.....Total loss: 80.74897071403892\n",
      "Pure loss: 81.51561807980072.....Total loss: 81.51561807980072\n",
      "epoch 459 learning rate:  0.012178649237472767   Training loss:   80.74897071403892  Valing loss:   81.51561807980072\n",
      "Pure loss: 80.2345334467439.....Total loss: 80.2345334467439\n",
      "Pure loss: 81.02413925479004.....Total loss: 81.02413925479004\n",
      "epoch 460 learning rate:  0.01217391304347826   Training loss:   80.2345334467439  Valing loss:   81.02413925479004\n",
      "Pure loss: 78.82900944953111.....Total loss: 78.82900944953111\n",
      "Pure loss: 78.50337976265999.....Total loss: 78.50337976265999\n",
      "epoch 461 learning rate:  0.012169197396963123   Training loss:   78.82900944953111  Valing loss:   78.50337976265999\n",
      "Pure loss: 78.55156206139434.....Total loss: 78.55156206139434\n",
      "Pure loss: 77.74690989526309.....Total loss: 77.74690989526309\n",
      "epoch 462 learning rate:  0.012164502164502164   Training loss:   78.55156206139434  Valing loss:   77.74690989526309\n",
      "Pure loss: 78.86214618314325.....Total loss: 78.86214618314325\n",
      "Pure loss: 78.5043026609363.....Total loss: 78.5043026609363\n",
      "epoch 463 learning rate:  0.012159827213822895   Training loss:   78.86214618314325  Valing loss:   78.5043026609363\n",
      "Pure loss: 79.87329877828671.....Total loss: 79.87329877828671\n",
      "Pure loss: 81.03485409150011.....Total loss: 81.03485409150011\n",
      "epoch 464 learning rate:  0.012155172413793103   Training loss:   79.87329877828671  Valing loss:   81.03485409150011\n",
      "Pure loss: 80.07000108705962.....Total loss: 80.07000108705962\n",
      "Pure loss: 81.37474641533589.....Total loss: 81.37474641533589\n",
      "epoch 465 learning rate:  0.012150537634408601   Training loss:   80.07000108705962  Valing loss:   81.37474641533589\n",
      "Pure loss: 81.3006321381669.....Total loss: 81.3006321381669\n",
      "Pure loss: 83.76733714609563.....Total loss: 83.76733714609563\n",
      "epoch 466 learning rate:  0.012145922746781116   Training loss:   81.3006321381669  Valing loss:   83.76733714609563\n",
      "Pure loss: 81.69141078697795.....Total loss: 81.69141078697795\n",
      "Pure loss: 84.47671628569722.....Total loss: 84.47671628569722\n",
      "epoch 467 learning rate:  0.012141327623126339   Training loss:   81.69141078697795  Valing loss:   84.47671628569722\n",
      "Pure loss: 82.15428021094655.....Total loss: 82.15428021094655\n",
      "Pure loss: 85.2226877536588.....Total loss: 85.2226877536588\n",
      "epoch 468 learning rate:  0.012136752136752138   Training loss:   82.15428021094655  Valing loss:   85.2226877536588\n",
      "Pure loss: 83.41113910378543.....Total loss: 83.41113910378543\n",
      "Pure loss: 86.93697204869575.....Total loss: 86.93697204869575\n",
      "epoch 469 learning rate:  0.012132196162046908   Training loss:   83.41113910378543  Valing loss:   86.93697204869575\n",
      "Pure loss: 81.93133769707866.....Total loss: 81.93133769707866\n",
      "Pure loss: 84.81397109289703.....Total loss: 84.81397109289703\n",
      "epoch 470 learning rate:  0.012127659574468085   Training loss:   81.93133769707866  Valing loss:   84.81397109289703\n",
      "Pure loss: 79.39288426137031.....Total loss: 79.39288426137031\n",
      "Pure loss: 80.79848641566169.....Total loss: 80.79848641566169\n",
      "epoch 471 learning rate:  0.012123142250530785   Training loss:   79.39288426137031  Valing loss:   80.79848641566169\n",
      "Pure loss: 80.01398028757599.....Total loss: 80.01398028757599\n",
      "Pure loss: 81.77645105675673.....Total loss: 81.77645105675673\n",
      "epoch 472 learning rate:  0.01211864406779661   Training loss:   80.01398028757599  Valing loss:   81.77645105675673\n",
      "Pure loss: 82.96994459761166.....Total loss: 82.96994459761166\n",
      "Pure loss: 86.6985098578826.....Total loss: 86.6985098578826\n",
      "epoch 473 learning rate:  0.01211416490486258   Training loss:   82.96994459761166  Valing loss:   86.6985098578826\n",
      "Pure loss: 84.53346047844543.....Total loss: 84.53346047844543\n",
      "Pure loss: 88.705499756121.....Total loss: 88.705499756121\n",
      "epoch 474 learning rate:  0.012109704641350211   Training loss:   84.53346047844543  Valing loss:   88.705499756121\n",
      "Pure loss: 79.97219283565849.....Total loss: 79.97219283565849\n",
      "Pure loss: 81.94261801628974.....Total loss: 81.94261801628974\n",
      "epoch 475 learning rate:  0.012105263157894737   Training loss:   79.97219283565849  Valing loss:   81.94261801628974\n",
      "Pure loss: 80.17002686417534.....Total loss: 80.17002686417534\n",
      "Pure loss: 82.23949719581577.....Total loss: 82.23949719581577\n",
      "epoch 476 learning rate:  0.012100840336134453   Training loss:   80.17002686417534  Valing loss:   82.23949719581577\n",
      "Pure loss: 80.90531641531226.....Total loss: 80.90531641531226\n",
      "Pure loss: 83.21643985959086.....Total loss: 83.21643985959086\n",
      "epoch 477 learning rate:  0.01209643605870021   Training loss:   80.90531641531226  Valing loss:   83.21643985959086\n",
      "Pure loss: 79.75151918726304.....Total loss: 79.75151918726304\n",
      "Pure loss: 82.03003443439088.....Total loss: 82.03003443439088\n",
      "epoch 478 learning rate:  0.012092050209205021   Training loss:   79.75151918726304  Valing loss:   82.03003443439088\n",
      "Pure loss: 78.2810514398584.....Total loss: 78.2810514398584\n",
      "Pure loss: 79.69026520619232.....Total loss: 79.69026520619232\n",
      "epoch 479 learning rate:  0.01208768267223382   Training loss:   78.2810514398584  Valing loss:   79.69026520619232\n",
      "Pure loss: 77.54902010018007.....Total loss: 77.54902010018007\n",
      "Pure loss: 78.42388229941479.....Total loss: 78.42388229941479\n",
      "epoch 480 learning rate:  0.012083333333333333   Training loss:   77.54902010018007  Valing loss:   78.42388229941479\n",
      "Pure loss: 77.10946490107726.....Total loss: 77.10946490107726\n",
      "Pure loss: 77.60735367362983.....Total loss: 77.60735367362983\n",
      "epoch 481 learning rate:  0.01207900207900208   Training loss:   77.10946490107726  Valing loss:   77.60735367362983\n",
      "Pure loss: 77.17808217608318.....Total loss: 77.17808217608318\n",
      "Pure loss: 77.74763914974649.....Total loss: 77.74763914974649\n",
      "epoch 482 learning rate:  0.012074688796680498   Training loss:   77.17808217608318  Valing loss:   77.74763914974649\n",
      "Pure loss: 77.39034794255346.....Total loss: 77.39034794255346\n",
      "Pure loss: 78.16477150001661.....Total loss: 78.16477150001661\n",
      "epoch 483 learning rate:  0.012070393374741202   Training loss:   77.39034794255346  Valing loss:   78.16477150001661\n",
      "Pure loss: 77.71955418491508.....Total loss: 77.71955418491508\n",
      "Pure loss: 78.65512449122397.....Total loss: 78.65512449122397\n",
      "epoch 484 learning rate:  0.012066115702479339   Training loss:   77.71955418491508  Valing loss:   78.65512449122397\n",
      "Pure loss: 76.9807910477431.....Total loss: 76.9807910477431\n",
      "Pure loss: 77.37187448752435.....Total loss: 77.37187448752435\n",
      "epoch 485 learning rate:  0.012061855670103093   Training loss:   76.9807910477431  Valing loss:   77.37187448752435\n",
      "Pure loss: 76.60010022573736.....Total loss: 76.60010022573736\n",
      "Pure loss: 76.62122752889796.....Total loss: 76.62122752889796\n",
      "epoch 486 learning rate:  0.01205761316872428   Training loss:   76.60010022573736  Valing loss:   76.62122752889796\n",
      "Pure loss: 76.01105822839979.....Total loss: 76.01105822839979\n",
      "Pure loss: 75.49346108764412.....Total loss: 75.49346108764412\n",
      "epoch 487 learning rate:  0.012053388090349077   Training loss:   76.01105822839979  Valing loss:   75.49346108764412\n",
      "Pure loss: 75.48563572581602.....Total loss: 75.48563572581602\n",
      "Pure loss: 74.28541321127494.....Total loss: 74.28541321127494\n",
      "epoch 488 learning rate:  0.012049180327868853   Training loss:   75.48563572581602  Valing loss:   74.28541321127494\n",
      "Pure loss: 74.40362311105906.....Total loss: 74.40362311105906\n",
      "Pure loss: 71.30629180626403.....Total loss: 71.30629180626403\n",
      "epoch 489 learning rate:  0.012044989775051125   Training loss:   74.40362311105906  Valing loss:   71.30629180626403\n",
      "Pure loss: 74.80856681472167.....Total loss: 74.80856681472167\n",
      "Pure loss: 71.58658128734841.....Total loss: 71.58658128734841\n",
      "epoch 490 learning rate:  0.012040816326530613   Training loss:   74.80856681472167  Valing loss:   71.58658128734841\n",
      "Pure loss: 74.88152479213414.....Total loss: 74.88152479213414\n",
      "Pure loss: 71.26406466823764.....Total loss: 71.26406466823764\n",
      "epoch 491 learning rate:  0.012036659877800407   Training loss:   74.88152479213414  Valing loss:   71.26406466823764\n",
      "Pure loss: 75.43930843547199.....Total loss: 75.43930843547199\n",
      "Pure loss: 70.56333258224143.....Total loss: 70.56333258224143\n",
      "epoch 492 learning rate:  0.012032520325203253   Training loss:   75.43930843547199  Valing loss:   70.56333258224143\n",
      "Pure loss: 74.25553582497568.....Total loss: 74.25553582497568\n",
      "Pure loss: 69.81338221257461.....Total loss: 69.81338221257461\n",
      "epoch 493 learning rate:  0.012028397565922921   Training loss:   74.25553582497568  Valing loss:   69.81338221257461\n",
      "Pure loss: 75.32536973959951.....Total loss: 75.32536973959951\n",
      "Pure loss: 70.70045312151277.....Total loss: 70.70045312151277\n",
      "epoch 494 learning rate:  0.012024291497975709   Training loss:   75.32536973959951  Valing loss:   70.70045312151277\n",
      "Pure loss: 75.64898984804942.....Total loss: 75.64898984804942\n",
      "Pure loss: 71.05370737193113.....Total loss: 71.05370737193113\n",
      "epoch 495 learning rate:  0.012020202020202021   Training loss:   75.64898984804942  Valing loss:   71.05370737193113\n",
      "Pure loss: 75.02023110927945.....Total loss: 75.02023110927945\n",
      "Pure loss: 71.18347275176015.....Total loss: 71.18347275176015\n",
      "epoch 496 learning rate:  0.012016129032258065   Training loss:   75.02023110927945  Valing loss:   71.18347275176015\n",
      "Pure loss: 74.95933959089741.....Total loss: 74.95933959089741\n",
      "Pure loss: 71.14958108410936.....Total loss: 71.14958108410936\n",
      "epoch 497 learning rate:  0.012012072434607647   Training loss:   74.95933959089741  Valing loss:   71.14958108410936\n",
      "Pure loss: 74.20776048825296.....Total loss: 74.20776048825296\n",
      "Pure loss: 73.06539713777076.....Total loss: 73.06539713777076\n",
      "epoch 498 learning rate:  0.012008032128514055   Training loss:   74.20776048825296  Valing loss:   73.06539713777076\n",
      "Pure loss: 80.68856218038977.....Total loss: 80.68856218038977\n",
      "Pure loss: 84.7427179922616.....Total loss: 84.7427179922616\n",
      "epoch 499 learning rate:  0.012004008016032065   Training loss:   80.68856218038977  Valing loss:   84.7427179922616\n",
      "Pure loss: 78.3268979837397.....Total loss: 78.3268979837397\n",
      "Pure loss: 81.17799523496082.....Total loss: 81.17799523496082\n",
      "epoch 500 learning rate:  0.012   Training loss:   78.3268979837397  Valing loss:   81.17799523496082\n",
      "Pure loss: 75.96981975097611.....Total loss: 75.96981975097611\n",
      "Pure loss: 77.56073300317738.....Total loss: 77.56073300317738\n",
      "epoch 501 learning rate:  0.011996007984031935   Training loss:   75.96981975097611  Valing loss:   77.56073300317738\n",
      "Pure loss: 76.05752356707085.....Total loss: 76.05752356707085\n",
      "Pure loss: 77.73107926006331.....Total loss: 77.73107926006331\n",
      "epoch 502 learning rate:  0.01199203187250996   Training loss:   76.05752356707085  Valing loss:   77.73107926006331\n",
      "Pure loss: 75.68190261794723.....Total loss: 75.68190261794723\n",
      "Pure loss: 77.11103791685309.....Total loss: 77.11103791685309\n",
      "epoch 503 learning rate:  0.01198807157057654   Training loss:   75.68190261794723  Valing loss:   77.11103791685309\n",
      "Pure loss: 74.75819976109958.....Total loss: 74.75819976109958\n",
      "Pure loss: 75.46823613321568.....Total loss: 75.46823613321568\n",
      "epoch 504 learning rate:  0.011984126984126984   Training loss:   74.75819976109958  Valing loss:   75.46823613321568\n",
      "Pure loss: 76.49472610987357.....Total loss: 76.49472610987357\n",
      "Pure loss: 78.51781701206946.....Total loss: 78.51781701206946\n",
      "epoch 505 learning rate:  0.01198019801980198   Training loss:   76.49472610987357  Valing loss:   78.51781701206946\n",
      "Pure loss: 77.0083490678742.....Total loss: 77.0083490678742\n",
      "Pure loss: 79.24175527834892.....Total loss: 79.24175527834892\n",
      "epoch 506 learning rate:  0.011976284584980238   Training loss:   77.0083490678742  Valing loss:   79.24175527834892\n",
      "Pure loss: 76.53804386444848.....Total loss: 76.53804386444848\n",
      "Pure loss: 78.53128917990006.....Total loss: 78.53128917990006\n",
      "epoch 507 learning rate:  0.011972386587771203   Training loss:   76.53804386444848  Valing loss:   78.53128917990006\n",
      "Pure loss: 75.06453584776376.....Total loss: 75.06453584776376\n",
      "Pure loss: 76.16512186903456.....Total loss: 76.16512186903456\n",
      "epoch 508 learning rate:  0.011968503937007874   Training loss:   75.06453584776376  Valing loss:   76.16512186903456\n",
      "Pure loss: 75.5869755229682.....Total loss: 75.5869755229682\n",
      "Pure loss: 76.90577631241341.....Total loss: 76.90577631241341\n",
      "epoch 509 learning rate:  0.011964636542239686   Training loss:   75.5869755229682  Valing loss:   76.90577631241341\n",
      "Pure loss: 72.70455833965369.....Total loss: 72.70455833965369\n",
      "Pure loss: 72.0134659289508.....Total loss: 72.0134659289508\n",
      "epoch 510 learning rate:  0.01196078431372549   Training loss:   72.70455833965369  Valing loss:   72.0134659289508\n",
      "Pure loss: 71.28072146748791.....Total loss: 71.28072146748791\n",
      "Pure loss: 68.80222357540524.....Total loss: 68.80222357540524\n",
      "epoch 511 learning rate:  0.011956947162426615   Training loss:   71.28072146748791  Valing loss:   68.80222357540524\n",
      "Pure loss: 71.42322469025333.....Total loss: 71.42322469025333\n",
      "Pure loss: 69.22591755662044.....Total loss: 69.22591755662044\n",
      "epoch 512 learning rate:  0.011953125   Training loss:   71.42322469025333  Valing loss:   69.22591755662044\n",
      "Pure loss: 70.97248477655414.....Total loss: 70.97248477655414\n",
      "Pure loss: 68.14239111829067.....Total loss: 68.14239111829067\n",
      "epoch 513 learning rate:  0.011949317738791423   Training loss:   70.97248477655414  Valing loss:   68.14239111829067\n",
      "Pure loss: 70.71722814924438.....Total loss: 70.71722814924438\n",
      "Pure loss: 67.17987476918701.....Total loss: 67.17987476918701\n",
      "epoch 514 learning rate:  0.011945525291828794   Training loss:   70.71722814924438  Valing loss:   67.17987476918701\n",
      "Pure loss: 70.79111563727568.....Total loss: 70.79111563727568\n",
      "Pure loss: 67.45293215119611.....Total loss: 67.45293215119611\n",
      "epoch 515 learning rate:  0.011941747572815534   Training loss:   70.79111563727568  Valing loss:   67.45293215119611\n",
      "Pure loss: 70.88086587272545.....Total loss: 70.88086587272545\n",
      "Pure loss: 67.84356988563988.....Total loss: 67.84356988563988\n",
      "epoch 516 learning rate:  0.011937984496124031   Training loss:   70.88086587272545  Valing loss:   67.84356988563988\n",
      "Pure loss: 70.96119200468189.....Total loss: 70.96119200468189\n",
      "Pure loss: 68.08353591393393.....Total loss: 68.08353591393393\n",
      "epoch 517 learning rate:  0.011934235976789168   Training loss:   70.96119200468189  Valing loss:   68.08353591393393\n",
      "Pure loss: 70.89768789578473.....Total loss: 70.89768789578473\n",
      "Pure loss: 68.1052306099934.....Total loss: 68.1052306099934\n",
      "epoch 518 learning rate:  0.01193050193050193   Training loss:   70.89768789578473  Valing loss:   68.1052306099934\n",
      "Pure loss: 70.95215820327687.....Total loss: 70.95215820327687\n",
      "Pure loss: 68.31725453414923.....Total loss: 68.31725453414923\n",
      "epoch 519 learning rate:  0.011926782273603083   Training loss:   70.95215820327687  Valing loss:   68.31725453414923\n",
      "Pure loss: 70.97102976657538.....Total loss: 70.97102976657538\n",
      "Pure loss: 68.43506216075036.....Total loss: 68.43506216075036\n",
      "epoch 520 learning rate:  0.011923076923076923   Training loss:   70.97102976657538  Valing loss:   68.43506216075036\n",
      "Pure loss: 72.77603170525015.....Total loss: 72.77603170525015\n",
      "Pure loss: 73.26376373245697.....Total loss: 73.26376373245697\n",
      "epoch 521 learning rate:  0.011919385796545105   Training loss:   72.77603170525015  Valing loss:   73.26376373245697\n",
      "Pure loss: 74.11207680103786.....Total loss: 74.11207680103786\n",
      "Pure loss: 75.48410672381605.....Total loss: 75.48410672381605\n",
      "epoch 522 learning rate:  0.011915708812260537   Training loss:   74.11207680103786  Valing loss:   75.48410672381605\n",
      "Pure loss: 75.17605211002432.....Total loss: 75.17605211002432\n",
      "Pure loss: 76.94111107259441.....Total loss: 76.94111107259441\n",
      "epoch 523 learning rate:  0.011912045889101339   Training loss:   75.17605211002432  Valing loss:   76.94111107259441\n",
      "Pure loss: 74.82308393127126.....Total loss: 74.82308393127126\n",
      "Pure loss: 76.46499299469592.....Total loss: 76.46499299469592\n",
      "epoch 524 learning rate:  0.011908396946564885   Training loss:   74.82308393127126  Valing loss:   76.46499299469592\n",
      "Pure loss: 74.08391430017404.....Total loss: 74.08391430017404\n",
      "Pure loss: 75.28822362112228.....Total loss: 75.28822362112228\n",
      "epoch 525 learning rate:  0.011904761904761904   Training loss:   74.08391430017404  Valing loss:   75.28822362112228\n",
      "Pure loss: 72.68432384571017.....Total loss: 72.68432384571017\n",
      "Pure loss: 73.13113836730959.....Total loss: 73.13113836730959\n",
      "epoch 526 learning rate:  0.011901140684410647   Training loss:   72.68432384571017  Valing loss:   73.13113836730959\n",
      "Pure loss: 70.05516309952347.....Total loss: 70.05516309952347\n",
      "Pure loss: 67.76867701018143.....Total loss: 67.76867701018143\n",
      "epoch 527 learning rate:  0.01189753320683112   Training loss:   70.05516309952347  Valing loss:   67.76867701018143\n",
      "Pure loss: 70.66793104380808.....Total loss: 70.66793104380808\n",
      "Pure loss: 69.11362185384637.....Total loss: 69.11362185384637\n",
      "epoch 528 learning rate:  0.011893939393939394   Training loss:   70.66793104380808  Valing loss:   69.11362185384637\n",
      "Pure loss: 71.00199882572689.....Total loss: 71.00199882572689\n",
      "Pure loss: 69.6435728643205.....Total loss: 69.6435728643205\n",
      "epoch 529 learning rate:  0.011890359168241967   Training loss:   71.00199882572689  Valing loss:   69.6435728643205\n",
      "Pure loss: 70.00167554326649.....Total loss: 70.00167554326649\n",
      "Pure loss: 68.26947584592561.....Total loss: 68.26947584592561\n",
      "epoch 530 learning rate:  0.011886792452830188   Training loss:   70.00167554326649  Valing loss:   68.26947584592561\n",
      "Pure loss: 70.56178746651163.....Total loss: 70.56178746651163\n",
      "Pure loss: 69.56481492637016.....Total loss: 69.56481492637016\n",
      "epoch 531 learning rate:  0.011883239171374765   Training loss:   70.56178746651163  Valing loss:   69.56481492637016\n",
      "Pure loss: 71.22884631896288.....Total loss: 71.22884631896288\n",
      "Pure loss: 70.71140201251376.....Total loss: 70.71140201251376\n",
      "epoch 532 learning rate:  0.0118796992481203   Training loss:   71.22884631896288  Valing loss:   70.71140201251376\n",
      "Pure loss: 71.1516144493728.....Total loss: 71.1516144493728\n",
      "Pure loss: 70.58700744534771.....Total loss: 70.58700744534771\n",
      "epoch 533 learning rate:  0.011876172607879924   Training loss:   71.1516144493728  Valing loss:   70.58700744534771\n",
      "Pure loss: 71.94531349366505.....Total loss: 71.94531349366505\n",
      "Pure loss: 72.02956694955385.....Total loss: 72.02956694955385\n",
      "epoch 534 learning rate:  0.011872659176029962   Training loss:   71.94531349366505  Valing loss:   72.02956694955385\n",
      "Pure loss: 72.44401356458472.....Total loss: 72.44401356458472\n",
      "Pure loss: 72.71970400708636.....Total loss: 72.71970400708636\n",
      "epoch 535 learning rate:  0.011869158878504673   Training loss:   72.44401356458472  Valing loss:   72.71970400708636\n",
      "Pure loss: 73.47184148661799.....Total loss: 73.47184148661799\n",
      "Pure loss: 74.36404585157487.....Total loss: 74.36404585157487\n",
      "epoch 536 learning rate:  0.011865671641791045   Training loss:   73.47184148661799  Valing loss:   74.36404585157487\n",
      "Pure loss: 73.29245473280437.....Total loss: 73.29245473280437\n",
      "Pure loss: 74.11250600037077.....Total loss: 74.11250600037077\n",
      "epoch 537 learning rate:  0.01186219739292365   Training loss:   73.29245473280437  Valing loss:   74.11250600037077\n",
      "Pure loss: 73.38686438847216.....Total loss: 73.38686438847216\n",
      "Pure loss: 74.22644432769874.....Total loss: 74.22644432769874\n",
      "epoch 538 learning rate:  0.011858736059479555   Training loss:   73.38686438847216  Valing loss:   74.22644432769874\n",
      "Pure loss: 75.43954791109284.....Total loss: 75.43954791109284\n",
      "Pure loss: 76.96330808894395.....Total loss: 76.96330808894395\n",
      "epoch 539 learning rate:  0.011855287569573283   Training loss:   75.43954791109284  Valing loss:   76.96330808894395\n",
      "Pure loss: 75.53975545604513.....Total loss: 75.53975545604513\n",
      "Pure loss: 77.11427132429715.....Total loss: 77.11427132429715\n",
      "epoch 540 learning rate:  0.011851851851851853   Training loss:   75.53975545604513  Valing loss:   77.11427132429715\n",
      "Pure loss: 71.92670034719782.....Total loss: 71.92670034719782\n",
      "Pure loss: 72.68056988976704.....Total loss: 72.68056988976704\n",
      "epoch 541 learning rate:  0.011848428835489835   Training loss:   71.92670034719782  Valing loss:   72.68056988976704\n",
      "Pure loss: 70.3641492323028.....Total loss: 70.3641492323028\n",
      "Pure loss: 70.04128043225947.....Total loss: 70.04128043225947\n",
      "epoch 542 learning rate:  0.011845018450184501   Training loss:   70.3641492323028  Valing loss:   70.04128043225947\n",
      "Pure loss: 68.61079260295804.....Total loss: 68.61079260295804\n",
      "Pure loss: 66.67728465644178.....Total loss: 66.67728465644178\n",
      "epoch 543 learning rate:  0.011841620626151014   Training loss:   68.61079260295804  Valing loss:   66.67728465644178\n",
      "Pure loss: 67.50511677354913.....Total loss: 67.50511677354913\n",
      "Pure loss: 64.29251648290075.....Total loss: 64.29251648290075\n",
      "epoch 544 learning rate:  0.011838235294117648   Training loss:   67.50511677354913  Valing loss:   64.29251648290075\n",
      "Pure loss: 67.26154179876357.....Total loss: 67.26154179876357\n",
      "Pure loss: 63.44984307093744.....Total loss: 63.44984307093744\n",
      "epoch 545 learning rate:  0.011834862385321101   Training loss:   67.26154179876357  Valing loss:   63.44984307093744\n",
      "Pure loss: 67.3503936691318.....Total loss: 67.3503936691318\n",
      "Pure loss: 62.921160587028446.....Total loss: 62.921160587028446\n",
      "epoch 546 learning rate:  0.011831501831501832   Training loss:   67.3503936691318  Valing loss:   62.921160587028446\n",
      "Pure loss: 67.27579036083291.....Total loss: 67.27579036083291\n",
      "Pure loss: 64.74892462854987.....Total loss: 64.74892462854987\n",
      "epoch 547 learning rate:  0.011828153564899451   Training loss:   67.27579036083291  Valing loss:   64.74892462854987\n",
      "Pure loss: 67.2484130882421.....Total loss: 67.2484130882421\n",
      "Pure loss: 64.60158445015432.....Total loss: 64.60158445015432\n",
      "epoch 548 learning rate:  0.011824817518248175   Training loss:   67.2484130882421  Valing loss:   64.60158445015432\n",
      "Pure loss: 67.73019097422488.....Total loss: 67.73019097422488\n",
      "Pure loss: 64.92652721274837.....Total loss: 64.92652721274837\n",
      "epoch 549 learning rate:  0.011821493624772313   Training loss:   67.73019097422488  Valing loss:   64.92652721274837\n",
      "Pure loss: 67.81209216384403.....Total loss: 67.81209216384403\n",
      "Pure loss: 62.61662869071796.....Total loss: 62.61662869071796\n",
      "epoch 550 learning rate:  0.011818181818181818   Training loss:   67.81209216384403  Valing loss:   62.61662869071796\n",
      "Pure loss: 67.10688319419432.....Total loss: 67.10688319419432\n",
      "Pure loss: 61.60499447007022.....Total loss: 61.60499447007022\n",
      "epoch 551 learning rate:  0.011814882032667878   Training loss:   67.10688319419432  Valing loss:   61.60499447007022\n",
      "Pure loss: 67.13407537570257.....Total loss: 67.13407537570257\n",
      "Pure loss: 61.54055545026571.....Total loss: 61.54055545026571\n",
      "epoch 552 learning rate:  0.01181159420289855   Training loss:   67.13407537570257  Valing loss:   61.54055545026571\n",
      "Pure loss: 66.64616361772427.....Total loss: 66.64616361772427\n",
      "Pure loss: 64.82967991559359.....Total loss: 64.82967991559359\n",
      "epoch 553 learning rate:  0.011808318264014466   Training loss:   66.64616361772427  Valing loss:   64.82967991559359\n",
      "Pure loss: 66.19522597740809.....Total loss: 66.19522597740809\n",
      "Pure loss: 62.48745383066637.....Total loss: 62.48745383066637\n",
      "epoch 554 learning rate:  0.011805054151624549   Training loss:   66.19522597740809  Valing loss:   62.48745383066637\n",
      "Pure loss: 66.21339800723763.....Total loss: 66.21339800723763\n",
      "Pure loss: 62.3321372116455.....Total loss: 62.3321372116455\n",
      "epoch 555 learning rate:  0.011801801801801803   Training loss:   66.21339800723763  Valing loss:   62.3321372116455\n",
      "Pure loss: 65.64938735500752.....Total loss: 65.64938735500752\n",
      "Pure loss: 61.88063603512536.....Total loss: 61.88063603512536\n",
      "epoch 556 learning rate:  0.011798561151079138   Training loss:   65.64938735500752  Valing loss:   61.88063603512536\n",
      "Pure loss: 65.66589193707905.....Total loss: 65.66589193707905\n",
      "Pure loss: 61.65673258523452.....Total loss: 61.65673258523452\n",
      "epoch 557 learning rate:  0.011795332136445243   Training loss:   65.66589193707905  Valing loss:   61.65673258523452\n",
      "Pure loss: 65.48713661898275.....Total loss: 65.48713661898275\n",
      "Pure loss: 61.59369498558342.....Total loss: 61.59369498558342\n",
      "epoch 558 learning rate:  0.011792114695340502   Training loss:   65.48713661898275  Valing loss:   61.59369498558342\n",
      "Pure loss: 65.98308343605392.....Total loss: 65.98308343605392\n",
      "Pure loss: 60.911624591668044.....Total loss: 60.911624591668044\n",
      "epoch 559 learning rate:  0.011788908765652951   Training loss:   65.98308343605392  Valing loss:   60.911624591668044\n",
      "Pure loss: 65.08359330552359.....Total loss: 65.08359330552359\n",
      "Pure loss: 62.605677494336746.....Total loss: 62.605677494336746\n",
      "epoch 560 learning rate:  0.011785714285714287   Training loss:   65.08359330552359  Valing loss:   62.605677494336746\n",
      "Pure loss: 65.11603971867349.....Total loss: 65.11603971867349\n",
      "Pure loss: 62.76522702053676.....Total loss: 62.76522702053676\n",
      "epoch 561 learning rate:  0.011782531194295901   Training loss:   65.11603971867349  Valing loss:   62.76522702053676\n",
      "Pure loss: 64.76450561087103.....Total loss: 64.76450561087103\n",
      "Pure loss: 60.55979156351265.....Total loss: 60.55979156351265\n",
      "epoch 562 learning rate:  0.011779359430604982   Training loss:   64.76450561087103  Valing loss:   60.55979156351265\n",
      "Pure loss: 64.98579955902869.....Total loss: 64.98579955902869\n",
      "Pure loss: 59.98058175129028.....Total loss: 59.98058175129028\n",
      "epoch 563 learning rate:  0.01177619893428064   Training loss:   64.98579955902869  Valing loss:   59.98058175129028\n",
      "Pure loss: 64.68556589514284.....Total loss: 64.68556589514284\n",
      "Pure loss: 60.14502103879085.....Total loss: 60.14502103879085\n",
      "epoch 564 learning rate:  0.011773049645390072   Training loss:   64.68556589514284  Valing loss:   60.14502103879085\n",
      "Pure loss: 65.36230722592471.....Total loss: 65.36230722592471\n",
      "Pure loss: 59.73302198862353.....Total loss: 59.73302198862353\n",
      "epoch 565 learning rate:  0.011769911504424779   Training loss:   65.36230722592471  Valing loss:   59.73302198862353\n",
      "Pure loss: 64.9546724460488.....Total loss: 64.9546724460488\n",
      "Pure loss: 59.58995731146537.....Total loss: 59.58995731146537\n",
      "epoch 566 learning rate:  0.01176678445229682   Training loss:   64.9546724460488  Valing loss:   59.58995731146537\n",
      "Pure loss: 64.8674889370911.....Total loss: 64.8674889370911\n",
      "Pure loss: 59.66182480147806.....Total loss: 59.66182480147806\n",
      "epoch 567 learning rate:  0.011763668430335097   Training loss:   64.8674889370911  Valing loss:   59.66182480147806\n",
      "Pure loss: 64.12136099436903.....Total loss: 64.12136099436903\n",
      "Pure loss: 62.065195804365715.....Total loss: 62.065195804365715\n",
      "epoch 568 learning rate:  0.01176056338028169   Training loss:   64.12136099436903  Valing loss:   62.065195804365715\n",
      "Pure loss: 64.45958710090684.....Total loss: 64.45958710090684\n",
      "Pure loss: 62.78594636833805.....Total loss: 62.78594636833805\n",
      "epoch 569 learning rate:  0.011757469244288225   Training loss:   64.45958710090684  Valing loss:   62.78594636833805\n",
      "Pure loss: 64.11314254300396.....Total loss: 64.11314254300396\n",
      "Pure loss: 61.885880378995026.....Total loss: 61.885880378995026\n",
      "epoch 570 learning rate:  0.011754385964912281   Training loss:   64.11314254300396  Valing loss:   61.885880378995026\n",
      "Pure loss: 65.05707201245765.....Total loss: 65.05707201245765\n",
      "Pure loss: 64.2990010885998.....Total loss: 64.2990010885998\n",
      "epoch 571 learning rate:  0.011751313485113836   Training loss:   65.05707201245765  Valing loss:   64.2990010885998\n",
      "Pure loss: 64.76035654050963.....Total loss: 64.76035654050963\n",
      "Pure loss: 63.68142061003296.....Total loss: 63.68142061003296\n",
      "epoch 572 learning rate:  0.011748251748251748   Training loss:   64.76035654050963  Valing loss:   63.68142061003296\n",
      "Pure loss: 65.32646803433289.....Total loss: 65.32646803433289\n",
      "Pure loss: 64.7887625307602.....Total loss: 64.7887625307602\n",
      "epoch 573 learning rate:  0.01174520069808028   Training loss:   65.32646803433289  Valing loss:   64.7887625307602\n",
      "Pure loss: 68.22595656661012.....Total loss: 68.22595656661012\n",
      "Pure loss: 69.40531616752622.....Total loss: 69.40531616752622\n",
      "epoch 574 learning rate:  0.011742160278745644   Training loss:   68.22595656661012  Valing loss:   69.40531616752622\n",
      "Pure loss: 65.57091840000429.....Total loss: 65.57091840000429\n",
      "Pure loss: 65.14949921377593.....Total loss: 65.14949921377593\n",
      "epoch 575 learning rate:  0.01173913043478261   Training loss:   65.57091840000429  Valing loss:   65.14949921377593\n",
      "Pure loss: 66.18722185929234.....Total loss: 66.18722185929234\n",
      "Pure loss: 66.06418974551688.....Total loss: 66.06418974551688\n",
      "epoch 576 learning rate:  0.01173611111111111   Training loss:   66.18722185929234  Valing loss:   66.06418974551688\n",
      "Pure loss: 67.75593210367846.....Total loss: 67.75593210367846\n",
      "Pure loss: 68.56731476941607.....Total loss: 68.56731476941607\n",
      "epoch 577 learning rate:  0.011733102253032928   Training loss:   67.75593210367846  Valing loss:   68.56731476941607\n",
      "Pure loss: 66.09250496494916.....Total loss: 66.09250496494916\n",
      "Pure loss: 65.96666154959367.....Total loss: 65.96666154959367\n",
      "epoch 578 learning rate:  0.011730103806228374   Training loss:   66.09250496494916  Valing loss:   65.96666154959367\n",
      "Pure loss: 65.52058492401845.....Total loss: 65.52058492401845\n",
      "Pure loss: 65.1306082469389.....Total loss: 65.1306082469389\n",
      "epoch 579 learning rate:  0.011727115716753023   Training loss:   65.52058492401845  Valing loss:   65.1306082469389\n",
      "Pure loss: 65.04116551873965.....Total loss: 65.04116551873965\n",
      "Pure loss: 64.30449377690307.....Total loss: 64.30449377690307\n",
      "epoch 580 learning rate:  0.011724137931034483   Training loss:   65.04116551873965  Valing loss:   64.30449377690307\n",
      "Pure loss: 63.47142647202216.....Total loss: 63.47142647202216\n",
      "Pure loss: 61.14007084816753.....Total loss: 61.14007084816753\n",
      "epoch 581 learning rate:  0.011721170395869191   Training loss:   63.47142647202216  Valing loss:   61.14007084816753\n",
      "Pure loss: 63.270433358105294.....Total loss: 63.270433358105294\n",
      "Pure loss: 60.925598174491135.....Total loss: 60.925598174491135\n",
      "epoch 582 learning rate:  0.011718213058419243   Training loss:   63.270433358105294  Valing loss:   60.925598174491135\n",
      "Pure loss: 64.50549313086465.....Total loss: 64.50549313086465\n",
      "Pure loss: 63.77190920035911.....Total loss: 63.77190920035911\n",
      "epoch 583 learning rate:  0.011715265866209263   Training loss:   64.50549313086465  Valing loss:   63.77190920035911\n",
      "Pure loss: 64.20893348957433.....Total loss: 64.20893348957433\n",
      "Pure loss: 63.25717934999495.....Total loss: 63.25717934999495\n",
      "epoch 584 learning rate:  0.011712328767123288   Training loss:   64.20893348957433  Valing loss:   63.25717934999495\n",
      "Pure loss: 63.37731813381052.....Total loss: 63.37731813381052\n",
      "Pure loss: 62.01557769780275.....Total loss: 62.01557769780275\n",
      "epoch 585 learning rate:  0.011709401709401709   Training loss:   63.37731813381052  Valing loss:   62.01557769780275\n",
      "Pure loss: 64.5384616509147.....Total loss: 64.5384616509147\n",
      "Pure loss: 64.46803743516487.....Total loss: 64.46803743516487\n",
      "epoch 586 learning rate:  0.011706484641638225   Training loss:   64.5384616509147  Valing loss:   64.46803743516487\n",
      "Pure loss: 65.25153728517799.....Total loss: 65.25153728517799\n",
      "Pure loss: 65.70881696132732.....Total loss: 65.70881696132732\n",
      "epoch 587 learning rate:  0.011703577512776831   Training loss:   65.25153728517799  Valing loss:   65.70881696132732\n",
      "Pure loss: 66.2977926041923.....Total loss: 66.2977926041923\n",
      "Pure loss: 67.53611288267196.....Total loss: 67.53611288267196\n",
      "epoch 588 learning rate:  0.011700680272108844   Training loss:   66.2977926041923  Valing loss:   67.53611288267196\n",
      "Pure loss: 65.57313917833812.....Total loss: 65.57313917833812\n",
      "Pure loss: 66.27367231263126.....Total loss: 66.27367231263126\n",
      "epoch 589 learning rate:  0.01169779286926995   Training loss:   65.57313917833812  Valing loss:   66.27367231263126\n",
      "Pure loss: 65.14751551499337.....Total loss: 65.14751551499337\n",
      "Pure loss: 65.51758784303026.....Total loss: 65.51758784303026\n",
      "epoch 590 learning rate:  0.011694915254237288   Training loss:   65.14751551499337  Valing loss:   65.51758784303026\n",
      "Pure loss: 64.04579976286553.....Total loss: 64.04579976286553\n",
      "Pure loss: 63.54760984250541.....Total loss: 63.54760984250541\n",
      "epoch 591 learning rate:  0.011692047377326565   Training loss:   64.04579976286553  Valing loss:   63.54760984250541\n",
      "Pure loss: 64.07134301985906.....Total loss: 64.07134301985906\n",
      "Pure loss: 63.56708608873141.....Total loss: 63.56708608873141\n",
      "epoch 592 learning rate:  0.01168918918918919   Training loss:   64.07134301985906  Valing loss:   63.56708608873141\n",
      "Pure loss: 64.06337025264774.....Total loss: 64.06337025264774\n",
      "Pure loss: 63.567111524547066.....Total loss: 63.567111524547066\n",
      "epoch 593 learning rate:  0.011686340640809444   Training loss:   64.06337025264774  Valing loss:   63.567111524547066\n",
      "Pure loss: 64.21695065951309.....Total loss: 64.21695065951309\n",
      "Pure loss: 64.04623938780577.....Total loss: 64.04623938780577\n",
      "epoch 594 learning rate:  0.011683501683501683   Training loss:   64.21695065951309  Valing loss:   64.04623938780577\n",
      "Pure loss: 66.5655690336262.....Total loss: 66.5655690336262\n",
      "Pure loss: 68.34227445910422.....Total loss: 68.34227445910422\n",
      "epoch 595 learning rate:  0.011680672268907562   Training loss:   66.5655690336262  Valing loss:   68.34227445910422\n",
      "Pure loss: 65.03376043496299.....Total loss: 65.03376043496299\n",
      "Pure loss: 66.08286693964037.....Total loss: 66.08286693964037\n",
      "epoch 596 learning rate:  0.011677852348993288   Training loss:   65.03376043496299  Valing loss:   66.08286693964037\n",
      "Pure loss: 64.3425392258291.....Total loss: 64.3425392258291\n",
      "Pure loss: 64.88247776180529.....Total loss: 64.88247776180529\n",
      "epoch 597 learning rate:  0.011675041876046902   Training loss:   64.3425392258291  Valing loss:   64.88247776180529\n",
      "Pure loss: 64.71847789868129.....Total loss: 64.71847789868129\n",
      "Pure loss: 65.54243302784072.....Total loss: 65.54243302784072\n",
      "epoch 598 learning rate:  0.011672240802675586   Training loss:   64.71847789868129  Valing loss:   65.54243302784072\n",
      "Pure loss: 65.34555981013251.....Total loss: 65.34555981013251\n",
      "Pure loss: 66.65917046373755.....Total loss: 66.65917046373755\n",
      "epoch 599 learning rate:  0.011669449081803005   Training loss:   65.34555981013251  Valing loss:   66.65917046373755\n",
      "Pure loss: 65.30444792992391.....Total loss: 65.30444792992391\n",
      "Pure loss: 66.59446914753802.....Total loss: 66.59446914753802\n",
      "epoch 600 learning rate:  0.011666666666666667   Training loss:   65.30444792992391  Valing loss:   66.59446914753802\n",
      "Pure loss: 64.13574327573316.....Total loss: 64.13574327573316\n",
      "Pure loss: 64.70013352410656.....Total loss: 64.70013352410656\n",
      "epoch 601 learning rate:  0.011663893510815308   Training loss:   64.13574327573316  Valing loss:   64.70013352410656\n",
      "Pure loss: 63.662798025779914.....Total loss: 63.662798025779914\n",
      "Pure loss: 63.77400949953454.....Total loss: 63.77400949953454\n",
      "epoch 602 learning rate:  0.011661129568106313   Training loss:   63.662798025779914  Valing loss:   63.77400949953454\n",
      "Pure loss: 63.80988999211861.....Total loss: 63.80988999211861\n",
      "Pure loss: 64.0181053648111.....Total loss: 64.0181053648111\n",
      "epoch 603 learning rate:  0.01165837479270315   Training loss:   63.80988999211861  Valing loss:   64.0181053648111\n",
      "Pure loss: 71.5263764303871.....Total loss: 71.5263764303871\n",
      "Pure loss: 76.14804347936055.....Total loss: 76.14804347936055\n",
      "epoch 604 learning rate:  0.011655629139072848   Training loss:   71.5263764303871  Valing loss:   76.14804347936055\n",
      "Pure loss: 68.19191523538427.....Total loss: 68.19191523538427\n",
      "Pure loss: 71.58783171146852.....Total loss: 71.58783171146852\n",
      "epoch 605 learning rate:  0.011652892561983472   Training loss:   68.19191523538427  Valing loss:   71.58783171146852\n",
      "Pure loss: 67.04935899667858.....Total loss: 67.04935899667858\n",
      "Pure loss: 69.94199332439443.....Total loss: 69.94199332439443\n",
      "epoch 606 learning rate:  0.011650165016501651   Training loss:   67.04935899667858  Valing loss:   69.94199332439443\n",
      "Pure loss: 67.09185993283953.....Total loss: 67.09185993283953\n",
      "Pure loss: 70.00956753536457.....Total loss: 70.00956753536457\n",
      "epoch 607 learning rate:  0.011647446457990115   Training loss:   67.09185993283953  Valing loss:   70.00956753536457\n",
      "Pure loss: 64.94213761080064.....Total loss: 64.94213761080064\n",
      "Pure loss: 66.90272835046181.....Total loss: 66.90272835046181\n",
      "epoch 608 learning rate:  0.011644736842105263   Training loss:   64.94213761080064  Valing loss:   66.90272835046181\n",
      "Pure loss: 63.848113450144716.....Total loss: 63.848113450144716\n",
      "Pure loss: 65.04813795356486.....Total loss: 65.04813795356486\n",
      "epoch 609 learning rate:  0.011642036124794745   Training loss:   63.848113450144716  Valing loss:   65.04813795356486\n",
      "Pure loss: 63.79423527435986.....Total loss: 63.79423527435986\n",
      "Pure loss: 65.11103827918892.....Total loss: 65.11103827918892\n",
      "epoch 610 learning rate:  0.011639344262295083   Training loss:   63.79423527435986  Valing loss:   65.11103827918892\n",
      "Pure loss: 65.24571777025463.....Total loss: 65.24571777025463\n",
      "Pure loss: 67.6317165528385.....Total loss: 67.6317165528385\n",
      "epoch 611 learning rate:  0.011636661211129296   Training loss:   65.24571777025463  Valing loss:   67.6317165528385\n",
      "Pure loss: 62.4228699132889.....Total loss: 62.4228699132889\n",
      "Pure loss: 62.737927952547814.....Total loss: 62.737927952547814\n",
      "epoch 612 learning rate:  0.011633986928104575   Training loss:   62.4228699132889  Valing loss:   62.737927952547814\n",
      "Pure loss: 62.45296039674022.....Total loss: 62.45296039674022\n",
      "Pure loss: 62.80009061834782.....Total loss: 62.80009061834782\n",
      "epoch 613 learning rate:  0.011631321370309951   Training loss:   62.45296039674022  Valing loss:   62.80009061834782\n",
      "Pure loss: 64.97843776063365.....Total loss: 64.97843776063365\n",
      "Pure loss: 67.25147256099507.....Total loss: 67.25147256099507\n",
      "epoch 614 learning rate:  0.011628664495114007   Training loss:   64.97843776063365  Valing loss:   67.25147256099507\n",
      "Pure loss: 64.50445610141068.....Total loss: 64.50445610141068\n",
      "Pure loss: 66.51421103513425.....Total loss: 66.51421103513425\n",
      "epoch 615 learning rate:  0.011626016260162603   Training loss:   64.50445610141068  Valing loss:   66.51421103513425\n",
      "Pure loss: 63.75218714622939.....Total loss: 63.75218714622939\n",
      "Pure loss: 65.27827786456392.....Total loss: 65.27827786456392\n",
      "epoch 616 learning rate:  0.011623376623376624   Training loss:   63.75218714622939  Valing loss:   65.27827786456392\n",
      "Pure loss: 62.81070102372692.....Total loss: 62.81070102372692\n",
      "Pure loss: 63.731878679815.....Total loss: 63.731878679815\n",
      "epoch 617 learning rate:  0.011620745542949756   Training loss:   62.81070102372692  Valing loss:   63.731878679815\n",
      "Pure loss: 61.65098869629237.....Total loss: 61.65098869629237\n",
      "Pure loss: 61.64337576043765.....Total loss: 61.64337576043765\n",
      "epoch 618 learning rate:  0.011618122977346278   Training loss:   61.65098869629237  Valing loss:   61.64337576043765\n",
      "Pure loss: 63.588228383489565.....Total loss: 63.588228383489565\n",
      "Pure loss: 65.11523787015774.....Total loss: 65.11523787015774\n",
      "epoch 619 learning rate:  0.011615508885298869   Training loss:   63.588228383489565  Valing loss:   65.11523787015774\n",
      "Pure loss: 61.6424869012766.....Total loss: 61.6424869012766\n",
      "Pure loss: 61.78952738544394.....Total loss: 61.78952738544394\n",
      "epoch 620 learning rate:  0.011612903225806452   Training loss:   61.6424869012766  Valing loss:   61.78952738544394\n",
      "Pure loss: 60.29664860921912.....Total loss: 60.29664860921912\n",
      "Pure loss: 58.96418951092898.....Total loss: 58.96418951092898\n",
      "epoch 621 learning rate:  0.011610305958132046   Training loss:   60.29664860921912  Valing loss:   58.96418951092898\n",
      "Pure loss: 59.827543028291885.....Total loss: 59.827543028291885\n",
      "Pure loss: 57.7420135943483.....Total loss: 57.7420135943483\n",
      "epoch 622 learning rate:  0.011607717041800643   Training loss:   59.827543028291885  Valing loss:   57.7420135943483\n",
      "Pure loss: 59.77365945745528.....Total loss: 59.77365945745528\n",
      "Pure loss: 57.420582090776215.....Total loss: 57.420582090776215\n",
      "epoch 623 learning rate:  0.011605136436597111   Training loss:   59.77365945745528  Valing loss:   57.420582090776215\n",
      "Pure loss: 59.47950019667911.....Total loss: 59.47950019667911\n",
      "Pure loss: 55.59392179588554.....Total loss: 55.59392179588554\n",
      "epoch 624 learning rate:  0.011602564102564102   Training loss:   59.47950019667911  Valing loss:   55.59392179588554\n",
      "Pure loss: 59.62863389088066.....Total loss: 59.62863389088066\n",
      "Pure loss: 58.51438766111562.....Total loss: 58.51438766111562\n",
      "epoch 625 learning rate:  0.011600000000000001   Training loss:   59.62863389088066  Valing loss:   58.51438766111562\n",
      "Pure loss: 59.474814229792685.....Total loss: 59.474814229792685\n",
      "Pure loss: 58.09886572965578.....Total loss: 58.09886572965578\n",
      "epoch 626 learning rate:  0.01159744408945687   Training loss:   59.474814229792685  Valing loss:   58.09886572965578\n",
      "Pure loss: 59.50360716986866.....Total loss: 59.50360716986866\n",
      "Pure loss: 58.09882563707837.....Total loss: 58.09882563707837\n",
      "epoch 627 learning rate:  0.011594896331738437   Training loss:   59.50360716986866  Valing loss:   58.09882563707837\n",
      "Pure loss: 59.4448240754498.....Total loss: 59.4448240754498\n",
      "Pure loss: 58.0723760300839.....Total loss: 58.0723760300839\n",
      "epoch 628 learning rate:  0.011592356687898089   Training loss:   59.4448240754498  Valing loss:   58.0723760300839\n",
      "Pure loss: 58.178316707705584.....Total loss: 58.178316707705584\n",
      "Pure loss: 54.55371929048619.....Total loss: 54.55371929048619\n",
      "epoch 629 learning rate:  0.011589825119236884   Training loss:   58.178316707705584  Valing loss:   54.55371929048619\n",
      "Pure loss: 58.1084693123192.....Total loss: 58.1084693123192\n",
      "Pure loss: 53.321319866288896.....Total loss: 53.321319866288896\n",
      "epoch 630 learning rate:  0.011587301587301587   Training loss:   58.1084693123192  Valing loss:   53.321319866288896\n",
      "Pure loss: 58.91552843292231.....Total loss: 58.91552843292231\n",
      "Pure loss: 54.168845508736894.....Total loss: 54.168845508736894\n",
      "epoch 631 learning rate:  0.011584786053882726   Training loss:   58.91552843292231  Valing loss:   54.168845508736894\n",
      "Pure loss: 58.321518347111564.....Total loss: 58.321518347111564\n",
      "Pure loss: 54.134332103608195.....Total loss: 54.134332103608195\n",
      "epoch 632 learning rate:  0.011582278481012658   Training loss:   58.321518347111564  Valing loss:   54.134332103608195\n",
      "Pure loss: 59.07663957061762.....Total loss: 59.07663957061762\n",
      "Pure loss: 54.85409753668988.....Total loss: 54.85409753668988\n",
      "epoch 633 learning rate:  0.011579778830963665   Training loss:   59.07663957061762  Valing loss:   54.85409753668988\n",
      "Pure loss: 58.856646888284764.....Total loss: 58.856646888284764\n",
      "Pure loss: 57.0138759634273.....Total loss: 57.0138759634273\n",
      "epoch 634 learning rate:  0.011577287066246058   Training loss:   58.856646888284764  Valing loss:   57.0138759634273\n",
      "Pure loss: 58.69406154364138.....Total loss: 58.69406154364138\n",
      "Pure loss: 56.175597128830375.....Total loss: 56.175597128830375\n",
      "epoch 635 learning rate:  0.0115748031496063   Training loss:   58.69406154364138  Valing loss:   56.175597128830375\n",
      "Pure loss: 58.70770985998968.....Total loss: 58.70770985998968\n",
      "Pure loss: 56.345763070677286.....Total loss: 56.345763070677286\n",
      "epoch 636 learning rate:  0.011572327044025157   Training loss:   58.70770985998968  Valing loss:   56.345763070677286\n",
      "Pure loss: 58.67732622897942.....Total loss: 58.67732622897942\n",
      "Pure loss: 56.63958997079683.....Total loss: 56.63958997079683\n",
      "epoch 637 learning rate:  0.011569858712715856   Training loss:   58.67732622897942  Valing loss:   56.63958997079683\n",
      "Pure loss: 58.49669959104747.....Total loss: 58.49669959104747\n",
      "Pure loss: 56.676866334920405.....Total loss: 56.676866334920405\n",
      "epoch 638 learning rate:  0.011567398119122257   Training loss:   58.49669959104747  Valing loss:   56.676866334920405\n",
      "Pure loss: 58.156038970451455.....Total loss: 58.156038970451455\n",
      "Pure loss: 56.68130603091306.....Total loss: 56.68130603091306\n",
      "epoch 639 learning rate:  0.011564945226917058   Training loss:   58.156038970451455  Valing loss:   56.68130603091306\n",
      "Pure loss: 58.2615662119795.....Total loss: 58.2615662119795\n",
      "Pure loss: 56.9919265211872.....Total loss: 56.9919265211872\n",
      "epoch 640 learning rate:  0.0115625   Training loss:   58.2615662119795  Valing loss:   56.9919265211872\n",
      "Pure loss: 57.968710601904085.....Total loss: 57.968710601904085\n",
      "Pure loss: 56.19642311535116.....Total loss: 56.19642311535116\n",
      "epoch 641 learning rate:  0.0115600624024961   Training loss:   57.968710601904085  Valing loss:   56.19642311535116\n",
      "Pure loss: 57.88585708170029.....Total loss: 57.88585708170029\n",
      "Pure loss: 55.443399347943455.....Total loss: 55.443399347943455\n",
      "epoch 642 learning rate:  0.011557632398753894   Training loss:   57.88585708170029  Valing loss:   55.443399347943455\n",
      "Pure loss: 57.8737127875209.....Total loss: 57.8737127875209\n",
      "Pure loss: 55.7897985356945.....Total loss: 55.7897985356945\n",
      "epoch 643 learning rate:  0.011555209953343701   Training loss:   57.8737127875209  Valing loss:   55.7897985356945\n",
      "Pure loss: 57.571402784402636.....Total loss: 57.571402784402636\n",
      "Pure loss: 54.41561912730034.....Total loss: 54.41561912730034\n",
      "epoch 644 learning rate:  0.0115527950310559   Training loss:   57.571402784402636  Valing loss:   54.41561912730034\n",
      "Pure loss: 57.58374442526089.....Total loss: 57.58374442526089\n",
      "Pure loss: 54.38965798138211.....Total loss: 54.38965798138211\n",
      "epoch 645 learning rate:  0.011550387596899225   Training loss:   57.58374442526089  Valing loss:   54.38965798138211\n",
      "Pure loss: 57.60021316367084.....Total loss: 57.60021316367084\n",
      "Pure loss: 54.191292908826284.....Total loss: 54.191292908826284\n",
      "epoch 646 learning rate:  0.011547987616099072   Training loss:   57.60021316367084  Valing loss:   54.191292908826284\n",
      "Pure loss: 57.472835187799824.....Total loss: 57.472835187799824\n",
      "Pure loss: 54.20709775211034.....Total loss: 54.20709775211034\n",
      "epoch 647 learning rate:  0.011545595054095827   Training loss:   57.472835187799824  Valing loss:   54.20709775211034\n",
      "Pure loss: 57.44857403144168.....Total loss: 57.44857403144168\n",
      "Pure loss: 54.30147685055399.....Total loss: 54.30147685055399\n",
      "epoch 648 learning rate:  0.01154320987654321   Training loss:   57.44857403144168  Valing loss:   54.30147685055399\n",
      "Pure loss: 57.599305313206884.....Total loss: 57.599305313206884\n",
      "Pure loss: 55.28044176028759.....Total loss: 55.28044176028759\n",
      "epoch 649 learning rate:  0.011540832049306626   Training loss:   57.599305313206884  Valing loss:   55.28044176028759\n",
      "Pure loss: 57.50457433132131.....Total loss: 57.50457433132131\n",
      "Pure loss: 55.40529087273491.....Total loss: 55.40529087273491\n",
      "epoch 650 learning rate:  0.011538461538461539   Training loss:   57.50457433132131  Valing loss:   55.40529087273491\n",
      "Pure loss: 57.49504671581631.....Total loss: 57.49504671581631\n",
      "Pure loss: 55.37595612595405.....Total loss: 55.37595612595405\n",
      "epoch 651 learning rate:  0.01153609831029186   Training loss:   57.49504671581631  Valing loss:   55.37595612595405\n",
      "Pure loss: 56.80721356401622.....Total loss: 56.80721356401622\n",
      "Pure loss: 53.30043365316652.....Total loss: 53.30043365316652\n",
      "epoch 652 learning rate:  0.011533742331288344   Training loss:   56.80721356401622  Valing loss:   53.30043365316652\n",
      "Pure loss: 56.79764430308571.....Total loss: 56.79764430308571\n",
      "Pure loss: 53.0688748996738.....Total loss: 53.0688748996738\n",
      "epoch 653 learning rate:  0.011531393568147015   Training loss:   56.79764430308571  Valing loss:   53.0688748996738\n",
      "Pure loss: 56.79665103918515.....Total loss: 56.79665103918515\n",
      "Pure loss: 53.175743678173035.....Total loss: 53.175743678173035\n",
      "epoch 654 learning rate:  0.011529051987767585   Training loss:   56.79665103918515  Valing loss:   53.175743678173035\n",
      "Pure loss: 56.61035392898928.....Total loss: 56.61035392898928\n",
      "Pure loss: 53.331612554944535.....Total loss: 53.331612554944535\n",
      "epoch 655 learning rate:  0.011526717557251908   Training loss:   56.61035392898928  Valing loss:   53.331612554944535\n",
      "Pure loss: 56.656209418359815.....Total loss: 56.656209418359815\n",
      "Pure loss: 53.2322760571288.....Total loss: 53.2322760571288\n",
      "epoch 656 learning rate:  0.011524390243902439   Training loss:   56.656209418359815  Valing loss:   53.2322760571288\n",
      "Pure loss: 56.8093164302363.....Total loss: 56.8093164302363\n",
      "Pure loss: 54.136227582688306.....Total loss: 54.136227582688306\n",
      "epoch 657 learning rate:  0.0115220700152207   Training loss:   56.8093164302363  Valing loss:   54.136227582688306\n",
      "Pure loss: 57.4239365010189.....Total loss: 57.4239365010189\n",
      "Pure loss: 55.969551487921116.....Total loss: 55.969551487921116\n",
      "epoch 658 learning rate:  0.011519756838905775   Training loss:   57.4239365010189  Valing loss:   55.969551487921116\n",
      "Pure loss: 58.026766096112304.....Total loss: 58.026766096112304\n",
      "Pure loss: 57.18355030264313.....Total loss: 57.18355030264313\n",
      "epoch 659 learning rate:  0.011517450682852808   Training loss:   58.026766096112304  Valing loss:   57.18355030264313\n",
      "Pure loss: 58.37016918052406.....Total loss: 58.37016918052406\n",
      "Pure loss: 57.79190017655517.....Total loss: 57.79190017655517\n",
      "epoch 660 learning rate:  0.011515151515151515   Training loss:   58.37016918052406  Valing loss:   57.79190017655517\n",
      "Pure loss: 59.271357288650904.....Total loss: 59.271357288650904\n",
      "Pure loss: 59.45826470222125.....Total loss: 59.45826470222125\n",
      "epoch 661 learning rate:  0.01151285930408472   Training loss:   59.271357288650904  Valing loss:   59.45826470222125\n",
      "Pure loss: 59.052081895090055.....Total loss: 59.052081895090055\n",
      "Pure loss: 59.201984577192185.....Total loss: 59.201984577192185\n",
      "epoch 662 learning rate:  0.011510574018126888   Training loss:   59.052081895090055  Valing loss:   59.201984577192185\n",
      "Pure loss: 58.737239007244675.....Total loss: 58.737239007244675\n",
      "Pure loss: 58.69365954616351.....Total loss: 58.69365954616351\n",
      "epoch 663 learning rate:  0.011508295625942685   Training loss:   58.737239007244675  Valing loss:   58.69365954616351\n",
      "Pure loss: 59.51144822765233.....Total loss: 59.51144822765233\n",
      "Pure loss: 59.92441260079272.....Total loss: 59.92441260079272\n",
      "epoch 664 learning rate:  0.011506024096385542   Training loss:   59.51144822765233  Valing loss:   59.92441260079272\n",
      "Pure loss: 59.10537389549919.....Total loss: 59.10537389549919\n",
      "Pure loss: 59.38323393075424.....Total loss: 59.38323393075424\n",
      "epoch 665 learning rate:  0.011503759398496242   Training loss:   59.10537389549919  Valing loss:   59.38323393075424\n",
      "Pure loss: 59.34033810941033.....Total loss: 59.34033810941033\n",
      "Pure loss: 59.72899217415359.....Total loss: 59.72899217415359\n",
      "epoch 666 learning rate:  0.0115015015015015   Training loss:   59.34033810941033  Valing loss:   59.72899217415359\n",
      "Pure loss: 58.97708729005505.....Total loss: 58.97708729005505\n",
      "Pure loss: 59.236918274558384.....Total loss: 59.236918274558384\n",
      "epoch 667 learning rate:  0.011499250374812593   Training loss:   58.97708729005505  Valing loss:   59.236918274558384\n",
      "Pure loss: 59.30486210859479.....Total loss: 59.30486210859479\n",
      "Pure loss: 59.723510685421495.....Total loss: 59.723510685421495\n",
      "epoch 668 learning rate:  0.011497005988023952   Training loss:   59.30486210859479  Valing loss:   59.723510685421495\n",
      "Pure loss: 58.432721207552646.....Total loss: 58.432721207552646\n",
      "Pure loss: 58.24952340772377.....Total loss: 58.24952340772377\n",
      "epoch 669 learning rate:  0.01149476831091181   Training loss:   58.432721207552646  Valing loss:   58.24952340772377\n",
      "Pure loss: 56.95326530480612.....Total loss: 56.95326530480612\n",
      "Pure loss: 55.32423656273599.....Total loss: 55.32423656273599\n",
      "epoch 670 learning rate:  0.011492537313432836   Training loss:   56.95326530480612  Valing loss:   55.32423656273599\n",
      "Pure loss: 57.33614603698888.....Total loss: 57.33614603698888\n",
      "Pure loss: 56.18196581752634.....Total loss: 56.18196581752634\n",
      "epoch 671 learning rate:  0.011490312965722803   Training loss:   57.33614603698888  Valing loss:   56.18196581752634\n",
      "Pure loss: 56.909497894229396.....Total loss: 56.909497894229396\n",
      "Pure loss: 55.681273625349746.....Total loss: 55.681273625349746\n",
      "epoch 672 learning rate:  0.011488095238095238   Training loss:   56.909497894229396  Valing loss:   55.681273625349746\n",
      "Pure loss: 56.69116060051089.....Total loss: 56.69116060051089\n",
      "Pure loss: 55.148894842589236.....Total loss: 55.148894842589236\n",
      "epoch 673 learning rate:  0.011485884101040119   Training loss:   56.69116060051089  Valing loss:   55.148894842589236\n",
      "Pure loss: 56.75684059465989.....Total loss: 56.75684059465989\n",
      "Pure loss: 55.3103062236742.....Total loss: 55.3103062236742\n",
      "epoch 674 learning rate:  0.011483679525222553   Training loss:   56.75684059465989  Valing loss:   55.3103062236742\n",
      "Pure loss: 56.05904901767445.....Total loss: 56.05904901767445\n",
      "Pure loss: 53.27834063216979.....Total loss: 53.27834063216979\n",
      "epoch 675 learning rate:  0.011481481481481481   Training loss:   56.05904901767445  Valing loss:   53.27834063216979\n",
      "Pure loss: 56.01252011605569.....Total loss: 56.01252011605569\n",
      "Pure loss: 51.760019530370855.....Total loss: 51.760019530370855\n",
      "epoch 676 learning rate:  0.011479289940828403   Training loss:   56.01252011605569  Valing loss:   51.760019530370855\n",
      "Pure loss: 56.031181870025826.....Total loss: 56.031181870025826\n",
      "Pure loss: 51.975555377285204.....Total loss: 51.975555377285204\n",
      "epoch 677 learning rate:  0.011477104874446087   Training loss:   56.031181870025826  Valing loss:   51.975555377285204\n",
      "Pure loss: 56.630004747918356.....Total loss: 56.630004747918356\n",
      "Pure loss: 50.587271283461426.....Total loss: 50.587271283461426\n",
      "epoch 678 learning rate:  0.011474926253687316   Training loss:   56.630004747918356  Valing loss:   50.587271283461426\n",
      "Pure loss: 56.157567019795955.....Total loss: 56.157567019795955\n",
      "Pure loss: 50.257622253893715.....Total loss: 50.257622253893715\n",
      "epoch 679 learning rate:  0.011472754050073638   Training loss:   56.157567019795955  Valing loss:   50.257622253893715\n",
      "Pure loss: 56.440043143377736.....Total loss: 56.440043143377736\n",
      "Pure loss: 50.160183748956115.....Total loss: 50.160183748956115\n",
      "epoch 680 learning rate:  0.011470588235294118   Training loss:   56.440043143377736  Valing loss:   50.160183748956115\n",
      "Pure loss: 56.327971044662405.....Total loss: 56.327971044662405\n",
      "Pure loss: 50.196235230868076.....Total loss: 50.196235230868076\n",
      "epoch 681 learning rate:  0.011468428781204111   Training loss:   56.327971044662405  Valing loss:   50.196235230868076\n",
      "Pure loss: 57.00795745333631.....Total loss: 57.00795745333631\n",
      "Pure loss: 50.006583615438686.....Total loss: 50.006583615438686\n",
      "epoch 682 learning rate:  0.011466275659824048   Training loss:   57.00795745333631  Valing loss:   50.006583615438686\n",
      "Pure loss: 57.14034138027466.....Total loss: 57.14034138027466\n",
      "Pure loss: 50.04575692513272.....Total loss: 50.04575692513272\n",
      "epoch 683 learning rate:  0.011464128843338214   Training loss:   57.14034138027466  Valing loss:   50.04575692513272\n",
      "Pure loss: 56.50523475396994.....Total loss: 56.50523475396994\n",
      "Pure loss: 50.01790633930576.....Total loss: 50.01790633930576\n",
      "epoch 684 learning rate:  0.011461988304093567   Training loss:   56.50523475396994  Valing loss:   50.01790633930576\n",
      "Pure loss: 56.06715700567759.....Total loss: 56.06715700567759\n",
      "Pure loss: 50.368785204653136.....Total loss: 50.368785204653136\n",
      "epoch 685 learning rate:  0.01145985401459854   Training loss:   56.06715700567759  Valing loss:   50.368785204653136\n",
      "Pure loss: 56.16688477041512.....Total loss: 56.16688477041512\n",
      "Pure loss: 50.38966386952385.....Total loss: 50.38966386952385\n",
      "epoch 686 learning rate:  0.011457725947521867   Training loss:   56.16688477041512  Valing loss:   50.38966386952385\n",
      "Pure loss: 55.96027584889212.....Total loss: 55.96027584889212\n",
      "Pure loss: 50.48071356749379.....Total loss: 50.48071356749379\n",
      "epoch 687 learning rate:  0.011455604075691413   Training loss:   55.96027584889212  Valing loss:   50.48071356749379\n",
      "Pure loss: 55.91507327541643.....Total loss: 55.91507327541643\n",
      "Pure loss: 50.466051340694335.....Total loss: 50.466051340694335\n",
      "epoch 688 learning rate:  0.011453488372093024   Training loss:   55.91507327541643  Valing loss:   50.466051340694335\n",
      "Pure loss: 56.29536494917581.....Total loss: 56.29536494917581\n",
      "Pure loss: 50.19431884776253.....Total loss: 50.19431884776253\n",
      "epoch 689 learning rate:  0.011451378809869376   Training loss:   56.29536494917581  Valing loss:   50.19431884776253\n",
      "Pure loss: 55.78154981520742.....Total loss: 55.78154981520742\n",
      "Pure loss: 49.73703147185548.....Total loss: 49.73703147185548\n",
      "epoch 690 learning rate:  0.01144927536231884   Training loss:   55.78154981520742  Valing loss:   49.73703147185548\n",
      "Pure loss: 56.323309557858536.....Total loss: 56.323309557858536\n",
      "Pure loss: 49.61945674049712.....Total loss: 49.61945674049712\n",
      "epoch 691 learning rate:  0.011447178002894355   Training loss:   56.323309557858536  Valing loss:   49.61945674049712\n",
      "Pure loss: 57.23804175033239.....Total loss: 57.23804175033239\n",
      "Pure loss: 49.74192059229958.....Total loss: 49.74192059229958\n",
      "epoch 692 learning rate:  0.011445086705202312   Training loss:   57.23804175033239  Valing loss:   49.74192059229958\n",
      "Pure loss: 57.048041752437676.....Total loss: 57.048041752437676\n",
      "Pure loss: 49.72200901544049.....Total loss: 49.72200901544049\n",
      "epoch 693 learning rate:  0.011443001443001442   Training loss:   57.048041752437676  Valing loss:   49.72200901544049\n",
      "Pure loss: 56.11932079304772.....Total loss: 56.11932079304772\n",
      "Pure loss: 49.2531502131421.....Total loss: 49.2531502131421\n",
      "epoch 694 learning rate:  0.011440922190201729   Training loss:   56.11932079304772  Valing loss:   49.2531502131421\n",
      "Pure loss: 56.31678692722196.....Total loss: 56.31678692722196\n",
      "Pure loss: 49.26569686213849.....Total loss: 49.26569686213849\n",
      "epoch 695 learning rate:  0.011438848920863309   Training loss:   56.31678692722196  Valing loss:   49.26569686213849\n",
      "Pure loss: 56.14934587999397.....Total loss: 56.14934587999397\n",
      "Pure loss: 49.388498669322786.....Total loss: 49.388498669322786\n",
      "epoch 696 learning rate:  0.011436781609195402   Training loss:   56.14934587999397  Valing loss:   49.388498669322786\n",
      "Pure loss: 55.371279060349345.....Total loss: 55.371279060349345\n",
      "Pure loss: 49.80897403794148.....Total loss: 49.80897403794148\n",
      "epoch 697 learning rate:  0.011434720229555237   Training loss:   55.371279060349345  Valing loss:   49.80897403794148\n",
      "Pure loss: 55.85443633528843.....Total loss: 55.85443633528843\n",
      "Pure loss: 49.57063136719757.....Total loss: 49.57063136719757\n",
      "epoch 698 learning rate:  0.011432664756446992   Training loss:   55.85443633528843  Valing loss:   49.57063136719757\n",
      "Pure loss: 57.18174169376717.....Total loss: 57.18174169376717\n",
      "Pure loss: 49.04448737953373.....Total loss: 49.04448737953373\n",
      "epoch 699 learning rate:  0.011430615164520744   Training loss:   57.18174169376717  Valing loss:   49.04448737953373\n",
      "Pure loss: 57.648343203225274.....Total loss: 57.648343203225274\n",
      "Pure loss: 49.15201158629682.....Total loss: 49.15201158629682\n",
      "epoch 700 learning rate:  0.011428571428571429   Training loss:   57.648343203225274  Valing loss:   49.15201158629682\n",
      "Pure loss: 57.96213048274069.....Total loss: 57.96213048274069\n",
      "Pure loss: 49.30278948943759.....Total loss: 49.30278948943759\n",
      "epoch 701 learning rate:  0.011426533523537803   Training loss:   57.96213048274069  Valing loss:   49.30278948943759\n",
      "Pure loss: 57.18251681390672.....Total loss: 57.18251681390672\n",
      "Pure loss: 48.96440947215399.....Total loss: 48.96440947215399\n",
      "epoch 702 learning rate:  0.011424501424501425   Training loss:   57.18251681390672  Valing loss:   48.96440947215399\n",
      "Pure loss: 57.619322878543535.....Total loss: 57.619322878543535\n",
      "Pure loss: 48.93629772784272.....Total loss: 48.93629772784272\n",
      "epoch 703 learning rate:  0.011422475106685634   Training loss:   57.619322878543535  Valing loss:   48.93629772784272\n",
      "Pure loss: 58.366212030032244.....Total loss: 58.366212030032244\n",
      "Pure loss: 49.34653850075877.....Total loss: 49.34653850075877\n",
      "epoch 704 learning rate:  0.011420454545454546   Training loss:   58.366212030032244  Valing loss:   49.34653850075877\n",
      "Pure loss: 59.79832452975953.....Total loss: 59.79832452975953\n",
      "Pure loss: 49.975109481578336.....Total loss: 49.975109481578336\n",
      "epoch 705 learning rate:  0.011418439716312057   Training loss:   59.79832452975953  Valing loss:   49.975109481578336\n",
      "Pure loss: 56.43231559518119.....Total loss: 56.43231559518119\n",
      "Pure loss: 47.8998388462283.....Total loss: 47.8998388462283\n",
      "epoch 706 learning rate:  0.01141643059490085   Training loss:   56.43231559518119  Valing loss:   47.8998388462283\n",
      "Pure loss: 57.3315643212473.....Total loss: 57.3315643212473\n",
      "Pure loss: 48.01004314069572.....Total loss: 48.01004314069572\n",
      "epoch 707 learning rate:  0.011414427157001415   Training loss:   57.3315643212473  Valing loss:   48.01004314069572\n",
      "Pure loss: 58.559262905200384.....Total loss: 58.559262905200384\n",
      "Pure loss: 48.1418749304165.....Total loss: 48.1418749304165\n",
      "epoch 708 learning rate:  0.011412429378531073   Training loss:   58.559262905200384  Valing loss:   48.1418749304165\n",
      "Pure loss: 57.731299135738396.....Total loss: 57.731299135738396\n",
      "Pure loss: 47.95717126641611.....Total loss: 47.95717126641611\n",
      "epoch 709 learning rate:  0.011410437235543018   Training loss:   57.731299135738396  Valing loss:   47.95717126641611\n",
      "Pure loss: 54.748570863115106.....Total loss: 54.748570863115106\n",
      "Pure loss: 47.90381255069684.....Total loss: 47.90381255069684\n",
      "epoch 710 learning rate:  0.011408450704225352   Training loss:   54.748570863115106  Valing loss:   47.90381255069684\n",
      "Pure loss: 54.63611153715603.....Total loss: 54.63611153715603\n",
      "Pure loss: 48.15967896462621.....Total loss: 48.15967896462621\n",
      "epoch 711 learning rate:  0.01140646976090014   Training loss:   54.63611153715603  Valing loss:   48.15967896462621\n",
      "Pure loss: 54.6780119524902.....Total loss: 54.6780119524902\n",
      "Pure loss: 48.09631479063199.....Total loss: 48.09631479063199\n",
      "epoch 712 learning rate:  0.011404494382022472   Training loss:   54.6780119524902  Valing loss:   48.09631479063199\n",
      "Pure loss: 54.64407646860581.....Total loss: 54.64407646860581\n",
      "Pure loss: 48.157791838568166.....Total loss: 48.157791838568166\n",
      "epoch 713 learning rate:  0.011402524544179523   Training loss:   54.64407646860581  Valing loss:   48.157791838568166\n",
      "Pure loss: 54.77942975193304.....Total loss: 54.77942975193304\n",
      "Pure loss: 47.87072369145328.....Total loss: 47.87072369145328\n",
      "epoch 714 learning rate:  0.011400560224089636   Training loss:   54.77942975193304  Valing loss:   47.87072369145328\n",
      "Pure loss: 54.73257195967424.....Total loss: 54.73257195967424\n",
      "Pure loss: 48.01405553890159.....Total loss: 48.01405553890159\n",
      "epoch 715 learning rate:  0.0113986013986014   Training loss:   54.73257195967424  Valing loss:   48.01405553890159\n",
      "Pure loss: 54.90527026081641.....Total loss: 54.90527026081641\n",
      "Pure loss: 47.84735114409343.....Total loss: 47.84735114409343\n",
      "epoch 716 learning rate:  0.011396648044692738   Training loss:   54.90527026081641  Valing loss:   47.84735114409343\n",
      "Pure loss: 54.3414988000043.....Total loss: 54.3414988000043\n",
      "Pure loss: 48.95398764590737.....Total loss: 48.95398764590737\n",
      "epoch 717 learning rate:  0.011394700139470014   Training loss:   54.3414988000043  Valing loss:   48.95398764590737\n",
      "Pure loss: 54.25401784580043.....Total loss: 54.25401784580043\n",
      "Pure loss: 48.471065163501365.....Total loss: 48.471065163501365\n",
      "epoch 718 learning rate:  0.011392757660167131   Training loss:   54.25401784580043  Valing loss:   48.471065163501365\n",
      "Pure loss: 54.241171346840176.....Total loss: 54.241171346840176\n",
      "Pure loss: 48.414366955921096.....Total loss: 48.414366955921096\n",
      "epoch 719 learning rate:  0.011390820584144645   Training loss:   54.241171346840176  Valing loss:   48.414366955921096\n",
      "Pure loss: 54.35541470657353.....Total loss: 54.35541470657353\n",
      "Pure loss: 48.93921737884136.....Total loss: 48.93921737884136\n",
      "epoch 720 learning rate:  0.01138888888888889   Training loss:   54.35541470657353  Valing loss:   48.93921737884136\n",
      "Pure loss: 54.35579859817673.....Total loss: 54.35579859817673\n",
      "Pure loss: 48.77048189947159.....Total loss: 48.77048189947159\n",
      "epoch 721 learning rate:  0.011386962552011095   Training loss:   54.35579859817673  Valing loss:   48.77048189947159\n",
      "Pure loss: 54.2801576087587.....Total loss: 54.2801576087587\n",
      "Pure loss: 48.555426725276526.....Total loss: 48.555426725276526\n",
      "epoch 722 learning rate:  0.011385041551246538   Training loss:   54.2801576087587  Valing loss:   48.555426725276526\n",
      "Pure loss: 54.367970492055086.....Total loss: 54.367970492055086\n",
      "Pure loss: 48.270485841834514.....Total loss: 48.270485841834514\n",
      "epoch 723 learning rate:  0.011383125864453666   Training loss:   54.367970492055086  Valing loss:   48.270485841834514\n",
      "Pure loss: 54.27557417944252.....Total loss: 54.27557417944252\n",
      "Pure loss: 49.096585248903175.....Total loss: 49.096585248903175\n",
      "epoch 724 learning rate:  0.01138121546961326   Training loss:   54.27557417944252  Valing loss:   49.096585248903175\n",
      "Pure loss: 54.00785999110442.....Total loss: 54.00785999110442\n",
      "Pure loss: 47.93988093365525.....Total loss: 47.93988093365525\n",
      "epoch 725 learning rate:  0.011379310344827587   Training loss:   54.00785999110442  Valing loss:   47.93988093365525\n",
      "Pure loss: 54.082459153747315.....Total loss: 54.082459153747315\n",
      "Pure loss: 47.738162924339925.....Total loss: 47.738162924339925\n",
      "epoch 726 learning rate:  0.01137741046831956   Training loss:   54.082459153747315  Valing loss:   47.738162924339925\n",
      "Pure loss: 54.16088888154755.....Total loss: 54.16088888154755\n",
      "Pure loss: 47.628487237880535.....Total loss: 47.628487237880535\n",
      "epoch 727 learning rate:  0.011375515818431912   Training loss:   54.16088888154755  Valing loss:   47.628487237880535\n",
      "Pure loss: 54.58998523156911.....Total loss: 54.58998523156911\n",
      "Pure loss: 47.30089310935715.....Total loss: 47.30089310935715\n",
      "epoch 728 learning rate:  0.011373626373626374   Training loss:   54.58998523156911  Valing loss:   47.30089310935715\n",
      "Pure loss: 56.14043969403308.....Total loss: 56.14043969403308\n",
      "Pure loss: 47.13668976427728.....Total loss: 47.13668976427728\n",
      "epoch 729 learning rate:  0.011371742112482854   Training loss:   56.14043969403308  Valing loss:   47.13668976427728\n",
      "Pure loss: 55.85883377202874.....Total loss: 55.85883377202874\n",
      "Pure loss: 47.05115653730329.....Total loss: 47.05115653730329\n",
      "epoch 730 learning rate:  0.01136986301369863   Training loss:   55.85883377202874  Valing loss:   47.05115653730329\n",
      "Pure loss: 56.342277974393056.....Total loss: 56.342277974393056\n",
      "Pure loss: 47.148843367729036.....Total loss: 47.148843367729036\n",
      "epoch 731 learning rate:  0.011367989056087551   Training loss:   56.342277974393056  Valing loss:   47.148843367729036\n",
      "Pure loss: 57.165651762914344.....Total loss: 57.165651762914344\n",
      "Pure loss: 47.41273162549979.....Total loss: 47.41273162549979\n",
      "epoch 732 learning rate:  0.011366120218579235   Training loss:   57.165651762914344  Valing loss:   47.41273162549979\n",
      "Pure loss: 58.27508074979512.....Total loss: 58.27508074979512\n",
      "Pure loss: 47.89905023481811.....Total loss: 47.89905023481811\n",
      "epoch 733 learning rate:  0.011364256480218281   Training loss:   58.27508074979512  Valing loss:   47.89905023481811\n",
      "Pure loss: 58.74309901502612.....Total loss: 58.74309901502612\n",
      "Pure loss: 48.14187916817574.....Total loss: 48.14187916817574\n",
      "epoch 734 learning rate:  0.011362397820163487   Training loss:   58.74309901502612  Valing loss:   48.14187916817574\n",
      "Pure loss: 59.256101081997244.....Total loss: 59.256101081997244\n",
      "Pure loss: 48.303125319481005.....Total loss: 48.303125319481005\n",
      "epoch 735 learning rate:  0.011360544217687075   Training loss:   59.256101081997244  Valing loss:   48.303125319481005\n",
      "Pure loss: 60.52726572045235.....Total loss: 60.52726572045235\n",
      "Pure loss: 49.028057967145514.....Total loss: 49.028057967145514\n",
      "epoch 736 learning rate:  0.011358695652173914   Training loss:   60.52726572045235  Valing loss:   49.028057967145514\n",
      "Pure loss: 59.785305001274054.....Total loss: 59.785305001274054\n",
      "Pure loss: 48.659669305397195.....Total loss: 48.659669305397195\n",
      "epoch 737 learning rate:  0.011356852103120759   Training loss:   59.785305001274054  Valing loss:   48.659669305397195\n",
      "Pure loss: 60.11711027204108.....Total loss: 60.11711027204108\n",
      "Pure loss: 48.91773860088085.....Total loss: 48.91773860088085\n",
      "epoch 738 learning rate:  0.011355013550135502   Training loss:   60.11711027204108  Valing loss:   48.91773860088085\n",
      "Pure loss: 59.96341325653675.....Total loss: 59.96341325653675\n",
      "Pure loss: 48.82249560941597.....Total loss: 48.82249560941597\n",
      "epoch 739 learning rate:  0.011353179972936401   Training loss:   59.96341325653675  Valing loss:   48.82249560941597\n",
      "Pure loss: 59.53557068807553.....Total loss: 59.53557068807553\n",
      "Pure loss: 48.56203593298291.....Total loss: 48.56203593298291\n",
      "epoch 740 learning rate:  0.011351351351351352   Training loss:   59.53557068807553  Valing loss:   48.56203593298291\n",
      "Pure loss: 58.21755451818555.....Total loss: 58.21755451818555\n",
      "Pure loss: 47.674655885124785.....Total loss: 47.674655885124785\n",
      "epoch 741 learning rate:  0.01134952766531714   Training loss:   58.21755451818555  Valing loss:   47.674655885124785\n",
      "Pure loss: 59.12400772081052.....Total loss: 59.12400772081052\n",
      "Pure loss: 48.394594818752424.....Total loss: 48.394594818752424\n",
      "epoch 742 learning rate:  0.011347708894878707   Training loss:   59.12400772081052  Valing loss:   48.394594818752424\n",
      "Pure loss: 56.86141973358672.....Total loss: 56.86141973358672\n",
      "Pure loss: 47.42996483528092.....Total loss: 47.42996483528092\n",
      "epoch 743 learning rate:  0.011345895020188426   Training loss:   56.86141973358672  Valing loss:   47.42996483528092\n",
      "Pure loss: 58.27120160590414.....Total loss: 58.27120160590414\n",
      "Pure loss: 47.99432127280934.....Total loss: 47.99432127280934\n",
      "epoch 744 learning rate:  0.011344086021505377   Training loss:   58.27120160590414  Valing loss:   47.99432127280934\n",
      "Pure loss: 59.6464728767971.....Total loss: 59.6464728767971\n",
      "Pure loss: 48.72957003602821.....Total loss: 48.72957003602821\n",
      "epoch 745 learning rate:  0.011342281879194631   Training loss:   59.6464728767971  Valing loss:   48.72957003602821\n",
      "Pure loss: 60.5483172913837.....Total loss: 60.5483172913837\n",
      "Pure loss: 49.25299224217423.....Total loss: 49.25299224217423\n",
      "epoch 746 learning rate:  0.011340482573726541   Training loss:   60.5483172913837  Valing loss:   49.25299224217423\n",
      "Pure loss: 59.89882700074577.....Total loss: 59.89882700074577\n",
      "Pure loss: 48.88714678119967.....Total loss: 48.88714678119967\n",
      "epoch 747 learning rate:  0.011338688085676037   Training loss:   59.89882700074577  Valing loss:   48.88714678119967\n",
      "Pure loss: 57.74285451264788.....Total loss: 57.74285451264788\n",
      "Pure loss: 47.97528831720589.....Total loss: 47.97528831720589\n",
      "epoch 748 learning rate:  0.011336898395721925   Training loss:   57.74285451264788  Valing loss:   47.97528831720589\n",
      "Pure loss: 58.032119692656295.....Total loss: 58.032119692656295\n",
      "Pure loss: 48.08883354437099.....Total loss: 48.08883354437099\n",
      "epoch 749 learning rate:  0.011335113484646196   Training loss:   58.032119692656295  Valing loss:   48.08883354437099\n",
      "Pure loss: 57.632236269313026.....Total loss: 57.632236269313026\n",
      "Pure loss: 47.93374093108949.....Total loss: 47.93374093108949\n",
      "epoch 750 learning rate:  0.011333333333333334   Training loss:   57.632236269313026  Valing loss:   47.93374093108949\n",
      "Pure loss: 57.53009256287654.....Total loss: 57.53009256287654\n",
      "Pure loss: 47.87952289985834.....Total loss: 47.87952289985834\n",
      "epoch 751 learning rate:  0.011331557922769641   Training loss:   57.53009256287654  Valing loss:   47.87952289985834\n",
      "Pure loss: 57.47138517528471.....Total loss: 57.47138517528471\n",
      "Pure loss: 47.8504204443689.....Total loss: 47.8504204443689\n",
      "epoch 752 learning rate:  0.011329787234042554   Training loss:   57.47138517528471  Valing loss:   47.8504204443689\n",
      "Pure loss: 58.21894593648286.....Total loss: 58.21894593648286\n",
      "Pure loss: 47.87699019485827.....Total loss: 47.87699019485827\n",
      "epoch 753 learning rate:  0.011328021248339974   Training loss:   58.21894593648286  Valing loss:   47.87699019485827\n",
      "Pure loss: 56.3960811638056.....Total loss: 56.3960811638056\n",
      "Pure loss: 46.91449893864147.....Total loss: 46.91449893864147\n",
      "epoch 754 learning rate:  0.011326259946949602   Training loss:   56.3960811638056  Valing loss:   46.91449893864147\n",
      "Pure loss: 55.37845704063357.....Total loss: 55.37845704063357\n",
      "Pure loss: 46.38699113740994.....Total loss: 46.38699113740994\n",
      "epoch 755 learning rate:  0.01132450331125828   Training loss:   55.37845704063357  Valing loss:   46.38699113740994\n",
      "Pure loss: 55.38771009498353.....Total loss: 55.38771009498353\n",
      "Pure loss: 46.389692514709424.....Total loss: 46.389692514709424\n",
      "epoch 756 learning rate:  0.011322751322751323   Training loss:   55.38771009498353  Valing loss:   46.389692514709424\n",
      "Pure loss: 55.603675390364295.....Total loss: 55.603675390364295\n",
      "Pure loss: 46.42630994775956.....Total loss: 46.42630994775956\n",
      "epoch 757 learning rate:  0.011321003963011889   Training loss:   55.603675390364295  Valing loss:   46.42630994775956\n",
      "Pure loss: 55.12454315822213.....Total loss: 55.12454315822213\n",
      "Pure loss: 46.298284346283296.....Total loss: 46.298284346283296\n",
      "epoch 758 learning rate:  0.011319261213720316   Training loss:   55.12454315822213  Valing loss:   46.298284346283296\n",
      "Pure loss: 54.992196777445905.....Total loss: 54.992196777445905\n",
      "Pure loss: 46.30449091284026.....Total loss: 46.30449091284026\n",
      "epoch 759 learning rate:  0.011317523056653492   Training loss:   54.992196777445905  Valing loss:   46.30449091284026\n",
      "Pure loss: 55.17624449930765.....Total loss: 55.17624449930765\n",
      "Pure loss: 46.302282355993185.....Total loss: 46.302282355993185\n",
      "epoch 760 learning rate:  0.011315789473684211   Training loss:   55.17624449930765  Valing loss:   46.302282355993185\n",
      "Pure loss: 55.645030448751946.....Total loss: 55.645030448751946\n",
      "Pure loss: 46.415792802056934.....Total loss: 46.415792802056934\n",
      "epoch 761 learning rate:  0.011314060446780553   Training loss:   55.645030448751946  Valing loss:   46.415792802056934\n",
      "Pure loss: 55.28284336295731.....Total loss: 55.28284336295731\n",
      "Pure loss: 46.23494131937714.....Total loss: 46.23494131937714\n",
      "epoch 762 learning rate:  0.01131233595800525   Training loss:   55.28284336295731  Valing loss:   46.23494131937714\n",
      "Pure loss: 52.04183246194848.....Total loss: 52.04183246194848\n",
      "Pure loss: 46.58822813629095.....Total loss: 46.58822813629095\n",
      "epoch 763 learning rate:  0.011310615989515072   Training loss:   52.04183246194848  Valing loss:   46.58822813629095\n",
      "Pure loss: 51.96910730638438.....Total loss: 51.96910730638438\n",
      "Pure loss: 46.69793548518472.....Total loss: 46.69793548518472\n",
      "epoch 764 learning rate:  0.01130890052356021   Training loss:   51.96910730638438  Valing loss:   46.69793548518472\n",
      "Pure loss: 52.016680803022695.....Total loss: 52.016680803022695\n",
      "Pure loss: 46.67671506415794.....Total loss: 46.67671506415794\n",
      "epoch 765 learning rate:  0.011307189542483661   Training loss:   52.016680803022695  Valing loss:   46.67671506415794\n",
      "Pure loss: 52.01611407207994.....Total loss: 52.01611407207994\n",
      "Pure loss: 46.67716026479405.....Total loss: 46.67716026479405\n",
      "epoch 766 learning rate:  0.011305483028720626   Training loss:   52.01611407207994  Valing loss:   46.67716026479405\n",
      "Pure loss: 52.55204007780685.....Total loss: 52.55204007780685\n",
      "Pure loss: 45.99758383192366.....Total loss: 45.99758383192366\n",
      "epoch 767 learning rate:  0.011303780964797915   Training loss:   52.55204007780685  Valing loss:   45.99758383192366\n",
      "Pure loss: 52.406791955760426.....Total loss: 52.406791955760426\n",
      "Pure loss: 46.015624296654956.....Total loss: 46.015624296654956\n",
      "epoch 768 learning rate:  0.011302083333333334   Training loss:   52.406791955760426  Valing loss:   46.015624296654956\n",
      "Pure loss: 51.86221253670361.....Total loss: 51.86221253670361\n",
      "Pure loss: 46.408149037760566.....Total loss: 46.408149037760566\n",
      "epoch 769 learning rate:  0.011300390117035111   Training loss:   51.86221253670361  Valing loss:   46.408149037760566\n",
      "Pure loss: 51.4006018940503.....Total loss: 51.4006018940503\n",
      "Pure loss: 47.548335592168264.....Total loss: 47.548335592168264\n",
      "epoch 770 learning rate:  0.0112987012987013   Training loss:   51.4006018940503  Valing loss:   47.548335592168264\n",
      "Pure loss: 51.38976111227127.....Total loss: 51.38976111227127\n",
      "Pure loss: 47.513893398112856.....Total loss: 47.513893398112856\n",
      "epoch 771 learning rate:  0.011297016861219197   Training loss:   51.38976111227127  Valing loss:   47.513893398112856\n",
      "Pure loss: 51.5649374300791.....Total loss: 51.5649374300791\n",
      "Pure loss: 47.12829797943718.....Total loss: 47.12829797943718\n",
      "epoch 772 learning rate:  0.011295336787564766   Training loss:   51.5649374300791  Valing loss:   47.12829797943718\n",
      "Pure loss: 51.35301900263305.....Total loss: 51.35301900263305\n",
      "Pure loss: 47.69662624193588.....Total loss: 47.69662624193588\n",
      "epoch 773 learning rate:  0.01129366106080207   Training loss:   51.35301900263305  Valing loss:   47.69662624193588\n",
      "Pure loss: 51.381767478907285.....Total loss: 51.381767478907285\n",
      "Pure loss: 46.63817089831774.....Total loss: 46.63817089831774\n",
      "epoch 774 learning rate:  0.011291989664082687   Training loss:   51.381767478907285  Valing loss:   46.63817089831774\n",
      "Pure loss: 51.38431599240787.....Total loss: 51.38431599240787\n",
      "Pure loss: 46.63361656184065.....Total loss: 46.63361656184065\n",
      "epoch 775 learning rate:  0.011290322580645162   Training loss:   51.38431599240787  Valing loss:   46.63361656184065\n",
      "Pure loss: 51.08664568905787.....Total loss: 51.08664568905787\n",
      "Pure loss: 47.90274982453082.....Total loss: 47.90274982453082\n",
      "epoch 776 learning rate:  0.011288659793814434   Training loss:   51.08664568905787  Valing loss:   47.90274982453082\n",
      "Pure loss: 51.087530112056136.....Total loss: 51.087530112056136\n",
      "Pure loss: 47.91258576310504.....Total loss: 47.91258576310504\n",
      "epoch 777 learning rate:  0.011287001287001287   Training loss:   51.087530112056136  Valing loss:   47.91258576310504\n",
      "Pure loss: 51.08333619375502.....Total loss: 51.08333619375502\n",
      "Pure loss: 47.91625000006943.....Total loss: 47.91625000006943\n",
      "epoch 778 learning rate:  0.0112853470437018   Training loss:   51.08333619375502  Valing loss:   47.91625000006943\n",
      "Pure loss: 51.27057098060902.....Total loss: 51.27057098060902\n",
      "Pure loss: 46.187732622731374.....Total loss: 46.187732622731374\n",
      "epoch 779 learning rate:  0.01128369704749679   Training loss:   51.27057098060902  Valing loss:   46.187732622731374\n",
      "Pure loss: 51.427134262699425.....Total loss: 51.427134262699425\n",
      "Pure loss: 46.24892222372579.....Total loss: 46.24892222372579\n",
      "epoch 780 learning rate:  0.011282051282051283   Training loss:   51.427134262699425  Valing loss:   46.24892222372579\n",
      "Pure loss: 51.06958409216088.....Total loss: 51.06958409216088\n",
      "Pure loss: 46.965882399081224.....Total loss: 46.965882399081224\n",
      "epoch 781 learning rate:  0.011280409731113956   Training loss:   51.06958409216088  Valing loss:   46.965882399081224\n",
      "Pure loss: 51.504500191523604.....Total loss: 51.504500191523604\n",
      "Pure loss: 46.08004535189571.....Total loss: 46.08004535189571\n",
      "epoch 782 learning rate:  0.011278772378516624   Training loss:   51.504500191523604  Valing loss:   46.08004535189571\n",
      "Pure loss: 51.76546523814654.....Total loss: 51.76546523814654\n",
      "Pure loss: 45.793290559910346.....Total loss: 45.793290559910346\n",
      "epoch 783 learning rate:  0.011277139208173692   Training loss:   51.76546523814654  Valing loss:   45.793290559910346\n",
      "Pure loss: 51.13436525421594.....Total loss: 51.13436525421594\n",
      "Pure loss: 46.14210553176786.....Total loss: 46.14210553176786\n",
      "epoch 784 learning rate:  0.011275510204081633   Training loss:   51.13436525421594  Valing loss:   46.14210553176786\n",
      "Pure loss: 50.83812642479582.....Total loss: 50.83812642479582\n",
      "Pure loss: 46.06131902439509.....Total loss: 46.06131902439509\n",
      "epoch 785 learning rate:  0.011273885350318471   Training loss:   50.83812642479582  Valing loss:   46.06131902439509\n",
      "Pure loss: 50.84238134167315.....Total loss: 50.84238134167315\n",
      "Pure loss: 46.06162866592058.....Total loss: 46.06162866592058\n",
      "epoch 786 learning rate:  0.011272264631043256   Training loss:   50.84238134167315  Valing loss:   46.06162866592058\n",
      "Pure loss: 50.70796402671399.....Total loss: 50.70796402671399\n",
      "Pure loss: 46.57388251091409.....Total loss: 46.57388251091409\n",
      "epoch 787 learning rate:  0.011270648030495553   Training loss:   50.70796402671399  Valing loss:   46.57388251091409\n",
      "Pure loss: 50.690925831042.....Total loss: 50.690925831042\n",
      "Pure loss: 46.886913959706824.....Total loss: 46.886913959706824\n",
      "epoch 788 learning rate:  0.011269035532994924   Training loss:   50.690925831042  Valing loss:   46.886913959706824\n",
      "Pure loss: 50.65962923839331.....Total loss: 50.65962923839331\n",
      "Pure loss: 46.801507388477695.....Total loss: 46.801507388477695\n",
      "epoch 789 learning rate:  0.01126742712294043   Training loss:   50.65962923839331  Valing loss:   46.801507388477695\n",
      "Pure loss: 50.868596294494864.....Total loss: 50.868596294494864\n",
      "Pure loss: 48.10793237575709.....Total loss: 48.10793237575709\n",
      "epoch 790 learning rate:  0.011265822784810127   Training loss:   50.868596294494864  Valing loss:   48.10793237575709\n",
      "Pure loss: 52.25296266797878.....Total loss: 52.25296266797878\n",
      "Pure loss: 51.61308154462968.....Total loss: 51.61308154462968\n",
      "epoch 791 learning rate:  0.011264222503160556   Training loss:   52.25296266797878  Valing loss:   51.61308154462968\n",
      "Pure loss: 51.18001293197184.....Total loss: 51.18001293197184\n",
      "Pure loss: 49.455041336041795.....Total loss: 49.455041336041795\n",
      "epoch 792 learning rate:  0.011262626262626262   Training loss:   51.18001293197184  Valing loss:   49.455041336041795\n",
      "Pure loss: 51.213434539736.....Total loss: 51.213434539736\n",
      "Pure loss: 49.51431646501953.....Total loss: 49.51431646501953\n",
      "epoch 793 learning rate:  0.011261034047919293   Training loss:   51.213434539736  Valing loss:   49.51431646501953\n",
      "Pure loss: 50.77138975849249.....Total loss: 50.77138975849249\n",
      "Pure loss: 48.68697588572639.....Total loss: 48.68697588572639\n",
      "epoch 794 learning rate:  0.011259445843828715   Training loss:   50.77138975849249  Valing loss:   48.68697588572639\n",
      "Pure loss: 50.51621973485328.....Total loss: 50.51621973485328\n",
      "Pure loss: 47.80229032001289.....Total loss: 47.80229032001289\n",
      "epoch 795 learning rate:  0.011257861635220126   Training loss:   50.51621973485328  Valing loss:   47.80229032001289\n",
      "Pure loss: 50.80661156394678.....Total loss: 50.80661156394678\n",
      "Pure loss: 48.96250262722017.....Total loss: 48.96250262722017\n",
      "epoch 796 learning rate:  0.011256281407035176   Training loss:   50.80661156394678  Valing loss:   48.96250262722017\n",
      "Pure loss: 50.45250379388276.....Total loss: 50.45250379388276\n",
      "Pure loss: 47.92080578967326.....Total loss: 47.92080578967326\n",
      "epoch 797 learning rate:  0.011254705144291091   Training loss:   50.45250379388276  Valing loss:   47.92080578967326\n",
      "Pure loss: 50.39773822274539.....Total loss: 50.39773822274539\n",
      "Pure loss: 47.47905339299312.....Total loss: 47.47905339299312\n",
      "epoch 798 learning rate:  0.0112531328320802   Training loss:   50.39773822274539  Valing loss:   47.47905339299312\n",
      "Pure loss: 50.41779835435481.....Total loss: 50.41779835435481\n",
      "Pure loss: 47.505141090083555.....Total loss: 47.505141090083555\n",
      "epoch 799 learning rate:  0.011251564455569463   Training loss:   50.41779835435481  Valing loss:   47.505141090083555\n",
      "Pure loss: 50.2845341035585.....Total loss: 50.2845341035585\n",
      "Pure loss: 47.561071007289804.....Total loss: 47.561071007289804\n",
      "epoch 800 learning rate:  0.01125   Training loss:   50.2845341035585  Valing loss:   47.561071007289804\n",
      "Pure loss: 50.10563946365779.....Total loss: 50.10563946365779\n",
      "Pure loss: 46.52754884698392.....Total loss: 46.52754884698392\n",
      "epoch 801 learning rate:  0.011248439450686641   Training loss:   50.10563946365779  Valing loss:   46.52754884698392\n",
      "Pure loss: 50.16741187252739.....Total loss: 50.16741187252739\n",
      "Pure loss: 46.09571507159096.....Total loss: 46.09571507159096\n",
      "epoch 802 learning rate:  0.011246882793017457   Training loss:   50.16741187252739  Valing loss:   46.09571507159096\n",
      "Pure loss: 50.547902403157664.....Total loss: 50.547902403157664\n",
      "Pure loss: 45.465120129211286.....Total loss: 45.465120129211286\n",
      "epoch 803 learning rate:  0.0112453300124533   Training loss:   50.547902403157664  Valing loss:   45.465120129211286\n",
      "Pure loss: 50.55843635092876.....Total loss: 50.55843635092876\n",
      "Pure loss: 45.46949136147996.....Total loss: 45.46949136147996\n",
      "epoch 804 learning rate:  0.011243781094527363   Training loss:   50.55843635092876  Valing loss:   45.46949136147996\n",
      "Pure loss: 50.361826149450245.....Total loss: 50.361826149450245\n",
      "Pure loss: 45.50394763339468.....Total loss: 45.50394763339468\n",
      "epoch 805 learning rate:  0.011242236024844721   Training loss:   50.361826149450245  Valing loss:   45.50394763339468\n",
      "Pure loss: 50.000663165203754.....Total loss: 50.000663165203754\n",
      "Pure loss: 45.43798958006832.....Total loss: 45.43798958006832\n",
      "epoch 806 learning rate:  0.011240694789081886   Training loss:   50.000663165203754  Valing loss:   45.43798958006832\n",
      "Pure loss: 50.16179027443028.....Total loss: 50.16179027443028\n",
      "Pure loss: 45.10893290212433.....Total loss: 45.10893290212433\n",
      "epoch 807 learning rate:  0.011239157372986369   Training loss:   50.16179027443028  Valing loss:   45.10893290212433\n",
      "Pure loss: 50.94459548908467.....Total loss: 50.94459548908467\n",
      "Pure loss: 44.56869128635582.....Total loss: 44.56869128635582\n",
      "epoch 808 learning rate:  0.011237623762376237   Training loss:   50.94459548908467  Valing loss:   44.56869128635582\n",
      "Pure loss: 50.41582512468315.....Total loss: 50.41582512468315\n",
      "Pure loss: 44.41572600219032.....Total loss: 44.41572600219032\n",
      "epoch 809 learning rate:  0.011236093943139678   Training loss:   50.41582512468315  Valing loss:   44.41572600219032\n",
      "Pure loss: 49.89976318812327.....Total loss: 49.89976318812327\n",
      "Pure loss: 44.5365578619332.....Total loss: 44.5365578619332\n",
      "epoch 810 learning rate:  0.011234567901234569   Training loss:   49.89976318812327  Valing loss:   44.5365578619332\n",
      "Pure loss: 49.85357476148891.....Total loss: 49.85357476148891\n",
      "Pure loss: 44.863173954531824.....Total loss: 44.863173954531824\n",
      "epoch 811 learning rate:  0.01123304562268804   Training loss:   49.85357476148891  Valing loss:   44.863173954531824\n",
      "Pure loss: 49.730347619482735.....Total loss: 49.730347619482735\n",
      "Pure loss: 46.11371011084122.....Total loss: 46.11371011084122\n",
      "epoch 812 learning rate:  0.01123152709359606   Training loss:   49.730347619482735  Valing loss:   46.11371011084122\n",
      "Pure loss: 49.90405464767049.....Total loss: 49.90405464767049\n",
      "Pure loss: 46.81241552200446.....Total loss: 46.81241552200446\n",
      "epoch 813 learning rate:  0.011230012300123002   Training loss:   49.90405464767049  Valing loss:   46.81241552200446\n",
      "Pure loss: 50.45870285460046.....Total loss: 50.45870285460046\n",
      "Pure loss: 48.23370114182581.....Total loss: 48.23370114182581\n",
      "epoch 814 learning rate:  0.011228501228501228   Training loss:   50.45870285460046  Valing loss:   48.23370114182581\n",
      "Pure loss: 51.631005900034765.....Total loss: 51.631005900034765\n",
      "Pure loss: 50.890665712232604.....Total loss: 50.890665712232604\n",
      "epoch 815 learning rate:  0.011226993865030675   Training loss:   51.631005900034765  Valing loss:   50.890665712232604\n",
      "Pure loss: 52.984937016926196.....Total loss: 52.984937016926196\n",
      "Pure loss: 53.419032266012806.....Total loss: 53.419032266012806\n",
      "epoch 816 learning rate:  0.011225490196078432   Training loss:   52.984937016926196  Valing loss:   53.419032266012806\n",
      "Pure loss: 51.61049190049468.....Total loss: 51.61049190049468\n",
      "Pure loss: 51.354984329187175.....Total loss: 51.354984329187175\n",
      "epoch 817 learning rate:  0.011223990208078335   Training loss:   51.61049190049468  Valing loss:   51.354984329187175\n",
      "Pure loss: 52.110530684564814.....Total loss: 52.110530684564814\n",
      "Pure loss: 52.26772830782848.....Total loss: 52.26772830782848\n",
      "epoch 818 learning rate:  0.011222493887530563   Training loss:   52.110530684564814  Valing loss:   52.26772830782848\n",
      "Pure loss: 51.7445313433726.....Total loss: 51.7445313433726\n",
      "Pure loss: 51.60223178782563.....Total loss: 51.60223178782563\n",
      "epoch 819 learning rate:  0.011221001221001222   Training loss:   51.7445313433726  Valing loss:   51.60223178782563\n",
      "Pure loss: 54.300575086293954.....Total loss: 54.300575086293954\n",
      "Pure loss: 56.02062843349602.....Total loss: 56.02062843349602\n",
      "epoch 820 learning rate:  0.01121951219512195   Training loss:   54.300575086293954  Valing loss:   56.02062843349602\n",
      "Pure loss: 53.29306653122087.....Total loss: 53.29306653122087\n",
      "Pure loss: 54.65114237564053.....Total loss: 54.65114237564053\n",
      "epoch 821 learning rate:  0.011218026796589526   Training loss:   53.29306653122087  Valing loss:   54.65114237564053\n",
      "Pure loss: 52.59101754245689.....Total loss: 52.59101754245689\n",
      "Pure loss: 53.82185405925573.....Total loss: 53.82185405925573\n",
      "epoch 822 learning rate:  0.01121654501216545   Training loss:   52.59101754245689  Valing loss:   53.82185405925573\n",
      "Pure loss: 51.83428080882206.....Total loss: 51.83428080882206\n",
      "Pure loss: 52.79145940359512.....Total loss: 52.79145940359512\n",
      "epoch 823 learning rate:  0.011215066828675577   Training loss:   51.83428080882206  Valing loss:   52.79145940359512\n",
      "Pure loss: 51.18198315786423.....Total loss: 51.18198315786423\n",
      "Pure loss: 51.58461489185368.....Total loss: 51.58461489185368\n",
      "epoch 824 learning rate:  0.011213592233009708   Training loss:   51.18198315786423  Valing loss:   51.58461489185368\n",
      "Pure loss: 51.608523919437765.....Total loss: 51.608523919437765\n",
      "Pure loss: 52.34059050003727.....Total loss: 52.34059050003727\n",
      "epoch 825 learning rate:  0.011212121212121211   Training loss:   51.608523919437765  Valing loss:   52.34059050003727\n",
      "Pure loss: 51.08704831217792.....Total loss: 51.08704831217792\n",
      "Pure loss: 51.41851002891784.....Total loss: 51.41851002891784\n",
      "epoch 826 learning rate:  0.011210653753026635   Training loss:   51.08704831217792  Valing loss:   51.41851002891784\n",
      "Pure loss: 51.130116990161575.....Total loss: 51.130116990161575\n",
      "Pure loss: 51.48646318402976.....Total loss: 51.48646318402976\n",
      "epoch 827 learning rate:  0.01120918984280532   Training loss:   51.130116990161575  Valing loss:   51.48646318402976\n",
      "Pure loss: 50.098841157924625.....Total loss: 50.098841157924625\n",
      "Pure loss: 49.49072244733178.....Total loss: 49.49072244733178\n",
      "epoch 828 learning rate:  0.011207729468599034   Training loss:   50.098841157924625  Valing loss:   49.49072244733178\n",
      "Pure loss: 49.516649230875885.....Total loss: 49.516649230875885\n",
      "Pure loss: 47.967437054139346.....Total loss: 47.967437054139346\n",
      "epoch 829 learning rate:  0.01120627261761158   Training loss:   49.516649230875885  Valing loss:   47.967437054139346\n",
      "Pure loss: 49.344038210520466.....Total loss: 49.344038210520466\n",
      "Pure loss: 46.738674839975154.....Total loss: 46.738674839975154\n",
      "epoch 830 learning rate:  0.011204819277108435   Training loss:   49.344038210520466  Valing loss:   46.738674839975154\n",
      "Pure loss: 49.2376072519473.....Total loss: 49.2376072519473\n",
      "Pure loss: 46.73502303865772.....Total loss: 46.73502303865772\n",
      "epoch 831 learning rate:  0.011203369434416365   Training loss:   49.2376072519473  Valing loss:   46.73502303865772\n",
      "Pure loss: 49.08980904056689.....Total loss: 49.08980904056689\n",
      "Pure loss: 44.85676188711139.....Total loss: 44.85676188711139\n",
      "epoch 832 learning rate:  0.011201923076923078   Training loss:   49.08980904056689  Valing loss:   44.85676188711139\n",
      "Pure loss: 49.230822552520586.....Total loss: 49.230822552520586\n",
      "Pure loss: 44.661239769463826.....Total loss: 44.661239769463826\n",
      "epoch 833 learning rate:  0.01120048019207683   Training loss:   49.230822552520586  Valing loss:   44.661239769463826\n",
      "Pure loss: 50.35730359100839.....Total loss: 50.35730359100839\n",
      "Pure loss: 44.23129475175854.....Total loss: 44.23129475175854\n",
      "epoch 834 learning rate:  0.01119904076738609   Training loss:   50.35730359100839  Valing loss:   44.23129475175854\n",
      "Pure loss: 49.95240458015766.....Total loss: 49.95240458015766\n",
      "Pure loss: 43.97650046074586.....Total loss: 43.97650046074586\n",
      "epoch 835 learning rate:  0.011197604790419162   Training loss:   49.95240458015766  Valing loss:   43.97650046074586\n",
      "Pure loss: 50.11998676240987.....Total loss: 50.11998676240987\n",
      "Pure loss: 44.09333356263236.....Total loss: 44.09333356263236\n",
      "epoch 836 learning rate:  0.011196172248803827   Training loss:   50.11998676240987  Valing loss:   44.09333356263236\n",
      "Pure loss: 48.805377201680535.....Total loss: 48.805377201680535\n",
      "Pure loss: 44.12587521206953.....Total loss: 44.12587521206953\n",
      "epoch 837 learning rate:  0.011194743130227001   Training loss:   48.805377201680535  Valing loss:   44.12587521206953\n",
      "Pure loss: 48.77807685210424.....Total loss: 48.77807685210424\n",
      "Pure loss: 44.17978440843531.....Total loss: 44.17978440843531\n",
      "epoch 838 learning rate:  0.011193317422434368   Training loss:   48.77807685210424  Valing loss:   44.17978440843531\n",
      "Pure loss: 48.43788316228055.....Total loss: 48.43788316228055\n",
      "Pure loss: 45.82320986922664.....Total loss: 45.82320986922664\n",
      "epoch 839 learning rate:  0.011191895113230037   Training loss:   48.43788316228055  Valing loss:   45.82320986922664\n",
      "Pure loss: 48.651623159224194.....Total loss: 48.651623159224194\n",
      "Pure loss: 44.75545945912868.....Total loss: 44.75545945912868\n",
      "epoch 840 learning rate:  0.011190476190476192   Training loss:   48.651623159224194  Valing loss:   44.75545945912868\n",
      "Pure loss: 48.88844560180995.....Total loss: 48.88844560180995\n",
      "Pure loss: 44.40745645758589.....Total loss: 44.40745645758589\n",
      "epoch 841 learning rate:  0.011189060642092748   Training loss:   48.88844560180995  Valing loss:   44.40745645758589\n",
      "Pure loss: 49.292032976815776.....Total loss: 49.292032976815776\n",
      "Pure loss: 44.489145307998925.....Total loss: 44.489145307998925\n",
      "epoch 842 learning rate:  0.011187648456057008   Training loss:   49.292032976815776  Valing loss:   44.489145307998925\n",
      "Pure loss: 49.22508992082233.....Total loss: 49.22508992082233\n",
      "Pure loss: 44.44047106708075.....Total loss: 44.44047106708075\n",
      "epoch 843 learning rate:  0.011186239620403321   Training loss:   49.22508992082233  Valing loss:   44.44047106708075\n",
      "Pure loss: 48.35539613567292.....Total loss: 48.35539613567292\n",
      "Pure loss: 46.59788412984389.....Total loss: 46.59788412984389\n",
      "epoch 844 learning rate:  0.011184834123222749   Training loss:   48.35539613567292  Valing loss:   46.59788412984389\n",
      "Pure loss: 48.390002651535596.....Total loss: 48.390002651535596\n",
      "Pure loss: 44.85082345100834.....Total loss: 44.85082345100834\n",
      "epoch 845 learning rate:  0.011183431952662722   Training loss:   48.390002651535596  Valing loss:   44.85082345100834\n",
      "Pure loss: 48.578594755080054.....Total loss: 48.578594755080054\n",
      "Pure loss: 44.611798732907396.....Total loss: 44.611798732907396\n",
      "epoch 846 learning rate:  0.011182033096926714   Training loss:   48.578594755080054  Valing loss:   44.611798732907396\n",
      "Pure loss: 48.03163368082301.....Total loss: 48.03163368082301\n",
      "Pure loss: 44.797861156877225.....Total loss: 44.797861156877225\n",
      "epoch 847 learning rate:  0.011180637544273907   Training loss:   48.03163368082301  Valing loss:   44.797861156877225\n",
      "Pure loss: 48.11977772265046.....Total loss: 48.11977772265046\n",
      "Pure loss: 44.85349331880562.....Total loss: 44.85349331880562\n",
      "epoch 848 learning rate:  0.011179245283018868   Training loss:   48.11977772265046  Valing loss:   44.85349331880562\n",
      "Pure loss: 48.06154456469424.....Total loss: 48.06154456469424\n",
      "Pure loss: 45.12886593818329.....Total loss: 45.12886593818329\n",
      "epoch 849 learning rate:  0.011177856301531213   Training loss:   48.06154456469424  Valing loss:   45.12886593818329\n",
      "Pure loss: 48.087423756217575.....Total loss: 48.087423756217575\n",
      "Pure loss: 45.158950383088126.....Total loss: 45.158950383088126\n",
      "epoch 850 learning rate:  0.011176470588235295   Training loss:   48.087423756217575  Valing loss:   45.158950383088126\n",
      "Pure loss: 48.38517378003322.....Total loss: 48.38517378003322\n",
      "Pure loss: 44.94747682327212.....Total loss: 44.94747682327212\n",
      "epoch 851 learning rate:  0.01117508813160987   Training loss:   48.38517378003322  Valing loss:   44.94747682327212\n",
      "Pure loss: 49.05792306100791.....Total loss: 49.05792306100791\n",
      "Pure loss: 44.29705567333837.....Total loss: 44.29705567333837\n",
      "epoch 852 learning rate:  0.011173708920187793   Training loss:   49.05792306100791  Valing loss:   44.29705567333837\n",
      "Pure loss: 49.349036707566526.....Total loss: 49.349036707566526\n",
      "Pure loss: 44.073927950045594.....Total loss: 44.073927950045594\n",
      "epoch 853 learning rate:  0.011172332942555686   Training loss:   49.349036707566526  Valing loss:   44.073927950045594\n",
      "Pure loss: 48.53836276692319.....Total loss: 48.53836276692319\n",
      "Pure loss: 43.96232175638143.....Total loss: 43.96232175638143\n",
      "epoch 854 learning rate:  0.01117096018735363   Training loss:   48.53836276692319  Valing loss:   43.96232175638143\n",
      "Pure loss: 48.72062659435178.....Total loss: 48.72062659435178\n",
      "Pure loss: 44.079504000205965.....Total loss: 44.079504000205965\n",
      "epoch 855 learning rate:  0.011169590643274854   Training loss:   48.72062659435178  Valing loss:   44.079504000205965\n",
      "Pure loss: 49.72894237559306.....Total loss: 49.72894237559306\n",
      "Pure loss: 43.84218385011304.....Total loss: 43.84218385011304\n",
      "epoch 856 learning rate:  0.011168224299065421   Training loss:   49.72894237559306  Valing loss:   43.84218385011304\n",
      "Pure loss: 51.65849605174604.....Total loss: 51.65849605174604\n",
      "Pure loss: 44.20151794657107.....Total loss: 44.20151794657107\n",
      "epoch 857 learning rate:  0.01116686114352392   Training loss:   51.65849605174604  Valing loss:   44.20151794657107\n",
      "Pure loss: 48.477568271361825.....Total loss: 48.477568271361825\n",
      "Pure loss: 45.31287610049285.....Total loss: 45.31287610049285\n",
      "epoch 858 learning rate:  0.011165501165501166   Training loss:   48.477568271361825  Valing loss:   45.31287610049285\n",
      "Pure loss: 48.95645597543114.....Total loss: 48.95645597543114\n",
      "Pure loss: 45.412662463488694.....Total loss: 45.412662463488694\n",
      "epoch 859 learning rate:  0.011164144353899884   Training loss:   48.95645597543114  Valing loss:   45.412662463488694\n",
      "Pure loss: 49.78835376203809.....Total loss: 49.78835376203809\n",
      "Pure loss: 45.35615681148616.....Total loss: 45.35615681148616\n",
      "epoch 860 learning rate:  0.011162790697674419   Training loss:   49.78835376203809  Valing loss:   45.35615681148616\n",
      "Pure loss: 49.221376026609775.....Total loss: 49.221376026609775\n",
      "Pure loss: 45.070734561785684.....Total loss: 45.070734561785684\n",
      "epoch 861 learning rate:  0.011161440185830429   Training loss:   49.221376026609775  Valing loss:   45.070734561785684\n",
      "Pure loss: 48.44980756564621.....Total loss: 48.44980756564621\n",
      "Pure loss: 44.82575898601228.....Total loss: 44.82575898601228\n",
      "epoch 862 learning rate:  0.011160092807424594   Training loss:   48.44980756564621  Valing loss:   44.82575898601228\n",
      "Pure loss: 48.833072876988446.....Total loss: 48.833072876988446\n",
      "Pure loss: 44.644553576649635.....Total loss: 44.644553576649635\n",
      "epoch 863 learning rate:  0.011158748551564311   Training loss:   48.833072876988446  Valing loss:   44.644553576649635\n",
      "Pure loss: 49.22465515224046.....Total loss: 49.22465515224046\n",
      "Pure loss: 44.79729746158566.....Total loss: 44.79729746158566\n",
      "epoch 864 learning rate:  0.011157407407407408   Training loss:   49.22465515224046  Valing loss:   44.79729746158566\n",
      "Pure loss: 48.91204903850981.....Total loss: 48.91204903850981\n",
      "Pure loss: 44.57846762526486.....Total loss: 44.57846762526486\n",
      "epoch 865 learning rate:  0.01115606936416185   Training loss:   48.91204903850981  Valing loss:   44.57846762526486\n",
      "Pure loss: 48.749604997135336.....Total loss: 48.749604997135336\n",
      "Pure loss: 44.459635181377294.....Total loss: 44.459635181377294\n",
      "epoch 866 learning rate:  0.01115473441108545   Training loss:   48.749604997135336  Valing loss:   44.459635181377294\n",
      "Pure loss: 48.23999477930102.....Total loss: 48.23999477930102\n",
      "Pure loss: 45.13760378333259.....Total loss: 45.13760378333259\n",
      "epoch 867 learning rate:  0.011153402537485584   Training loss:   48.23999477930102  Valing loss:   45.13760378333259\n",
      "Pure loss: 48.22110040729098.....Total loss: 48.22110040729098\n",
      "Pure loss: 45.14582559531387.....Total loss: 45.14582559531387\n",
      "epoch 868 learning rate:  0.011152073732718894   Training loss:   48.22110040729098  Valing loss:   45.14582559531387\n",
      "Pure loss: 47.68738728927348.....Total loss: 47.68738728927348\n",
      "Pure loss: 45.78785832250949.....Total loss: 45.78785832250949\n",
      "epoch 869 learning rate:  0.011150747986191024   Training loss:   47.68738728927348  Valing loss:   45.78785832250949\n",
      "Pure loss: 47.054678655874305.....Total loss: 47.054678655874305\n",
      "Pure loss: 45.52763931476971.....Total loss: 45.52763931476971\n",
      "epoch 870 learning rate:  0.011149425287356322   Training loss:   47.054678655874305  Valing loss:   45.52763931476971\n",
      "Pure loss: 47.56469924095668.....Total loss: 47.56469924095668\n",
      "Pure loss: 47.62789888935014.....Total loss: 47.62789888935014\n",
      "epoch 871 learning rate:  0.011148105625717566   Training loss:   47.56469924095668  Valing loss:   47.62789888935014\n",
      "Pure loss: 47.342612074395525.....Total loss: 47.342612074395525\n",
      "Pure loss: 47.064510630897146.....Total loss: 47.064510630897146\n",
      "epoch 872 learning rate:  0.011146788990825688   Training loss:   47.342612074395525  Valing loss:   47.064510630897146\n",
      "Pure loss: 47.652478848723774.....Total loss: 47.652478848723774\n",
      "Pure loss: 47.810378396204165.....Total loss: 47.810378396204165\n",
      "epoch 873 learning rate:  0.011145475372279496   Training loss:   47.652478848723774  Valing loss:   47.810378396204165\n",
      "Pure loss: 47.86814338744549.....Total loss: 47.86814338744549\n",
      "Pure loss: 48.24624149442915.....Total loss: 48.24624149442915\n",
      "epoch 874 learning rate:  0.0111441647597254   Training loss:   47.86814338744549  Valing loss:   48.24624149442915\n",
      "Pure loss: 47.355991452202495.....Total loss: 47.355991452202495\n",
      "Pure loss: 47.26700859780249.....Total loss: 47.26700859780249\n",
      "epoch 875 learning rate:  0.011142857142857144   Training loss:   47.355991452202495  Valing loss:   47.26700859780249\n",
      "Pure loss: 47.35461055447603.....Total loss: 47.35461055447603\n",
      "Pure loss: 47.22583742278752.....Total loss: 47.22583742278752\n",
      "epoch 876 learning rate:  0.011141552511415525   Training loss:   47.35461055447603  Valing loss:   47.22583742278752\n",
      "Pure loss: 52.3381627614101.....Total loss: 52.3381627614101\n",
      "Pure loss: 55.75917205129946.....Total loss: 55.75917205129946\n",
      "epoch 877 learning rate:  0.011140250855188142   Training loss:   52.3381627614101  Valing loss:   55.75917205129946\n",
      "Pure loss: 52.03823905540699.....Total loss: 52.03823905540699\n",
      "Pure loss: 55.29262576191633.....Total loss: 55.29262576191633\n",
      "epoch 878 learning rate:  0.011138952164009112   Training loss:   52.03823905540699  Valing loss:   55.29262576191633\n",
      "Pure loss: 51.51370829160272.....Total loss: 51.51370829160272\n",
      "Pure loss: 54.532735493639116.....Total loss: 54.532735493639116\n",
      "epoch 879 learning rate:  0.011137656427758817   Training loss:   51.51370829160272  Valing loss:   54.532735493639116\n",
      "Pure loss: 51.24941236226856.....Total loss: 51.24941236226856\n",
      "Pure loss: 54.144489030799676.....Total loss: 54.144489030799676\n",
      "epoch 880 learning rate:  0.011136363636363637   Training loss:   51.24941236226856  Valing loss:   54.144489030799676\n",
      "Pure loss: 49.06823371101831.....Total loss: 49.06823371101831\n",
      "Pure loss: 50.66882810324003.....Total loss: 50.66882810324003\n",
      "epoch 881 learning rate:  0.011135073779795687   Training loss:   49.06823371101831  Valing loss:   50.66882810324003\n",
      "Pure loss: 50.07334745688328.....Total loss: 50.07334745688328\n",
      "Pure loss: 52.32904306792405.....Total loss: 52.32904306792405\n",
      "epoch 882 learning rate:  0.011133786848072563   Training loss:   50.07334745688328  Valing loss:   52.32904306792405\n",
      "Pure loss: 50.50395383736913.....Total loss: 50.50395383736913\n",
      "Pure loss: 52.98420057065478.....Total loss: 52.98420057065478\n",
      "epoch 883 learning rate:  0.011132502831257078   Training loss:   50.50395383736913  Valing loss:   52.98420057065478\n",
      "Pure loss: 53.2158315912141.....Total loss: 53.2158315912141\n",
      "Pure loss: 57.07834144878793.....Total loss: 57.07834144878793\n",
      "epoch 884 learning rate:  0.011131221719457013   Training loss:   53.2158315912141  Valing loss:   57.07834144878793\n",
      "Pure loss: 53.87345933699346.....Total loss: 53.87345933699346\n",
      "Pure loss: 57.957845410381175.....Total loss: 57.957845410381175\n",
      "epoch 885 learning rate:  0.01112994350282486   Training loss:   53.87345933699346  Valing loss:   57.957845410381175\n",
      "Pure loss: 52.23447359276672.....Total loss: 52.23447359276672\n",
      "Pure loss: 55.655527116043324.....Total loss: 55.655527116043324\n",
      "epoch 886 learning rate:  0.011128668171557563   Training loss:   52.23447359276672  Valing loss:   55.655527116043324\n",
      "Pure loss: 51.088593581353656.....Total loss: 51.088593581353656\n",
      "Pure loss: 53.99000360411118.....Total loss: 53.99000360411118\n",
      "epoch 887 learning rate:  0.01112739571589628   Training loss:   51.088593581353656  Valing loss:   53.99000360411118\n",
      "Pure loss: 51.62334864115217.....Total loss: 51.62334864115217\n",
      "Pure loss: 54.84899524908417.....Total loss: 54.84899524908417\n",
      "epoch 888 learning rate:  0.011126126126126127   Training loss:   51.62334864115217  Valing loss:   54.84899524908417\n",
      "Pure loss: 50.51477214248771.....Total loss: 50.51477214248771\n",
      "Pure loss: 53.218362506961014.....Total loss: 53.218362506961014\n",
      "epoch 889 learning rate:  0.011124859392575927   Training loss:   50.51477214248771  Valing loss:   53.218362506961014\n",
      "Pure loss: 50.39731855326716.....Total loss: 50.39731855326716\n",
      "Pure loss: 53.037250761503756.....Total loss: 53.037250761503756\n",
      "epoch 890 learning rate:  0.011123595505617978   Training loss:   50.39731855326716  Valing loss:   53.037250761503756\n",
      "Pure loss: 49.940122486257614.....Total loss: 49.940122486257614\n",
      "Pure loss: 52.29368228192317.....Total loss: 52.29368228192317\n",
      "epoch 891 learning rate:  0.01112233445566779   Training loss:   49.940122486257614  Valing loss:   52.29368228192317\n",
      "Pure loss: 47.64972640085129.....Total loss: 47.64972640085129\n",
      "Pure loss: 48.53224281376095.....Total loss: 48.53224281376095\n",
      "epoch 892 learning rate:  0.011121076233183856   Training loss:   47.64972640085129  Valing loss:   48.53224281376095\n",
      "Pure loss: 46.02524490723585.....Total loss: 46.02524490723585\n",
      "Pure loss: 45.103842954173956.....Total loss: 45.103842954173956\n",
      "epoch 893 learning rate:  0.011119820828667413   Training loss:   46.02524490723585  Valing loss:   45.103842954173956\n",
      "Pure loss: 45.922783447903505.....Total loss: 45.922783447903505\n",
      "Pure loss: 45.19582283773824.....Total loss: 45.19582283773824\n",
      "epoch 894 learning rate:  0.011118568232662192   Training loss:   45.922783447903505  Valing loss:   45.19582283773824\n",
      "Pure loss: 45.768382506926436.....Total loss: 45.768382506926436\n",
      "Pure loss: 45.223481200523445.....Total loss: 45.223481200523445\n",
      "epoch 895 learning rate:  0.01111731843575419   Training loss:   45.768382506926436  Valing loss:   45.223481200523445\n",
      "Pure loss: 46.25874654952424.....Total loss: 46.25874654952424\n",
      "Pure loss: 46.33118954628501.....Total loss: 46.33118954628501\n",
      "epoch 896 learning rate:  0.011116071428571428   Training loss:   46.25874654952424  Valing loss:   46.33118954628501\n",
      "Pure loss: 48.10341701905893.....Total loss: 48.10341701905893\n",
      "Pure loss: 49.786217794429746.....Total loss: 49.786217794429746\n",
      "epoch 897 learning rate:  0.011114827201783724   Training loss:   48.10341701905893  Valing loss:   49.786217794429746\n",
      "Pure loss: 49.61192703460934.....Total loss: 49.61192703460934\n",
      "Pure loss: 52.245367305944576.....Total loss: 52.245367305944576\n",
      "epoch 898 learning rate:  0.01111358574610245   Training loss:   49.61192703460934  Valing loss:   52.245367305944576\n",
      "Pure loss: 49.88178997538163.....Total loss: 49.88178997538163\n",
      "Pure loss: 52.637658835030386.....Total loss: 52.637658835030386\n",
      "epoch 899 learning rate:  0.011112347052280312   Training loss:   49.88178997538163  Valing loss:   52.637658835030386\n",
      "Pure loss: 48.54996562481822.....Total loss: 48.54996562481822\n",
      "Pure loss: 50.72957513060397.....Total loss: 50.72957513060397\n",
      "epoch 900 learning rate:  0.011111111111111112   Training loss:   48.54996562481822  Valing loss:   50.72957513060397\n",
      "Pure loss: 48.52479011395124.....Total loss: 48.52479011395124\n",
      "Pure loss: 50.69155485495556.....Total loss: 50.69155485495556\n",
      "epoch 901 learning rate:  0.011109877913429524   Training loss:   48.52479011395124  Valing loss:   50.69155485495556\n",
      "Pure loss: 48.40502944484998.....Total loss: 48.40502944484998\n",
      "Pure loss: 50.50907255475347.....Total loss: 50.50907255475347\n",
      "epoch 902 learning rate:  0.011108647450110865   Training loss:   48.40502944484998  Valing loss:   50.50907255475347\n",
      "Pure loss: 47.999956145229035.....Total loss: 47.999956145229035\n",
      "Pure loss: 49.8737672403796.....Total loss: 49.8737672403796\n",
      "epoch 903 learning rate:  0.011107419712070875   Training loss:   47.999956145229035  Valing loss:   49.8737672403796\n",
      "Pure loss: 46.730082984167446.....Total loss: 46.730082984167446\n",
      "Pure loss: 47.7364808631943.....Total loss: 47.7364808631943\n",
      "epoch 904 learning rate:  0.011106194690265487   Training loss:   46.730082984167446  Valing loss:   47.7364808631943\n",
      "Pure loss: 46.36130550093384.....Total loss: 46.36130550093384\n",
      "Pure loss: 47.0694973702518.....Total loss: 47.0694973702518\n",
      "epoch 905 learning rate:  0.011104972375690608   Training loss:   46.36130550093384  Valing loss:   47.0694973702518\n",
      "Pure loss: 46.298826024055636.....Total loss: 46.298826024055636\n",
      "Pure loss: 46.92105418316237.....Total loss: 46.92105418316237\n",
      "epoch 906 learning rate:  0.011103752759381899   Training loss:   46.298826024055636  Valing loss:   46.92105418316237\n",
      "Pure loss: 45.30499977723886.....Total loss: 45.30499977723886\n",
      "Pure loss: 44.60324843553772.....Total loss: 44.60324843553772\n",
      "epoch 907 learning rate:  0.011102535832414554   Training loss:   45.30499977723886  Valing loss:   44.60324843553772\n",
      "Pure loss: 45.31137583705203.....Total loss: 45.31137583705203\n",
      "Pure loss: 44.610760864024876.....Total loss: 44.610760864024876\n",
      "epoch 908 learning rate:  0.011101321585903084   Training loss:   45.31137583705203  Valing loss:   44.610760864024876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 45.324000199440924.....Total loss: 45.324000199440924\n",
      "Pure loss: 44.44565612503182.....Total loss: 44.44565612503182\n",
      "epoch 909 learning rate:  0.011100110011001101   Training loss:   45.324000199440924  Valing loss:   44.44565612503182\n",
      "Pure loss: 45.35884333178374.....Total loss: 45.35884333178374\n",
      "Pure loss: 44.610347117674905.....Total loss: 44.610347117674905\n",
      "epoch 910 learning rate:  0.011098901098901099   Training loss:   45.35884333178374  Valing loss:   44.610347117674905\n",
      "Pure loss: 45.380532105582944.....Total loss: 45.380532105582944\n",
      "Pure loss: 44.72676345752114.....Total loss: 44.72676345752114\n",
      "epoch 911 learning rate:  0.011097694840834248   Training loss:   45.380532105582944  Valing loss:   44.72676345752114\n",
      "Pure loss: 46.65922352106166.....Total loss: 46.65922352106166\n",
      "Pure loss: 47.50056897757489.....Total loss: 47.50056897757489\n",
      "epoch 912 learning rate:  0.011096491228070176   Training loss:   46.65922352106166  Valing loss:   47.50056897757489\n",
      "Pure loss: 46.99978675235186.....Total loss: 46.99978675235186\n",
      "Pure loss: 48.12089341774255.....Total loss: 48.12089341774255\n",
      "epoch 913 learning rate:  0.011095290251916759   Training loss:   46.99978675235186  Valing loss:   48.12089341774255\n",
      "Pure loss: 47.52132820816337.....Total loss: 47.52132820816337\n",
      "Pure loss: 48.98217890979438.....Total loss: 48.98217890979438\n",
      "epoch 914 learning rate:  0.011094091903719912   Training loss:   47.52132820816337  Valing loss:   48.98217890979438\n",
      "Pure loss: 47.494496459253085.....Total loss: 47.494496459253085\n",
      "Pure loss: 48.934682140763286.....Total loss: 48.934682140763286\n",
      "epoch 915 learning rate:  0.011092896174863389   Training loss:   47.494496459253085  Valing loss:   48.934682140763286\n",
      "Pure loss: 47.8884696125793.....Total loss: 47.8884696125793\n",
      "Pure loss: 49.56508080966278.....Total loss: 49.56508080966278\n",
      "epoch 916 learning rate:  0.01109170305676856   Training loss:   47.8884696125793  Valing loss:   49.56508080966278\n",
      "Pure loss: 48.64540031606758.....Total loss: 48.64540031606758\n",
      "Pure loss: 50.74650931991331.....Total loss: 50.74650931991331\n",
      "epoch 917 learning rate:  0.01109051254089422   Training loss:   48.64540031606758  Valing loss:   50.74650931991331\n",
      "Pure loss: 49.49383162472717.....Total loss: 49.49383162472717\n",
      "Pure loss: 51.946147081926206.....Total loss: 51.946147081926206\n",
      "epoch 918 learning rate:  0.011089324618736384   Training loss:   49.49383162472717  Valing loss:   51.946147081926206\n",
      "Pure loss: 50.34903201982488.....Total loss: 50.34903201982488\n",
      "Pure loss: 53.18982063649694.....Total loss: 53.18982063649694\n",
      "epoch 919 learning rate:  0.011088139281828075   Training loss:   50.34903201982488  Valing loss:   53.18982063649694\n",
      "Pure loss: 48.10937824413592.....Total loss: 48.10937824413592\n",
      "Pure loss: 49.85245825021961.....Total loss: 49.85245825021961\n",
      "epoch 920 learning rate:  0.011086956521739131   Training loss:   48.10937824413592  Valing loss:   49.85245825021961\n",
      "Pure loss: 46.22015587388243.....Total loss: 46.22015587388243\n",
      "Pure loss: 46.791730060694356.....Total loss: 46.791730060694356\n",
      "epoch 921 learning rate:  0.011085776330076005   Training loss:   46.22015587388243  Valing loss:   46.791730060694356\n",
      "Pure loss: 46.07091224576258.....Total loss: 46.07091224576258\n",
      "Pure loss: 46.59742490984136.....Total loss: 46.59742490984136\n",
      "epoch 922 learning rate:  0.011084598698481562   Training loss:   46.07091224576258  Valing loss:   46.59742490984136\n",
      "Pure loss: 46.05960264494411.....Total loss: 46.05960264494411\n",
      "Pure loss: 46.58005545994893.....Total loss: 46.58005545994893\n",
      "epoch 923 learning rate:  0.011083423618634887   Training loss:   46.05960264494411  Valing loss:   46.58005545994893\n",
      "Pure loss: 46.22478483804929.....Total loss: 46.22478483804929\n",
      "Pure loss: 46.87977346128455.....Total loss: 46.87977346128455\n",
      "epoch 924 learning rate:  0.011082251082251082   Training loss:   46.22478483804929  Valing loss:   46.87977346128455\n",
      "Pure loss: 46.215156456640365.....Total loss: 46.215156456640365\n",
      "Pure loss: 46.86332028613291.....Total loss: 46.86332028613291\n",
      "epoch 925 learning rate:  0.01108108108108108   Training loss:   46.215156456640365  Valing loss:   46.86332028613291\n",
      "Pure loss: 44.87313022736575.....Total loss: 44.87313022736575\n",
      "Pure loss: 44.278976187201415.....Total loss: 44.278976187201415\n",
      "epoch 926 learning rate:  0.011079913606911447   Training loss:   44.87313022736575  Valing loss:   44.278976187201415\n",
      "Pure loss: 44.73444318902697.....Total loss: 44.73444318902697\n",
      "Pure loss: 43.98661633950015.....Total loss: 43.98661633950015\n",
      "epoch 927 learning rate:  0.011078748651564186   Training loss:   44.73444318902697  Valing loss:   43.98661633950015\n",
      "Pure loss: 44.3497144228593.....Total loss: 44.3497144228593\n",
      "Pure loss: 42.92729929394144.....Total loss: 42.92729929394144\n",
      "epoch 928 learning rate:  0.011077586206896552   Training loss:   44.3497144228593  Valing loss:   42.92729929394144\n",
      "Pure loss: 44.44505242341094.....Total loss: 44.44505242341094\n",
      "Pure loss: 43.243115524198245.....Total loss: 43.243115524198245\n",
      "epoch 929 learning rate:  0.011076426264800862   Training loss:   44.44505242341094  Valing loss:   43.243115524198245\n",
      "Pure loss: 44.08280618228449.....Total loss: 44.08280618228449\n",
      "Pure loss: 41.79703081354844.....Total loss: 41.79703081354844\n",
      "epoch 930 learning rate:  0.011075268817204302   Training loss:   44.08280618228449  Valing loss:   41.79703081354844\n",
      "Pure loss: 44.44918937646.....Total loss: 44.44918937646\n",
      "Pure loss: 43.67457297935032.....Total loss: 43.67457297935032\n",
      "epoch 931 learning rate:  0.011074113856068744   Training loss:   44.44918937646  Valing loss:   43.67457297935032\n",
      "Pure loss: 43.49232191928873.....Total loss: 43.49232191928873\n",
      "Pure loss: 41.31337633848865.....Total loss: 41.31337633848865\n",
      "epoch 932 learning rate:  0.011072961373390557   Training loss:   43.49232191928873  Valing loss:   41.31337633848865\n",
      "Pure loss: 43.69552507183871.....Total loss: 43.69552507183871\n",
      "Pure loss: 41.849974799590925.....Total loss: 41.849974799590925\n",
      "epoch 933 learning rate:  0.011071811361200428   Training loss:   43.69552507183871  Valing loss:   41.849974799590925\n",
      "Pure loss: 43.650397120624405.....Total loss: 43.650397120624405\n",
      "Pure loss: 41.77342429659468.....Total loss: 41.77342429659468\n",
      "epoch 934 learning rate:  0.011070663811563169   Training loss:   43.650397120624405  Valing loss:   41.77342429659468\n",
      "Pure loss: 43.31679280601985.....Total loss: 43.31679280601985\n",
      "Pure loss: 40.82506900566536.....Total loss: 40.82506900566536\n",
      "epoch 935 learning rate:  0.01106951871657754   Training loss:   43.31679280601985  Valing loss:   40.82506900566536\n",
      "Pure loss: 43.17009444545797.....Total loss: 43.17009444545797\n",
      "Pure loss: 40.001753206665256.....Total loss: 40.001753206665256\n",
      "epoch 936 learning rate:  0.011068376068376068   Training loss:   43.17009444545797  Valing loss:   40.001753206665256\n",
      "Pure loss: 43.50024917879475.....Total loss: 43.50024917879475\n",
      "Pure loss: 41.56303740572151.....Total loss: 41.56303740572151\n",
      "epoch 937 learning rate:  0.011067235859124867   Training loss:   43.50024917879475  Valing loss:   41.56303740572151\n",
      "Pure loss: 43.24793361380848.....Total loss: 43.24793361380848\n",
      "Pure loss: 40.62296078373617.....Total loss: 40.62296078373617\n",
      "epoch 938 learning rate:  0.011066098081023455   Training loss:   43.24793361380848  Valing loss:   40.62296078373617\n",
      "Pure loss: 43.06595283599037.....Total loss: 43.06595283599037\n",
      "Pure loss: 39.89200437801508.....Total loss: 39.89200437801508\n",
      "epoch 939 learning rate:  0.01106496272630458   Training loss:   43.06595283599037  Valing loss:   39.89200437801508\n",
      "Pure loss: 43.07612120195044.....Total loss: 43.07612120195044\n",
      "Pure loss: 39.96279307666044.....Total loss: 39.96279307666044\n",
      "epoch 940 learning rate:  0.011063829787234043   Training loss:   43.07612120195044  Valing loss:   39.96279307666044\n",
      "Pure loss: 42.95179655793059.....Total loss: 42.95179655793059\n",
      "Pure loss: 39.36503783990451.....Total loss: 39.36503783990451\n",
      "epoch 941 learning rate:  0.01106269925611052   Training loss:   42.95179655793059  Valing loss:   39.36503783990451\n",
      "Pure loss: 42.961578088727656.....Total loss: 42.961578088727656\n",
      "Pure loss: 39.47271701281499.....Total loss: 39.47271701281499\n",
      "epoch 942 learning rate:  0.011061571125265393   Training loss:   42.961578088727656  Valing loss:   39.47271701281499\n",
      "Pure loss: 43.10861797850159.....Total loss: 43.10861797850159\n",
      "Pure loss: 39.961287239938706.....Total loss: 39.961287239938706\n",
      "epoch 943 learning rate:  0.011060445387062566   Training loss:   43.10861797850159  Valing loss:   39.961287239938706\n",
      "Pure loss: 43.19663208296055.....Total loss: 43.19663208296055\n",
      "Pure loss: 40.357203722491555.....Total loss: 40.357203722491555\n",
      "epoch 944 learning rate:  0.011059322033898306   Training loss:   43.19663208296055  Valing loss:   40.357203722491555\n",
      "Pure loss: 43.15435116302369.....Total loss: 43.15435116302369\n",
      "Pure loss: 40.103254505525435.....Total loss: 40.103254505525435\n",
      "epoch 945 learning rate:  0.011058201058201059   Training loss:   43.15435116302369  Valing loss:   40.103254505525435\n",
      "Pure loss: 43.73797036639863.....Total loss: 43.73797036639863\n",
      "Pure loss: 41.88211013636755.....Total loss: 41.88211013636755\n",
      "epoch 946 learning rate:  0.01105708245243129   Training loss:   43.73797036639863  Valing loss:   41.88211013636755\n",
      "Pure loss: 44.22832746664131.....Total loss: 44.22832746664131\n",
      "Pure loss: 43.030978187998016.....Total loss: 43.030978187998016\n",
      "epoch 947 learning rate:  0.01105596620908131   Training loss:   44.22832746664131  Valing loss:   43.030978187998016\n",
      "Pure loss: 43.40792302576071.....Total loss: 43.40792302576071\n",
      "Pure loss: 41.76221058974934.....Total loss: 41.76221058974934\n",
      "epoch 948 learning rate:  0.011054852320675105   Training loss:   43.40792302576071  Valing loss:   41.76221058974934\n",
      "Pure loss: 43.627117303441324.....Total loss: 43.627117303441324\n",
      "Pure loss: 42.17910502555493.....Total loss: 42.17910502555493\n",
      "epoch 949 learning rate:  0.011053740779768177   Training loss:   43.627117303441324  Valing loss:   42.17910502555493\n",
      "Pure loss: 42.89603948717963.....Total loss: 42.89603948717963\n",
      "Pure loss: 40.076424761441565.....Total loss: 40.076424761441565\n",
      "epoch 950 learning rate:  0.01105263157894737   Training loss:   42.89603948717963  Valing loss:   40.076424761441565\n",
      "Pure loss: 42.76672490416444.....Total loss: 42.76672490416444\n",
      "Pure loss: 39.86594907554355.....Total loss: 39.86594907554355\n",
      "epoch 951 learning rate:  0.011051524710830705   Training loss:   42.76672490416444  Valing loss:   39.86594907554355\n",
      "Pure loss: 42.613399883785135.....Total loss: 42.613399883785135\n",
      "Pure loss: 39.35454402705384.....Total loss: 39.35454402705384\n",
      "epoch 952 learning rate:  0.011050420168067227   Training loss:   42.613399883785135  Valing loss:   39.35454402705384\n",
      "Pure loss: 43.86211834600843.....Total loss: 43.86211834600843\n",
      "Pure loss: 44.02723009777627.....Total loss: 44.02723009777627\n",
      "epoch 953 learning rate:  0.011049317943336832   Training loss:   43.86211834600843  Valing loss:   44.02723009777627\n",
      "Pure loss: 43.449238007330074.....Total loss: 43.449238007330074\n",
      "Pure loss: 43.231889890490315.....Total loss: 43.231889890490315\n",
      "epoch 954 learning rate:  0.011048218029350104   Training loss:   43.449238007330074  Valing loss:   43.231889890490315\n",
      "Pure loss: 43.194822917327166.....Total loss: 43.194822917327166\n",
      "Pure loss: 42.727026402050896.....Total loss: 42.727026402050896\n",
      "epoch 955 learning rate:  0.011047120418848169   Training loss:   43.194822917327166  Valing loss:   42.727026402050896\n",
      "Pure loss: 42.89802983835964.....Total loss: 42.89802983835964\n",
      "Pure loss: 42.05403744524328.....Total loss: 42.05403744524328\n",
      "epoch 956 learning rate:  0.01104602510460251   Training loss:   42.89802983835964  Valing loss:   42.05403744524328\n",
      "Pure loss: 47.38990179619407.....Total loss: 47.38990179619407\n",
      "Pure loss: 49.91438566004279.....Total loss: 49.91438566004279\n",
      "epoch 957 learning rate:  0.011044932079414838   Training loss:   47.38990179619407  Valing loss:   49.91438566004279\n",
      "Pure loss: 50.11087669633874.....Total loss: 50.11087669633874\n",
      "Pure loss: 53.89323790859552.....Total loss: 53.89323790859552\n",
      "epoch 958 learning rate:  0.01104384133611691   Training loss:   50.11087669633874  Valing loss:   53.89323790859552\n",
      "Pure loss: 51.60703849083952.....Total loss: 51.60703849083952\n",
      "Pure loss: 55.90191486118719.....Total loss: 55.90191486118719\n",
      "epoch 959 learning rate:  0.011042752867570386   Training loss:   51.60703849083952  Valing loss:   55.90191486118719\n",
      "Pure loss: 52.086491931654464.....Total loss: 52.086491931654464\n",
      "Pure loss: 56.48911388245144.....Total loss: 56.48911388245144\n",
      "epoch 960 learning rate:  0.011041666666666667   Training loss:   52.086491931654464  Valing loss:   56.48911388245144\n",
      "Pure loss: 50.37746373454329.....Total loss: 50.37746373454329\n",
      "Pure loss: 54.27231180300355.....Total loss: 54.27231180300355\n",
      "epoch 961 learning rate:  0.011040582726326744   Training loss:   50.37746373454329  Valing loss:   54.27231180300355\n",
      "Pure loss: 48.767849390453115.....Total loss: 48.767849390453115\n",
      "Pure loss: 52.16615655981263.....Total loss: 52.16615655981263\n",
      "epoch 962 learning rate:  0.01103950103950104   Training loss:   48.767849390453115  Valing loss:   52.16615655981263\n",
      "Pure loss: 51.36331852576104.....Total loss: 51.36331852576104\n",
      "Pure loss: 55.7201710432304.....Total loss: 55.7201710432304\n",
      "epoch 963 learning rate:  0.011038421599169262   Training loss:   51.36331852576104  Valing loss:   55.7201710432304\n",
      "Pure loss: 52.822663473437046.....Total loss: 52.822663473437046\n",
      "Pure loss: 57.52158841241543.....Total loss: 57.52158841241543\n",
      "epoch 964 learning rate:  0.011037344398340249   Training loss:   52.822663473437046  Valing loss:   57.52158841241543\n",
      "Pure loss: 53.28081575593023.....Total loss: 53.28081575593023\n",
      "Pure loss: 57.96055103683702.....Total loss: 57.96055103683702\n",
      "epoch 965 learning rate:  0.011036269430051813   Training loss:   53.28081575593023  Valing loss:   57.96055103683702\n",
      "Pure loss: 53.525792690257944.....Total loss: 53.525792690257944\n",
      "Pure loss: 58.28515376420874.....Total loss: 58.28515376420874\n",
      "epoch 966 learning rate:  0.0110351966873706   Training loss:   53.525792690257944  Valing loss:   58.28515376420874\n",
      "Pure loss: 51.49817164117964.....Total loss: 51.49817164117964\n",
      "Pure loss: 55.62540295094693.....Total loss: 55.62540295094693\n",
      "epoch 967 learning rate:  0.011034126163391934   Training loss:   51.49817164117964  Valing loss:   55.62540295094693\n",
      "Pure loss: 50.168646666219836.....Total loss: 50.168646666219836\n",
      "Pure loss: 53.89422456757026.....Total loss: 53.89422456757026\n",
      "epoch 968 learning rate:  0.01103305785123967   Training loss:   50.168646666219836  Valing loss:   53.89422456757026\n",
      "Pure loss: 46.42178928340243.....Total loss: 46.42178928340243\n",
      "Pure loss: 48.58442201043983.....Total loss: 48.58442201043983\n",
      "epoch 969 learning rate:  0.011031991744066048   Training loss:   46.42178928340243  Valing loss:   48.58442201043983\n",
      "Pure loss: 48.18254236018256.....Total loss: 48.18254236018256\n",
      "Pure loss: 51.13755401064176.....Total loss: 51.13755401064176\n",
      "epoch 970 learning rate:  0.011030927835051546   Training loss:   48.18254236018256  Valing loss:   51.13755401064176\n",
      "Pure loss: 45.37473557570905.....Total loss: 45.37473557570905\n",
      "Pure loss: 47.15237464613897.....Total loss: 47.15237464613897\n",
      "epoch 971 learning rate:  0.011029866117404738   Training loss:   45.37473557570905  Valing loss:   47.15237464613897\n",
      "Pure loss: 45.64755906450142.....Total loss: 45.64755906450142\n",
      "Pure loss: 47.54264698470473.....Total loss: 47.54264698470473\n",
      "epoch 972 learning rate:  0.01102880658436214   Training loss:   45.64755906450142  Valing loss:   47.54264698470473\n",
      "Pure loss: 44.37673700643602.....Total loss: 44.37673700643602\n",
      "Pure loss: 45.62071647402236.....Total loss: 45.62071647402236\n",
      "epoch 973 learning rate:  0.011027749229188078   Training loss:   44.37673700643602  Valing loss:   45.62071647402236\n",
      "Pure loss: 44.51058321849944.....Total loss: 44.51058321849944\n",
      "Pure loss: 45.82375346454453.....Total loss: 45.82375346454453\n",
      "epoch 974 learning rate:  0.011026694045174538   Training loss:   44.51058321849944  Valing loss:   45.82375346454453\n",
      "Pure loss: 43.96552719989517.....Total loss: 43.96552719989517\n",
      "Pure loss: 44.98084253318092.....Total loss: 44.98084253318092\n",
      "epoch 975 learning rate:  0.011025641025641027   Training loss:   43.96552719989517  Valing loss:   44.98084253318092\n",
      "Pure loss: 44.88768069495598.....Total loss: 44.88768069495598\n",
      "Pure loss: 46.50958378537136.....Total loss: 46.50958378537136\n",
      "epoch 976 learning rate:  0.011024590163934426   Training loss:   44.88768069495598  Valing loss:   46.50958378537136\n",
      "Pure loss: 45.09445351592961.....Total loss: 45.09445351592961\n",
      "Pure loss: 46.82901071148567.....Total loss: 46.82901071148567\n",
      "epoch 977 learning rate:  0.011023541453428863   Training loss:   45.09445351592961  Valing loss:   46.82901071148567\n",
      "Pure loss: 44.22289727157029.....Total loss: 44.22289727157029\n",
      "Pure loss: 45.46963709075234.....Total loss: 45.46963709075234\n",
      "epoch 978 learning rate:  0.011022494887525562   Training loss:   44.22289727157029  Valing loss:   45.46963709075234\n",
      "Pure loss: 44.18263742430273.....Total loss: 44.18263742430273\n",
      "Pure loss: 45.40204827763422.....Total loss: 45.40204827763422\n",
      "epoch 979 learning rate:  0.011021450459652707   Training loss:   44.18263742430273  Valing loss:   45.40204827763422\n",
      "Pure loss: 44.537974601862864.....Total loss: 44.537974601862864\n",
      "Pure loss: 45.963271058613834.....Total loss: 45.963271058613834\n",
      "epoch 980 learning rate:  0.011020408163265306   Training loss:   44.537974601862864  Valing loss:   45.963271058613834\n",
      "Pure loss: 44.451201132618316.....Total loss: 44.451201132618316\n",
      "Pure loss: 45.83993736749197.....Total loss: 45.83993736749197\n",
      "epoch 981 learning rate:  0.011019367991845056   Training loss:   44.451201132618316  Valing loss:   45.83993736749197\n",
      "Pure loss: 45.064111811610196.....Total loss: 45.064111811610196\n",
      "Pure loss: 46.75999014696321.....Total loss: 46.75999014696321\n",
      "epoch 982 learning rate:  0.011018329938900204   Training loss:   45.064111811610196  Valing loss:   46.75999014696321\n",
      "Pure loss: 44.22071642880522.....Total loss: 44.22071642880522\n",
      "Pure loss: 45.422912068878155.....Total loss: 45.422912068878155\n",
      "epoch 983 learning rate:  0.011017293997965413   Training loss:   44.22071642880522  Valing loss:   45.422912068878155\n",
      "Pure loss: 42.154771704119625.....Total loss: 42.154771704119625\n",
      "Pure loss: 41.7114930123997.....Total loss: 41.7114930123997\n",
      "epoch 984 learning rate:  0.011016260162601627   Training loss:   42.154771704119625  Valing loss:   41.7114930123997\n",
      "Pure loss: 41.015407398822866.....Total loss: 41.015407398822866\n",
      "Pure loss: 38.97860761315873.....Total loss: 38.97860761315873\n",
      "epoch 985 learning rate:  0.01101522842639594   Training loss:   41.015407398822866  Valing loss:   38.97860761315873\n",
      "Pure loss: 40.950369230605936.....Total loss: 40.950369230605936\n",
      "Pure loss: 38.74652842919734.....Total loss: 38.74652842919734\n",
      "epoch 986 learning rate:  0.01101419878296146   Training loss:   40.950369230605936  Valing loss:   38.74652842919734\n",
      "Pure loss: 40.77634160093719.....Total loss: 40.77634160093719\n",
      "Pure loss: 37.503461149265995.....Total loss: 37.503461149265995\n",
      "epoch 987 learning rate:  0.011013171225937184   Training loss:   40.77634160093719  Valing loss:   37.503461149265995\n",
      "Pure loss: 41.201452024575815.....Total loss: 41.201452024575815\n",
      "Pure loss: 39.82170513252165.....Total loss: 39.82170513252165\n",
      "epoch 988 learning rate:  0.011012145748987855   Training loss:   41.201452024575815  Valing loss:   39.82170513252165\n",
      "Pure loss: 42.82069232898068.....Total loss: 42.82069232898068\n",
      "Pure loss: 43.166710101933354.....Total loss: 43.166710101933354\n",
      "epoch 989 learning rate:  0.011011122345803842   Training loss:   42.82069232898068  Valing loss:   43.166710101933354\n",
      "Pure loss: 41.00400535651481.....Total loss: 41.00400535651481\n",
      "Pure loss: 39.79714529983728.....Total loss: 39.79714529983728\n",
      "epoch 990 learning rate:  0.01101010101010101   Training loss:   41.00400535651481  Valing loss:   39.79714529983728\n",
      "Pure loss: 41.020821201201294.....Total loss: 41.020821201201294\n",
      "Pure loss: 39.830275072794144.....Total loss: 39.830275072794144\n",
      "epoch 991 learning rate:  0.011009081735620586   Training loss:   41.020821201201294  Valing loss:   39.830275072794144\n",
      "Pure loss: 41.059902731026526.....Total loss: 41.059902731026526\n",
      "Pure loss: 39.88774192234677.....Total loss: 39.88774192234677\n",
      "epoch 992 learning rate:  0.011008064516129033   Training loss:   41.059902731026526  Valing loss:   39.88774192234677\n",
      "Pure loss: 40.91277748230638.....Total loss: 40.91277748230638\n",
      "Pure loss: 39.564355238582166.....Total loss: 39.564355238582166\n",
      "epoch 993 learning rate:  0.011007049345417925   Training loss:   40.91277748230638  Valing loss:   39.564355238582166\n",
      "Pure loss: 40.59779429998454.....Total loss: 40.59779429998454\n",
      "Pure loss: 38.72270497351963.....Total loss: 38.72270497351963\n",
      "epoch 994 learning rate:  0.011006036217303823   Training loss:   40.59779429998454  Valing loss:   38.72270497351963\n",
      "Pure loss: 40.31762048903574.....Total loss: 40.31762048903574\n",
      "Pure loss: 37.25910490571412.....Total loss: 37.25910490571412\n",
      "epoch 995 learning rate:  0.011005025125628141   Training loss:   40.31762048903574  Valing loss:   37.25910490571412\n",
      "Pure loss: 40.316524509165305.....Total loss: 40.316524509165305\n",
      "Pure loss: 37.0973783827427.....Total loss: 37.0973783827427\n",
      "epoch 996 learning rate:  0.011004016064257029   Training loss:   40.316524509165305  Valing loss:   37.0973783827427\n",
      "Pure loss: 40.397024714147506.....Total loss: 40.397024714147506\n",
      "Pure loss: 36.636385435143254.....Total loss: 36.636385435143254\n",
      "epoch 997 learning rate:  0.011003009027081245   Training loss:   40.397024714147506  Valing loss:   36.636385435143254\n",
      "Pure loss: 40.359167196794836.....Total loss: 40.359167196794836\n",
      "Pure loss: 36.73784725922398.....Total loss: 36.73784725922398\n",
      "epoch 998 learning rate:  0.011002004008016032   Training loss:   40.359167196794836  Valing loss:   36.73784725922398\n",
      "Pure loss: 40.35672572034828.....Total loss: 40.35672572034828\n",
      "Pure loss: 36.73859182663247.....Total loss: 36.73859182663247\n",
      "epoch 999 learning rate:  0.011001001001001001   Training loss:   40.35672572034828  Valing loss:   36.73859182663247\n",
      "Pure loss: 40.821372206657955.....Total loss: 40.821372206657955\n",
      "Pure loss: 39.70263938327144.....Total loss: 39.70263938327144\n",
      "epoch 1000 learning rate:  0.011   Training loss:   40.821372206657955  Valing loss:   39.70263938327144\n",
      "Pure loss: 40.799851294434994.....Total loss: 40.799851294434994\n",
      "Pure loss: 39.65781609633962.....Total loss: 39.65781609633962\n",
      "epoch 1001 learning rate:  0.010999000999000998   Training loss:   40.799851294434994  Valing loss:   39.65781609633962\n",
      "Pure loss: 41.353359145503354.....Total loss: 41.353359145503354\n",
      "Pure loss: 40.701140957446974.....Total loss: 40.701140957446974\n",
      "epoch 1002 learning rate:  0.010998003992015969   Training loss:   41.353359145503354  Valing loss:   40.701140957446974\n",
      "Pure loss: 41.38021095575279.....Total loss: 41.38021095575279\n",
      "Pure loss: 40.74930103174939.....Total loss: 40.74930103174939\n",
      "epoch 1003 learning rate:  0.010997008973080757   Training loss:   41.38021095575279  Valing loss:   40.74930103174939\n",
      "Pure loss: 41.343784405114945.....Total loss: 41.343784405114945\n",
      "Pure loss: 40.673317771637905.....Total loss: 40.673317771637905\n",
      "epoch 1004 learning rate:  0.010996015936254981   Training loss:   41.343784405114945  Valing loss:   40.673317771637905\n",
      "Pure loss: 40.537564054953876.....Total loss: 40.537564054953876\n",
      "Pure loss: 38.77873976254171.....Total loss: 38.77873976254171\n",
      "epoch 1005 learning rate:  0.01099502487562189   Training loss:   40.537564054953876  Valing loss:   38.77873976254171\n",
      "Pure loss: 40.19943409786862.....Total loss: 40.19943409786862\n",
      "Pure loss: 37.0023730772371.....Total loss: 37.0023730772371\n",
      "epoch 1006 learning rate:  0.010994035785288271   Training loss:   40.19943409786862  Valing loss:   37.0023730772371\n",
      "Pure loss: 40.23443540694429.....Total loss: 40.23443540694429\n",
      "Pure loss: 36.7686352087723.....Total loss: 36.7686352087723\n",
      "epoch 1007 learning rate:  0.01099304865938431   Training loss:   40.23443540694429  Valing loss:   36.7686352087723\n",
      "Pure loss: 40.636100446365816.....Total loss: 40.636100446365816\n",
      "Pure loss: 35.78999264183049.....Total loss: 35.78999264183049\n",
      "epoch 1008 learning rate:  0.010992063492063492   Training loss:   40.636100446365816  Valing loss:   35.78999264183049\n",
      "Pure loss: 40.953011495900796.....Total loss: 40.953011495900796\n",
      "Pure loss: 35.72846954018134.....Total loss: 35.72846954018134\n",
      "epoch 1009 learning rate:  0.010991080277502478   Training loss:   40.953011495900796  Valing loss:   35.72846954018134\n",
      "Pure loss: 40.5254070669554.....Total loss: 40.5254070669554\n",
      "Pure loss: 35.81537867461185.....Total loss: 35.81537867461185\n",
      "epoch 1010 learning rate:  0.01099009900990099   Training loss:   40.5254070669554  Valing loss:   35.81537867461185\n",
      "Pure loss: 41.313659965789796.....Total loss: 41.313659965789796\n",
      "Pure loss: 35.29331341512746.....Total loss: 35.29331341512746\n",
      "epoch 1011 learning rate:  0.010989119683481702   Training loss:   41.313659965789796  Valing loss:   35.29331341512746\n",
      "Pure loss: 42.42023393846872.....Total loss: 42.42023393846872\n",
      "Pure loss: 35.349633838547604.....Total loss: 35.349633838547604\n",
      "epoch 1012 learning rate:  0.010988142292490118   Training loss:   42.42023393846872  Valing loss:   35.349633838547604\n",
      "Pure loss: 42.18403571867856.....Total loss: 42.18403571867856\n",
      "Pure loss: 35.27425092153023.....Total loss: 35.27425092153023\n",
      "epoch 1013 learning rate:  0.010987166831194472   Training loss:   42.18403571867856  Valing loss:   35.27425092153023\n",
      "Pure loss: 41.99906046732794.....Total loss: 41.99906046732794\n",
      "Pure loss: 35.252071132696344.....Total loss: 35.252071132696344\n",
      "epoch 1014 learning rate:  0.010986193293885602   Training loss:   41.99906046732794  Valing loss:   35.252071132696344\n",
      "Pure loss: 41.7347881162729.....Total loss: 41.7347881162729\n",
      "Pure loss: 35.2252120036321.....Total loss: 35.2252120036321\n",
      "epoch 1015 learning rate:  0.010985221674876847   Training loss:   41.7347881162729  Valing loss:   35.2252120036321\n",
      "Pure loss: 42.388122100766694.....Total loss: 42.388122100766694\n",
      "Pure loss: 35.35254014007631.....Total loss: 35.35254014007631\n",
      "epoch 1016 learning rate:  0.010984251968503937   Training loss:   42.388122100766694  Valing loss:   35.35254014007631\n",
      "Pure loss: 43.57374747249256.....Total loss: 43.57374747249256\n",
      "Pure loss: 35.75012623960945.....Total loss: 35.75012623960945\n",
      "epoch 1017 learning rate:  0.010983284169124877   Training loss:   43.57374747249256  Valing loss:   35.75012623960945\n",
      "Pure loss: 40.922323738280944.....Total loss: 40.922323738280944\n",
      "Pure loss: 35.20824544693222.....Total loss: 35.20824544693222\n",
      "epoch 1018 learning rate:  0.010982318271119843   Training loss:   40.922323738280944  Valing loss:   35.20824544693222\n",
      "Pure loss: 41.20304527575556.....Total loss: 41.20304527575556\n",
      "Pure loss: 35.21095600952311.....Total loss: 35.21095600952311\n",
      "epoch 1019 learning rate:  0.01098135426889107   Training loss:   41.20304527575556  Valing loss:   35.21095600952311\n",
      "Pure loss: 40.86301825187897.....Total loss: 40.86301825187897\n",
      "Pure loss: 35.21832685596671.....Total loss: 35.21832685596671\n",
      "epoch 1020 learning rate:  0.010980392156862745   Training loss:   40.86301825187897  Valing loss:   35.21832685596671\n",
      "Pure loss: 41.51963944718731.....Total loss: 41.51963944718731\n",
      "Pure loss: 35.16370782012711.....Total loss: 35.16370782012711\n",
      "epoch 1021 learning rate:  0.010979431929480902   Training loss:   41.51963944718731  Valing loss:   35.16370782012711\n",
      "Pure loss: 41.35558413975418.....Total loss: 41.35558413975418\n",
      "Pure loss: 35.165851787199045.....Total loss: 35.165851787199045\n",
      "epoch 1022 learning rate:  0.010978473581213307   Training loss:   41.35558413975418  Valing loss:   35.165851787199045\n",
      "Pure loss: 42.19241748500544.....Total loss: 42.19241748500544\n",
      "Pure loss: 35.31370714334868.....Total loss: 35.31370714334868\n",
      "epoch 1023 learning rate:  0.010977517106549366   Training loss:   42.19241748500544  Valing loss:   35.31370714334868\n",
      "Pure loss: 43.408301563319746.....Total loss: 43.408301563319746\n",
      "Pure loss: 35.76201089154625.....Total loss: 35.76201089154625\n",
      "epoch 1024 learning rate:  0.0109765625   Training loss:   43.408301563319746  Valing loss:   35.76201089154625\n",
      "Pure loss: 44.991562228073256.....Total loss: 44.991562228073256\n",
      "Pure loss: 36.34945960034361.....Total loss: 36.34945960034361\n",
      "epoch 1025 learning rate:  0.01097560975609756   Training loss:   44.991562228073256  Valing loss:   36.34945960034361\n",
      "Pure loss: 44.40851458798092.....Total loss: 44.40851458798092\n",
      "Pure loss: 35.946684953469116.....Total loss: 35.946684953469116\n",
      "epoch 1026 learning rate:  0.010974658869395712   Training loss:   44.40851458798092  Valing loss:   35.946684953469116\n",
      "Pure loss: 44.40883145622995.....Total loss: 44.40883145622995\n",
      "Pure loss: 35.94686643185659.....Total loss: 35.94686643185659\n",
      "epoch 1027 learning rate:  0.010973709834469329   Training loss:   44.40883145622995  Valing loss:   35.94686643185659\n",
      "Pure loss: 46.0807130889005.....Total loss: 46.0807130889005\n",
      "Pure loss: 36.88040076818781.....Total loss: 36.88040076818781\n",
      "epoch 1028 learning rate:  0.010972762645914397   Training loss:   46.0807130889005  Valing loss:   36.88040076818781\n",
      "Pure loss: 45.919869809702334.....Total loss: 45.919869809702334\n",
      "Pure loss: 36.80270109893064.....Total loss: 36.80270109893064\n",
      "epoch 1029 learning rate:  0.01097181729834791   Training loss:   45.919869809702334  Valing loss:   36.80270109893064\n",
      "Pure loss: 43.433100463604916.....Total loss: 43.433100463604916\n",
      "Pure loss: 35.499140833402414.....Total loss: 35.499140833402414\n",
      "epoch 1030 learning rate:  0.010970873786407768   Training loss:   43.433100463604916  Valing loss:   35.499140833402414\n",
      "Pure loss: 42.73262744287567.....Total loss: 42.73262744287567\n",
      "Pure loss: 35.18313691958623.....Total loss: 35.18313691958623\n",
      "epoch 1031 learning rate:  0.010969932104752668   Training loss:   42.73262744287567  Valing loss:   35.18313691958623\n",
      "Pure loss: 41.77375429204074.....Total loss: 41.77375429204074\n",
      "Pure loss: 35.004601614646546.....Total loss: 35.004601614646546\n",
      "epoch 1032 learning rate:  0.010968992248062016   Training loss:   41.77375429204074  Valing loss:   35.004601614646546\n",
      "Pure loss: 41.930859719676846.....Total loss: 41.930859719676846\n",
      "Pure loss: 35.01526763843506.....Total loss: 35.01526763843506\n",
      "epoch 1033 learning rate:  0.010968054211035819   Training loss:   41.930859719676846  Valing loss:   35.01526763843506\n",
      "Pure loss: 42.046689192350584.....Total loss: 42.046689192350584\n",
      "Pure loss: 35.04315322562453.....Total loss: 35.04315322562453\n",
      "epoch 1034 learning rate:  0.010967117988394585   Training loss:   42.046689192350584  Valing loss:   35.04315322562453\n",
      "Pure loss: 41.85992513608568.....Total loss: 41.85992513608568\n",
      "Pure loss: 35.02410272631407.....Total loss: 35.02410272631407\n",
      "epoch 1035 learning rate:  0.010966183574879227   Training loss:   41.85992513608568  Valing loss:   35.02410272631407\n",
      "Pure loss: 42.24184762484354.....Total loss: 42.24184762484354\n",
      "Pure loss: 35.11321269267193.....Total loss: 35.11321269267193\n",
      "epoch 1036 learning rate:  0.010965250965250965   Training loss:   42.24184762484354  Valing loss:   35.11321269267193\n",
      "Pure loss: 43.03964651387138.....Total loss: 43.03964651387138\n",
      "Pure loss: 35.34116624522231.....Total loss: 35.34116624522231\n",
      "epoch 1037 learning rate:  0.010964320154291225   Training loss:   43.03964651387138  Valing loss:   35.34116624522231\n",
      "Pure loss: 41.68556114702371.....Total loss: 41.68556114702371\n",
      "Pure loss: 34.890945262249225.....Total loss: 34.890945262249225\n",
      "epoch 1038 learning rate:  0.010963391136801542   Training loss:   41.68556114702371  Valing loss:   34.890945262249225\n",
      "Pure loss: 41.88617076684696.....Total loss: 41.88617076684696\n",
      "Pure loss: 34.92969440566738.....Total loss: 34.92969440566738\n",
      "epoch 1039 learning rate:  0.010962463907603465   Training loss:   41.88617076684696  Valing loss:   34.92969440566738\n",
      "Pure loss: 41.929599806928245.....Total loss: 41.929599806928245\n",
      "Pure loss: 34.9362640355002.....Total loss: 34.9362640355002\n",
      "epoch 1040 learning rate:  0.010961538461538462   Training loss:   41.929599806928245  Valing loss:   34.9362640355002\n",
      "Pure loss: 42.32750779409814.....Total loss: 42.32750779409814\n",
      "Pure loss: 35.0671963072882.....Total loss: 35.0671963072882\n",
      "epoch 1041 learning rate:  0.01096061479346782   Training loss:   42.32750779409814  Valing loss:   35.0671963072882\n",
      "Pure loss: 41.29606335334735.....Total loss: 41.29606335334735\n",
      "Pure loss: 34.88012216856331.....Total loss: 34.88012216856331\n",
      "epoch 1042 learning rate:  0.010959692898272554   Training loss:   41.29606335334735  Valing loss:   34.88012216856331\n",
      "Pure loss: 42.79768325837454.....Total loss: 42.79768325837454\n",
      "Pure loss: 35.12377530782788.....Total loss: 35.12377530782788\n",
      "epoch 1043 learning rate:  0.010958772770853309   Training loss:   42.79768325837454  Valing loss:   35.12377530782788\n",
      "Pure loss: 41.83563056923398.....Total loss: 41.83563056923398\n",
      "Pure loss: 34.767112771873144.....Total loss: 34.767112771873144\n",
      "epoch 1044 learning rate:  0.010957854406130268   Training loss:   41.83563056923398  Valing loss:   34.767112771873144\n",
      "Pure loss: 41.41217189871317.....Total loss: 41.41217189871317\n",
      "Pure loss: 34.75062668344492.....Total loss: 34.75062668344492\n",
      "epoch 1045 learning rate:  0.010956937799043063   Training loss:   41.41217189871317  Valing loss:   34.75062668344492\n",
      "Pure loss: 38.897307043089796.....Total loss: 38.897307043089796\n",
      "Pure loss: 35.85037179853932.....Total loss: 35.85037179853932\n",
      "epoch 1046 learning rate:  0.01095602294455067   Training loss:   38.897307043089796  Valing loss:   35.85037179853932\n",
      "Pure loss: 38.91588895490611.....Total loss: 38.91588895490611\n",
      "Pure loss: 36.0510258032751.....Total loss: 36.0510258032751\n",
      "epoch 1047 learning rate:  0.010955109837631328   Training loss:   38.91588895490611  Valing loss:   36.0510258032751\n",
      "Pure loss: 38.92603240455532.....Total loss: 38.92603240455532\n",
      "Pure loss: 36.07675421306106.....Total loss: 36.07675421306106\n",
      "epoch 1048 learning rate:  0.010954198473282443   Training loss:   38.92603240455532  Valing loss:   36.07675421306106\n",
      "Pure loss: 38.90668252996053.....Total loss: 38.90668252996053\n",
      "Pure loss: 36.02699447610665.....Total loss: 36.02699447610665\n",
      "epoch 1049 learning rate:  0.010953288846520496   Training loss:   38.90668252996053  Valing loss:   36.02699447610665\n",
      "Pure loss: 39.00304561640934.....Total loss: 39.00304561640934\n",
      "Pure loss: 36.23289390579978.....Total loss: 36.23289390579978\n",
      "epoch 1050 learning rate:  0.010952380952380953   Training loss:   39.00304561640934  Valing loss:   36.23289390579978\n",
      "Pure loss: 38.96323629314437.....Total loss: 38.96323629314437\n",
      "Pure loss: 36.01055608008997.....Total loss: 36.01055608008997\n",
      "epoch 1051 learning rate:  0.010951474785918174   Training loss:   38.96323629314437  Valing loss:   36.01055608008997\n",
      "Pure loss: 39.08541361483405.....Total loss: 39.08541361483405\n",
      "Pure loss: 34.98258744275875.....Total loss: 34.98258744275875\n",
      "epoch 1052 learning rate:  0.010950570342205323   Training loss:   39.08541361483405  Valing loss:   34.98258744275875\n",
      "Pure loss: 39.12307321261487.....Total loss: 39.12307321261487\n",
      "Pure loss: 35.11193184048325.....Total loss: 35.11193184048325\n",
      "epoch 1053 learning rate:  0.010949667616334284   Training loss:   39.12307321261487  Valing loss:   35.11193184048325\n",
      "Pure loss: 39.068281830071555.....Total loss: 39.068281830071555\n",
      "Pure loss: 34.90574316243076.....Total loss: 34.90574316243076\n",
      "epoch 1054 learning rate:  0.01094876660341556   Training loss:   39.068281830071555  Valing loss:   34.90574316243076\n",
      "Pure loss: 39.14137840572492.....Total loss: 39.14137840572492\n",
      "Pure loss: 34.74063396678673.....Total loss: 34.74063396678673\n",
      "epoch 1055 learning rate:  0.010947867298578199   Training loss:   39.14137840572492  Valing loss:   34.74063396678673\n",
      "Pure loss: 38.988895883959295.....Total loss: 38.988895883959295\n",
      "Pure loss: 35.01598454813881.....Total loss: 35.01598454813881\n",
      "epoch 1056 learning rate:  0.010946969696969698   Training loss:   38.988895883959295  Valing loss:   35.01598454813881\n",
      "Pure loss: 38.99222910646059.....Total loss: 38.99222910646059\n",
      "Pure loss: 34.95967415361122.....Total loss: 34.95967415361122\n",
      "epoch 1057 learning rate:  0.010946073793755912   Training loss:   38.99222910646059  Valing loss:   34.95967415361122\n",
      "Pure loss: 38.86036713801565.....Total loss: 38.86036713801565\n",
      "Pure loss: 35.294588307866256.....Total loss: 35.294588307866256\n",
      "epoch 1058 learning rate:  0.010945179584120984   Training loss:   38.86036713801565  Valing loss:   35.294588307866256\n",
      "Pure loss: 39.2124714002453.....Total loss: 39.2124714002453\n",
      "Pure loss: 34.419210568383214.....Total loss: 34.419210568383214\n",
      "epoch 1059 learning rate:  0.010944287063267234   Training loss:   39.2124714002453  Valing loss:   34.419210568383214\n",
      "Pure loss: 39.20516734998438.....Total loss: 39.20516734998438\n",
      "Pure loss: 34.428480912668775.....Total loss: 34.428480912668775\n",
      "epoch 1060 learning rate:  0.010943396226415094   Training loss:   39.20516734998438  Valing loss:   34.428480912668775\n",
      "Pure loss: 39.19904843040201.....Total loss: 39.19904843040201\n",
      "Pure loss: 34.5623329989893.....Total loss: 34.5623329989893\n",
      "epoch 1061 learning rate:  0.010942507068803016   Training loss:   39.19904843040201  Valing loss:   34.5623329989893\n",
      "Pure loss: 39.285028468062116.....Total loss: 39.285028468062116\n",
      "Pure loss: 34.442782300941566.....Total loss: 34.442782300941566\n",
      "epoch 1062 learning rate:  0.010941619585687382   Training loss:   39.285028468062116  Valing loss:   34.442782300941566\n",
      "Pure loss: 39.29444998502172.....Total loss: 39.29444998502172\n",
      "Pure loss: 34.354936747328615.....Total loss: 34.354936747328615\n",
      "epoch 1063 learning rate:  0.010940733772342427   Training loss:   39.29444998502172  Valing loss:   34.354936747328615\n",
      "Pure loss: 39.07037369506192.....Total loss: 39.07037369506192\n",
      "Pure loss: 34.64653592693545.....Total loss: 34.64653592693545\n",
      "epoch 1064 learning rate:  0.01093984962406015   Training loss:   39.07037369506192  Valing loss:   34.64653592693545\n",
      "Pure loss: 39.01328661762965.....Total loss: 39.01328661762965\n",
      "Pure loss: 34.802819777432845.....Total loss: 34.802819777432845\n",
      "epoch 1065 learning rate:  0.010938967136150234   Training loss:   39.01328661762965  Valing loss:   34.802819777432845\n",
      "Pure loss: 38.67333673498341.....Total loss: 38.67333673498341\n",
      "Pure loss: 34.33077905405096.....Total loss: 34.33077905405096\n",
      "epoch 1066 learning rate:  0.010938086303939963   Training loss:   38.67333673498341  Valing loss:   34.33077905405096\n",
      "Pure loss: 38.61513770460097.....Total loss: 38.61513770460097\n",
      "Pure loss: 34.45938185915872.....Total loss: 34.45938185915872\n",
      "epoch 1067 learning rate:  0.010937207122774134   Training loss:   38.61513770460097  Valing loss:   34.45938185915872\n",
      "Pure loss: 38.678785096644454.....Total loss: 38.678785096644454\n",
      "Pure loss: 34.31624649871373.....Total loss: 34.31624649871373\n",
      "epoch 1068 learning rate:  0.010936329588014982   Training loss:   38.678785096644454  Valing loss:   34.31624649871373\n",
      "Pure loss: 39.13401550925722.....Total loss: 39.13401550925722\n",
      "Pure loss: 34.03031708000151.....Total loss: 34.03031708000151\n",
      "epoch 1069 learning rate:  0.010935453695042096   Training loss:   39.13401550925722  Valing loss:   34.03031708000151\n",
      "Pure loss: 39.435842020539.....Total loss: 39.435842020539\n",
      "Pure loss: 34.125119467730464.....Total loss: 34.125119467730464\n",
      "epoch 1070 learning rate:  0.010934579439252337   Training loss:   39.435842020539  Valing loss:   34.125119467730464\n",
      "Pure loss: 39.55601362506747.....Total loss: 39.55601362506747\n",
      "Pure loss: 34.10800742937022.....Total loss: 34.10800742937022\n",
      "epoch 1071 learning rate:  0.010933706816059758   Training loss:   39.55601362506747  Valing loss:   34.10800742937022\n",
      "Pure loss: 39.593072136906706.....Total loss: 39.593072136906706\n",
      "Pure loss: 34.14972530908687.....Total loss: 34.14972530908687\n",
      "epoch 1072 learning rate:  0.010932835820895522   Training loss:   39.593072136906706  Valing loss:   34.14972530908687\n",
      "Pure loss: 39.26772507391434.....Total loss: 39.26772507391434\n",
      "Pure loss: 34.10252875512834.....Total loss: 34.10252875512834\n",
      "epoch 1073 learning rate:  0.010931966449207828   Training loss:   39.26772507391434  Valing loss:   34.10252875512834\n",
      "Pure loss: 39.4167291164332.....Total loss: 39.4167291164332\n",
      "Pure loss: 34.11382490476081.....Total loss: 34.11382490476081\n",
      "epoch 1074 learning rate:  0.010931098696461826   Training loss:   39.4167291164332  Valing loss:   34.11382490476081\n",
      "Pure loss: 39.70553233889704.....Total loss: 39.70553233889704\n",
      "Pure loss: 34.16655332711569.....Total loss: 34.16655332711569\n",
      "epoch 1075 learning rate:  0.010930232558139534   Training loss:   39.70553233889704  Valing loss:   34.16655332711569\n",
      "Pure loss: 39.68673464765648.....Total loss: 39.68673464765648\n",
      "Pure loss: 34.154149136507094.....Total loss: 34.154149136507094\n",
      "epoch 1076 learning rate:  0.010929368029739777   Training loss:   39.68673464765648  Valing loss:   34.154149136507094\n",
      "Pure loss: 39.57091556769747.....Total loss: 39.57091556769747\n",
      "Pure loss: 33.92623505988959.....Total loss: 33.92623505988959\n",
      "epoch 1077 learning rate:  0.010928505106778088   Training loss:   39.57091556769747  Valing loss:   33.92623505988959\n",
      "Pure loss: 39.531792070604446.....Total loss: 39.531792070604446\n",
      "Pure loss: 33.911338899698386.....Total loss: 33.911338899698386\n",
      "epoch 1078 learning rate:  0.010927643784786643   Training loss:   39.531792070604446  Valing loss:   33.911338899698386\n",
      "Pure loss: 39.93424476280048.....Total loss: 39.93424476280048\n",
      "Pure loss: 34.09868903599648.....Total loss: 34.09868903599648\n",
      "epoch 1079 learning rate:  0.01092678405931418   Training loss:   39.93424476280048  Valing loss:   34.09868903599648\n",
      "Pure loss: 39.06638028310796.....Total loss: 39.06638028310796\n",
      "Pure loss: 34.15254895919685.....Total loss: 34.15254895919685\n",
      "epoch 1080 learning rate:  0.010925925925925926   Training loss:   39.06638028310796  Valing loss:   34.15254895919685\n",
      "Pure loss: 39.02202337118965.....Total loss: 39.02202337118965\n",
      "Pure loss: 34.17959175644531.....Total loss: 34.17959175644531\n",
      "epoch 1081 learning rate:  0.010925069380203515   Training loss:   39.02202337118965  Valing loss:   34.17959175644531\n",
      "Pure loss: 38.98506000051821.....Total loss: 38.98506000051821\n",
      "Pure loss: 34.16808435271122.....Total loss: 34.16808435271122\n",
      "epoch 1082 learning rate:  0.010924214417744917   Training loss:   38.98506000051821  Valing loss:   34.16808435271122\n",
      "Pure loss: 39.561969858772116.....Total loss: 39.561969858772116\n",
      "Pure loss: 34.07872535202863.....Total loss: 34.07872535202863\n",
      "epoch 1083 learning rate:  0.010923361034164358   Training loss:   39.561969858772116  Valing loss:   34.07872535202863\n",
      "Pure loss: 39.2953972582507.....Total loss: 39.2953972582507\n",
      "Pure loss: 33.93055346170186.....Total loss: 33.93055346170186\n",
      "epoch 1084 learning rate:  0.01092250922509225   Training loss:   39.2953972582507  Valing loss:   33.93055346170186\n",
      "Pure loss: 39.03032824969325.....Total loss: 39.03032824969325\n",
      "Pure loss: 33.97239059447665.....Total loss: 33.97239059447665\n",
      "epoch 1085 learning rate:  0.010921658986175115   Training loss:   39.03032824969325  Valing loss:   33.97239059447665\n",
      "Pure loss: 38.260477834805066.....Total loss: 38.260477834805066\n",
      "Pure loss: 34.771003808483414.....Total loss: 34.771003808483414\n",
      "epoch 1086 learning rate:  0.010920810313075506   Training loss:   38.260477834805066  Valing loss:   34.771003808483414\n",
      "Pure loss: 38.21604574966682.....Total loss: 38.21604574966682\n",
      "Pure loss: 35.0764565806165.....Total loss: 35.0764565806165\n",
      "epoch 1087 learning rate:  0.010919963201471941   Training loss:   38.21604574966682  Valing loss:   35.0764565806165\n",
      "Pure loss: 38.212209596994064.....Total loss: 38.212209596994064\n",
      "Pure loss: 35.03642832750539.....Total loss: 35.03642832750539\n",
      "epoch 1088 learning rate:  0.010919117647058824   Training loss:   38.212209596994064  Valing loss:   35.03642832750539\n",
      "Pure loss: 38.21074731380155.....Total loss: 38.21074731380155\n",
      "Pure loss: 35.085523387274435.....Total loss: 35.085523387274435\n",
      "epoch 1089 learning rate:  0.010918273645546372   Training loss:   38.21074731380155  Valing loss:   35.085523387274435\n",
      "Pure loss: 38.286544437721616.....Total loss: 38.286544437721616\n",
      "Pure loss: 35.5095826163321.....Total loss: 35.5095826163321\n",
      "epoch 1090 learning rate:  0.01091743119266055   Training loss:   38.286544437721616  Valing loss:   35.5095826163321\n",
      "Pure loss: 38.689563073642475.....Total loss: 38.689563073642475\n",
      "Pure loss: 36.73684668577857.....Total loss: 36.73684668577857\n",
      "epoch 1091 learning rate:  0.010916590284142989   Training loss:   38.689563073642475  Valing loss:   36.73684668577857\n",
      "Pure loss: 38.707922022729576.....Total loss: 38.707922022729576\n",
      "Pure loss: 36.79030959016528.....Total loss: 36.79030959016528\n",
      "epoch 1092 learning rate:  0.010915750915750916   Training loss:   38.707922022729576  Valing loss:   36.79030959016528\n",
      "Pure loss: 38.85531622531273.....Total loss: 38.85531622531273\n",
      "Pure loss: 37.04591770265156.....Total loss: 37.04591770265156\n",
      "epoch 1093 learning rate:  0.01091491308325709   Training loss:   38.85531622531273  Valing loss:   37.04591770265156\n",
      "Pure loss: 38.7141224871641.....Total loss: 38.7141224871641\n",
      "Pure loss: 36.89358562287197.....Total loss: 36.89358562287197\n",
      "epoch 1094 learning rate:  0.010914076782449727   Training loss:   38.7141224871641  Valing loss:   36.89358562287197\n",
      "Pure loss: 38.51518927845802.....Total loss: 38.51518927845802\n",
      "Pure loss: 36.242877674693865.....Total loss: 36.242877674693865\n",
      "epoch 1095 learning rate:  0.01091324200913242   Training loss:   38.51518927845802  Valing loss:   36.242877674693865\n",
      "Pure loss: 38.38010567696128.....Total loss: 38.38010567696128\n",
      "Pure loss: 35.96207597179252.....Total loss: 35.96207597179252\n",
      "epoch 1096 learning rate:  0.010912408759124088   Training loss:   38.38010567696128  Valing loss:   35.96207597179252\n",
      "Pure loss: 38.437720808532525.....Total loss: 38.437720808532525\n",
      "Pure loss: 36.15175495427881.....Total loss: 36.15175495427881\n",
      "epoch 1097 learning rate:  0.010911577028258889   Training loss:   38.437720808532525  Valing loss:   36.15175495427881\n",
      "Pure loss: 38.181213305785114.....Total loss: 38.181213305785114\n",
      "Pure loss: 34.7618080786368.....Total loss: 34.7618080786368\n",
      "epoch 1098 learning rate:  0.010910746812386157   Training loss:   38.181213305785114  Valing loss:   34.7618080786368\n",
      "Pure loss: 38.161095517186176.....Total loss: 38.161095517186176\n",
      "Pure loss: 34.64311747933055.....Total loss: 34.64311747933055\n",
      "epoch 1099 learning rate:  0.010909918107370338   Training loss:   38.161095517186176  Valing loss:   34.64311747933055\n",
      "Pure loss: 38.3051674439111.....Total loss: 38.3051674439111\n",
      "Pure loss: 35.22353628305491.....Total loss: 35.22353628305491\n",
      "epoch 1100 learning rate:  0.01090909090909091   Training loss:   38.3051674439111  Valing loss:   35.22353628305491\n",
      "Pure loss: 38.17856739684015.....Total loss: 38.17856739684015\n",
      "Pure loss: 34.9358995753383.....Total loss: 34.9358995753383\n",
      "epoch 1101 learning rate:  0.010908265213442325   Training loss:   38.17856739684015  Valing loss:   34.9358995753383\n",
      "Pure loss: 38.41971590337465.....Total loss: 38.41971590337465\n",
      "Pure loss: 36.06187360899384.....Total loss: 36.06187360899384\n",
      "epoch 1102 learning rate:  0.010907441016333938   Training loss:   38.41971590337465  Valing loss:   36.06187360899384\n",
      "Pure loss: 39.23361067865127.....Total loss: 39.23361067865127\n",
      "Pure loss: 38.06498210993143.....Total loss: 38.06498210993143\n",
      "epoch 1103 learning rate:  0.010906618313689937   Training loss:   39.23361067865127  Valing loss:   38.06498210993143\n",
      "Pure loss: 40.565083912921054.....Total loss: 40.565083912921054\n",
      "Pure loss: 40.709261966943295.....Total loss: 40.709261966943295\n",
      "epoch 1104 learning rate:  0.010905797101449276   Training loss:   40.565083912921054  Valing loss:   40.709261966943295\n",
      "Pure loss: 39.31871764376936.....Total loss: 39.31871764376936\n",
      "Pure loss: 38.68759087075641.....Total loss: 38.68759087075641\n",
      "epoch 1105 learning rate:  0.010904977375565611   Training loss:   39.31871764376936  Valing loss:   38.68759087075641\n",
      "Pure loss: 38.29605510546468.....Total loss: 38.29605510546468\n",
      "Pure loss: 36.44382496035918.....Total loss: 36.44382496035918\n",
      "epoch 1106 learning rate:  0.010904159132007233   Training loss:   38.29605510546468  Valing loss:   36.44382496035918\n",
      "Pure loss: 38.14818619071791.....Total loss: 38.14818619071791\n",
      "Pure loss: 36.08799675159695.....Total loss: 36.08799675159695\n",
      "epoch 1107 learning rate:  0.010903342366757001   Training loss:   38.14818619071791  Valing loss:   36.08799675159695\n",
      "Pure loss: 37.91710636728809.....Total loss: 37.91710636728809\n",
      "Pure loss: 35.76448905677179.....Total loss: 35.76448905677179\n",
      "epoch 1108 learning rate:  0.010902527075812275   Training loss:   37.91710636728809  Valing loss:   35.76448905677179\n",
      "Pure loss: 37.85759796976727.....Total loss: 37.85759796976727\n",
      "Pure loss: 35.54315291875918.....Total loss: 35.54315291875918\n",
      "epoch 1109 learning rate:  0.010901713255184851   Training loss:   37.85759796976727  Valing loss:   35.54315291875918\n",
      "Pure loss: 37.765669679931726.....Total loss: 37.765669679931726\n",
      "Pure loss: 34.889846175161225.....Total loss: 34.889846175161225\n",
      "epoch 1110 learning rate:  0.0109009009009009   Training loss:   37.765669679931726  Valing loss:   34.889846175161225\n",
      "Pure loss: 37.77625368317555.....Total loss: 37.77625368317555\n",
      "Pure loss: 34.8358115054133.....Total loss: 34.8358115054133\n",
      "epoch 1111 learning rate:  0.0109000900090009   Training loss:   37.77625368317555  Valing loss:   34.8358115054133\n",
      "Pure loss: 37.77985423887197.....Total loss: 37.77985423887197\n",
      "Pure loss: 34.490326676393764.....Total loss: 34.490326676393764\n",
      "epoch 1112 learning rate:  0.010899280575539568   Training loss:   37.77985423887197  Valing loss:   34.490326676393764\n",
      "Pure loss: 38.23895372843061.....Total loss: 38.23895372843061\n",
      "Pure loss: 33.50903062811989.....Total loss: 33.50903062811989\n",
      "epoch 1113 learning rate:  0.010898472596585804   Training loss:   38.23895372843061  Valing loss:   33.50903062811989\n",
      "Pure loss: 37.83952761922931.....Total loss: 37.83952761922931\n",
      "Pure loss: 33.56373708513761.....Total loss: 33.56373708513761\n",
      "epoch 1114 learning rate:  0.010897666068222622   Training loss:   37.83952761922931  Valing loss:   33.56373708513761\n",
      "Pure loss: 37.838250248855054.....Total loss: 37.838250248855054\n",
      "Pure loss: 33.56676043731164.....Total loss: 33.56676043731164\n",
      "epoch 1115 learning rate:  0.010896860986547085   Training loss:   37.838250248855054  Valing loss:   33.56676043731164\n",
      "Pure loss: 37.6177401843592.....Total loss: 37.6177401843592\n",
      "Pure loss: 34.36751127568663.....Total loss: 34.36751127568663\n",
      "epoch 1116 learning rate:  0.010896057347670251   Training loss:   37.6177401843592  Valing loss:   34.36751127568663\n",
      "Pure loss: 37.61760218004417.....Total loss: 37.61760218004417\n",
      "Pure loss: 34.23101277112452.....Total loss: 34.23101277112452\n",
      "epoch 1117 learning rate:  0.0108952551477171   Training loss:   37.61760218004417  Valing loss:   34.23101277112452\n",
      "Pure loss: 37.74821814793929.....Total loss: 37.74821814793929\n",
      "Pure loss: 33.484704997850905.....Total loss: 33.484704997850905\n",
      "epoch 1118 learning rate:  0.010894454382826477   Training loss:   37.74821814793929  Valing loss:   33.484704997850905\n",
      "Pure loss: 37.68989345657574.....Total loss: 37.68989345657574\n",
      "Pure loss: 33.59618466177775.....Total loss: 33.59618466177775\n",
      "epoch 1119 learning rate:  0.010893655049151027   Training loss:   37.68989345657574  Valing loss:   33.59618466177775\n",
      "Pure loss: 37.95149139353561.....Total loss: 37.95149139353561\n",
      "Pure loss: 33.322783986939505.....Total loss: 33.322783986939505\n",
      "epoch 1120 learning rate:  0.010892857142857143   Training loss:   37.95149139353561  Valing loss:   33.322783986939505\n",
      "Pure loss: 37.677718084382846.....Total loss: 37.677718084382846\n",
      "Pure loss: 33.50942387676417.....Total loss: 33.50942387676417\n",
      "epoch 1121 learning rate:  0.010892060660124889   Training loss:   37.677718084382846  Valing loss:   33.50942387676417\n",
      "Pure loss: 37.668612794646926.....Total loss: 37.668612794646926\n",
      "Pure loss: 33.52951078828064.....Total loss: 33.52951078828064\n",
      "epoch 1122 learning rate:  0.01089126559714795   Training loss:   37.668612794646926  Valing loss:   33.52951078828064\n",
      "Pure loss: 37.51465119223596.....Total loss: 37.51465119223596\n",
      "Pure loss: 34.3750868727651.....Total loss: 34.3750868727651\n",
      "epoch 1123 learning rate:  0.010890471950133571   Training loss:   37.51465119223596  Valing loss:   34.3750868727651\n",
      "Pure loss: 37.50720027950206.....Total loss: 37.50720027950206\n",
      "Pure loss: 34.290900783675106.....Total loss: 34.290900783675106\n",
      "epoch 1124 learning rate:  0.010889679715302492   Training loss:   37.50720027950206  Valing loss:   34.290900783675106\n",
      "Pure loss: 37.5109364814297.....Total loss: 37.5109364814297\n",
      "Pure loss: 34.372942240070735.....Total loss: 34.372942240070735\n",
      "epoch 1125 learning rate:  0.010888888888888889   Training loss:   37.5109364814297  Valing loss:   34.372942240070735\n",
      "Pure loss: 37.506775181085835.....Total loss: 37.506775181085835\n",
      "Pure loss: 34.295014698654875.....Total loss: 34.295014698654875\n",
      "epoch 1126 learning rate:  0.01088809946714032   Training loss:   37.506775181085835  Valing loss:   34.295014698654875\n",
      "Pure loss: 37.50520225411001.....Total loss: 37.50520225411001\n",
      "Pure loss: 34.23793346510199.....Total loss: 34.23793346510199\n",
      "epoch 1127 learning rate:  0.010887311446317658   Training loss:   37.50520225411001  Valing loss:   34.23793346510199\n",
      "Pure loss: 37.49743621926367.....Total loss: 37.49743621926367\n",
      "Pure loss: 34.589488613518434.....Total loss: 34.589488613518434\n",
      "epoch 1128 learning rate:  0.010886524822695035   Training loss:   37.49743621926367  Valing loss:   34.589488613518434\n",
      "Pure loss: 37.79512800182389.....Total loss: 37.79512800182389\n",
      "Pure loss: 35.960860387727166.....Total loss: 35.960860387727166\n",
      "epoch 1129 learning rate:  0.010885739592559788   Training loss:   37.79512800182389  Valing loss:   35.960860387727166\n",
      "Pure loss: 37.55727859536371.....Total loss: 37.55727859536371\n",
      "Pure loss: 35.28252809945463.....Total loss: 35.28252809945463\n",
      "epoch 1130 learning rate:  0.01088495575221239   Training loss:   37.55727859536371  Valing loss:   35.28252809945463\n",
      "Pure loss: 37.4440830914013.....Total loss: 37.4440830914013\n",
      "Pure loss: 34.726480600924106.....Total loss: 34.726480600924106\n",
      "epoch 1131 learning rate:  0.010884173297966402   Training loss:   37.4440830914013  Valing loss:   34.726480600924106\n",
      "Pure loss: 37.41538166040545.....Total loss: 37.41538166040545\n",
      "Pure loss: 34.288353754655496.....Total loss: 34.288353754655496\n",
      "epoch 1132 learning rate:  0.01088339222614841   Training loss:   37.41538166040545  Valing loss:   34.288353754655496\n",
      "Pure loss: 37.41742635787878.....Total loss: 37.41742635787878\n",
      "Pure loss: 34.214588738338.....Total loss: 34.214588738338\n",
      "epoch 1133 learning rate:  0.01088261253309797   Training loss:   37.41742635787878  Valing loss:   34.214588738338\n",
      "Pure loss: 37.50436655095238.....Total loss: 37.50436655095238\n",
      "Pure loss: 33.8385578459703.....Total loss: 33.8385578459703\n",
      "epoch 1134 learning rate:  0.010881834215167549   Training loss:   37.50436655095238  Valing loss:   33.8385578459703\n",
      "Pure loss: 37.458791025651585.....Total loss: 37.458791025651585\n",
      "Pure loss: 33.94433691080897.....Total loss: 33.94433691080897\n",
      "epoch 1135 learning rate:  0.010881057268722467   Training loss:   37.458791025651585  Valing loss:   33.94433691080897\n",
      "Pure loss: 37.645635824069416.....Total loss: 37.645635824069416\n",
      "Pure loss: 33.501902156708965.....Total loss: 33.501902156708965\n",
      "epoch 1136 learning rate:  0.010880281690140846   Training loss:   37.645635824069416  Valing loss:   33.501902156708965\n",
      "Pure loss: 37.89103635041067.....Total loss: 37.89103635041067\n",
      "Pure loss: 33.282498024546875.....Total loss: 33.282498024546875\n",
      "epoch 1137 learning rate:  0.010879507475813545   Training loss:   37.89103635041067  Valing loss:   33.282498024546875\n",
      "Pure loss: 37.34190733023531.....Total loss: 37.34190733023531\n",
      "Pure loss: 33.60623825522975.....Total loss: 33.60623825522975\n",
      "epoch 1138 learning rate:  0.010878734622144113   Training loss:   37.34190733023531  Valing loss:   33.60623825522975\n",
      "Pure loss: 38.155494808465846.....Total loss: 38.155494808465846\n",
      "Pure loss: 32.747473770219415.....Total loss: 32.747473770219415\n",
      "epoch 1139 learning rate:  0.010877963125548727   Training loss:   38.155494808465846  Valing loss:   32.747473770219415\n",
      "Pure loss: 36.98133336388097.....Total loss: 36.98133336388097\n",
      "Pure loss: 32.98181337510063.....Total loss: 32.98181337510063\n",
      "epoch 1140 learning rate:  0.01087719298245614   Training loss:   36.98133336388097  Valing loss:   32.98181337510063\n",
      "Pure loss: 36.98042337383863.....Total loss: 36.98042337383863\n",
      "Pure loss: 32.985562680525724.....Total loss: 32.985562680525724\n",
      "epoch 1141 learning rate:  0.010876424189307624   Training loss:   36.98042337383863  Valing loss:   32.985562680525724\n",
      "Pure loss: 36.934120327437206.....Total loss: 36.934120327437206\n",
      "Pure loss: 33.11607238028228.....Total loss: 33.11607238028228\n",
      "epoch 1142 learning rate:  0.010875656742556918   Training loss:   36.934120327437206  Valing loss:   33.11607238028228\n",
      "Pure loss: 36.950295028508066.....Total loss: 36.950295028508066\n",
      "Pure loss: 33.17871665089237.....Total loss: 33.17871665089237\n",
      "epoch 1143 learning rate:  0.010874890638670167   Training loss:   36.950295028508066  Valing loss:   33.17871665089237\n",
      "Pure loss: 36.94182670434427.....Total loss: 36.94182670434427\n",
      "Pure loss: 33.15802607907217.....Total loss: 33.15802607907217\n",
      "epoch 1144 learning rate:  0.010874125874125874   Training loss:   36.94182670434427  Valing loss:   33.15802607907217\n",
      "Pure loss: 37.19820376576366.....Total loss: 37.19820376576366\n",
      "Pure loss: 32.546188757717935.....Total loss: 32.546188757717935\n",
      "epoch 1145 learning rate:  0.010873362445414847   Training loss:   37.19820376576366  Valing loss:   32.546188757717935\n",
      "Pure loss: 37.20174589782356.....Total loss: 37.20174589782356\n",
      "Pure loss: 32.54033187088877.....Total loss: 32.54033187088877\n",
      "epoch 1146 learning rate:  0.01087260034904014   Training loss:   37.20174589782356  Valing loss:   32.54033187088877\n",
      "Pure loss: 36.99562658580908.....Total loss: 36.99562658580908\n",
      "Pure loss: 32.75159050276502.....Total loss: 32.75159050276502\n",
      "epoch 1147 learning rate:  0.010871839581517   Training loss:   36.99562658580908  Valing loss:   32.75159050276502\n",
      "Pure loss: 36.894068150323456.....Total loss: 36.894068150323456\n",
      "Pure loss: 33.21125393061548.....Total loss: 33.21125393061548\n",
      "epoch 1148 learning rate:  0.010871080139372822   Training loss:   36.894068150323456  Valing loss:   33.21125393061548\n",
      "Pure loss: 37.192941478809395.....Total loss: 37.192941478809395\n",
      "Pure loss: 32.429718600872775.....Total loss: 32.429718600872775\n",
      "epoch 1149 learning rate:  0.010870322019147085   Training loss:   37.192941478809395  Valing loss:   32.429718600872775\n",
      "Pure loss: 37.49259576691269.....Total loss: 37.49259576691269\n",
      "Pure loss: 32.2257971624685.....Total loss: 32.2257971624685\n",
      "epoch 1150 learning rate:  0.010869565217391304   Training loss:   37.49259576691269  Valing loss:   32.2257971624685\n",
      "Pure loss: 37.319212003024475.....Total loss: 37.319212003024475\n",
      "Pure loss: 32.29385334687351.....Total loss: 32.29385334687351\n",
      "epoch 1151 learning rate:  0.010868809730668984   Training loss:   37.319212003024475  Valing loss:   32.29385334687351\n",
      "Pure loss: 36.74237481050463.....Total loss: 36.74237481050463\n",
      "Pure loss: 32.83786975167539.....Total loss: 32.83786975167539\n",
      "epoch 1152 learning rate:  0.010868055555555556   Training loss:   36.74237481050463  Valing loss:   32.83786975167539\n",
      "Pure loss: 36.73275009087902.....Total loss: 36.73275009087902\n",
      "Pure loss: 32.79361775207021.....Total loss: 32.79361775207021\n",
      "epoch 1153 learning rate:  0.010867302688638334   Training loss:   36.73275009087902  Valing loss:   32.79361775207021\n",
      "Pure loss: 36.72486031338649.....Total loss: 36.72486031338649\n",
      "Pure loss: 33.062109861865984.....Total loss: 33.062109861865984\n",
      "epoch 1154 learning rate:  0.010866551126516464   Training loss:   36.72486031338649  Valing loss:   33.062109861865984\n",
      "Pure loss: 36.72346625860204.....Total loss: 36.72346625860204\n",
      "Pure loss: 33.00375448376673.....Total loss: 33.00375448376673\n",
      "epoch 1155 learning rate:  0.010865800865800867   Training loss:   36.72346625860204  Valing loss:   33.00375448376673\n",
      "Pure loss: 36.6677269590071.....Total loss: 36.6677269590071\n",
      "Pure loss: 32.73266914770399.....Total loss: 32.73266914770399\n",
      "epoch 1156 learning rate:  0.010865051903114186   Training loss:   36.6677269590071  Valing loss:   32.73266914770399\n",
      "Pure loss: 36.800038260188316.....Total loss: 36.800038260188316\n",
      "Pure loss: 32.35495028749948.....Total loss: 32.35495028749948\n",
      "epoch 1157 learning rate:  0.010864304235090752   Training loss:   36.800038260188316  Valing loss:   32.35495028749948\n",
      "Pure loss: 37.465723192927065.....Total loss: 37.465723192927065\n",
      "Pure loss: 31.77467299002292.....Total loss: 31.77467299002292\n",
      "epoch 1158 learning rate:  0.010863557858376512   Training loss:   37.465723192927065  Valing loss:   31.77467299002292\n",
      "Pure loss: 38.40454732025861.....Total loss: 38.40454732025861\n",
      "Pure loss: 31.878249113312673.....Total loss: 31.878249113312673\n",
      "epoch 1159 learning rate:  0.01086281276962899   Training loss:   38.40454732025861  Valing loss:   31.878249113312673\n",
      "Pure loss: 38.887467256295174.....Total loss: 38.887467256295174\n",
      "Pure loss: 32.010957902497175.....Total loss: 32.010957902497175\n",
      "epoch 1160 learning rate:  0.010862068965517242   Training loss:   38.887467256295174  Valing loss:   32.010957902497175\n",
      "Pure loss: 39.998370100615816.....Total loss: 39.998370100615816\n",
      "Pure loss: 32.30352123224869.....Total loss: 32.30352123224869\n",
      "epoch 1161 learning rate:  0.010861326442721793   Training loss:   39.998370100615816  Valing loss:   32.30352123224869\n",
      "Pure loss: 39.68650654884292.....Total loss: 39.68650654884292\n",
      "Pure loss: 32.18773004622164.....Total loss: 32.18773004622164\n",
      "epoch 1162 learning rate:  0.010860585197934597   Training loss:   39.68650654884292  Valing loss:   32.18773004622164\n",
      "Pure loss: 40.053408281033754.....Total loss: 40.053408281033754\n",
      "Pure loss: 32.37658717093976.....Total loss: 32.37658717093976\n",
      "epoch 1163 learning rate:  0.010859845227858985   Training loss:   40.053408281033754  Valing loss:   32.37658717093976\n",
      "Pure loss: 38.4486079451993.....Total loss: 38.4486079451993\n",
      "Pure loss: 31.730077050906164.....Total loss: 31.730077050906164\n",
      "epoch 1164 learning rate:  0.010859106529209623   Training loss:   38.4486079451993  Valing loss:   31.730077050906164\n",
      "Pure loss: 38.72698437977272.....Total loss: 38.72698437977272\n",
      "Pure loss: 31.81769403432875.....Total loss: 31.81769403432875\n",
      "epoch 1165 learning rate:  0.010858369098712446   Training loss:   38.72698437977272  Valing loss:   31.81769403432875\n",
      "Pure loss: 39.62939497796473.....Total loss: 39.62939497796473\n",
      "Pure loss: 32.14122195902588.....Total loss: 32.14122195902588\n",
      "epoch 1166 learning rate:  0.01085763293310463   Training loss:   39.62939497796473  Valing loss:   32.14122195902588\n",
      "Pure loss: 39.93814628056396.....Total loss: 39.93814628056396\n",
      "Pure loss: 32.36941320542533.....Total loss: 32.36941320542533\n",
      "epoch 1167 learning rate:  0.010856898029134534   Training loss:   39.93814628056396  Valing loss:   32.36941320542533\n",
      "Pure loss: 39.51294425719782.....Total loss: 39.51294425719782\n",
      "Pure loss: 32.19620195199717.....Total loss: 32.19620195199717\n",
      "epoch 1168 learning rate:  0.010856164383561644   Training loss:   39.51294425719782  Valing loss:   32.19620195199717\n",
      "Pure loss: 37.736522342936915.....Total loss: 37.736522342936915\n",
      "Pure loss: 31.80688159277782.....Total loss: 31.80688159277782\n",
      "epoch 1169 learning rate:  0.010855431993156544   Training loss:   37.736522342936915  Valing loss:   31.80688159277782\n",
      "Pure loss: 37.0612533123296.....Total loss: 37.0612533123296\n",
      "Pure loss: 31.895545956117186.....Total loss: 31.895545956117186\n",
      "epoch 1170 learning rate:  0.010854700854700855   Training loss:   37.0612533123296  Valing loss:   31.895545956117186\n",
      "Pure loss: 37.200298594800714.....Total loss: 37.200298594800714\n",
      "Pure loss: 31.963112728890646.....Total loss: 31.963112728890646\n",
      "epoch 1171 learning rate:  0.01085397096498719   Training loss:   37.200298594800714  Valing loss:   31.963112728890646\n",
      "Pure loss: 37.19225657623551.....Total loss: 37.19225657623551\n",
      "Pure loss: 31.957277699692547.....Total loss: 31.957277699692547\n",
      "epoch 1172 learning rate:  0.010853242320819113   Training loss:   37.19225657623551  Valing loss:   31.957277699692547\n",
      "Pure loss: 36.39164711949933.....Total loss: 36.39164711949933\n",
      "Pure loss: 32.20234695578969.....Total loss: 32.20234695578969\n",
      "epoch 1173 learning rate:  0.010852514919011083   Training loss:   36.39164711949933  Valing loss:   32.20234695578969\n",
      "Pure loss: 36.19044640766338.....Total loss: 36.19044640766338\n",
      "Pure loss: 32.746947188899924.....Total loss: 32.746947188899924\n",
      "epoch 1174 learning rate:  0.010851788756388417   Training loss:   36.19044640766338  Valing loss:   32.746947188899924\n",
      "Pure loss: 36.17931002739117.....Total loss: 36.17931002739117\n",
      "Pure loss: 33.39291576383525.....Total loss: 33.39291576383525\n",
      "epoch 1175 learning rate:  0.010851063829787235   Training loss:   36.17931002739117  Valing loss:   33.39291576383525\n",
      "Pure loss: 36.110490968655796.....Total loss: 36.110490968655796\n",
      "Pure loss: 32.792670548315016.....Total loss: 32.792670548315016\n",
      "epoch 1176 learning rate:  0.010850340136054421   Training loss:   36.110490968655796  Valing loss:   32.792670548315016\n",
      "Pure loss: 36.117495656204404.....Total loss: 36.117495656204404\n",
      "Pure loss: 32.90536329113199.....Total loss: 32.90536329113199\n",
      "epoch 1177 learning rate:  0.01084961767204758   Training loss:   36.117495656204404  Valing loss:   32.90536329113199\n",
      "Pure loss: 36.156273151696205.....Total loss: 36.156273151696205\n",
      "Pure loss: 32.12371618034178.....Total loss: 32.12371618034178\n",
      "epoch 1178 learning rate:  0.010848896434634974   Training loss:   36.156273151696205  Valing loss:   32.12371618034178\n",
      "Pure loss: 36.282900674361144.....Total loss: 36.282900674361144\n",
      "Pure loss: 31.712585352787592.....Total loss: 31.712585352787592\n",
      "epoch 1179 learning rate:  0.010848176420695506   Training loss:   36.282900674361144  Valing loss:   31.712585352787592\n",
      "Pure loss: 36.43166527314814.....Total loss: 36.43166527314814\n",
      "Pure loss: 31.58008134518993.....Total loss: 31.58008134518993\n",
      "epoch 1180 learning rate:  0.010847457627118645   Training loss:   36.43166527314814  Valing loss:   31.58008134518993\n",
      "Pure loss: 36.17667950459592.....Total loss: 36.17667950459592\n",
      "Pure loss: 31.806825851109473.....Total loss: 31.806825851109473\n",
      "epoch 1181 learning rate:  0.010846740050804403   Training loss:   36.17667950459592  Valing loss:   31.806825851109473\n",
      "Pure loss: 36.113722436103636.....Total loss: 36.113722436103636\n",
      "Pure loss: 31.902310299992582.....Total loss: 31.902310299992582\n",
      "epoch 1182 learning rate:  0.010846023688663282   Training loss:   36.113722436103636  Valing loss:   31.902310299992582\n",
      "Pure loss: 36.562026358204974.....Total loss: 36.562026358204974\n",
      "Pure loss: 31.5418561031401.....Total loss: 31.5418561031401\n",
      "epoch 1183 learning rate:  0.01084530853761623   Training loss:   36.562026358204974  Valing loss:   31.5418561031401\n",
      "Pure loss: 36.52704549176678.....Total loss: 36.52704549176678\n",
      "Pure loss: 31.560206496344193.....Total loss: 31.560206496344193\n",
      "epoch 1184 learning rate:  0.010844594594594595   Training loss:   36.52704549176678  Valing loss:   31.560206496344193\n",
      "Pure loss: 36.18032996019548.....Total loss: 36.18032996019548\n",
      "Pure loss: 31.835655527136925.....Total loss: 31.835655527136925\n",
      "epoch 1185 learning rate:  0.010843881856540085   Training loss:   36.18032996019548  Valing loss:   31.835655527136925\n",
      "Pure loss: 36.28649394442984.....Total loss: 36.28649394442984\n",
      "Pure loss: 31.84443887810992.....Total loss: 31.84443887810992\n",
      "epoch 1186 learning rate:  0.010843170320404723   Training loss:   36.28649394442984  Valing loss:   31.84443887810992\n",
      "Pure loss: 36.32329716573255.....Total loss: 36.32329716573255\n",
      "Pure loss: 31.806466052137157.....Total loss: 31.806466052137157\n",
      "epoch 1187 learning rate:  0.0108424599831508   Training loss:   36.32329716573255  Valing loss:   31.806466052137157\n",
      "Pure loss: 35.83363561386471.....Total loss: 35.83363561386471\n",
      "Pure loss: 32.28344952796141.....Total loss: 32.28344952796141\n",
      "epoch 1188 learning rate:  0.010841750841750842   Training loss:   35.83363561386471  Valing loss:   32.28344952796141\n",
      "Pure loss: 35.87455059030253.....Total loss: 35.87455059030253\n",
      "Pure loss: 32.72998463107023.....Total loss: 32.72998463107023\n",
      "epoch 1189 learning rate:  0.010841042893187552   Training loss:   35.87455059030253  Valing loss:   32.72998463107023\n",
      "Pure loss: 36.058806363425646.....Total loss: 36.058806363425646\n",
      "Pure loss: 31.26267815788788.....Total loss: 31.26267815788788\n",
      "epoch 1190 learning rate:  0.010840336134453782   Training loss:   36.058806363425646  Valing loss:   31.26267815788788\n",
      "Pure loss: 36.11971565672538.....Total loss: 36.11971565672538\n",
      "Pure loss: 31.1935555338829.....Total loss: 31.1935555338829\n",
      "epoch 1191 learning rate:  0.010839630562552476   Training loss:   36.11971565672538  Valing loss:   31.1935555338829\n",
      "Pure loss: 36.50034755198585.....Total loss: 36.50034755198585\n",
      "Pure loss: 30.94628182868754.....Total loss: 30.94628182868754\n",
      "epoch 1192 learning rate:  0.010838926174496645   Training loss:   36.50034755198585  Valing loss:   30.94628182868754\n",
      "Pure loss: 35.9920482878752.....Total loss: 35.9920482878752\n",
      "Pure loss: 31.189248530247273.....Total loss: 31.189248530247273\n",
      "epoch 1193 learning rate:  0.010838222967309304   Training loss:   35.9920482878752  Valing loss:   31.189248530247273\n",
      "Pure loss: 35.93455809501702.....Total loss: 35.93455809501702\n",
      "Pure loss: 31.492811585205352.....Total loss: 31.492811585205352\n",
      "epoch 1194 learning rate:  0.01083752093802345   Training loss:   35.93455809501702  Valing loss:   31.492811585205352\n",
      "Pure loss: 35.95630920145038.....Total loss: 35.95630920145038\n",
      "Pure loss: 31.37001602077354.....Total loss: 31.37001602077354\n",
      "epoch 1195 learning rate:  0.010836820083682008   Training loss:   35.95630920145038  Valing loss:   31.37001602077354\n",
      "Pure loss: 36.033225710435154.....Total loss: 36.033225710435154\n",
      "Pure loss: 32.34391277040743.....Total loss: 32.34391277040743\n",
      "epoch 1196 learning rate:  0.010836120401337792   Training loss:   36.033225710435154  Valing loss:   32.34391277040743\n",
      "Pure loss: 35.733245443332095.....Total loss: 35.733245443332095\n",
      "Pure loss: 31.884022559994364.....Total loss: 31.884022559994364\n",
      "epoch 1197 learning rate:  0.010835421888053467   Training loss:   35.733245443332095  Valing loss:   31.884022559994364\n",
      "Pure loss: 35.868478933799516.....Total loss: 35.868478933799516\n",
      "Pure loss: 31.4428706166456.....Total loss: 31.4428706166456\n",
      "epoch 1198 learning rate:  0.010834724540901503   Training loss:   35.868478933799516  Valing loss:   31.4428706166456\n",
      "Pure loss: 35.72874864526741.....Total loss: 35.72874864526741\n",
      "Pure loss: 31.843655875799957.....Total loss: 31.843655875799957\n",
      "epoch 1199 learning rate:  0.010834028356964138   Training loss:   35.72874864526741  Valing loss:   31.843655875799957\n",
      "Pure loss: 35.75547575521901.....Total loss: 35.75547575521901\n",
      "Pure loss: 31.78343964657627.....Total loss: 31.78343964657627\n",
      "epoch 1200 learning rate:  0.010833333333333334   Training loss:   35.75547575521901  Valing loss:   31.78343964657627\n",
      "Pure loss: 35.89597133835196.....Total loss: 35.89597133835196\n",
      "Pure loss: 31.41782719100092.....Total loss: 31.41782719100092\n",
      "epoch 1201 learning rate:  0.01083263946711074   Training loss:   35.89597133835196  Valing loss:   31.41782719100092\n",
      "Pure loss: 35.893989369272965.....Total loss: 35.893989369272965\n",
      "Pure loss: 31.419669253257535.....Total loss: 31.419669253257535\n",
      "epoch 1202 learning rate:  0.010831946755407653   Training loss:   35.893989369272965  Valing loss:   31.419669253257535\n",
      "Pure loss: 35.87740688908489.....Total loss: 35.87740688908489\n",
      "Pure loss: 31.415705036480464.....Total loss: 31.415705036480464\n",
      "epoch 1203 learning rate:  0.01083125519534497   Training loss:   35.87740688908489  Valing loss:   31.415705036480464\n",
      "Pure loss: 35.85599612351518.....Total loss: 35.85599612351518\n",
      "Pure loss: 31.420572664563355.....Total loss: 31.420572664563355\n",
      "epoch 1204 learning rate:  0.010830564784053157   Training loss:   35.85599612351518  Valing loss:   31.420572664563355\n",
      "Pure loss: 35.880487964749825.....Total loss: 35.880487964749825\n",
      "Pure loss: 31.375374682850094.....Total loss: 31.375374682850094\n",
      "epoch 1205 learning rate:  0.0108298755186722   Training loss:   35.880487964749825  Valing loss:   31.375374682850094\n",
      "Pure loss: 35.83207761949585.....Total loss: 35.83207761949585\n",
      "Pure loss: 31.508296140510147.....Total loss: 31.508296140510147\n",
      "epoch 1206 learning rate:  0.010829187396351575   Training loss:   35.83207761949585  Valing loss:   31.508296140510147\n",
      "Pure loss: 35.92926990919538.....Total loss: 35.92926990919538\n",
      "Pure loss: 31.36396466702258.....Total loss: 31.36396466702258\n",
      "epoch 1207 learning rate:  0.010828500414250207   Training loss:   35.92926990919538  Valing loss:   31.36396466702258\n",
      "Pure loss: 36.12233080285644.....Total loss: 36.12233080285644\n",
      "Pure loss: 31.1288977700015.....Total loss: 31.1288977700015\n",
      "epoch 1208 learning rate:  0.010827814569536425   Training loss:   36.12233080285644  Valing loss:   31.1288977700015\n",
      "Pure loss: 35.71091724734901.....Total loss: 35.71091724734901\n",
      "Pure loss: 31.687924611344712.....Total loss: 31.687924611344712\n",
      "epoch 1209 learning rate:  0.010827129859387923   Training loss:   35.71091724734901  Valing loss:   31.687924611344712\n",
      "Pure loss: 35.631972094270104.....Total loss: 35.631972094270104\n",
      "Pure loss: 32.09448079521803.....Total loss: 32.09448079521803\n",
      "epoch 1210 learning rate:  0.010826446280991735   Training loss:   35.631972094270104  Valing loss:   32.09448079521803\n",
      "Pure loss: 35.80135205540936.....Total loss: 35.80135205540936\n",
      "Pure loss: 32.551222855100065.....Total loss: 32.551222855100065\n",
      "epoch 1211 learning rate:  0.01082576383154418   Training loss:   35.80135205540936  Valing loss:   32.551222855100065\n",
      "Pure loss: 35.86518646541322.....Total loss: 35.86518646541322\n",
      "Pure loss: 32.833501371730485.....Total loss: 32.833501371730485\n",
      "epoch 1212 learning rate:  0.010825082508250825   Training loss:   35.86518646541322  Valing loss:   32.833501371730485\n",
      "Pure loss: 36.08126375630805.....Total loss: 36.08126375630805\n",
      "Pure loss: 33.48985711467213.....Total loss: 33.48985711467213\n",
      "epoch 1213 learning rate:  0.010824402308326464   Training loss:   36.08126375630805  Valing loss:   33.48985711467213\n",
      "Pure loss: 35.81477552538996.....Total loss: 35.81477552538996\n",
      "Pure loss: 33.12710630249963.....Total loss: 33.12710630249963\n",
      "epoch 1214 learning rate:  0.010823723228995058   Training loss:   35.81477552538996  Valing loss:   33.12710630249963\n",
      "Pure loss: 35.69366694454036.....Total loss: 35.69366694454036\n",
      "Pure loss: 32.79036569862236.....Total loss: 32.79036569862236\n",
      "epoch 1215 learning rate:  0.010823045267489713   Training loss:   35.69366694454036  Valing loss:   32.79036569862236\n",
      "Pure loss: 35.711316697036786.....Total loss: 35.711316697036786\n",
      "Pure loss: 32.85968836143908.....Total loss: 32.85968836143908\n",
      "epoch 1216 learning rate:  0.010822368421052633   Training loss:   35.711316697036786  Valing loss:   32.85968836143908\n",
      "Pure loss: 35.74775602264946.....Total loss: 35.74775602264946\n",
      "Pure loss: 32.99187064757664.....Total loss: 32.99187064757664\n",
      "epoch 1217 learning rate:  0.010821692686935086   Training loss:   35.74775602264946  Valing loss:   32.99187064757664\n",
      "Pure loss: 36.01703937922662.....Total loss: 36.01703937922662\n",
      "Pure loss: 33.79554604769447.....Total loss: 33.79554604769447\n",
      "epoch 1218 learning rate:  0.010821018062397374   Training loss:   36.01703937922662  Valing loss:   33.79554604769447\n",
      "Pure loss: 36.032533257832384.....Total loss: 36.032533257832384\n",
      "Pure loss: 33.83610209490289.....Total loss: 33.83610209490289\n",
      "epoch 1219 learning rate:  0.010820344544708777   Training loss:   36.032533257832384  Valing loss:   33.83610209490289\n",
      "Pure loss: 36.91633039338333.....Total loss: 36.91633039338333\n",
      "Pure loss: 35.75403726583026.....Total loss: 35.75403726583026\n",
      "epoch 1220 learning rate:  0.010819672131147541   Training loss:   36.91633039338333  Valing loss:   35.75403726583026\n",
      "Pure loss: 36.86521726904109.....Total loss: 36.86521726904109\n",
      "Pure loss: 35.65562045547004.....Total loss: 35.65562045547004\n",
      "epoch 1221 learning rate:  0.010819000819000819   Training loss:   36.86521726904109  Valing loss:   35.65562045547004\n",
      "Pure loss: 35.696979789033335.....Total loss: 35.696979789033335\n",
      "Pure loss: 33.03497171780557.....Total loss: 33.03497171780557\n",
      "epoch 1222 learning rate:  0.010818330605564648   Training loss:   35.696979789033335  Valing loss:   33.03497171780557\n",
      "Pure loss: 36.12837912518419.....Total loss: 36.12837912518419\n",
      "Pure loss: 34.12121219874645.....Total loss: 34.12121219874645\n",
      "epoch 1223 learning rate:  0.010817661488143908   Training loss:   36.12837912518419  Valing loss:   34.12121219874645\n",
      "Pure loss: 36.244029175012855.....Total loss: 36.244029175012855\n",
      "Pure loss: 34.418406610338025.....Total loss: 34.418406610338025\n",
      "epoch 1224 learning rate:  0.010816993464052288   Training loss:   36.244029175012855  Valing loss:   34.418406610338025\n",
      "Pure loss: 36.353194164568244.....Total loss: 36.353194164568244\n",
      "Pure loss: 34.626837230340925.....Total loss: 34.626837230340925\n",
      "epoch 1225 learning rate:  0.010816326530612244   Training loss:   36.353194164568244  Valing loss:   34.626837230340925\n",
      "Pure loss: 36.42354547755073.....Total loss: 36.42354547755073\n",
      "Pure loss: 34.75664707062767.....Total loss: 34.75664707062767\n",
      "epoch 1226 learning rate:  0.010815660685154976   Training loss:   36.42354547755073  Valing loss:   34.75664707062767\n",
      "Pure loss: 36.00795466301549.....Total loss: 36.00795466301549\n",
      "Pure loss: 33.90444086616917.....Total loss: 33.90444086616917\n",
      "epoch 1227 learning rate:  0.010814995925020375   Training loss:   36.00795466301549  Valing loss:   33.90444086616917\n",
      "Pure loss: 35.95595775234187.....Total loss: 35.95595775234187\n",
      "Pure loss: 33.78796083783002.....Total loss: 33.78796083783002\n",
      "epoch 1228 learning rate:  0.010814332247557004   Training loss:   35.95595775234187  Valing loss:   33.78796083783002\n",
      "Pure loss: 35.69461599998296.....Total loss: 35.69461599998296\n",
      "Pure loss: 33.010463851007025.....Total loss: 33.010463851007025\n",
      "epoch 1229 learning rate:  0.010813669650122051   Training loss:   35.69461599998296  Valing loss:   33.010463851007025\n",
      "Pure loss: 35.65070729370164.....Total loss: 35.65070729370164\n",
      "Pure loss: 32.95262395229652.....Total loss: 32.95262395229652\n",
      "epoch 1230 learning rate:  0.0108130081300813   Training loss:   35.65070729370164  Valing loss:   32.95262395229652\n",
      "Pure loss: 35.86168465599045.....Total loss: 35.86168465599045\n",
      "Pure loss: 33.57122137673508.....Total loss: 33.57122137673508\n",
      "epoch 1231 learning rate:  0.010812347684809098   Training loss:   35.86168465599045  Valing loss:   33.57122137673508\n",
      "Pure loss: 35.686854051802115.....Total loss: 35.686854051802115\n",
      "Pure loss: 33.15960816575371.....Total loss: 33.15960816575371\n",
      "epoch 1232 learning rate:  0.010811688311688312   Training loss:   35.686854051802115  Valing loss:   33.15960816575371\n",
      "Pure loss: 35.77154917367654.....Total loss: 35.77154917367654\n",
      "Pure loss: 33.41252521844902.....Total loss: 33.41252521844902\n",
      "epoch 1233 learning rate:  0.0108110300081103   Training loss:   35.77154917367654  Valing loss:   33.41252521844902\n",
      "Pure loss: 36.75765731448092.....Total loss: 36.75765731448092\n",
      "Pure loss: 35.72150307016698.....Total loss: 35.72150307016698\n",
      "epoch 1234 learning rate:  0.01081037277147488   Training loss:   36.75765731448092  Valing loss:   35.72150307016698\n",
      "Pure loss: 36.98615923507604.....Total loss: 36.98615923507604\n",
      "Pure loss: 36.113639449051725.....Total loss: 36.113639449051725\n",
      "epoch 1235 learning rate:  0.010809716599190283   Training loss:   36.98615923507604  Valing loss:   36.113639449051725\n",
      "Pure loss: 36.02960801196677.....Total loss: 36.02960801196677\n",
      "Pure loss: 34.278913618871414.....Total loss: 34.278913618871414\n",
      "epoch 1236 learning rate:  0.01080906148867314   Training loss:   36.02960801196677  Valing loss:   34.278913618871414\n",
      "Pure loss: 35.69401258701166.....Total loss: 35.69401258701166\n",
      "Pure loss: 33.51113492129495.....Total loss: 33.51113492129495\n",
      "epoch 1237 learning rate:  0.010808407437348424   Training loss:   35.69401258701166  Valing loss:   33.51113492129495\n",
      "Pure loss: 38.01605371440488.....Total loss: 38.01605371440488\n",
      "Pure loss: 38.257166088012944.....Total loss: 38.257166088012944\n",
      "epoch 1238 learning rate:  0.010807754442649435   Training loss:   38.01605371440488  Valing loss:   38.257166088012944\n",
      "Pure loss: 38.19339950604593.....Total loss: 38.19339950604593\n",
      "Pure loss: 38.38676645657409.....Total loss: 38.38676645657409\n",
      "epoch 1239 learning rate:  0.010807102502017756   Training loss:   38.19339950604593  Valing loss:   38.38676645657409\n",
      "Pure loss: 37.37597731707379.....Total loss: 37.37597731707379\n",
      "Pure loss: 37.26630068188734.....Total loss: 37.26630068188734\n",
      "epoch 1240 learning rate:  0.010806451612903226   Training loss:   37.37597731707379  Valing loss:   37.26630068188734\n",
      "Pure loss: 37.90703382780327.....Total loss: 37.90703382780327\n",
      "Pure loss: 38.038796135569584.....Total loss: 38.038796135569584\n",
      "epoch 1241 learning rate:  0.0108058017727639   Training loss:   37.90703382780327  Valing loss:   38.038796135569584\n",
      "Pure loss: 37.63798034487726.....Total loss: 37.63798034487726\n",
      "Pure loss: 37.62165980498636.....Total loss: 37.62165980498636\n",
      "epoch 1242 learning rate:  0.010805152979066023   Training loss:   37.63798034487726  Valing loss:   37.62165980498636\n",
      "Pure loss: 37.45843843657468.....Total loss: 37.45843843657468\n",
      "Pure loss: 37.34025987284118.....Total loss: 37.34025987284118\n",
      "epoch 1243 learning rate:  0.01080450522928399   Training loss:   37.45843843657468  Valing loss:   37.34025987284118\n",
      "Pure loss: 37.24142939549524.....Total loss: 37.24142939549524\n",
      "Pure loss: 36.993635113790916.....Total loss: 36.993635113790916\n",
      "epoch 1244 learning rate:  0.010803858520900322   Training loss:   37.24142939549524  Valing loss:   36.993635113790916\n",
      "Pure loss: 36.98407964412057.....Total loss: 36.98407964412057\n",
      "Pure loss: 36.48788424177731.....Total loss: 36.48788424177731\n",
      "epoch 1245 learning rate:  0.010803212851405623   Training loss:   36.98407964412057  Valing loss:   36.48788424177731\n",
      "Pure loss: 36.655302001296704.....Total loss: 36.655302001296704\n",
      "Pure loss: 35.89647615138516.....Total loss: 35.89647615138516\n",
      "epoch 1246 learning rate:  0.010802568218298555   Training loss:   36.655302001296704  Valing loss:   35.89647615138516\n",
      "Pure loss: 36.1088250212778.....Total loss: 36.1088250212778\n",
      "Pure loss: 34.89538616334583.....Total loss: 34.89538616334583\n",
      "epoch 1247 learning rate:  0.010801924619085806   Training loss:   36.1088250212778  Valing loss:   34.89538616334583\n",
      "Pure loss: 36.18613191191301.....Total loss: 36.18613191191301\n",
      "Pure loss: 34.95531553938451.....Total loss: 34.95531553938451\n",
      "epoch 1248 learning rate:  0.010801282051282052   Training loss:   36.18613191191301  Valing loss:   34.95531553938451\n",
      "Pure loss: 36.53758787505452.....Total loss: 36.53758787505452\n",
      "Pure loss: 35.69716024025961.....Total loss: 35.69716024025961\n",
      "epoch 1249 learning rate:  0.010800640512409927   Training loss:   36.53758787505452  Valing loss:   35.69716024025961\n",
      "Pure loss: 36.62804176716843.....Total loss: 36.62804176716843\n",
      "Pure loss: 35.865641788251864.....Total loss: 35.865641788251864\n",
      "epoch 1250 learning rate:  0.0108   Training loss:   36.62804176716843  Valing loss:   35.865641788251864\n",
      "Pure loss: 36.22294961865023.....Total loss: 36.22294961865023\n",
      "Pure loss: 35.03690177869536.....Total loss: 35.03690177869536\n",
      "epoch 1251 learning rate:  0.010799360511590727   Training loss:   36.22294961865023  Valing loss:   35.03690177869536\n",
      "Pure loss: 36.19161985787957.....Total loss: 36.19161985787957\n",
      "Pure loss: 34.97778078156401.....Total loss: 34.97778078156401\n",
      "epoch 1252 learning rate:  0.010798722044728435   Training loss:   36.19161985787957  Valing loss:   34.97778078156401\n",
      "Pure loss: 36.19128172803883.....Total loss: 36.19128172803883\n",
      "Pure loss: 34.97485172647362.....Total loss: 34.97485172647362\n",
      "epoch 1253 learning rate:  0.010798084596967279   Training loss:   36.19128172803883  Valing loss:   34.97485172647362\n",
      "Pure loss: 35.92049663494596.....Total loss: 35.92049663494596\n",
      "Pure loss: 34.32423280363616.....Total loss: 34.32423280363616\n",
      "epoch 1254 learning rate:  0.010797448165869219   Training loss:   35.92049663494596  Valing loss:   34.32423280363616\n",
      "Pure loss: 35.534431883437364.....Total loss: 35.534431883437364\n",
      "Pure loss: 33.073729256312795.....Total loss: 33.073729256312795\n",
      "epoch 1255 learning rate:  0.010796812749003985   Training loss:   35.534431883437364  Valing loss:   33.073729256312795\n",
      "Pure loss: 35.42384322232897.....Total loss: 35.42384322232897\n",
      "Pure loss: 32.72893593786895.....Total loss: 32.72893593786895\n",
      "epoch 1256 learning rate:  0.010796178343949045   Training loss:   35.42384322232897  Valing loss:   32.72893593786895\n",
      "Pure loss: 35.49883059787265.....Total loss: 35.49883059787265\n",
      "Pure loss: 33.34662130112637.....Total loss: 33.34662130112637\n",
      "epoch 1257 learning rate:  0.01079554494828958   Training loss:   35.49883059787265  Valing loss:   33.34662130112637\n",
      "Pure loss: 35.41589992628315.....Total loss: 35.41589992628315\n",
      "Pure loss: 33.29034282980495.....Total loss: 33.29034282980495\n",
      "epoch 1258 learning rate:  0.010794912559618442   Training loss:   35.41589992628315  Valing loss:   33.29034282980495\n",
      "Pure loss: 34.90809313769928.....Total loss: 34.90809313769928\n",
      "Pure loss: 32.21858504447103.....Total loss: 32.21858504447103\n",
      "epoch 1259 learning rate:  0.01079428117553614   Training loss:   34.90809313769928  Valing loss:   32.21858504447103\n",
      "Pure loss: 34.8287402019181.....Total loss: 34.8287402019181\n",
      "Pure loss: 31.557274083148858.....Total loss: 31.557274083148858\n",
      "epoch 1260 learning rate:  0.010793650793650795   Training loss:   34.8287402019181  Valing loss:   31.557274083148858\n",
      "Pure loss: 34.78959615594968.....Total loss: 34.78959615594968\n",
      "Pure loss: 31.517237343710587.....Total loss: 31.517237343710587\n",
      "epoch 1261 learning rate:  0.010793021411578113   Training loss:   34.78959615594968  Valing loss:   31.517237343710587\n",
      "Pure loss: 35.50632256710399.....Total loss: 35.50632256710399\n",
      "Pure loss: 31.918005490567182.....Total loss: 31.918005490567182\n",
      "epoch 1262 learning rate:  0.010792393026941363   Training loss:   35.50632256710399  Valing loss:   31.918005490567182\n",
      "Pure loss: 35.70813634224999.....Total loss: 35.70813634224999\n",
      "Pure loss: 31.695190019481274.....Total loss: 31.695190019481274\n",
      "epoch 1263 learning rate:  0.010791765637371338   Training loss:   35.70813634224999  Valing loss:   31.695190019481274\n",
      "Pure loss: 35.10150487408094.....Total loss: 35.10150487408094\n",
      "Pure loss: 31.91567296282455.....Total loss: 31.91567296282455\n",
      "epoch 1264 learning rate:  0.01079113924050633   Training loss:   35.10150487408094  Valing loss:   31.91567296282455\n",
      "Pure loss: 35.09648616468252.....Total loss: 35.09648616468252\n",
      "Pure loss: 31.94718442640598.....Total loss: 31.94718442640598\n",
      "epoch 1265 learning rate:  0.010790513833992096   Training loss:   35.09648616468252  Valing loss:   31.94718442640598\n",
      "Pure loss: 34.99299981655291.....Total loss: 34.99299981655291\n",
      "Pure loss: 32.45718187746673.....Total loss: 32.45718187746673\n",
      "epoch 1266 learning rate:  0.010789889415481833   Training loss:   34.99299981655291  Valing loss:   32.45718187746673\n",
      "Pure loss: 36.054941997493984.....Total loss: 36.054941997493984\n",
      "Pure loss: 35.180337330239.....Total loss: 35.180337330239\n",
      "epoch 1267 learning rate:  0.010789265982636149   Training loss:   36.054941997493984  Valing loss:   35.180337330239\n",
      "Pure loss: 35.29591653760733.....Total loss: 35.29591653760733\n",
      "Pure loss: 33.66602101058692.....Total loss: 33.66602101058692\n",
      "epoch 1268 learning rate:  0.010788643533123029   Training loss:   35.29591653760733  Valing loss:   33.66602101058692\n",
      "Pure loss: 35.20213612643326.....Total loss: 35.20213612643326\n",
      "Pure loss: 33.6571622258266.....Total loss: 33.6571622258266\n",
      "epoch 1269 learning rate:  0.010788022064617809   Training loss:   35.20213612643326  Valing loss:   33.6571622258266\n",
      "Pure loss: 35.079440061554855.....Total loss: 35.079440061554855\n",
      "Pure loss: 33.36865269933659.....Total loss: 33.36865269933659\n",
      "epoch 1270 learning rate:  0.01078740157480315   Training loss:   35.079440061554855  Valing loss:   33.36865269933659\n",
      "Pure loss: 35.072392083415735.....Total loss: 35.072392083415735\n",
      "Pure loss: 33.35260640281151.....Total loss: 33.35260640281151\n",
      "epoch 1271 learning rate:  0.010786782061369002   Training loss:   35.072392083415735  Valing loss:   33.35260640281151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 35.06222753301705.....Total loss: 35.06222753301705\n",
      "Pure loss: 33.194441514359234.....Total loss: 33.194441514359234\n",
      "epoch 1272 learning rate:  0.010786163522012579   Training loss:   35.06222753301705  Valing loss:   33.194441514359234\n",
      "Pure loss: 35.093737336690374.....Total loss: 35.093737336690374\n",
      "Pure loss: 33.17789037387094.....Total loss: 33.17789037387094\n",
      "epoch 1273 learning rate:  0.010785545954438334   Training loss:   35.093737336690374  Valing loss:   33.17789037387094\n",
      "Pure loss: 34.9798050001926.....Total loss: 34.9798050001926\n",
      "Pure loss: 33.24250313429481.....Total loss: 33.24250313429481\n",
      "epoch 1274 learning rate:  0.010784929356357928   Training loss:   34.9798050001926  Valing loss:   33.24250313429481\n",
      "Pure loss: 34.95768299081107.....Total loss: 34.95768299081107\n",
      "Pure loss: 33.33908934692478.....Total loss: 33.33908934692478\n",
      "epoch 1275 learning rate:  0.010784313725490196   Training loss:   34.95768299081107  Valing loss:   33.33908934692478\n",
      "Pure loss: 34.78884042084159.....Total loss: 34.78884042084159\n",
      "Pure loss: 32.97186266087397.....Total loss: 32.97186266087397\n",
      "epoch 1276 learning rate:  0.01078369905956113   Training loss:   34.78884042084159  Valing loss:   32.97186266087397\n",
      "Pure loss: 34.4378587068224.....Total loss: 34.4378587068224\n",
      "Pure loss: 31.826599928152053.....Total loss: 31.826599928152053\n",
      "epoch 1277 learning rate:  0.010783085356303837   Training loss:   34.4378587068224  Valing loss:   31.826599928152053\n",
      "Pure loss: 34.6728640339586.....Total loss: 34.6728640339586\n",
      "Pure loss: 31.946193532640102.....Total loss: 31.946193532640102\n",
      "epoch 1278 learning rate:  0.010782472613458528   Training loss:   34.6728640339586  Valing loss:   31.946193532640102\n",
      "Pure loss: 34.77444767307521.....Total loss: 34.77444767307521\n",
      "Pure loss: 31.33759005446566.....Total loss: 31.33759005446566\n",
      "epoch 1279 learning rate:  0.010781860828772478   Training loss:   34.77444767307521  Valing loss:   31.33759005446566\n",
      "Pure loss: 35.00948228928804.....Total loss: 35.00948228928804\n",
      "Pure loss: 31.268200443785776.....Total loss: 31.268200443785776\n",
      "epoch 1280 learning rate:  0.010781250000000001   Training loss:   35.00948228928804  Valing loss:   31.268200443785776\n",
      "Pure loss: 34.68505699927786.....Total loss: 34.68505699927786\n",
      "Pure loss: 31.41910590148567.....Total loss: 31.41910590148567\n",
      "epoch 1281 learning rate:  0.01078064012490242   Training loss:   34.68505699927786  Valing loss:   31.41910590148567\n",
      "Pure loss: 34.98320823041764.....Total loss: 34.98320823041764\n",
      "Pure loss: 31.67341512176506.....Total loss: 31.67341512176506\n",
      "epoch 1282 learning rate:  0.01078003120124805   Training loss:   34.98320823041764  Valing loss:   31.67341512176506\n",
      "Pure loss: 34.978614107509614.....Total loss: 34.978614107509614\n",
      "Pure loss: 31.71970044811147.....Total loss: 31.71970044811147\n",
      "epoch 1283 learning rate:  0.010779423226812159   Training loss:   34.978614107509614  Valing loss:   31.71970044811147\n",
      "Pure loss: 35.31222496438822.....Total loss: 35.31222496438822\n",
      "Pure loss: 31.632491807411526.....Total loss: 31.632491807411526\n",
      "epoch 1284 learning rate:  0.010778816199376947   Training loss:   35.31222496438822  Valing loss:   31.632491807411526\n",
      "Pure loss: 35.1991982001691.....Total loss: 35.1991982001691\n",
      "Pure loss: 31.5483207491992.....Total loss: 31.5483207491992\n",
      "epoch 1285 learning rate:  0.010778210116731518   Training loss:   35.1991982001691  Valing loss:   31.5483207491992\n",
      "Pure loss: 34.86906744004367.....Total loss: 34.86906744004367\n",
      "Pure loss: 32.168851831871116.....Total loss: 32.168851831871116\n",
      "epoch 1286 learning rate:  0.01077760497667185   Training loss:   34.86906744004367  Valing loss:   32.168851831871116\n",
      "Pure loss: 34.864729133578585.....Total loss: 34.864729133578585\n",
      "Pure loss: 31.90794173019588.....Total loss: 31.90794173019588\n",
      "epoch 1287 learning rate:  0.010777000777000778   Training loss:   34.864729133578585  Valing loss:   31.90794173019588\n",
      "Pure loss: 35.4882453567706.....Total loss: 35.4882453567706\n",
      "Pure loss: 32.37139446823392.....Total loss: 32.37139446823392\n",
      "epoch 1288 learning rate:  0.01077639751552795   Training loss:   35.4882453567706  Valing loss:   32.37139446823392\n",
      "Pure loss: 35.077232798179686.....Total loss: 35.077232798179686\n",
      "Pure loss: 32.30407069164005.....Total loss: 32.30407069164005\n",
      "epoch 1289 learning rate:  0.010775795190069822   Training loss:   35.077232798179686  Valing loss:   32.30407069164005\n",
      "Pure loss: 35.16314153487424.....Total loss: 35.16314153487424\n",
      "Pure loss: 31.897755110624466.....Total loss: 31.897755110624466\n",
      "epoch 1290 learning rate:  0.010775193798449613   Training loss:   35.16314153487424  Valing loss:   31.897755110624466\n",
      "Pure loss: 34.44160481486266.....Total loss: 34.44160481486266\n",
      "Pure loss: 31.54754911872406.....Total loss: 31.54754911872406\n",
      "epoch 1291 learning rate:  0.01077459333849729   Training loss:   34.44160481486266  Valing loss:   31.54754911872406\n",
      "Pure loss: 34.503169801904086.....Total loss: 34.503169801904086\n",
      "Pure loss: 30.99121119543232.....Total loss: 30.99121119543232\n",
      "epoch 1292 learning rate:  0.010773993808049536   Training loss:   34.503169801904086  Valing loss:   30.99121119543232\n",
      "Pure loss: 34.966289234648.....Total loss: 34.966289234648\n",
      "Pure loss: 31.312581128836403.....Total loss: 31.312581128836403\n",
      "epoch 1293 learning rate:  0.010773395204949729   Training loss:   34.966289234648  Valing loss:   31.312581128836403\n",
      "Pure loss: 35.1694745425663.....Total loss: 35.1694745425663\n",
      "Pure loss: 31.036615997946182.....Total loss: 31.036615997946182\n",
      "epoch 1294 learning rate:  0.010772797527047913   Training loss:   35.1694745425663  Valing loss:   31.036615997946182\n",
      "Pure loss: 35.62586947017005.....Total loss: 35.62586947017005\n",
      "Pure loss: 31.126196720050288.....Total loss: 31.126196720050288\n",
      "epoch 1295 learning rate:  0.010772200772200773   Training loss:   35.62586947017005  Valing loss:   31.126196720050288\n",
      "Pure loss: 35.16766783841962.....Total loss: 35.16766783841962\n",
      "Pure loss: 31.014653858108733.....Total loss: 31.014653858108733\n",
      "epoch 1296 learning rate:  0.010771604938271605   Training loss:   35.16766783841962  Valing loss:   31.014653858108733\n",
      "Pure loss: 34.66803980521381.....Total loss: 34.66803980521381\n",
      "Pure loss: 31.312908208482945.....Total loss: 31.312908208482945\n",
      "epoch 1297 learning rate:  0.0107710100231303   Training loss:   34.66803980521381  Valing loss:   31.312908208482945\n",
      "Pure loss: 34.83131518915822.....Total loss: 34.83131518915822\n",
      "Pure loss: 31.37932071308318.....Total loss: 31.37932071308318\n",
      "epoch 1298 learning rate:  0.010770416024653314   Training loss:   34.83131518915822  Valing loss:   31.37932071308318\n",
      "Pure loss: 34.676925821158186.....Total loss: 34.676925821158186\n",
      "Pure loss: 31.720329606463228.....Total loss: 31.720329606463228\n",
      "epoch 1299 learning rate:  0.010769822940723634   Training loss:   34.676925821158186  Valing loss:   31.720329606463228\n",
      "Pure loss: 34.90635380221904.....Total loss: 34.90635380221904\n",
      "Pure loss: 32.504054839644525.....Total loss: 32.504054839644525\n",
      "epoch 1300 learning rate:  0.010769230769230769   Training loss:   34.90635380221904  Valing loss:   32.504054839644525\n",
      "Pure loss: 34.86786876353248.....Total loss: 34.86786876353248\n",
      "Pure loss: 32.37136655814555.....Total loss: 32.37136655814555\n",
      "epoch 1301 learning rate:  0.010768639508070716   Training loss:   34.86786876353248  Valing loss:   32.37136655814555\n",
      "Pure loss: 34.85087854201333.....Total loss: 34.85087854201333\n",
      "Pure loss: 32.2518649011389.....Total loss: 32.2518649011389\n",
      "epoch 1302 learning rate:  0.010768049155145929   Training loss:   34.85087854201333  Valing loss:   32.2518649011389\n",
      "Pure loss: 34.926527923282926.....Total loss: 34.926527923282926\n",
      "Pure loss: 31.899126120650713.....Total loss: 31.899126120650713\n",
      "epoch 1303 learning rate:  0.010767459708365311   Training loss:   34.926527923282926  Valing loss:   31.899126120650713\n",
      "Pure loss: 34.93192919208425.....Total loss: 34.93192919208425\n",
      "Pure loss: 31.898750839003966.....Total loss: 31.898750839003966\n",
      "epoch 1304 learning rate:  0.010766871165644173   Training loss:   34.93192919208425  Valing loss:   31.898750839003966\n",
      "Pure loss: 34.948415599971625.....Total loss: 34.948415599971625\n",
      "Pure loss: 31.907767167556862.....Total loss: 31.907767167556862\n",
      "epoch 1305 learning rate:  0.010766283524904215   Training loss:   34.948415599971625  Valing loss:   31.907767167556862\n",
      "Pure loss: 35.604163572432554.....Total loss: 35.604163572432554\n",
      "Pure loss: 32.41395009189704.....Total loss: 32.41395009189704\n",
      "epoch 1306 learning rate:  0.010765696784073507   Training loss:   35.604163572432554  Valing loss:   32.41395009189704\n",
      "Pure loss: 35.78030334307832.....Total loss: 35.78030334307832\n",
      "Pure loss: 32.45015834960399.....Total loss: 32.45015834960399\n",
      "epoch 1307 learning rate:  0.010765110941086458   Training loss:   35.78030334307832  Valing loss:   32.45015834960399\n",
      "Pure loss: 35.13584385567346.....Total loss: 35.13584385567346\n",
      "Pure loss: 31.912831716818495.....Total loss: 31.912831716818495\n",
      "epoch 1308 learning rate:  0.010764525993883792   Training loss:   35.13584385567346  Valing loss:   31.912831716818495\n",
      "Pure loss: 34.9145904690334.....Total loss: 34.9145904690334\n",
      "Pure loss: 31.731502057758608.....Total loss: 31.731502057758608\n",
      "epoch 1309 learning rate:  0.010763941940412528   Training loss:   34.9145904690334  Valing loss:   31.731502057758608\n",
      "Pure loss: 34.79532980306985.....Total loss: 34.79532980306985\n",
      "Pure loss: 31.720376753589207.....Total loss: 31.720376753589207\n",
      "epoch 1310 learning rate:  0.010763358778625954   Training loss:   34.79532980306985  Valing loss:   31.720376753589207\n",
      "Pure loss: 34.96893042951188.....Total loss: 34.96893042951188\n",
      "Pure loss: 30.411036285786086.....Total loss: 30.411036285786086\n",
      "epoch 1311 learning rate:  0.0107627765064836   Training loss:   34.96893042951188  Valing loss:   30.411036285786086\n",
      "Pure loss: 34.84551178766283.....Total loss: 34.84551178766283\n",
      "Pure loss: 30.318858163780344.....Total loss: 30.318858163780344\n",
      "epoch 1312 learning rate:  0.01076219512195122   Training loss:   34.84551178766283  Valing loss:   30.318858163780344\n",
      "Pure loss: 35.196041204447134.....Total loss: 35.196041204447134\n",
      "Pure loss: 30.355855011998145.....Total loss: 30.355855011998145\n",
      "epoch 1313 learning rate:  0.010761614623000761   Training loss:   35.196041204447134  Valing loss:   30.355855011998145\n",
      "Pure loss: 36.47319393099388.....Total loss: 36.47319393099388\n",
      "Pure loss: 31.38663408200787.....Total loss: 31.38663408200787\n",
      "epoch 1314 learning rate:  0.01076103500761035   Training loss:   36.47319393099388  Valing loss:   31.38663408200787\n",
      "Pure loss: 35.957443204854464.....Total loss: 35.957443204854464\n",
      "Pure loss: 30.96305473397479.....Total loss: 30.96305473397479\n",
      "epoch 1315 learning rate:  0.01076045627376426   Training loss:   35.957443204854464  Valing loss:   30.96305473397479\n",
      "Pure loss: 35.27604246167618.....Total loss: 35.27604246167618\n",
      "Pure loss: 30.809254589739243.....Total loss: 30.809254589739243\n",
      "epoch 1316 learning rate:  0.010759878419452888   Training loss:   35.27604246167618  Valing loss:   30.809254589739243\n",
      "Pure loss: 35.22916874527208.....Total loss: 35.22916874527208\n",
      "Pure loss: 30.81747466216092.....Total loss: 30.81747466216092\n",
      "epoch 1317 learning rate:  0.010759301442672741   Training loss:   35.22916874527208  Valing loss:   30.81747466216092\n",
      "Pure loss: 35.575670729000095.....Total loss: 35.575670729000095\n",
      "Pure loss: 30.85894027302568.....Total loss: 30.85894027302568\n",
      "epoch 1318 learning rate:  0.010758725341426405   Training loss:   35.575670729000095  Valing loss:   30.85894027302568\n",
      "Pure loss: 34.5948203356077.....Total loss: 34.5948203356077\n",
      "Pure loss: 30.727033639272463.....Total loss: 30.727033639272463\n",
      "epoch 1319 learning rate:  0.010758150113722517   Training loss:   34.5948203356077  Valing loss:   30.727033639272463\n",
      "Pure loss: 34.64924379317982.....Total loss: 34.64924379317982\n",
      "Pure loss: 30.644438716378296.....Total loss: 30.644438716378296\n",
      "epoch 1320 learning rate:  0.010757575757575757   Training loss:   34.64924379317982  Valing loss:   30.644438716378296\n",
      "Pure loss: 34.86314488476873.....Total loss: 34.86314488476873\n",
      "Pure loss: 30.548702990215236.....Total loss: 30.548702990215236\n",
      "epoch 1321 learning rate:  0.010757002271006812   Training loss:   34.86314488476873  Valing loss:   30.548702990215236\n",
      "Pure loss: 35.17509089487116.....Total loss: 35.17509089487116\n",
      "Pure loss: 30.565444325388007.....Total loss: 30.565444325388007\n",
      "epoch 1322 learning rate:  0.01075642965204236   Training loss:   35.17509089487116  Valing loss:   30.565444325388007\n",
      "Pure loss: 35.34588572359976.....Total loss: 35.34588572359976\n",
      "Pure loss: 30.372915211537194.....Total loss: 30.372915211537194\n",
      "epoch 1323 learning rate:  0.010755857898715042   Training loss:   35.34588572359976  Valing loss:   30.372915211537194\n",
      "Pure loss: 35.23724265649003.....Total loss: 35.23724265649003\n",
      "Pure loss: 30.358113429711747.....Total loss: 30.358113429711747\n",
      "epoch 1324 learning rate:  0.010755287009063445   Training loss:   35.23724265649003  Valing loss:   30.358113429711747\n",
      "Pure loss: 35.09457990943097.....Total loss: 35.09457990943097\n",
      "Pure loss: 30.448343745530718.....Total loss: 30.448343745530718\n",
      "epoch 1325 learning rate:  0.010754716981132076   Training loss:   35.09457990943097  Valing loss:   30.448343745530718\n",
      "Pure loss: 34.26848269927487.....Total loss: 34.26848269927487\n",
      "Pure loss: 30.96030839334155.....Total loss: 30.96030839334155\n",
      "epoch 1326 learning rate:  0.010754147812971343   Training loss:   34.26848269927487  Valing loss:   30.96030839334155\n",
      "Pure loss: 33.7663662397311.....Total loss: 33.7663662397311\n",
      "Pure loss: 30.601542015840113.....Total loss: 30.601542015840113\n",
      "epoch 1327 learning rate:  0.010753579502637528   Training loss:   33.7663662397311  Valing loss:   30.601542015840113\n",
      "Pure loss: 33.302017454506284.....Total loss: 33.302017454506284\n",
      "Pure loss: 30.567724207956733.....Total loss: 30.567724207956733\n",
      "epoch 1328 learning rate:  0.010753012048192772   Training loss:   33.302017454506284  Valing loss:   30.567724207956733\n",
      "Pure loss: 33.29563089675069.....Total loss: 33.29563089675069\n",
      "Pure loss: 30.13520155777528.....Total loss: 30.13520155777528\n",
      "epoch 1329 learning rate:  0.01075244544770504   Training loss:   33.29563089675069  Valing loss:   30.13520155777528\n",
      "Pure loss: 33.61386432160006.....Total loss: 33.61386432160006\n",
      "Pure loss: 29.0722943031949.....Total loss: 29.0722943031949\n",
      "epoch 1330 learning rate:  0.01075187969924812   Training loss:   33.61386432160006  Valing loss:   29.0722943031949\n",
      "Pure loss: 32.8475776368302.....Total loss: 32.8475776368302\n",
      "Pure loss: 28.800001991627763.....Total loss: 28.800001991627763\n",
      "epoch 1331 learning rate:  0.010751314800901577   Training loss:   32.8475776368302  Valing loss:   28.800001991627763\n",
      "Pure loss: 32.787779135424536.....Total loss: 32.787779135424536\n",
      "Pure loss: 30.139122562978418.....Total loss: 30.139122562978418\n",
      "epoch 1332 learning rate:  0.010750750750750751   Training loss:   32.787779135424536  Valing loss:   30.139122562978418\n",
      "Pure loss: 32.778434788089854.....Total loss: 32.778434788089854\n",
      "Pure loss: 30.09361047086739.....Total loss: 30.09361047086739\n",
      "epoch 1333 learning rate:  0.010750187546886721   Training loss:   32.778434788089854  Valing loss:   30.09361047086739\n",
      "Pure loss: 32.77980759429039.....Total loss: 32.77980759429039\n",
      "Pure loss: 30.099359352322633.....Total loss: 30.099359352322633\n",
      "epoch 1334 learning rate:  0.010749625187406298   Training loss:   32.77980759429039  Valing loss:   30.099359352322633\n",
      "Pure loss: 32.72150777238783.....Total loss: 32.72150777238783\n",
      "Pure loss: 29.77824739630842.....Total loss: 29.77824739630842\n",
      "epoch 1335 learning rate:  0.010749063670411986   Training loss:   32.72150777238783  Valing loss:   29.77824739630842\n",
      "Pure loss: 32.7588592716685.....Total loss: 32.7588592716685\n",
      "Pure loss: 29.04458286681115.....Total loss: 29.04458286681115\n",
      "epoch 1336 learning rate:  0.010748502994011977   Training loss:   32.7588592716685  Valing loss:   29.04458286681115\n",
      "Pure loss: 32.791175071746096.....Total loss: 32.791175071746096\n",
      "Pure loss: 28.936334890403646.....Total loss: 28.936334890403646\n",
      "epoch 1337 learning rate:  0.01074794315632012   Training loss:   32.791175071746096  Valing loss:   28.936334890403646\n",
      "Pure loss: 32.89789704005351.....Total loss: 32.89789704005351\n",
      "Pure loss: 28.764607796178407.....Total loss: 28.764607796178407\n",
      "epoch 1338 learning rate:  0.010747384155455904   Training loss:   32.89789704005351  Valing loss:   28.764607796178407\n",
      "Pure loss: 32.54551614235948.....Total loss: 32.54551614235948\n",
      "Pure loss: 29.185611528300875.....Total loss: 29.185611528300875\n",
      "epoch 1339 learning rate:  0.010746825989544436   Training loss:   32.54551614235948  Valing loss:   29.185611528300875\n",
      "Pure loss: 32.55012203314222.....Total loss: 32.55012203314222\n",
      "Pure loss: 29.269419522441535.....Total loss: 29.269419522441535\n",
      "epoch 1340 learning rate:  0.010746268656716419   Training loss:   32.55012203314222  Valing loss:   29.269419522441535\n",
      "Pure loss: 32.554921325150474.....Total loss: 32.554921325150474\n",
      "Pure loss: 29.15184572976136.....Total loss: 29.15184572976136\n",
      "epoch 1341 learning rate:  0.010745712155108129   Training loss:   32.554921325150474  Valing loss:   29.15184572976136\n",
      "Pure loss: 32.64133313349798.....Total loss: 32.64133313349798\n",
      "Pure loss: 29.16320215392144.....Total loss: 29.16320215392144\n",
      "epoch 1342 learning rate:  0.0107451564828614   Training loss:   32.64133313349798  Valing loss:   29.16320215392144\n",
      "Pure loss: 32.72733816592603.....Total loss: 32.72733816592603\n",
      "Pure loss: 29.001613214510517.....Total loss: 29.001613214510517\n",
      "epoch 1343 learning rate:  0.010744601638123604   Training loss:   32.72733816592603  Valing loss:   29.001613214510517\n",
      "Pure loss: 32.400600519816294.....Total loss: 32.400600519816294\n",
      "Pure loss: 29.001201119897512.....Total loss: 29.001201119897512\n",
      "epoch 1344 learning rate:  0.010744047619047618   Training loss:   32.400600519816294  Valing loss:   29.001201119897512\n",
      "Pure loss: 32.394616374421645.....Total loss: 32.394616374421645\n",
      "Pure loss: 29.15161644951689.....Total loss: 29.15161644951689\n",
      "epoch 1345 learning rate:  0.010743494423791822   Training loss:   32.394616374421645  Valing loss:   29.15161644951689\n",
      "Pure loss: 32.36287081131181.....Total loss: 32.36287081131181\n",
      "Pure loss: 28.833412276590416.....Total loss: 28.833412276590416\n",
      "epoch 1346 learning rate:  0.010742942050520059   Training loss:   32.36287081131181  Valing loss:   28.833412276590416\n",
      "Pure loss: 32.502546947728156.....Total loss: 32.502546947728156\n",
      "Pure loss: 28.695454806763763.....Total loss: 28.695454806763763\n",
      "epoch 1347 learning rate:  0.010742390497401633   Training loss:   32.502546947728156  Valing loss:   28.695454806763763\n",
      "Pure loss: 32.67922082149274.....Total loss: 32.67922082149274\n",
      "Pure loss: 28.814031405496713.....Total loss: 28.814031405496713\n",
      "epoch 1348 learning rate:  0.010741839762611276   Training loss:   32.67922082149274  Valing loss:   28.814031405496713\n",
      "Pure loss: 32.79392971245568.....Total loss: 32.79392971245568\n",
      "Pure loss: 28.611749597532338.....Total loss: 28.611749597532338\n",
      "epoch 1349 learning rate:  0.010741289844329132   Training loss:   32.79392971245568  Valing loss:   28.611749597532338\n",
      "Pure loss: 32.849399227954464.....Total loss: 32.849399227954464\n",
      "Pure loss: 28.56430880656339.....Total loss: 28.56430880656339\n",
      "epoch 1350 learning rate:  0.010740740740740742   Training loss:   32.849399227954464  Valing loss:   28.56430880656339\n",
      "Pure loss: 32.913537716252236.....Total loss: 32.913537716252236\n",
      "Pure loss: 28.56943871024205.....Total loss: 28.56943871024205\n",
      "epoch 1351 learning rate:  0.01074019245003701   Training loss:   32.913537716252236  Valing loss:   28.56943871024205\n",
      "Pure loss: 32.76635356596192.....Total loss: 32.76635356596192\n",
      "Pure loss: 28.592776487222498.....Total loss: 28.592776487222498\n",
      "epoch 1352 learning rate:  0.010739644970414202   Training loss:   32.76635356596192  Valing loss:   28.592776487222498\n",
      "Pure loss: 33.271806503987214.....Total loss: 33.271806503987214\n",
      "Pure loss: 28.788014341845745.....Total loss: 28.788014341845745\n",
      "epoch 1353 learning rate:  0.01073909830007391   Training loss:   33.271806503987214  Valing loss:   28.788014341845745\n",
      "Pure loss: 32.646738409777456.....Total loss: 32.646738409777456\n",
      "Pure loss: 28.85476332691919.....Total loss: 28.85476332691919\n",
      "epoch 1354 learning rate:  0.010738552437223043   Training loss:   32.646738409777456  Valing loss:   28.85476332691919\n",
      "Pure loss: 32.55176001447485.....Total loss: 32.55176001447485\n",
      "Pure loss: 28.823224053261686.....Total loss: 28.823224053261686\n",
      "epoch 1355 learning rate:  0.010738007380073801   Training loss:   32.55176001447485  Valing loss:   28.823224053261686\n",
      "Pure loss: 32.481857614547074.....Total loss: 32.481857614547074\n",
      "Pure loss: 29.01924014395909.....Total loss: 29.01924014395909\n",
      "epoch 1356 learning rate:  0.010737463126843658   Training loss:   32.481857614547074  Valing loss:   29.01924014395909\n",
      "Pure loss: 32.88357210109455.....Total loss: 32.88357210109455\n",
      "Pure loss: 29.187226123856806.....Total loss: 29.187226123856806\n",
      "epoch 1357 learning rate:  0.010736919675755343   Training loss:   32.88357210109455  Valing loss:   29.187226123856806\n",
      "Pure loss: 32.84280859078024.....Total loss: 32.84280859078024\n",
      "Pure loss: 29.16043838336353.....Total loss: 29.16043838336353\n",
      "epoch 1358 learning rate:  0.010736377025036818   Training loss:   32.84280859078024  Valing loss:   29.16043838336353\n",
      "Pure loss: 33.084403855943684.....Total loss: 33.084403855943684\n",
      "Pure loss: 29.095048091933823.....Total loss: 29.095048091933823\n",
      "epoch 1359 learning rate:  0.010735835172921267   Training loss:   33.084403855943684  Valing loss:   29.095048091933823\n",
      "Pure loss: 32.474694732889155.....Total loss: 32.474694732889155\n",
      "Pure loss: 28.710955103258744.....Total loss: 28.710955103258744\n",
      "epoch 1360 learning rate:  0.010735294117647058   Training loss:   32.474694732889155  Valing loss:   28.710955103258744\n",
      "Pure loss: 32.17940894928704.....Total loss: 32.17940894928704\n",
      "Pure loss: 28.63649455914612.....Total loss: 28.63649455914612\n",
      "epoch 1361 learning rate:  0.010734753857457752   Training loss:   32.17940894928704  Valing loss:   28.63649455914612\n",
      "Pure loss: 32.552393209383276.....Total loss: 32.552393209383276\n",
      "Pure loss: 27.67762694301958.....Total loss: 27.67762694301958\n",
      "epoch 1362 learning rate:  0.010734214390602057   Training loss:   32.552393209383276  Valing loss:   27.67762694301958\n",
      "Pure loss: 32.1683066244398.....Total loss: 32.1683066244398\n",
      "Pure loss: 27.81959090068547.....Total loss: 27.81959090068547\n",
      "epoch 1363 learning rate:  0.010733675715333823   Training loss:   32.1683066244398  Valing loss:   27.81959090068547\n",
      "Pure loss: 32.14021447390185.....Total loss: 32.14021447390185\n",
      "Pure loss: 27.815172258328708.....Total loss: 27.815172258328708\n",
      "epoch 1364 learning rate:  0.010733137829912023   Training loss:   32.14021447390185  Valing loss:   27.815172258328708\n",
      "Pure loss: 32.131818567177284.....Total loss: 32.131818567177284\n",
      "Pure loss: 27.82774578365348.....Total loss: 27.82774578365348\n",
      "epoch 1365 learning rate:  0.010732600732600732   Training loss:   32.131818567177284  Valing loss:   27.82774578365348\n",
      "Pure loss: 32.420474137024165.....Total loss: 32.420474137024165\n",
      "Pure loss: 27.61399791792965.....Total loss: 27.61399791792965\n",
      "epoch 1366 learning rate:  0.010732064421669107   Training loss:   32.420474137024165  Valing loss:   27.61399791792965\n",
      "Pure loss: 32.34844588619123.....Total loss: 32.34844588619123\n",
      "Pure loss: 27.61628941300866.....Total loss: 27.61628941300866\n",
      "epoch 1367 learning rate:  0.010731528895391368   Training loss:   32.34844588619123  Valing loss:   27.61628941300866\n",
      "Pure loss: 32.328434079007266.....Total loss: 32.328434079007266\n",
      "Pure loss: 27.62735851705987.....Total loss: 27.62735851705987\n",
      "epoch 1368 learning rate:  0.010730994152046784   Training loss:   32.328434079007266  Valing loss:   27.62735851705987\n",
      "Pure loss: 32.39285899057126.....Total loss: 32.39285899057126\n",
      "Pure loss: 27.615254232348178.....Total loss: 27.615254232348178\n",
      "epoch 1369 learning rate:  0.01073046018991965   Training loss:   32.39285899057126  Valing loss:   27.615254232348178\n",
      "Pure loss: 32.181832106758286.....Total loss: 32.181832106758286\n",
      "Pure loss: 27.69407259163177.....Total loss: 27.69407259163177\n",
      "epoch 1370 learning rate:  0.01072992700729927   Training loss:   32.181832106758286  Valing loss:   27.69407259163177\n",
      "Pure loss: 31.909905760394352.....Total loss: 31.909905760394352\n",
      "Pure loss: 28.67322616518376.....Total loss: 28.67322616518376\n",
      "epoch 1371 learning rate:  0.010729394602479942   Training loss:   31.909905760394352  Valing loss:   28.67322616518376\n",
      "Pure loss: 32.06431847984029.....Total loss: 32.06431847984029\n",
      "Pure loss: 29.332024860224124.....Total loss: 29.332024860224124\n",
      "epoch 1372 learning rate:  0.010728862973760933   Training loss:   32.06431847984029  Valing loss:   29.332024860224124\n",
      "Pure loss: 31.688280885995205.....Total loss: 31.688280885995205\n",
      "Pure loss: 28.52403664557687.....Total loss: 28.52403664557687\n",
      "epoch 1373 learning rate:  0.010728332119446468   Training loss:   31.688280885995205  Valing loss:   28.52403664557687\n",
      "Pure loss: 31.873222608026317.....Total loss: 31.873222608026317\n",
      "Pure loss: 29.19073076674328.....Total loss: 29.19073076674328\n",
      "epoch 1374 learning rate:  0.010727802037845706   Training loss:   31.873222608026317  Valing loss:   29.19073076674328\n",
      "Pure loss: 31.779159524901655.....Total loss: 31.779159524901655\n",
      "Pure loss: 28.925251766657016.....Total loss: 28.925251766657016\n",
      "epoch 1375 learning rate:  0.010727272727272728   Training loss:   31.779159524901655  Valing loss:   28.925251766657016\n",
      "Pure loss: 32.206334718312945.....Total loss: 32.206334718312945\n",
      "Pure loss: 30.06811019567252.....Total loss: 30.06811019567252\n",
      "epoch 1376 learning rate:  0.010726744186046511   Training loss:   32.206334718312945  Valing loss:   30.06811019567252\n",
      "Pure loss: 32.23491805125241.....Total loss: 32.23491805125241\n",
      "Pure loss: 30.12741799409731.....Total loss: 30.12741799409731\n",
      "epoch 1377 learning rate:  0.010726216412490922   Training loss:   32.23491805125241  Valing loss:   30.12741799409731\n",
      "Pure loss: 32.09423339961123.....Total loss: 32.09423339961123\n",
      "Pure loss: 29.803511689807078.....Total loss: 29.803511689807078\n",
      "epoch 1378 learning rate:  0.010725689404934688   Training loss:   32.09423339961123  Valing loss:   29.803511689807078\n",
      "Pure loss: 32.03153345745394.....Total loss: 32.03153345745394\n",
      "Pure loss: 29.64807811772484.....Total loss: 29.64807811772484\n",
      "epoch 1379 learning rate:  0.010725163161711385   Training loss:   32.03153345745394  Valing loss:   29.64807811772484\n",
      "Pure loss: 33.09206135703364.....Total loss: 33.09206135703364\n",
      "Pure loss: 31.959505501639455.....Total loss: 31.959505501639455\n",
      "epoch 1380 learning rate:  0.010724637681159421   Training loss:   33.09206135703364  Valing loss:   31.959505501639455\n",
      "Pure loss: 33.588814276844445.....Total loss: 33.588814276844445\n",
      "Pure loss: 32.82951120005032.....Total loss: 32.82951120005032\n",
      "epoch 1381 learning rate:  0.010724112961622014   Training loss:   33.588814276844445  Valing loss:   32.82951120005032\n",
      "Pure loss: 34.89098363720044.....Total loss: 34.89098363720044\n",
      "Pure loss: 34.88635394498625.....Total loss: 34.88635394498625\n",
      "epoch 1382 learning rate:  0.010723589001447178   Training loss:   34.89098363720044  Valing loss:   34.88635394498625\n",
      "Pure loss: 34.800109648118884.....Total loss: 34.800109648118884\n",
      "Pure loss: 34.75013284907144.....Total loss: 34.75013284907144\n",
      "epoch 1383 learning rate:  0.010723065798987708   Training loss:   34.800109648118884  Valing loss:   34.75013284907144\n",
      "Pure loss: 35.855149937395986.....Total loss: 35.855149937395986\n",
      "Pure loss: 36.25263823122578.....Total loss: 36.25263823122578\n",
      "epoch 1384 learning rate:  0.010722543352601156   Training loss:   35.855149937395986  Valing loss:   36.25263823122578\n",
      "Pure loss: 34.712477198553344.....Total loss: 34.712477198553344\n",
      "Pure loss: 34.648614320158565.....Total loss: 34.648614320158565\n",
      "epoch 1385 learning rate:  0.01072202166064982   Training loss:   34.712477198553344  Valing loss:   34.648614320158565\n",
      "Pure loss: 35.01441066191502.....Total loss: 35.01441066191502\n",
      "Pure loss: 35.09263785389275.....Total loss: 35.09263785389275\n",
      "epoch 1386 learning rate:  0.010721500721500722   Training loss:   35.01441066191502  Valing loss:   35.09263785389275\n",
      "Pure loss: 33.668032421092825.....Total loss: 33.668032421092825\n",
      "Pure loss: 33.033921052379206.....Total loss: 33.033921052379206\n",
      "epoch 1387 learning rate:  0.010720980533525595   Training loss:   33.668032421092825  Valing loss:   33.033921052379206\n",
      "Pure loss: 33.644543972894255.....Total loss: 33.644543972894255\n",
      "Pure loss: 32.99481145607796.....Total loss: 32.99481145607796\n",
      "epoch 1388 learning rate:  0.010720461095100865   Training loss:   33.644543972894255  Valing loss:   32.99481145607796\n",
      "Pure loss: 32.75114176121915.....Total loss: 32.75114176121915\n",
      "Pure loss: 31.480366841669774.....Total loss: 31.480366841669774\n",
      "epoch 1389 learning rate:  0.010719942404607632   Training loss:   32.75114176121915  Valing loss:   31.480366841669774\n",
      "Pure loss: 31.92602056806762.....Total loss: 31.92602056806762\n",
      "Pure loss: 29.889409376377156.....Total loss: 29.889409376377156\n",
      "epoch 1390 learning rate:  0.010719424460431655   Training loss:   31.92602056806762  Valing loss:   29.889409376377156\n",
      "Pure loss: 31.92886989219612.....Total loss: 31.92886989219612\n",
      "Pure loss: 29.895393662010864.....Total loss: 29.895393662010864\n",
      "epoch 1391 learning rate:  0.010718907260963336   Training loss:   31.92886989219612  Valing loss:   29.895393662010864\n",
      "Pure loss: 31.360267137404687.....Total loss: 31.360267137404687\n",
      "Pure loss: 28.362815068911843.....Total loss: 28.362815068911843\n",
      "epoch 1392 learning rate:  0.010718390804597702   Training loss:   31.360267137404687  Valing loss:   28.362815068911843\n",
      "Pure loss: 31.362513143503566.....Total loss: 31.362513143503566\n",
      "Pure loss: 28.40609489717047.....Total loss: 28.40609489717047\n",
      "epoch 1393 learning rate:  0.010717875089734387   Training loss:   31.362513143503566  Valing loss:   28.40609489717047\n",
      "Pure loss: 31.30933235106262.....Total loss: 31.30933235106262\n",
      "Pure loss: 28.020399236018903.....Total loss: 28.020399236018903\n",
      "epoch 1394 learning rate:  0.010717360114777618   Training loss:   31.30933235106262  Valing loss:   28.020399236018903\n",
      "Pure loss: 31.35400284585027.....Total loss: 31.35400284585027\n",
      "Pure loss: 28.26325139875168.....Total loss: 28.26325139875168\n",
      "epoch 1395 learning rate:  0.010716845878136202   Training loss:   31.35400284585027  Valing loss:   28.26325139875168\n",
      "Pure loss: 31.408709371393755.....Total loss: 31.408709371393755\n",
      "Pure loss: 28.522128808463687.....Total loss: 28.522128808463687\n",
      "epoch 1396 learning rate:  0.010716332378223496   Training loss:   31.408709371393755  Valing loss:   28.522128808463687\n",
      "Pure loss: 31.325795979621983.....Total loss: 31.325795979621983\n",
      "Pure loss: 28.085853355534283.....Total loss: 28.085853355534283\n",
      "epoch 1397 learning rate:  0.01071581961345741   Training loss:   31.325795979621983  Valing loss:   28.085853355534283\n",
      "Pure loss: 31.3134455576458.....Total loss: 31.3134455576458\n",
      "Pure loss: 27.782347337941406.....Total loss: 27.782347337941406\n",
      "epoch 1398 learning rate:  0.010715307582260372   Training loss:   31.3134455576458  Valing loss:   27.782347337941406\n",
      "Pure loss: 31.317478388589212.....Total loss: 31.317478388589212\n",
      "Pure loss: 27.714045496677237.....Total loss: 27.714045496677237\n",
      "epoch 1399 learning rate:  0.010714796283059328   Training loss:   31.317478388589212  Valing loss:   27.714045496677237\n",
      "Pure loss: 31.309418142424562.....Total loss: 31.309418142424562\n",
      "Pure loss: 27.7085936225091.....Total loss: 27.7085936225091\n",
      "epoch 1400 learning rate:  0.010714285714285714   Training loss:   31.309418142424562  Valing loss:   27.7085936225091\n",
      "Pure loss: 31.301537513601094.....Total loss: 31.301537513601094\n",
      "Pure loss: 27.826497041207524.....Total loss: 27.826497041207524\n",
      "epoch 1401 learning rate:  0.010713775874375447   Training loss:   31.301537513601094  Valing loss:   27.826497041207524\n",
      "Pure loss: 31.321679511956784.....Total loss: 31.321679511956784\n",
      "Pure loss: 27.610500118658358.....Total loss: 27.610500118658358\n",
      "epoch 1402 learning rate:  0.010713266761768902   Training loss:   31.321679511956784  Valing loss:   27.610500118658358\n",
      "Pure loss: 31.243013300581193.....Total loss: 31.243013300581193\n",
      "Pure loss: 27.654630467129973.....Total loss: 27.654630467129973\n",
      "epoch 1403 learning rate:  0.010712758374910905   Training loss:   31.243013300581193  Valing loss:   27.654630467129973\n",
      "Pure loss: 31.243197511990907.....Total loss: 31.243197511990907\n",
      "Pure loss: 28.10656203407956.....Total loss: 28.10656203407956\n",
      "epoch 1404 learning rate:  0.010712250712250713   Training loss:   31.243197511990907  Valing loss:   28.10656203407956\n",
      "Pure loss: 31.257592588830594.....Total loss: 31.257592588830594\n",
      "Pure loss: 28.26258003446127.....Total loss: 28.26258003446127\n",
      "epoch 1405 learning rate:  0.010711743772241994   Training loss:   31.257592588830594  Valing loss:   28.26258003446127\n",
      "Pure loss: 31.233118088694912.....Total loss: 31.233118088694912\n",
      "Pure loss: 27.906484023226856.....Total loss: 27.906484023226856\n",
      "epoch 1406 learning rate:  0.010711237553342816   Training loss:   31.233118088694912  Valing loss:   27.906484023226856\n",
      "Pure loss: 31.334416630472578.....Total loss: 31.334416630472578\n",
      "Pure loss: 28.570943290965126.....Total loss: 28.570943290965126\n",
      "epoch 1407 learning rate:  0.010710732054015636   Training loss:   31.334416630472578  Valing loss:   28.570943290965126\n",
      "Pure loss: 31.20824565176768.....Total loss: 31.20824565176768\n",
      "Pure loss: 27.90900020394458.....Total loss: 27.90900020394458\n",
      "epoch 1408 learning rate:  0.010710227272727272   Training loss:   31.20824565176768  Valing loss:   27.90900020394458\n",
      "Pure loss: 31.200816964279472.....Total loss: 31.200816964279472\n",
      "Pure loss: 27.636110674319127.....Total loss: 27.636110674319127\n",
      "epoch 1409 learning rate:  0.0107097232079489   Training loss:   31.200816964279472  Valing loss:   27.636110674319127\n",
      "Pure loss: 31.33125815075346.....Total loss: 31.33125815075346\n",
      "Pure loss: 27.336667838084985.....Total loss: 27.336667838084985\n",
      "epoch 1410 learning rate:  0.010709219858156028   Training loss:   31.33125815075346  Valing loss:   27.336667838084985\n",
      "Pure loss: 31.245690123916823.....Total loss: 31.245690123916823\n",
      "Pure loss: 27.318622437938647.....Total loss: 27.318622437938647\n",
      "epoch 1411 learning rate:  0.010708717221828491   Training loss:   31.245690123916823  Valing loss:   27.318622437938647\n",
      "Pure loss: 31.154500686828595.....Total loss: 31.154500686828595\n",
      "Pure loss: 27.46176246330586.....Total loss: 27.46176246330586\n",
      "epoch 1412 learning rate:  0.010708215297450425   Training loss:   31.154500686828595  Valing loss:   27.46176246330586\n",
      "Pure loss: 31.134413468781972.....Total loss: 31.134413468781972\n",
      "Pure loss: 27.591287273394414.....Total loss: 27.591287273394414\n",
      "epoch 1413 learning rate:  0.010707714083510261   Training loss:   31.134413468781972  Valing loss:   27.591287273394414\n",
      "Pure loss: 31.13223679625105.....Total loss: 31.13223679625105\n",
      "Pure loss: 27.56983935253301.....Total loss: 27.56983935253301\n",
      "epoch 1414 learning rate:  0.010707213578500708   Training loss:   31.13223679625105  Valing loss:   27.56983935253301\n",
      "Pure loss: 31.141620327065187.....Total loss: 31.141620327065187\n",
      "Pure loss: 27.46679535094617.....Total loss: 27.46679535094617\n",
      "epoch 1415 learning rate:  0.010706713780918729   Training loss:   31.141620327065187  Valing loss:   27.46679535094617\n",
      "Pure loss: 31.136901350334778.....Total loss: 31.136901350334778\n",
      "Pure loss: 27.48763869973775.....Total loss: 27.48763869973775\n",
      "epoch 1416 learning rate:  0.010706214689265537   Training loss:   31.136901350334778  Valing loss:   27.48763869973775\n",
      "Pure loss: 31.33725497750603.....Total loss: 31.33725497750603\n",
      "Pure loss: 27.48264579792682.....Total loss: 27.48264579792682\n",
      "epoch 1417 learning rate:  0.010705716302046578   Training loss:   31.33725497750603  Valing loss:   27.48264579792682\n",
      "Pure loss: 31.208851642028517.....Total loss: 31.208851642028517\n",
      "Pure loss: 27.902987249921306.....Total loss: 27.902987249921306\n",
      "epoch 1418 learning rate:  0.01070521861777151   Training loss:   31.208851642028517  Valing loss:   27.902987249921306\n",
      "Pure loss: 31.158652000992234.....Total loss: 31.158652000992234\n",
      "Pure loss: 27.99736877552311.....Total loss: 27.99736877552311\n",
      "epoch 1419 learning rate:  0.010704721634954194   Training loss:   31.158652000992234  Valing loss:   27.99736877552311\n",
      "Pure loss: 31.135179267022483.....Total loss: 31.135179267022483\n",
      "Pure loss: 27.9987549002569.....Total loss: 27.9987549002569\n",
      "epoch 1420 learning rate:  0.010704225352112677   Training loss:   31.135179267022483  Valing loss:   27.9987549002569\n",
      "Pure loss: 31.155360182225976.....Total loss: 31.155360182225976\n",
      "Pure loss: 27.476246373753742.....Total loss: 27.476246373753742\n",
      "epoch 1421 learning rate:  0.010703729767769177   Training loss:   31.155360182225976  Valing loss:   27.476246373753742\n",
      "Pure loss: 31.181763044822464.....Total loss: 31.181763044822464\n",
      "Pure loss: 27.377258130479095.....Total loss: 27.377258130479095\n",
      "epoch 1422 learning rate:  0.010703234880450071   Training loss:   31.181763044822464  Valing loss:   27.377258130479095\n",
      "Pure loss: 31.18709237238411.....Total loss: 31.18709237238411\n",
      "Pure loss: 27.365764546420326.....Total loss: 27.365764546420326\n",
      "epoch 1423 learning rate:  0.010702740688685874   Training loss:   31.18709237238411  Valing loss:   27.365764546420326\n",
      "Pure loss: 31.097633264656512.....Total loss: 31.097633264656512\n",
      "Pure loss: 27.726926600361306.....Total loss: 27.726926600361306\n",
      "epoch 1424 learning rate:  0.010702247191011236   Training loss:   31.097633264656512  Valing loss:   27.726926600361306\n",
      "Pure loss: 31.181818034759274.....Total loss: 31.181818034759274\n",
      "Pure loss: 27.453693778930386.....Total loss: 27.453693778930386\n",
      "epoch 1425 learning rate:  0.010701754385964912   Training loss:   31.181818034759274  Valing loss:   27.453693778930386\n",
      "Pure loss: 31.400563373777054.....Total loss: 31.400563373777054\n",
      "Pure loss: 27.169784407909408.....Total loss: 27.169784407909408\n",
      "epoch 1426 learning rate:  0.010701262272089762   Training loss:   31.400563373777054  Valing loss:   27.169784407909408\n",
      "Pure loss: 31.500812526650723.....Total loss: 31.500812526650723\n",
      "Pure loss: 27.161473573545347.....Total loss: 27.161473573545347\n",
      "epoch 1427 learning rate:  0.010700770847932726   Training loss:   31.500812526650723  Valing loss:   27.161473573545347\n",
      "Pure loss: 31.368141241402444.....Total loss: 31.368141241402444\n",
      "Pure loss: 27.235004715855425.....Total loss: 27.235004715855425\n",
      "epoch 1428 learning rate:  0.010700280112044817   Training loss:   31.368141241402444  Valing loss:   27.235004715855425\n",
      "Pure loss: 31.361481436816405.....Total loss: 31.361481436816405\n",
      "Pure loss: 27.235113740249297.....Total loss: 27.235113740249297\n",
      "epoch 1429 learning rate:  0.010699790062981105   Training loss:   31.361481436816405  Valing loss:   27.235113740249297\n",
      "Pure loss: 31.406518089505163.....Total loss: 31.406518089505163\n",
      "Pure loss: 27.187287611597792.....Total loss: 27.187287611597792\n",
      "epoch 1430 learning rate:  0.010699300699300699   Training loss:   31.406518089505163  Valing loss:   27.187287611597792\n",
      "Pure loss: 31.988586531475875.....Total loss: 31.988586531475875\n",
      "Pure loss: 26.985596725634124.....Total loss: 26.985596725634124\n",
      "epoch 1431 learning rate:  0.010698812019566737   Training loss:   31.988586531475875  Valing loss:   26.985596725634124\n",
      "Pure loss: 32.01012836306503.....Total loss: 32.01012836306503\n",
      "Pure loss: 27.000304067035326.....Total loss: 27.000304067035326\n",
      "epoch 1432 learning rate:  0.010698324022346368   Training loss:   32.01012836306503  Valing loss:   27.000304067035326\n",
      "Pure loss: 32.03053924424792.....Total loss: 32.03053924424792\n",
      "Pure loss: 27.007417881154353.....Total loss: 27.007417881154353\n",
      "epoch 1433 learning rate:  0.010697836706210747   Training loss:   32.03053924424792  Valing loss:   27.007417881154353\n",
      "Pure loss: 31.518578091447026.....Total loss: 31.518578091447026\n",
      "Pure loss: 27.1671666147625.....Total loss: 27.1671666147625\n",
      "epoch 1434 learning rate:  0.010697350069735007   Training loss:   31.518578091447026  Valing loss:   27.1671666147625\n",
      "Pure loss: 31.697361280916176.....Total loss: 31.697361280916176\n",
      "Pure loss: 27.165475535759768.....Total loss: 27.165475535759768\n",
      "epoch 1435 learning rate:  0.010696864111498259   Training loss:   31.697361280916176  Valing loss:   27.165475535759768\n",
      "Pure loss: 31.421549495154174.....Total loss: 31.421549495154174\n",
      "Pure loss: 27.20617597540987.....Total loss: 27.20617597540987\n",
      "epoch 1436 learning rate:  0.010696378830083566   Training loss:   31.421549495154174  Valing loss:   27.20617597540987\n",
      "Pure loss: 31.237378046684995.....Total loss: 31.237378046684995\n",
      "Pure loss: 27.468142573319042.....Total loss: 27.468142573319042\n",
      "epoch 1437 learning rate:  0.010695894224077941   Training loss:   31.237378046684995  Valing loss:   27.468142573319042\n",
      "Pure loss: 31.545331177545712.....Total loss: 31.545331177545712\n",
      "Pure loss: 26.99724038927159.....Total loss: 26.99724038927159\n",
      "epoch 1438 learning rate:  0.010695410292072324   Training loss:   31.545331177545712  Valing loss:   26.99724038927159\n",
      "Pure loss: 31.41771088524987.....Total loss: 31.41771088524987\n",
      "Pure loss: 27.136731111015006.....Total loss: 27.136731111015006\n",
      "epoch 1439 learning rate:  0.010694927032661571   Training loss:   31.41771088524987  Valing loss:   27.136731111015006\n",
      "Pure loss: 31.907492659244774.....Total loss: 31.907492659244774\n",
      "Pure loss: 26.880475946892265.....Total loss: 26.880475946892265\n",
      "epoch 1440 learning rate:  0.010694444444444444   Training loss:   31.907492659244774  Valing loss:   26.880475946892265\n",
      "Pure loss: 31.971369539598577.....Total loss: 31.971369539598577\n",
      "Pure loss: 26.8784072388247.....Total loss: 26.8784072388247\n",
      "epoch 1441 learning rate:  0.010693962526023594   Training loss:   31.971369539598577  Valing loss:   26.8784072388247\n",
      "Pure loss: 32.00263798268402.....Total loss: 32.00263798268402\n",
      "Pure loss: 26.88896628313213.....Total loss: 26.88896628313213\n",
      "epoch 1442 learning rate:  0.010693481276005548   Training loss:   32.00263798268402  Valing loss:   26.88896628313213\n",
      "Pure loss: 32.50122261024718.....Total loss: 32.50122261024718\n",
      "Pure loss: 26.868571130436113.....Total loss: 26.868571130436113\n",
      "epoch 1443 learning rate:  0.010693000693000693   Training loss:   32.50122261024718  Valing loss:   26.868571130436113\n",
      "Pure loss: 32.42411021593694.....Total loss: 32.42411021593694\n",
      "Pure loss: 26.799541336154157.....Total loss: 26.799541336154157\n",
      "epoch 1444 learning rate:  0.01069252077562327   Training loss:   32.42411021593694  Valing loss:   26.799541336154157\n",
      "Pure loss: 32.2636847196883.....Total loss: 32.2636847196883\n",
      "Pure loss: 26.767771780936652.....Total loss: 26.767771780936652\n",
      "epoch 1445 learning rate:  0.01069204152249135   Training loss:   32.2636847196883  Valing loss:   26.767771780936652\n",
      "Pure loss: 32.432862391310664.....Total loss: 32.432862391310664\n",
      "Pure loss: 26.869799544799637.....Total loss: 26.869799544799637\n",
      "epoch 1446 learning rate:  0.010691562932226832   Training loss:   32.432862391310664  Valing loss:   26.869799544799637\n",
      "Pure loss: 32.26463261425366.....Total loss: 32.26463261425366\n",
      "Pure loss: 26.745587828286364.....Total loss: 26.745587828286364\n",
      "epoch 1447 learning rate:  0.010691085003455425   Training loss:   32.26463261425366  Valing loss:   26.745587828286364\n",
      "Pure loss: 31.829673890408767.....Total loss: 31.829673890408767\n",
      "Pure loss: 26.774490313422817.....Total loss: 26.774490313422817\n",
      "epoch 1448 learning rate:  0.01069060773480663   Training loss:   31.829673890408767  Valing loss:   26.774490313422817\n",
      "Pure loss: 31.054084836695868.....Total loss: 31.054084836695868\n",
      "Pure loss: 28.697406649420017.....Total loss: 28.697406649420017\n",
      "epoch 1449 learning rate:  0.010690131124913734   Training loss:   31.054084836695868  Valing loss:   28.697406649420017\n",
      "Pure loss: 31.43835618250148.....Total loss: 31.43835618250148\n",
      "Pure loss: 29.692150832380747.....Total loss: 29.692150832380747\n",
      "epoch 1450 learning rate:  0.010689655172413793   Training loss:   31.43835618250148  Valing loss:   29.692150832380747\n",
      "Pure loss: 31.412871962940496.....Total loss: 31.412871962940496\n",
      "Pure loss: 29.34263251861709.....Total loss: 29.34263251861709\n",
      "epoch 1451 learning rate:  0.010689179875947622   Training loss:   31.412871962940496  Valing loss:   29.34263251861709\n",
      "Pure loss: 31.018321350805977.....Total loss: 31.018321350805977\n",
      "Pure loss: 28.426982273487766.....Total loss: 28.426982273487766\n",
      "epoch 1452 learning rate:  0.01068870523415978   Training loss:   31.018321350805977  Valing loss:   28.426982273487766\n",
      "Pure loss: 30.85215937135391.....Total loss: 30.85215937135391\n",
      "Pure loss: 27.754503100820138.....Total loss: 27.754503100820138\n",
      "epoch 1453 learning rate:  0.010688231245698555   Training loss:   30.85215937135391  Valing loss:   27.754503100820138\n",
      "Pure loss: 30.83684858036225.....Total loss: 30.83684858036225\n",
      "Pure loss: 28.007237183575427.....Total loss: 28.007237183575427\n",
      "epoch 1454 learning rate:  0.010687757909215957   Training loss:   30.83684858036225  Valing loss:   28.007237183575427\n",
      "Pure loss: 31.094924763366123.....Total loss: 31.094924763366123\n",
      "Pure loss: 28.020544591202377.....Total loss: 28.020544591202377\n",
      "epoch 1455 learning rate:  0.010687285223367697   Training loss:   31.094924763366123  Valing loss:   28.020544591202377\n",
      "Pure loss: 31.069482025683417.....Total loss: 31.069482025683417\n",
      "Pure loss: 28.441268925652153.....Total loss: 28.441268925652153\n",
      "epoch 1456 learning rate:  0.010686813186813186   Training loss:   31.069482025683417  Valing loss:   28.441268925652153\n",
      "Pure loss: 31.029691891622235.....Total loss: 31.029691891622235\n",
      "Pure loss: 27.911590604750295.....Total loss: 27.911590604750295\n",
      "epoch 1457 learning rate:  0.010686341798215511   Training loss:   31.029691891622235  Valing loss:   27.911590604750295\n",
      "Pure loss: 31.058667315833024.....Total loss: 31.058667315833024\n",
      "Pure loss: 27.727461623157286.....Total loss: 27.727461623157286\n",
      "epoch 1458 learning rate:  0.010685871056241426   Training loss:   31.058667315833024  Valing loss:   27.727461623157286\n",
      "Pure loss: 31.044855139342772.....Total loss: 31.044855139342772\n",
      "Pure loss: 28.38924271624781.....Total loss: 28.38924271624781\n",
      "epoch 1459 learning rate:  0.010685400959561343   Training loss:   31.044855139342772  Valing loss:   28.38924271624781\n",
      "Pure loss: 31.072687874979962.....Total loss: 31.072687874979962\n",
      "Pure loss: 28.658130176039425.....Total loss: 28.658130176039425\n",
      "epoch 1460 learning rate:  0.010684931506849316   Training loss:   31.072687874979962  Valing loss:   28.658130176039425\n",
      "Pure loss: 31.034045735277278.....Total loss: 31.034045735277278\n",
      "Pure loss: 28.632516477342.....Total loss: 28.632516477342\n",
      "epoch 1461 learning rate:  0.010684462696783026   Training loss:   31.034045735277278  Valing loss:   28.632516477342\n",
      "Pure loss: 30.92479888256114.....Total loss: 30.92479888256114\n",
      "Pure loss: 27.95308797032377.....Total loss: 27.95308797032377\n",
      "epoch 1462 learning rate:  0.010683994528043776   Training loss:   30.92479888256114  Valing loss:   27.95308797032377\n",
      "Pure loss: 30.97569823241836.....Total loss: 30.97569823241836\n",
      "Pure loss: 27.617926617773488.....Total loss: 27.617926617773488\n",
      "epoch 1463 learning rate:  0.010683526999316474   Training loss:   30.97569823241836  Valing loss:   27.617926617773488\n",
      "Pure loss: 30.911841441771546.....Total loss: 30.911841441771546\n",
      "Pure loss: 27.617962855300867.....Total loss: 27.617962855300867\n",
      "epoch 1464 learning rate:  0.010683060109289618   Training loss:   30.911841441771546  Valing loss:   27.617962855300867\n",
      "Pure loss: 31.093144051281822.....Total loss: 31.093144051281822\n",
      "Pure loss: 27.212894253960222.....Total loss: 27.212894253960222\n",
      "epoch 1465 learning rate:  0.01068259385665529   Training loss:   31.093144051281822  Valing loss:   27.212894253960222\n",
      "Pure loss: 31.287592181767963.....Total loss: 31.287592181767963\n",
      "Pure loss: 27.180588266061523.....Total loss: 27.180588266061523\n",
      "epoch 1466 learning rate:  0.01068212824010914   Training loss:   31.287592181767963  Valing loss:   27.180588266061523\n",
      "Pure loss: 31.774985975570953.....Total loss: 31.774985975570953\n",
      "Pure loss: 27.6297037682461.....Total loss: 27.6297037682461\n",
      "epoch 1467 learning rate:  0.010681663258350375   Training loss:   31.774985975570953  Valing loss:   27.6297037682461\n",
      "Pure loss: 31.469235879177077.....Total loss: 31.469235879177077\n",
      "Pure loss: 27.758150337015262.....Total loss: 27.758150337015262\n",
      "epoch 1468 learning rate:  0.010681198910081744   Training loss:   31.469235879177077  Valing loss:   27.758150337015262\n",
      "Pure loss: 31.378254352864026.....Total loss: 31.378254352864026\n",
      "Pure loss: 27.6849177422584.....Total loss: 27.6849177422584\n",
      "epoch 1469 learning rate:  0.010680735194009531   Training loss:   31.378254352864026  Valing loss:   27.6849177422584\n",
      "Pure loss: 31.668457588716855.....Total loss: 31.668457588716855\n",
      "Pure loss: 27.878559300615198.....Total loss: 27.878559300615198\n",
      "epoch 1470 learning rate:  0.010680272108843538   Training loss:   31.668457588716855  Valing loss:   27.878559300615198\n",
      "Pure loss: 31.641422327106106.....Total loss: 31.641422327106106\n",
      "Pure loss: 27.869146695095427.....Total loss: 27.869146695095427\n",
      "epoch 1471 learning rate:  0.010679809653297076   Training loss:   31.641422327106106  Valing loss:   27.869146695095427\n",
      "Pure loss: 31.931525525599607.....Total loss: 31.931525525599607\n",
      "Pure loss: 30.54824902089591.....Total loss: 30.54824902089591\n",
      "epoch 1472 learning rate:  0.010679347826086956   Training loss:   31.931525525599607  Valing loss:   30.54824902089591\n",
      "Pure loss: 31.82817558101849.....Total loss: 31.82817558101849\n",
      "Pure loss: 30.450747514678294.....Total loss: 30.450747514678294\n",
      "epoch 1473 learning rate:  0.010678886625933469   Training loss:   31.82817558101849  Valing loss:   30.450747514678294\n",
      "Pure loss: 31.840013129338374.....Total loss: 31.840013129338374\n",
      "Pure loss: 30.66724010596295.....Total loss: 30.66724010596295\n",
      "epoch 1474 learning rate:  0.01067842605156038   Training loss:   31.840013129338374  Valing loss:   30.66724010596295\n",
      "Pure loss: 32.0578352665396.....Total loss: 32.0578352665396\n",
      "Pure loss: 31.047518811370878.....Total loss: 31.047518811370878\n",
      "epoch 1475 learning rate:  0.010677966101694915   Training loss:   32.0578352665396  Valing loss:   31.047518811370878\n",
      "Pure loss: 32.69428614357656.....Total loss: 32.69428614357656\n",
      "Pure loss: 32.18120424030979.....Total loss: 32.18120424030979\n",
      "epoch 1476 learning rate:  0.010677506775067751   Training loss:   32.69428614357656  Valing loss:   32.18120424030979\n",
      "Pure loss: 33.178258527885596.....Total loss: 33.178258527885596\n",
      "Pure loss: 33.11532553779387.....Total loss: 33.11532553779387\n",
      "epoch 1477 learning rate:  0.010677048070413   Training loss:   33.178258527885596  Valing loss:   33.11532553779387\n",
      "Pure loss: 33.22986344003101.....Total loss: 33.22986344003101\n",
      "Pure loss: 33.1970908707879.....Total loss: 33.1970908707879\n",
      "epoch 1478 learning rate:  0.0106765899864682   Training loss:   33.22986344003101  Valing loss:   33.1970908707879\n",
      "Pure loss: 32.95664406269195.....Total loss: 32.95664406269195\n",
      "Pure loss: 32.77413458773792.....Total loss: 32.77413458773792\n",
      "epoch 1479 learning rate:  0.010676132521974307   Training loss:   32.95664406269195  Valing loss:   32.77413458773792\n",
      "Pure loss: 33.108755627326644.....Total loss: 33.108755627326644\n",
      "Pure loss: 33.0263684384164.....Total loss: 33.0263684384164\n",
      "epoch 1480 learning rate:  0.010675675675675676   Training loss:   33.108755627326644  Valing loss:   33.0263684384164\n",
      "Pure loss: 33.05918043652347.....Total loss: 33.05918043652347\n",
      "Pure loss: 32.94958778477951.....Total loss: 32.94958778477951\n",
      "epoch 1481 learning rate:  0.010675219446320054   Training loss:   33.05918043652347  Valing loss:   32.94958778477951\n",
      "Pure loss: 31.470651284961505.....Total loss: 31.470651284961505\n",
      "Pure loss: 30.40582033980841.....Total loss: 30.40582033980841\n",
      "epoch 1482 learning rate:  0.01067476383265857   Training loss:   31.470651284961505  Valing loss:   30.40582033980841\n",
      "Pure loss: 31.46539806953003.....Total loss: 31.46539806953003\n",
      "Pure loss: 30.34790256510991.....Total loss: 30.34790256510991\n",
      "epoch 1483 learning rate:  0.010674308833445719   Training loss:   31.46539806953003  Valing loss:   30.34790256510991\n",
      "Pure loss: 31.50473598546991.....Total loss: 31.50473598546991\n",
      "Pure loss: 30.374403161701512.....Total loss: 30.374403161701512\n",
      "epoch 1484 learning rate:  0.010673854447439353   Training loss:   31.50473598546991  Valing loss:   30.374403161701512\n",
      "Pure loss: 30.601183931459985.....Total loss: 30.601183931459985\n",
      "Pure loss: 28.595925366234084.....Total loss: 28.595925366234084\n",
      "epoch 1485 learning rate:  0.010673400673400673   Training loss:   30.601183931459985  Valing loss:   28.595925366234084\n",
      "Pure loss: 30.528139912611493.....Total loss: 30.528139912611493\n",
      "Pure loss: 28.248251011598924.....Total loss: 28.248251011598924\n",
      "epoch 1486 learning rate:  0.010672947510094212   Training loss:   30.528139912611493  Valing loss:   28.248251011598924\n",
      "Pure loss: 31.02342297527884.....Total loss: 31.02342297527884\n",
      "Pure loss: 29.602944865209256.....Total loss: 29.602944865209256\n",
      "epoch 1487 learning rate:  0.010672494956287828   Training loss:   31.02342297527884  Valing loss:   29.602944865209256\n",
      "Pure loss: 33.60942315270807.....Total loss: 33.60942315270807\n",
      "Pure loss: 33.936704145509204.....Total loss: 33.936704145509204\n",
      "epoch 1488 learning rate:  0.010672043010752688   Training loss:   33.60942315270807  Valing loss:   33.936704145509204\n",
      "Pure loss: 33.595659472354.....Total loss: 33.595659472354\n",
      "Pure loss: 33.90885304194885.....Total loss: 33.90885304194885\n",
      "epoch 1489 learning rate:  0.010671591672263264   Training loss:   33.595659472354  Valing loss:   33.90885304194885\n",
      "Pure loss: 33.98113999848794.....Total loss: 33.98113999848794\n",
      "Pure loss: 34.491973189325115.....Total loss: 34.491973189325115\n",
      "epoch 1490 learning rate:  0.010671140939597316   Training loss:   33.98113999848794  Valing loss:   34.491973189325115\n",
      "Pure loss: 33.807145444827334.....Total loss: 33.807145444827334\n",
      "Pure loss: 34.056012907093795.....Total loss: 34.056012907093795\n",
      "epoch 1491 learning rate:  0.010670690811535882   Training loss:   33.807145444827334  Valing loss:   34.056012907093795\n",
      "Pure loss: 37.70136775753949.....Total loss: 37.70136775753949\n",
      "Pure loss: 39.32257910737818.....Total loss: 39.32257910737818\n",
      "epoch 1492 learning rate:  0.010670241286863271   Training loss:   37.70136775753949  Valing loss:   39.32257910737818\n",
      "Pure loss: 35.020287373973325.....Total loss: 35.020287373973325\n",
      "Pure loss: 35.698986283318405.....Total loss: 35.698986283318405\n",
      "epoch 1493 learning rate:  0.010669792364367046   Training loss:   35.020287373973325  Valing loss:   35.698986283318405\n",
      "Pure loss: 34.392955260824145.....Total loss: 34.392955260824145\n",
      "Pure loss: 34.79660752197699.....Total loss: 34.79660752197699\n",
      "epoch 1494 learning rate:  0.010669344042838019   Training loss:   34.392955260824145  Valing loss:   34.79660752197699\n",
      "Pure loss: 34.54859772900067.....Total loss: 34.54859772900067\n",
      "Pure loss: 34.8440536810198.....Total loss: 34.8440536810198\n",
      "epoch 1495 learning rate:  0.010668896321070234   Training loss:   34.54859772900067  Valing loss:   34.8440536810198\n",
      "Pure loss: 34.934790830703214.....Total loss: 34.934790830703214\n",
      "Pure loss: 35.47712878065107.....Total loss: 35.47712878065107\n",
      "epoch 1496 learning rate:  0.010668449197860962   Training loss:   34.934790830703214  Valing loss:   35.47712878065107\n",
      "Pure loss: 33.90743460725359.....Total loss: 33.90743460725359\n",
      "Pure loss: 33.75998049137909.....Total loss: 33.75998049137909\n",
      "epoch 1497 learning rate:  0.010668002672010688   Training loss:   33.90743460725359  Valing loss:   33.75998049137909\n",
      "Pure loss: 34.64958759379199.....Total loss: 34.64958759379199\n",
      "Pure loss: 34.99525103294122.....Total loss: 34.99525103294122\n",
      "epoch 1498 learning rate:  0.010667556742323097   Training loss:   34.64958759379199  Valing loss:   34.99525103294122\n",
      "Pure loss: 34.52023183635642.....Total loss: 34.52023183635642\n",
      "Pure loss: 34.903322658383686.....Total loss: 34.903322658383686\n",
      "epoch 1499 learning rate:  0.01066711140760507   Training loss:   34.52023183635642  Valing loss:   34.903322658383686\n",
      "Pure loss: 34.720205283391486.....Total loss: 34.720205283391486\n",
      "Pure loss: 35.26573783275295.....Total loss: 35.26573783275295\n",
      "epoch 1500 learning rate:  0.010666666666666666   Training loss:   34.720205283391486  Valing loss:   35.26573783275295\n",
      "Pure loss: 34.45740585885762.....Total loss: 34.45740585885762\n",
      "Pure loss: 34.86029223096829.....Total loss: 34.86029223096829\n",
      "epoch 1501 learning rate:  0.01066622251832112   Training loss:   34.45740585885762  Valing loss:   34.86029223096829\n",
      "Pure loss: 33.91519282557163.....Total loss: 33.91519282557163\n",
      "Pure loss: 34.012947424034905.....Total loss: 34.012947424034905\n",
      "epoch 1502 learning rate:  0.01066577896138482   Training loss:   33.91519282557163  Valing loss:   34.012947424034905\n",
      "Pure loss: 33.90709541871207.....Total loss: 33.90709541871207\n",
      "Pure loss: 34.21124100086618.....Total loss: 34.21124100086618\n",
      "epoch 1503 learning rate:  0.010665335994677312   Training loss:   33.90709541871207  Valing loss:   34.21124100086618\n",
      "Pure loss: 32.93931526457927.....Total loss: 32.93931526457927\n",
      "Pure loss: 32.62039529167995.....Total loss: 32.62039529167995\n",
      "epoch 1504 learning rate:  0.010664893617021276   Training loss:   32.93931526457927  Valing loss:   32.62039529167995\n",
      "Pure loss: 33.23766363486536.....Total loss: 33.23766363486536\n",
      "Pure loss: 33.218722945075406.....Total loss: 33.218722945075406\n",
      "epoch 1505 learning rate:  0.010664451827242525   Training loss:   33.23766363486536  Valing loss:   33.218722945075406\n",
      "Pure loss: 33.22973943469793.....Total loss: 33.22973943469793\n",
      "Pure loss: 33.18648566322325.....Total loss: 33.18648566322325\n",
      "epoch 1506 learning rate:  0.010664010624169986   Training loss:   33.22973943469793  Valing loss:   33.18648566322325\n",
      "Pure loss: 33.24239289504651.....Total loss: 33.24239289504651\n",
      "Pure loss: 33.20703602800866.....Total loss: 33.20703602800866\n",
      "epoch 1507 learning rate:  0.0106635700066357   Training loss:   33.24239289504651  Valing loss:   33.20703602800866\n",
      "Pure loss: 33.259084150888036.....Total loss: 33.259084150888036\n",
      "Pure loss: 33.2886957815275.....Total loss: 33.2886957815275\n",
      "epoch 1508 learning rate:  0.010663129973474801   Training loss:   33.259084150888036  Valing loss:   33.2886957815275\n",
      "Pure loss: 32.73446229783503.....Total loss: 32.73446229783503\n",
      "Pure loss: 32.4197367076875.....Total loss: 32.4197367076875\n",
      "epoch 1509 learning rate:  0.010662690523525514   Training loss:   32.73446229783503  Valing loss:   32.4197367076875\n",
      "Pure loss: 32.74444798462572.....Total loss: 32.74444798462572\n",
      "Pure loss: 32.43791715783347.....Total loss: 32.43791715783347\n",
      "epoch 1510 learning rate:  0.010662251655629139   Training loss:   32.74444798462572  Valing loss:   32.43791715783347\n",
      "Pure loss: 32.60704033180503.....Total loss: 32.60704033180503\n",
      "Pure loss: 32.19466537049356.....Total loss: 32.19466537049356\n",
      "epoch 1511 learning rate:  0.010661813368630046   Training loss:   32.60704033180503  Valing loss:   32.19466537049356\n",
      "Pure loss: 32.59482025074374.....Total loss: 32.59482025074374\n",
      "Pure loss: 32.1568139949837.....Total loss: 32.1568139949837\n",
      "epoch 1512 learning rate:  0.010661375661375662   Training loss:   32.59482025074374  Valing loss:   32.1568139949837\n",
      "Pure loss: 32.25779126806779.....Total loss: 32.25779126806779\n",
      "Pure loss: 31.554312777446725.....Total loss: 31.554312777446725\n",
      "epoch 1513 learning rate:  0.010660938532716458   Training loss:   32.25779126806779  Valing loss:   31.554312777446725\n",
      "Pure loss: 32.303098675834335.....Total loss: 32.303098675834335\n",
      "Pure loss: 31.59828119180844.....Total loss: 31.59828119180844\n",
      "epoch 1514 learning rate:  0.010660501981505945   Training loss:   32.303098675834335  Valing loss:   31.59828119180844\n",
      "Pure loss: 32.171023348375265.....Total loss: 32.171023348375265\n",
      "Pure loss: 31.344232847843674.....Total loss: 31.344232847843674\n",
      "epoch 1515 learning rate:  0.01066006600660066   Training loss:   32.171023348375265  Valing loss:   31.344232847843674\n",
      "Pure loss: 32.01094272441653.....Total loss: 32.01094272441653\n",
      "Pure loss: 30.990958987911217.....Total loss: 30.990958987911217\n",
      "epoch 1516 learning rate:  0.01065963060686016   Training loss:   32.01094272441653  Valing loss:   30.990958987911217\n",
      "Pure loss: 31.81750008155747.....Total loss: 31.81750008155747\n",
      "Pure loss: 30.901538754908778.....Total loss: 30.901538754908778\n",
      "epoch 1517 learning rate:  0.010659195781147001   Training loss:   31.81750008155747  Valing loss:   30.901538754908778\n",
      "Pure loss: 32.371138577550234.....Total loss: 32.371138577550234\n",
      "Pure loss: 31.21145899640973.....Total loss: 31.21145899640973\n",
      "epoch 1518 learning rate:  0.010658761528326746   Training loss:   32.371138577550234  Valing loss:   31.21145899640973\n",
      "Pure loss: 32.28067092853771.....Total loss: 32.28067092853771\n",
      "Pure loss: 30.805298569605092.....Total loss: 30.805298569605092\n",
      "epoch 1519 learning rate:  0.01065832784726794   Training loss:   32.28067092853771  Valing loss:   30.805298569605092\n",
      "Pure loss: 32.331047524643814.....Total loss: 32.331047524643814\n",
      "Pure loss: 30.85095395120845.....Total loss: 30.85095395120845\n",
      "epoch 1520 learning rate:  0.010657894736842106   Training loss:   32.331047524643814  Valing loss:   30.85095395120845\n",
      "Pure loss: 32.290116387054816.....Total loss: 32.290116387054816\n",
      "Pure loss: 30.773132412106182.....Total loss: 30.773132412106182\n",
      "epoch 1521 learning rate:  0.010657462195923735   Training loss:   32.290116387054816  Valing loss:   30.773132412106182\n",
      "Pure loss: 32.23933309960746.....Total loss: 32.23933309960746\n",
      "Pure loss: 30.604871491014165.....Total loss: 30.604871491014165\n",
      "epoch 1522 learning rate:  0.010657030223390276   Training loss:   32.23933309960746  Valing loss:   30.604871491014165\n",
      "Pure loss: 31.850298693569663.....Total loss: 31.850298693569663\n",
      "Pure loss: 30.34351899726059.....Total loss: 30.34351899726059\n",
      "epoch 1523 learning rate:  0.010656598818122127   Training loss:   31.850298693569663  Valing loss:   30.34351899726059\n",
      "Pure loss: 31.364135446012025.....Total loss: 31.364135446012025\n",
      "Pure loss: 30.087138869103274.....Total loss: 30.087138869103274\n",
      "epoch 1524 learning rate:  0.010656167979002625   Training loss:   31.364135446012025  Valing loss:   30.087138869103274\n",
      "Pure loss: 31.43219796855026.....Total loss: 31.43219796855026\n",
      "Pure loss: 30.24356835016283.....Total loss: 30.24356835016283\n",
      "epoch 1525 learning rate:  0.010655737704918032   Training loss:   31.43219796855026  Valing loss:   30.24356835016283\n",
      "Pure loss: 31.208729954296526.....Total loss: 31.208729954296526\n",
      "Pure loss: 29.714639044070402.....Total loss: 29.714639044070402\n",
      "epoch 1526 learning rate:  0.010655307994757536   Training loss:   31.208729954296526  Valing loss:   29.714639044070402\n",
      "Pure loss: 31.395402851314316.....Total loss: 31.395402851314316\n",
      "Pure loss: 30.166508352422355.....Total loss: 30.166508352422355\n",
      "epoch 1527 learning rate:  0.01065487884741323   Training loss:   31.395402851314316  Valing loss:   30.166508352422355\n",
      "Pure loss: 31.041598453931705.....Total loss: 31.041598453931705\n",
      "Pure loss: 28.919688158796188.....Total loss: 28.919688158796188\n",
      "epoch 1528 learning rate:  0.010654450261780106   Training loss:   31.041598453931705  Valing loss:   28.919688158796188\n",
      "Pure loss: 31.134875138329647.....Total loss: 31.134875138329647\n",
      "Pure loss: 28.74491113071095.....Total loss: 28.74491113071095\n",
      "epoch 1529 learning rate:  0.01065402223675605   Training loss:   31.134875138329647  Valing loss:   28.74491113071095\n",
      "Pure loss: 30.771568130440574.....Total loss: 30.771568130440574\n",
      "Pure loss: 28.672837629954678.....Total loss: 28.672837629954678\n",
      "epoch 1530 learning rate:  0.01065359477124183   Training loss:   30.771568130440574  Valing loss:   28.672837629954678\n",
      "Pure loss: 30.634696661616914.....Total loss: 30.634696661616914\n",
      "Pure loss: 28.651771655731494.....Total loss: 28.651771655731494\n",
      "epoch 1531 learning rate:  0.010653167864141085   Training loss:   30.634696661616914  Valing loss:   28.651771655731494\n",
      "Pure loss: 30.756060519512125.....Total loss: 30.756060519512125\n",
      "Pure loss: 28.920459059589053.....Total loss: 28.920459059589053\n",
      "epoch 1532 learning rate:  0.010652741514360313   Training loss:   30.756060519512125  Valing loss:   28.920459059589053\n",
      "Pure loss: 30.748925899636323.....Total loss: 30.748925899636323\n",
      "Pure loss: 28.05658674791506.....Total loss: 28.05658674791506\n",
      "epoch 1533 learning rate:  0.010652315720808872   Training loss:   30.748925899636323  Valing loss:   28.05658674791506\n",
      "Pure loss: 30.70686727790035.....Total loss: 30.70686727790035\n",
      "Pure loss: 27.879358648721325.....Total loss: 27.879358648721325\n",
      "epoch 1534 learning rate:  0.010651890482398957   Training loss:   30.70686727790035  Valing loss:   27.879358648721325\n",
      "Pure loss: 31.040528132558055.....Total loss: 31.040528132558055\n",
      "Pure loss: 27.867341915980933.....Total loss: 27.867341915980933\n",
      "epoch 1535 learning rate:  0.010651465798045602   Training loss:   31.040528132558055  Valing loss:   27.867341915980933\n",
      "Pure loss: 31.362659379318245.....Total loss: 31.362659379318245\n",
      "Pure loss: 27.9501452034982.....Total loss: 27.9501452034982\n",
      "epoch 1536 learning rate:  0.010651041666666666   Training loss:   31.362659379318245  Valing loss:   27.9501452034982\n",
      "Pure loss: 30.77223059952613.....Total loss: 30.77223059952613\n",
      "Pure loss: 27.516373839147615.....Total loss: 27.516373839147615\n",
      "epoch 1537 learning rate:  0.010650618087182824   Training loss:   30.77223059952613  Valing loss:   27.516373839147615\n",
      "Pure loss: 30.857954913591353.....Total loss: 30.857954913591353\n",
      "Pure loss: 27.31774257497989.....Total loss: 27.31774257497989\n",
      "epoch 1538 learning rate:  0.010650195058517555   Training loss:   30.857954913591353  Valing loss:   27.31774257497989\n",
      "Pure loss: 30.478024536761552.....Total loss: 30.478024536761552\n",
      "Pure loss: 27.59874498605591.....Total loss: 27.59874498605591\n",
      "epoch 1539 learning rate:  0.010649772579597141   Training loss:   30.478024536761552  Valing loss:   27.59874498605591\n",
      "Pure loss: 30.584211713333207.....Total loss: 30.584211713333207\n",
      "Pure loss: 27.36239578875521.....Total loss: 27.36239578875521\n",
      "epoch 1540 learning rate:  0.01064935064935065   Training loss:   30.584211713333207  Valing loss:   27.36239578875521\n",
      "Pure loss: 30.72267058483685.....Total loss: 30.72267058483685\n",
      "Pure loss: 27.465057457108752.....Total loss: 27.465057457108752\n",
      "epoch 1541 learning rate:  0.010648929266709928   Training loss:   30.72267058483685  Valing loss:   27.465057457108752\n",
      "Pure loss: 31.184956254322543.....Total loss: 31.184956254322543\n",
      "Pure loss: 27.83922997194497.....Total loss: 27.83922997194497\n",
      "epoch 1542 learning rate:  0.010648508430609598   Training loss:   31.184956254322543  Valing loss:   27.83922997194497\n",
      "Pure loss: 31.427351793819568.....Total loss: 31.427351793819568\n",
      "Pure loss: 27.882585573744535.....Total loss: 27.882585573744535\n",
      "epoch 1543 learning rate:  0.01064808813998704   Training loss:   31.427351793819568  Valing loss:   27.882585573744535\n",
      "Pure loss: 32.05404300766245.....Total loss: 32.05404300766245\n",
      "Pure loss: 27.442472401281194.....Total loss: 27.442472401281194\n",
      "epoch 1544 learning rate:  0.010647668393782384   Training loss:   32.05404300766245  Valing loss:   27.442472401281194\n",
      "Pure loss: 31.38936838298452.....Total loss: 31.38936838298452\n",
      "Pure loss: 27.249311176322898.....Total loss: 27.249311176322898\n",
      "epoch 1545 learning rate:  0.01064724919093851   Training loss:   31.38936838298452  Valing loss:   27.249311176322898\n",
      "Pure loss: 30.73615038717392.....Total loss: 30.73615038717392\n",
      "Pure loss: 27.036639739902878.....Total loss: 27.036639739902878\n",
      "epoch 1546 learning rate:  0.010646830530401035   Training loss:   30.73615038717392  Valing loss:   27.036639739902878\n",
      "Pure loss: 30.26629615810826.....Total loss: 30.26629615810826\n",
      "Pure loss: 27.32298205027807.....Total loss: 27.32298205027807\n",
      "epoch 1547 learning rate:  0.010646412411118293   Training loss:   30.26629615810826  Valing loss:   27.32298205027807\n",
      "Pure loss: 30.43102489843886.....Total loss: 30.43102489843886\n",
      "Pure loss: 27.05106674468067.....Total loss: 27.05106674468067\n",
      "epoch 1548 learning rate:  0.010645994832041344   Training loss:   30.43102489843886  Valing loss:   27.05106674468067\n",
      "Pure loss: 30.733709702345607.....Total loss: 30.733709702345607\n",
      "Pure loss: 26.804240949137174.....Total loss: 26.804240949137174\n",
      "epoch 1549 learning rate:  0.010645577792123951   Training loss:   30.733709702345607  Valing loss:   26.804240949137174\n",
      "Pure loss: 30.384390259798618.....Total loss: 30.384390259798618\n",
      "Pure loss: 27.09725174567867.....Total loss: 27.09725174567867\n",
      "epoch 1550 learning rate:  0.01064516129032258   Training loss:   30.384390259798618  Valing loss:   27.09725174567867\n",
      "Pure loss: 30.14844503569387.....Total loss: 30.14844503569387\n",
      "Pure loss: 27.290768604651454.....Total loss: 27.290768604651454\n",
      "epoch 1551 learning rate:  0.010644745325596389   Training loss:   30.14844503569387  Valing loss:   27.290768604651454\n",
      "Pure loss: 30.57012405688169.....Total loss: 30.57012405688169\n",
      "Pure loss: 27.084544702223717.....Total loss: 27.084544702223717\n",
      "epoch 1552 learning rate:  0.010644329896907216   Training loss:   30.57012405688169  Valing loss:   27.084544702223717\n",
      "Pure loss: 31.020792462438614.....Total loss: 31.020792462438614\n",
      "Pure loss: 27.121431471439237.....Total loss: 27.121431471439237\n",
      "epoch 1553 learning rate:  0.010643915003219576   Training loss:   31.020792462438614  Valing loss:   27.121431471439237\n",
      "Pure loss: 30.16674376091074.....Total loss: 30.16674376091074\n",
      "Pure loss: 27.19645938015334.....Total loss: 27.19645938015334\n",
      "epoch 1554 learning rate:  0.010643500643500644   Training loss:   30.16674376091074  Valing loss:   27.19645938015334\n",
      "Pure loss: 30.55358295113083.....Total loss: 30.55358295113083\n",
      "Pure loss: 26.657196981476517.....Total loss: 26.657196981476517\n",
      "epoch 1555 learning rate:  0.010643086816720257   Training loss:   30.55358295113083  Valing loss:   26.657196981476517\n",
      "Pure loss: 31.559203407540934.....Total loss: 31.559203407540934\n",
      "Pure loss: 26.92614254655569.....Total loss: 26.92614254655569\n",
      "epoch 1556 learning rate:  0.0106426735218509   Training loss:   31.559203407540934  Valing loss:   26.92614254655569\n",
      "Pure loss: 31.832702036570552.....Total loss: 31.832702036570552\n",
      "Pure loss: 26.99072832897481.....Total loss: 26.99072832897481\n",
      "epoch 1557 learning rate:  0.010642260757867695   Training loss:   31.832702036570552  Valing loss:   26.99072832897481\n",
      "Pure loss: 33.14360315357567.....Total loss: 33.14360315357567\n",
      "Pure loss: 27.79182589901168.....Total loss: 27.79182589901168\n",
      "epoch 1558 learning rate:  0.010641848523748396   Training loss:   33.14360315357567  Valing loss:   27.79182589901168\n",
      "Pure loss: 34.34505529902014.....Total loss: 34.34505529902014\n",
      "Pure loss: 28.23395115049636.....Total loss: 28.23395115049636\n",
      "epoch 1559 learning rate:  0.01064143681847338   Training loss:   34.34505529902014  Valing loss:   28.23395115049636\n",
      "Pure loss: 32.41521987595848.....Total loss: 32.41521987595848\n",
      "Pure loss: 26.50670654906489.....Total loss: 26.50670654906489\n",
      "epoch 1560 learning rate:  0.01064102564102564   Training loss:   32.41521987595848  Valing loss:   26.50670654906489\n",
      "Pure loss: 32.57885394169029.....Total loss: 32.57885394169029\n",
      "Pure loss: 26.61015402368764.....Total loss: 26.61015402368764\n",
      "epoch 1561 learning rate:  0.010640614990390776   Training loss:   32.57885394169029  Valing loss:   26.61015402368764\n",
      "Pure loss: 32.87322767557193.....Total loss: 32.87322767557193\n",
      "Pure loss: 26.792144236761064.....Total loss: 26.792144236761064\n",
      "epoch 1562 learning rate:  0.010640204865556979   Training loss:   32.87322767557193  Valing loss:   26.792144236761064\n",
      "Pure loss: 31.42953325573737.....Total loss: 31.42953325573737\n",
      "Pure loss: 25.96218493503365.....Total loss: 25.96218493503365\n",
      "epoch 1563 learning rate:  0.010639795265515036   Training loss:   31.42953325573737  Valing loss:   25.96218493503365\n",
      "Pure loss: 30.390529827991262.....Total loss: 30.390529827991262\n",
      "Pure loss: 25.259550798906144.....Total loss: 25.259550798906144\n",
      "epoch 1564 learning rate:  0.010639386189258313   Training loss:   30.390529827991262  Valing loss:   25.259550798906144\n",
      "Pure loss: 31.179783170111435.....Total loss: 31.179783170111435\n",
      "Pure loss: 25.86350035767349.....Total loss: 25.86350035767349\n",
      "epoch 1565 learning rate:  0.010638977635782748   Training loss:   31.179783170111435  Valing loss:   25.86350035767349\n",
      "Pure loss: 29.6911755257876.....Total loss: 29.6911755257876\n",
      "Pure loss: 25.40804724745088.....Total loss: 25.40804724745088\n",
      "epoch 1566 learning rate:  0.010638569604086845   Training loss:   29.6911755257876  Valing loss:   25.40804724745088\n",
      "Pure loss: 30.018890901376977.....Total loss: 30.018890901376977\n",
      "Pure loss: 25.406140304631965.....Total loss: 25.406140304631965\n",
      "epoch 1567 learning rate:  0.010638162093171666   Training loss:   30.018890901376977  Valing loss:   25.406140304631965\n",
      "Pure loss: 29.641429224910397.....Total loss: 29.641429224910397\n",
      "Pure loss: 25.356422767406023.....Total loss: 25.356422767406023\n",
      "epoch 1568 learning rate:  0.010637755102040817   Training loss:   29.641429224910397  Valing loss:   25.356422767406023\n",
      "Pure loss: 30.551704382500738.....Total loss: 30.551704382500738\n",
      "Pure loss: 25.54440955314567.....Total loss: 25.54440955314567\n",
      "epoch 1569 learning rate:  0.010637348629700446   Training loss:   30.551704382500738  Valing loss:   25.54440955314567\n",
      "Pure loss: 30.854317193000597.....Total loss: 30.854317193000597\n",
      "Pure loss: 25.68325003664381.....Total loss: 25.68325003664381\n",
      "epoch 1570 learning rate:  0.010636942675159236   Training loss:   30.854317193000597  Valing loss:   25.68325003664381\n",
      "Pure loss: 31.07122903062351.....Total loss: 31.07122903062351\n",
      "Pure loss: 25.799288002229023.....Total loss: 25.799288002229023\n",
      "epoch 1571 learning rate:  0.01063653723742839   Training loss:   31.07122903062351  Valing loss:   25.799288002229023\n",
      "Pure loss: 30.662686078403226.....Total loss: 30.662686078403226\n",
      "Pure loss: 25.451469587748697.....Total loss: 25.451469587748697\n",
      "epoch 1572 learning rate:  0.01063613231552163   Training loss:   30.662686078403226  Valing loss:   25.451469587748697\n",
      "Pure loss: 31.183797675696248.....Total loss: 31.183797675696248\n",
      "Pure loss: 25.696366030838462.....Total loss: 25.696366030838462\n",
      "epoch 1573 learning rate:  0.010635727908455182   Training loss:   31.183797675696248  Valing loss:   25.696366030838462\n",
      "Pure loss: 30.957652563012307.....Total loss: 30.957652563012307\n",
      "Pure loss: 25.466571458721113.....Total loss: 25.466571458721113\n",
      "epoch 1574 learning rate:  0.010635324015247777   Training loss:   30.957652563012307  Valing loss:   25.466571458721113\n",
      "Pure loss: 31.692017088295795.....Total loss: 31.692017088295795\n",
      "Pure loss: 25.835342201356948.....Total loss: 25.835342201356948\n",
      "epoch 1575 learning rate:  0.010634920634920636   Training loss:   31.692017088295795  Valing loss:   25.835342201356948\n",
      "Pure loss: 30.405261760374945.....Total loss: 30.405261760374945\n",
      "Pure loss: 25.19982327636561.....Total loss: 25.19982327636561\n",
      "epoch 1576 learning rate:  0.010634517766497462   Training loss:   30.405261760374945  Valing loss:   25.19982327636561\n",
      "Pure loss: 29.705308037258508.....Total loss: 29.705308037258508\n",
      "Pure loss: 24.75431328057616.....Total loss: 24.75431328057616\n",
      "epoch 1577 learning rate:  0.01063411540900444   Training loss:   29.705308037258508  Valing loss:   24.75431328057616\n",
      "Pure loss: 29.077750202277922.....Total loss: 29.077750202277922\n",
      "Pure loss: 24.57928611860265.....Total loss: 24.57928611860265\n",
      "epoch 1578 learning rate:  0.010633713561470215   Training loss:   29.077750202277922  Valing loss:   24.57928611860265\n",
      "Pure loss: 29.036032922740407.....Total loss: 29.036032922740407\n",
      "Pure loss: 24.585759998232376.....Total loss: 24.585759998232376\n",
      "epoch 1579 learning rate:  0.010633312222925902   Training loss:   29.036032922740407  Valing loss:   24.585759998232376\n",
      "Pure loss: 29.111182607412317.....Total loss: 29.111182607412317\n",
      "Pure loss: 24.520566364736766.....Total loss: 24.520566364736766\n",
      "epoch 1580 learning rate:  0.010632911392405063   Training loss:   29.111182607412317  Valing loss:   24.520566364736766\n",
      "Pure loss: 28.47773642709918.....Total loss: 28.47773642709918\n",
      "Pure loss: 25.426106025441346.....Total loss: 25.426106025441346\n",
      "epoch 1581 learning rate:  0.010632511068943707   Training loss:   28.47773642709918  Valing loss:   25.426106025441346\n",
      "Pure loss: 28.952473034591076.....Total loss: 28.952473034591076\n",
      "Pure loss: 27.23776307796468.....Total loss: 27.23776307796468\n",
      "epoch 1582 learning rate:  0.010632111251580279   Training loss:   28.952473034591076  Valing loss:   27.23776307796468\n",
      "Pure loss: 28.544402969897.....Total loss: 28.544402969897\n",
      "Pure loss: 25.986550009820352.....Total loss: 25.986550009820352\n",
      "epoch 1583 learning rate:  0.010631711939355654   Training loss:   28.544402969897  Valing loss:   25.986550009820352\n",
      "Pure loss: 28.54291528687014.....Total loss: 28.54291528687014\n",
      "Pure loss: 25.968849030897708.....Total loss: 25.968849030897708\n",
      "epoch 1584 learning rate:  0.010631313131313132   Training loss:   28.54291528687014  Valing loss:   25.968849030897708\n",
      "Pure loss: 28.29883881769631.....Total loss: 28.29883881769631\n",
      "Pure loss: 25.88892664602972.....Total loss: 25.88892664602972\n",
      "epoch 1585 learning rate:  0.010630914826498424   Training loss:   28.29883881769631  Valing loss:   25.88892664602972\n",
      "Pure loss: 28.238491217077605.....Total loss: 28.238491217077605\n",
      "Pure loss: 25.979970474575875.....Total loss: 25.979970474575875\n",
      "epoch 1586 learning rate:  0.010630517023959648   Training loss:   28.238491217077605  Valing loss:   25.979970474575875\n",
      "Pure loss: 28.200233426475517.....Total loss: 28.200233426475517\n",
      "Pure loss: 25.838727130255958.....Total loss: 25.838727130255958\n",
      "epoch 1587 learning rate:  0.010630119722747322   Training loss:   28.200233426475517  Valing loss:   25.838727130255958\n",
      "Pure loss: 28.057167655962648.....Total loss: 28.057167655962648\n",
      "Pure loss: 25.264291099962993.....Total loss: 25.264291099962993\n",
      "epoch 1588 learning rate:  0.010629722921914359   Training loss:   28.057167655962648  Valing loss:   25.264291099962993\n",
      "Pure loss: 28.04406529194407.....Total loss: 28.04406529194407\n",
      "Pure loss: 24.976169820465156.....Total loss: 24.976169820465156\n",
      "epoch 1589 learning rate:  0.010629326620516048   Training loss:   28.04406529194407  Valing loss:   24.976169820465156\n",
      "Pure loss: 28.247032622954244.....Total loss: 28.247032622954244\n",
      "Pure loss: 24.287540049348156.....Total loss: 24.287540049348156\n",
      "epoch 1590 learning rate:  0.010628930817610063   Training loss:   28.247032622954244  Valing loss:   24.287540049348156\n",
      "Pure loss: 28.236139802781093.....Total loss: 28.236139802781093\n",
      "Pure loss: 24.284794865290205.....Total loss: 24.284794865290205\n",
      "epoch 1591 learning rate:  0.010628535512256444   Training loss:   28.236139802781093  Valing loss:   24.284794865290205\n",
      "Pure loss: 28.04716529118245.....Total loss: 28.04716529118245\n",
      "Pure loss: 24.193292992146745.....Total loss: 24.193292992146745\n",
      "epoch 1592 learning rate:  0.010628140703517587   Training loss:   28.04716529118245  Valing loss:   24.193292992146745\n",
      "Pure loss: 28.08715993548313.....Total loss: 28.08715993548313\n",
      "Pure loss: 24.193114470522413.....Total loss: 24.193114470522413\n",
      "epoch 1593 learning rate:  0.010627746390458255   Training loss:   28.08715993548313  Valing loss:   24.193114470522413\n",
      "Pure loss: 28.03115962520651.....Total loss: 28.03115962520651\n",
      "Pure loss: 24.227681753712584.....Total loss: 24.227681753712584\n",
      "epoch 1594 learning rate:  0.010627352572145546   Training loss:   28.03115962520651  Valing loss:   24.227681753712584\n",
      "Pure loss: 28.095323917058433.....Total loss: 28.095323917058433\n",
      "Pure loss: 24.165107764821066.....Total loss: 24.165107764821066\n",
      "epoch 1595 learning rate:  0.010626959247648903   Training loss:   28.095323917058433  Valing loss:   24.165107764821066\n",
      "Pure loss: 27.944088883375134.....Total loss: 27.944088883375134\n",
      "Pure loss: 24.242045639541292.....Total loss: 24.242045639541292\n",
      "epoch 1596 learning rate:  0.0106265664160401   Training loss:   27.944088883375134  Valing loss:   24.242045639541292\n",
      "Pure loss: 28.042041878208572.....Total loss: 28.042041878208572\n",
      "Pure loss: 24.25836829348107.....Total loss: 24.25836829348107\n",
      "epoch 1597 learning rate:  0.010626174076393238   Training loss:   28.042041878208572  Valing loss:   24.25836829348107\n",
      "Pure loss: 28.305055320618273.....Total loss: 28.305055320618273\n",
      "Pure loss: 24.075218009696933.....Total loss: 24.075218009696933\n",
      "epoch 1598 learning rate:  0.01062578222778473   Training loss:   28.305055320618273  Valing loss:   24.075218009696933\n",
      "Pure loss: 28.625015506730584.....Total loss: 28.625015506730584\n",
      "Pure loss: 24.097912835737976.....Total loss: 24.097912835737976\n",
      "epoch 1599 learning rate:  0.01062539086929331   Training loss:   28.625015506730584  Valing loss:   24.097912835737976\n",
      "Pure loss: 28.40212223600927.....Total loss: 28.40212223600927\n",
      "Pure loss: 24.05679805033958.....Total loss: 24.05679805033958\n",
      "epoch 1600 learning rate:  0.010625   Training loss:   28.40212223600927  Valing loss:   24.05679805033958\n",
      "Pure loss: 29.0724863043165.....Total loss: 29.0724863043165\n",
      "Pure loss: 24.13154425855102.....Total loss: 24.13154425855102\n",
      "epoch 1601 learning rate:  0.010624609618988132   Training loss:   29.0724863043165  Valing loss:   24.13154425855102\n",
      "Pure loss: 28.733114417967215.....Total loss: 28.733114417967215\n",
      "Pure loss: 23.91878946990153.....Total loss: 23.91878946990153\n",
      "epoch 1602 learning rate:  0.01062421972534332   Training loss:   28.733114417967215  Valing loss:   23.91878946990153\n",
      "Pure loss: 28.798846860270764.....Total loss: 28.798846860270764\n",
      "Pure loss: 23.97562004521223.....Total loss: 23.97562004521223\n",
      "epoch 1603 learning rate:  0.010623830318153463   Training loss:   28.798846860270764  Valing loss:   23.97562004521223\n",
      "Pure loss: 28.598111392518213.....Total loss: 28.598111392518213\n",
      "Pure loss: 23.995371620512742.....Total loss: 23.995371620512742\n",
      "epoch 1604 learning rate:  0.010623441396508728   Training loss:   28.598111392518213  Valing loss:   23.995371620512742\n",
      "Pure loss: 28.763842194853808.....Total loss: 28.763842194853808\n",
      "Pure loss: 24.132315881890776.....Total loss: 24.132315881890776\n",
      "epoch 1605 learning rate:  0.010623052959501557   Training loss:   28.763842194853808  Valing loss:   24.132315881890776\n",
      "Pure loss: 28.736101811678182.....Total loss: 28.736101811678182\n",
      "Pure loss: 24.118359060251965.....Total loss: 24.118359060251965\n",
      "epoch 1606 learning rate:  0.010622665006226651   Training loss:   28.736101811678182  Valing loss:   24.118359060251965\n",
      "Pure loss: 28.904959187259966.....Total loss: 28.904959187259966\n",
      "Pure loss: 24.056357607504772.....Total loss: 24.056357607504772\n",
      "epoch 1607 learning rate:  0.010622277535780959   Training loss:   28.904959187259966  Valing loss:   24.056357607504772\n",
      "Pure loss: 27.991975674342868.....Total loss: 27.991975674342868\n",
      "Pure loss: 24.06208861164873.....Total loss: 24.06208861164873\n",
      "epoch 1608 learning rate:  0.010621890547263682   Training loss:   27.991975674342868  Valing loss:   24.06208861164873\n",
      "Pure loss: 27.640190916363625.....Total loss: 27.640190916363625\n",
      "Pure loss: 24.560534481329306.....Total loss: 24.560534481329306\n",
      "epoch 1609 learning rate:  0.010621504039776259   Training loss:   27.640190916363625  Valing loss:   24.560534481329306\n",
      "Pure loss: 27.641103586912138.....Total loss: 27.641103586912138\n",
      "Pure loss: 24.437885757587686.....Total loss: 24.437885757587686\n",
      "epoch 1610 learning rate:  0.01062111801242236   Training loss:   27.641103586912138  Valing loss:   24.437885757587686\n",
      "Pure loss: 27.83302163335275.....Total loss: 27.83302163335275\n",
      "Pure loss: 23.962804101452704.....Total loss: 23.962804101452704\n",
      "epoch 1611 learning rate:  0.010620732464307883   Training loss:   27.83302163335275  Valing loss:   23.962804101452704\n",
      "Pure loss: 28.095381687680888.....Total loss: 28.095381687680888\n",
      "Pure loss: 23.911170499872764.....Total loss: 23.911170499872764\n",
      "epoch 1612 learning rate:  0.010620347394540944   Training loss:   28.095381687680888  Valing loss:   23.911170499872764\n",
      "Pure loss: 28.885856428347267.....Total loss: 28.885856428347267\n",
      "Pure loss: 24.079680772419167.....Total loss: 24.079680772419167\n",
      "epoch 1613 learning rate:  0.010619962802231866   Training loss:   28.885856428347267  Valing loss:   24.079680772419167\n",
      "Pure loss: 28.102745841860006.....Total loss: 28.102745841860006\n",
      "Pure loss: 24.131844672352905.....Total loss: 24.131844672352905\n",
      "epoch 1614 learning rate:  0.010619578686493185   Training loss:   28.102745841860006  Valing loss:   24.131844672352905\n",
      "Pure loss: 27.929728607764275.....Total loss: 27.929728607764275\n",
      "Pure loss: 24.46860173555937.....Total loss: 24.46860173555937\n",
      "epoch 1615 learning rate:  0.010619195046439628   Training loss:   27.929728607764275  Valing loss:   24.46860173555937\n",
      "Pure loss: 28.072211205681892.....Total loss: 28.072211205681892\n",
      "Pure loss: 24.395673373527398.....Total loss: 24.395673373527398\n",
      "epoch 1616 learning rate:  0.01061881188118812   Training loss:   28.072211205681892  Valing loss:   24.395673373527398\n",
      "Pure loss: 27.777863300149296.....Total loss: 27.777863300149296\n",
      "Pure loss: 25.10118583157875.....Total loss: 25.10118583157875\n",
      "epoch 1617 learning rate:  0.010618429189857762   Training loss:   27.777863300149296  Valing loss:   25.10118583157875\n",
      "Pure loss: 28.248153295575754.....Total loss: 28.248153295575754\n",
      "Pure loss: 25.31538235936235.....Total loss: 25.31538235936235\n",
      "epoch 1618 learning rate:  0.01061804697156984   Training loss:   28.248153295575754  Valing loss:   25.31538235936235\n",
      "Pure loss: 28.183809161096434.....Total loss: 28.183809161096434\n",
      "Pure loss: 25.954217459441413.....Total loss: 25.954217459441413\n",
      "epoch 1619 learning rate:  0.010617665225447808   Training loss:   28.183809161096434  Valing loss:   25.954217459441413\n",
      "Pure loss: 28.202313692301765.....Total loss: 28.202313692301765\n",
      "Pure loss: 26.297740893977135.....Total loss: 26.297740893977135\n",
      "epoch 1620 learning rate:  0.010617283950617284   Training loss:   28.202313692301765  Valing loss:   26.297740893977135\n",
      "Pure loss: 28.1947106986393.....Total loss: 28.1947106986393\n",
      "Pure loss: 26.276459146076142.....Total loss: 26.276459146076142\n",
      "epoch 1621 learning rate:  0.010616903146206046   Training loss:   28.1947106986393  Valing loss:   26.276459146076142\n",
      "Pure loss: 28.049899788306117.....Total loss: 28.049899788306117\n",
      "Pure loss: 25.938099115920238.....Total loss: 25.938099115920238\n",
      "epoch 1622 learning rate:  0.01061652281134402   Training loss:   28.049899788306117  Valing loss:   25.938099115920238\n",
      "Pure loss: 28.04131779429035.....Total loss: 28.04131779429035\n",
      "Pure loss: 25.91319380204288.....Total loss: 25.91319380204288\n",
      "epoch 1623 learning rate:  0.010616142945163278   Training loss:   28.04131779429035  Valing loss:   25.91319380204288\n",
      "Pure loss: 27.95546004030435.....Total loss: 27.95546004030435\n",
      "Pure loss: 25.562809053463244.....Total loss: 25.562809053463244\n",
      "epoch 1624 learning rate:  0.010615763546798029   Training loss:   27.95546004030435  Valing loss:   25.562809053463244\n",
      "Pure loss: 28.050622742623297.....Total loss: 28.050622742623297\n",
      "Pure loss: 24.788770111320854.....Total loss: 24.788770111320854\n",
      "epoch 1625 learning rate:  0.010615384615384615   Training loss:   28.050622742623297  Valing loss:   24.788770111320854\n",
      "Pure loss: 28.312476741970375.....Total loss: 28.312476741970375\n",
      "Pure loss: 24.506147995055688.....Total loss: 24.506147995055688\n",
      "epoch 1626 learning rate:  0.010615006150061501   Training loss:   28.312476741970375  Valing loss:   24.506147995055688\n",
      "Pure loss: 28.115203246923173.....Total loss: 28.115203246923173\n",
      "Pure loss: 24.32864442130141.....Total loss: 24.32864442130141\n",
      "epoch 1627 learning rate:  0.010614628149969269   Training loss:   28.115203246923173  Valing loss:   24.32864442130141\n",
      "Pure loss: 28.256558126722243.....Total loss: 28.256558126722243\n",
      "Pure loss: 24.41548058017964.....Total loss: 24.41548058017964\n",
      "epoch 1628 learning rate:  0.010614250614250614   Training loss:   28.256558126722243  Valing loss:   24.41548058017964\n",
      "Pure loss: 27.366729186307726.....Total loss: 27.366729186307726\n",
      "Pure loss: 24.0082136054449.....Total loss: 24.0082136054449\n",
      "epoch 1629 learning rate:  0.010613873542050338   Training loss:   27.366729186307726  Valing loss:   24.0082136054449\n",
      "Pure loss: 27.563003792295362.....Total loss: 27.563003792295362\n",
      "Pure loss: 23.86263157032865.....Total loss: 23.86263157032865\n",
      "epoch 1630 learning rate:  0.010613496932515338   Training loss:   27.563003792295362  Valing loss:   23.86263157032865\n",
      "Pure loss: 27.58896491915993.....Total loss: 27.58896491915993\n",
      "Pure loss: 23.862702901019688.....Total loss: 23.862702901019688\n",
      "epoch 1631 learning rate:  0.010613120784794604   Training loss:   27.58896491915993  Valing loss:   23.862702901019688\n",
      "Pure loss: 27.324949741440822.....Total loss: 27.324949741440822\n",
      "Pure loss: 24.111826100623848.....Total loss: 24.111826100623848\n",
      "epoch 1632 learning rate:  0.010612745098039215   Training loss:   27.324949741440822  Valing loss:   24.111826100623848\n",
      "Pure loss: 27.329641791194597.....Total loss: 27.329641791194597\n",
      "Pure loss: 25.02280789938787.....Total loss: 25.02280789938787\n",
      "epoch 1633 learning rate:  0.010612369871402327   Training loss:   27.329641791194597  Valing loss:   25.02280789938787\n",
      "Pure loss: 27.33738178012355.....Total loss: 27.33738178012355\n",
      "Pure loss: 25.080177117452422.....Total loss: 25.080177117452422\n",
      "epoch 1634 learning rate:  0.010611995104039169   Training loss:   27.33738178012355  Valing loss:   25.080177117452422\n",
      "Pure loss: 27.2943316907666.....Total loss: 27.2943316907666\n",
      "Pure loss: 24.61138923619791.....Total loss: 24.61138923619791\n",
      "epoch 1635 learning rate:  0.010611620795107034   Training loss:   27.2943316907666  Valing loss:   24.61138923619791\n",
      "Pure loss: 27.255819145485248.....Total loss: 27.255819145485248\n",
      "Pure loss: 24.18297537110459.....Total loss: 24.18297537110459\n",
      "epoch 1636 learning rate:  0.01061124694376528   Training loss:   27.255819145485248  Valing loss:   24.18297537110459\n",
      "Pure loss: 27.249814286882668.....Total loss: 27.249814286882668\n",
      "Pure loss: 23.928364276481872.....Total loss: 23.928364276481872\n",
      "epoch 1637 learning rate:  0.010610873549175321   Training loss:   27.249814286882668  Valing loss:   23.928364276481872\n",
      "Pure loss: 27.122626070763552.....Total loss: 27.122626070763552\n",
      "Pure loss: 23.899529873160056.....Total loss: 23.899529873160056\n",
      "epoch 1638 learning rate:  0.01061050061050061   Training loss:   27.122626070763552  Valing loss:   23.899529873160056\n",
      "Pure loss: 27.126392457976213.....Total loss: 27.126392457976213\n",
      "Pure loss: 23.874351560070227.....Total loss: 23.874351560070227\n",
      "epoch 1639 learning rate:  0.01061012812690665   Training loss:   27.126392457976213  Valing loss:   23.874351560070227\n",
      "Pure loss: 27.162085116762725.....Total loss: 27.162085116762725\n",
      "Pure loss: 23.83496970353219.....Total loss: 23.83496970353219\n",
      "epoch 1640 learning rate:  0.010609756097560976   Training loss:   27.162085116762725  Valing loss:   23.83496970353219\n",
      "Pure loss: 27.102062120936424.....Total loss: 27.102062120936424\n",
      "Pure loss: 23.78277206680296.....Total loss: 23.78277206680296\n",
      "epoch 1641 learning rate:  0.010609384521633151   Training loss:   27.102062120936424  Valing loss:   23.78277206680296\n",
      "Pure loss: 27.05290856487143.....Total loss: 27.05290856487143\n",
      "Pure loss: 23.77553851644176.....Total loss: 23.77553851644176\n",
      "epoch 1642 learning rate:  0.010609013398294762   Training loss:   27.05290856487143  Valing loss:   23.77553851644176\n",
      "Pure loss: 27.13114760487083.....Total loss: 27.13114760487083\n",
      "Pure loss: 24.87282776639724.....Total loss: 24.87282776639724\n",
      "epoch 1643 learning rate:  0.010608642726719415   Training loss:   27.13114760487083  Valing loss:   24.87282776639724\n",
      "Pure loss: 26.88849771334057.....Total loss: 26.88849771334057\n",
      "Pure loss: 24.331040165849835.....Total loss: 24.331040165849835\n",
      "epoch 1644 learning rate:  0.010608272506082725   Training loss:   26.88849771334057  Valing loss:   24.331040165849835\n",
      "Pure loss: 26.95824716400496.....Total loss: 26.95824716400496\n",
      "Pure loss: 24.680759023931053.....Total loss: 24.680759023931053\n",
      "epoch 1645 learning rate:  0.01060790273556231   Training loss:   26.95824716400496  Valing loss:   24.680759023931053\n",
      "Pure loss: 26.975549685492197.....Total loss: 26.975549685492197\n",
      "Pure loss: 24.583634485241454.....Total loss: 24.583634485241454\n",
      "epoch 1646 learning rate:  0.010607533414337789   Training loss:   26.975549685492197  Valing loss:   24.583634485241454\n",
      "Pure loss: 27.127106685530382.....Total loss: 27.127106685530382\n",
      "Pure loss: 24.622181695999107.....Total loss: 24.622181695999107\n",
      "epoch 1647 learning rate:  0.010607164541590771   Training loss:   27.127106685530382  Valing loss:   24.622181695999107\n",
      "Pure loss: 27.004471085299663.....Total loss: 27.004471085299663\n",
      "Pure loss: 24.781468025334544.....Total loss: 24.781468025334544\n",
      "epoch 1648 learning rate:  0.010606796116504854   Training loss:   27.004471085299663  Valing loss:   24.781468025334544\n",
      "Pure loss: 27.093237410156064.....Total loss: 27.093237410156064\n",
      "Pure loss: 25.04834684069813.....Total loss: 25.04834684069813\n",
      "epoch 1649 learning rate:  0.010606428138265615   Training loss:   27.093237410156064  Valing loss:   25.04834684069813\n",
      "Pure loss: 27.092740986894565.....Total loss: 27.092740986894565\n",
      "Pure loss: 25.05088880552037.....Total loss: 25.05088880552037\n",
      "epoch 1650 learning rate:  0.010606060606060607   Training loss:   27.092740986894565  Valing loss:   25.05088880552037\n",
      "Pure loss: 27.78353389375027.....Total loss: 27.78353389375027\n",
      "Pure loss: 26.644389776370907.....Total loss: 26.644389776370907\n",
      "epoch 1651 learning rate:  0.010605693519079347   Training loss:   27.78353389375027  Valing loss:   26.644389776370907\n",
      "Pure loss: 28.35631539401183.....Total loss: 28.35631539401183\n",
      "Pure loss: 27.639835988737325.....Total loss: 27.639835988737325\n",
      "epoch 1652 learning rate:  0.010605326876513317   Training loss:   28.35631539401183  Valing loss:   27.639835988737325\n",
      "Pure loss: 28.581335842051793.....Total loss: 28.581335842051793\n",
      "Pure loss: 28.065796203618394.....Total loss: 28.065796203618394\n",
      "epoch 1653 learning rate:  0.01060496067755596   Training loss:   28.581335842051793  Valing loss:   28.065796203618394\n",
      "Pure loss: 27.48556978813777.....Total loss: 27.48556978813777\n",
      "Pure loss: 26.192387763923904.....Total loss: 26.192387763923904\n",
      "epoch 1654 learning rate:  0.010604594921402661   Training loss:   27.48556978813777  Valing loss:   26.192387763923904\n",
      "Pure loss: 27.507463871454622.....Total loss: 27.507463871454622\n",
      "Pure loss: 26.256872355078034.....Total loss: 26.256872355078034\n",
      "epoch 1655 learning rate:  0.010604229607250756   Training loss:   27.507463871454622  Valing loss:   26.256872355078034\n",
      "Pure loss: 26.994992260616247.....Total loss: 26.994992260616247\n",
      "Pure loss: 25.14328847827418.....Total loss: 25.14328847827418\n",
      "epoch 1656 learning rate:  0.010603864734299516   Training loss:   26.994992260616247  Valing loss:   25.14328847827418\n",
      "Pure loss: 27.164461179733213.....Total loss: 27.164461179733213\n",
      "Pure loss: 25.560419380184964.....Total loss: 25.560419380184964\n",
      "epoch 1657 learning rate:  0.010603500301750151   Training loss:   27.164461179733213  Valing loss:   25.560419380184964\n",
      "Pure loss: 27.13866108005628.....Total loss: 27.13866108005628\n",
      "Pure loss: 25.44253419161042.....Total loss: 25.44253419161042\n",
      "epoch 1658 learning rate:  0.01060313630880579   Training loss:   27.13866108005628  Valing loss:   25.44253419161042\n",
      "Pure loss: 26.71507455490539.....Total loss: 26.71507455490539\n",
      "Pure loss: 24.11758514374189.....Total loss: 24.11758514374189\n",
      "epoch 1659 learning rate:  0.010602772754671488   Training loss:   26.71507455490539  Valing loss:   24.11758514374189\n",
      "Pure loss: 26.71248050230553.....Total loss: 26.71248050230553\n",
      "Pure loss: 24.20150257124441.....Total loss: 24.20150257124441\n",
      "epoch 1660 learning rate:  0.010602409638554217   Training loss:   26.71248050230553  Valing loss:   24.20150257124441\n",
      "Pure loss: 26.78426765667914.....Total loss: 26.78426765667914\n",
      "Pure loss: 24.236954017680894.....Total loss: 24.236954017680894\n",
      "epoch 1661 learning rate:  0.010602046959662853   Training loss:   26.78426765667914  Valing loss:   24.236954017680894\n",
      "Pure loss: 26.87915724211887.....Total loss: 26.87915724211887\n",
      "Pure loss: 23.740492502399533.....Total loss: 23.740492502399533\n",
      "epoch 1662 learning rate:  0.010601684717208184   Training loss:   26.87915724211887  Valing loss:   23.740492502399533\n",
      "Pure loss: 26.873790060410652.....Total loss: 26.873790060410652\n",
      "Pure loss: 23.74200743730947.....Total loss: 23.74200743730947\n",
      "epoch 1663 learning rate:  0.010601322910402887   Training loss:   26.873790060410652  Valing loss:   23.74200743730947\n",
      "Pure loss: 26.84280990815909.....Total loss: 26.84280990815909\n",
      "Pure loss: 23.796572010968042.....Total loss: 23.796572010968042\n",
      "epoch 1664 learning rate:  0.010600961538461538   Training loss:   26.84280990815909  Valing loss:   23.796572010968042\n",
      "Pure loss: 26.79620250676143.....Total loss: 26.79620250676143\n",
      "Pure loss: 23.88526416098579.....Total loss: 23.88526416098579\n",
      "epoch 1665 learning rate:  0.0106006006006006   Training loss:   26.79620250676143  Valing loss:   23.88526416098579\n",
      "Pure loss: 26.756876284623505.....Total loss: 26.756876284623505\n",
      "Pure loss: 24.40620306881289.....Total loss: 24.40620306881289\n",
      "epoch 1666 learning rate:  0.010600240096038415   Training loss:   26.756876284623505  Valing loss:   24.40620306881289\n",
      "Pure loss: 26.700320597202506.....Total loss: 26.700320597202506\n",
      "Pure loss: 24.10006068365674.....Total loss: 24.10006068365674\n",
      "epoch 1667 learning rate:  0.0105998800239952   Training loss:   26.700320597202506  Valing loss:   24.10006068365674\n",
      "Pure loss: 26.74220959510542.....Total loss: 26.74220959510542\n",
      "Pure loss: 23.882562914229094.....Total loss: 23.882562914229094\n",
      "epoch 1668 learning rate:  0.010599520383693046   Training loss:   26.74220959510542  Valing loss:   23.882562914229094\n",
      "Pure loss: 26.780826441356673.....Total loss: 26.780826441356673\n",
      "Pure loss: 23.851019502630567.....Total loss: 23.851019502630567\n",
      "epoch 1669 learning rate:  0.010599161174355902   Training loss:   26.780826441356673  Valing loss:   23.851019502630567\n",
      "Pure loss: 26.5795364638491.....Total loss: 26.5795364638491\n",
      "Pure loss: 23.82529129330732.....Total loss: 23.82529129330732\n",
      "epoch 1670 learning rate:  0.010598802395209581   Training loss:   26.5795364638491  Valing loss:   23.82529129330732\n",
      "Pure loss: 26.565110669302335.....Total loss: 26.565110669302335\n",
      "Pure loss: 23.918118315043355.....Total loss: 23.918118315043355\n",
      "epoch 1671 learning rate:  0.010598444045481747   Training loss:   26.565110669302335  Valing loss:   23.918118315043355\n",
      "Pure loss: 26.5876213172287.....Total loss: 26.5876213172287\n",
      "Pure loss: 23.782612115507497.....Total loss: 23.782612115507497\n",
      "epoch 1672 learning rate:  0.010598086124401914   Training loss:   26.5876213172287  Valing loss:   23.782612115507497\n",
      "Pure loss: 26.505060451020952.....Total loss: 26.505060451020952\n",
      "Pure loss: 23.794237184608257.....Total loss: 23.794237184608257\n",
      "epoch 1673 learning rate:  0.010597728631201436   Training loss:   26.505060451020952  Valing loss:   23.794237184608257\n",
      "Pure loss: 26.426126251683588.....Total loss: 26.426126251683588\n",
      "Pure loss: 23.819117348531453.....Total loss: 23.819117348531453\n",
      "epoch 1674 learning rate:  0.0105973715651135   Training loss:   26.426126251683588  Valing loss:   23.819117348531453\n",
      "Pure loss: 26.5153602313518.....Total loss: 26.5153602313518\n",
      "Pure loss: 23.054853679005788.....Total loss: 23.054853679005788\n",
      "epoch 1675 learning rate:  0.010597014925373134   Training loss:   26.5153602313518  Valing loss:   23.054853679005788\n",
      "Pure loss: 26.454878060163225.....Total loss: 26.454878060163225\n",
      "Pure loss: 23.991436435243646.....Total loss: 23.991436435243646\n",
      "epoch 1676 learning rate:  0.010596658711217184   Training loss:   26.454878060163225  Valing loss:   23.991436435243646\n",
      "Pure loss: 26.675456158574423.....Total loss: 26.675456158574423\n",
      "Pure loss: 24.576718969845025.....Total loss: 24.576718969845025\n",
      "epoch 1677 learning rate:  0.010596302921884317   Training loss:   26.675456158574423  Valing loss:   24.576718969845025\n",
      "Pure loss: 26.79255212469072.....Total loss: 26.79255212469072\n",
      "Pure loss: 24.814399287515894.....Total loss: 24.814399287515894\n",
      "epoch 1678 learning rate:  0.010595947556615018   Training loss:   26.79255212469072  Valing loss:   24.814399287515894\n",
      "Pure loss: 26.54995570789512.....Total loss: 26.54995570789512\n",
      "Pure loss: 24.468553689825818.....Total loss: 24.468553689825818\n",
      "epoch 1679 learning rate:  0.010595592614651579   Training loss:   26.54995570789512  Valing loss:   24.468553689825818\n",
      "Pure loss: 26.51455513477905.....Total loss: 26.51455513477905\n",
      "Pure loss: 24.392117992250334.....Total loss: 24.392117992250334\n",
      "epoch 1680 learning rate:  0.010595238095238095   Training loss:   26.51455513477905  Valing loss:   24.392117992250334\n",
      "Pure loss: 26.274600778422997.....Total loss: 26.274600778422997\n",
      "Pure loss: 23.69274807892349.....Total loss: 23.69274807892349\n",
      "epoch 1681 learning rate:  0.010594883997620465   Training loss:   26.274600778422997  Valing loss:   23.69274807892349\n",
      "Pure loss: 26.230859444681197.....Total loss: 26.230859444681197\n",
      "Pure loss: 23.488852785342576.....Total loss: 23.488852785342576\n",
      "epoch 1682 learning rate:  0.010594530321046373   Training loss:   26.230859444681197  Valing loss:   23.488852785342576\n",
      "Pure loss: 26.310256208317252.....Total loss: 26.310256208317252\n",
      "Pure loss: 23.845446239224867.....Total loss: 23.845446239224867\n",
      "epoch 1683 learning rate:  0.0105941770647653   Training loss:   26.310256208317252  Valing loss:   23.845446239224867\n",
      "Pure loss: 26.2844654128804.....Total loss: 26.2844654128804\n",
      "Pure loss: 23.749372715404476.....Total loss: 23.749372715404476\n",
      "epoch 1684 learning rate:  0.010593824228028504   Training loss:   26.2844654128804  Valing loss:   23.749372715404476\n",
      "Pure loss: 26.481229581057082.....Total loss: 26.481229581057082\n",
      "Pure loss: 24.342989898433682.....Total loss: 24.342989898433682\n",
      "epoch 1685 learning rate:  0.01059347181008902   Training loss:   26.481229581057082  Valing loss:   24.342989898433682\n",
      "Pure loss: 26.290724705445847.....Total loss: 26.290724705445847\n",
      "Pure loss: 23.8512931912845.....Total loss: 23.8512931912845\n",
      "epoch 1686 learning rate:  0.01059311981020166   Training loss:   26.290724705445847  Valing loss:   23.8512931912845\n",
      "Pure loss: 26.20962791635934.....Total loss: 26.20962791635934\n",
      "Pure loss: 23.523150888242856.....Total loss: 23.523150888242856\n",
      "epoch 1687 learning rate:  0.010592768227623   Training loss:   26.20962791635934  Valing loss:   23.523150888242856\n",
      "Pure loss: 26.184811094603557.....Total loss: 26.184811094603557\n",
      "Pure loss: 23.417070348727215.....Total loss: 23.417070348727215\n",
      "epoch 1688 learning rate:  0.010592417061611376   Training loss:   26.184811094603557  Valing loss:   23.417070348727215\n",
      "Pure loss: 26.183571037182727.....Total loss: 26.183571037182727\n",
      "Pure loss: 22.987053694988425.....Total loss: 22.987053694988425\n",
      "epoch 1689 learning rate:  0.01059206631142688   Training loss:   26.183571037182727  Valing loss:   22.987053694988425\n",
      "Pure loss: 26.259242669016775.....Total loss: 26.259242669016775\n",
      "Pure loss: 23.48065431390361.....Total loss: 23.48065431390361\n",
      "epoch 1690 learning rate:  0.010591715976331361   Training loss:   26.259242669016775  Valing loss:   23.48065431390361\n",
      "Pure loss: 26.364345968985273.....Total loss: 26.364345968985273\n",
      "Pure loss: 23.67618866114739.....Total loss: 23.67618866114739\n",
      "epoch 1691 learning rate:  0.01059136605558841   Training loss:   26.364345968985273  Valing loss:   23.67618866114739\n",
      "Pure loss: 26.301636075835415.....Total loss: 26.301636075835415\n",
      "Pure loss: 23.492222298437348.....Total loss: 23.492222298437348\n",
      "epoch 1692 learning rate:  0.010591016548463356   Training loss:   26.301636075835415  Valing loss:   23.492222298437348\n",
      "Pure loss: 26.267955854670458.....Total loss: 26.267955854670458\n",
      "Pure loss: 23.16008036545153.....Total loss: 23.16008036545153\n",
      "epoch 1693 learning rate:  0.010590667454223272   Training loss:   26.267955854670458  Valing loss:   23.16008036545153\n",
      "Pure loss: 26.270677696120863.....Total loss: 26.270677696120863\n",
      "Pure loss: 23.183648859934216.....Total loss: 23.183648859934216\n",
      "epoch 1694 learning rate:  0.010590318772136955   Training loss:   26.270677696120863  Valing loss:   23.183648859934216\n",
      "Pure loss: 26.270197163860452.....Total loss: 26.270197163860452\n",
      "Pure loss: 23.17716561001388.....Total loss: 23.17716561001388\n",
      "epoch 1695 learning rate:  0.010589970501474926   Training loss:   26.270197163860452  Valing loss:   23.17716561001388\n",
      "Pure loss: 26.302210074269965.....Total loss: 26.302210074269965\n",
      "Pure loss: 22.942521312064805.....Total loss: 22.942521312064805\n",
      "epoch 1696 learning rate:  0.010589622641509434   Training loss:   26.302210074269965  Valing loss:   22.942521312064805\n",
      "Pure loss: 26.32761670561123.....Total loss: 26.32761670561123\n",
      "Pure loss: 22.784664697561023.....Total loss: 22.784664697561023\n",
      "epoch 1697 learning rate:  0.010589275191514437   Training loss:   26.32761670561123  Valing loss:   22.784664697561023\n",
      "Pure loss: 26.401015762850605.....Total loss: 26.401015762850605\n",
      "Pure loss: 22.614115739843037.....Total loss: 22.614115739843037\n",
      "epoch 1698 learning rate:  0.010588928150765608   Training loss:   26.401015762850605  Valing loss:   22.614115739843037\n",
      "Pure loss: 26.284650942459905.....Total loss: 26.284650942459905\n",
      "Pure loss: 23.481460198185385.....Total loss: 23.481460198185385\n",
      "epoch 1699 learning rate:  0.010588581518540318   Training loss:   26.284650942459905  Valing loss:   23.481460198185385\n",
      "Pure loss: 26.257560058867696.....Total loss: 26.257560058867696\n",
      "Pure loss: 23.320629447354055.....Total loss: 23.320629447354055\n",
      "epoch 1700 learning rate:  0.010588235294117647   Training loss:   26.257560058867696  Valing loss:   23.320629447354055\n",
      "Pure loss: 26.38180340264867.....Total loss: 26.38180340264867\n",
      "Pure loss: 23.86230253101219.....Total loss: 23.86230253101219\n",
      "epoch 1701 learning rate:  0.010587889476778366   Training loss:   26.38180340264867  Valing loss:   23.86230253101219\n",
      "Pure loss: 26.23053162243147.....Total loss: 26.23053162243147\n",
      "Pure loss: 22.96795219127829.....Total loss: 22.96795219127829\n",
      "epoch 1702 learning rate:  0.010587544065804935   Training loss:   26.23053162243147  Valing loss:   22.96795219127829\n",
      "Pure loss: 26.191433909763116.....Total loss: 26.191433909763116\n",
      "Pure loss: 22.864054861196923.....Total loss: 22.864054861196923\n",
      "epoch 1703 learning rate:  0.010587199060481504   Training loss:   26.191433909763116  Valing loss:   22.864054861196923\n",
      "Pure loss: 26.206837748325455.....Total loss: 26.206837748325455\n",
      "Pure loss: 22.743218320662237.....Total loss: 22.743218320662237\n",
      "epoch 1704 learning rate:  0.010586854460093897   Training loss:   26.206837748325455  Valing loss:   22.743218320662237\n",
      "Pure loss: 26.33392478792115.....Total loss: 26.33392478792115\n",
      "Pure loss: 22.46917220018961.....Total loss: 22.46917220018961\n",
      "epoch 1705 learning rate:  0.010586510263929618   Training loss:   26.33392478792115  Valing loss:   22.46917220018961\n",
      "Pure loss: 26.20756055325085.....Total loss: 26.20756055325085\n",
      "Pure loss: 22.788149931539376.....Total loss: 22.788149931539376\n",
      "epoch 1706 learning rate:  0.010586166471277842   Training loss:   26.20756055325085  Valing loss:   22.788149931539376\n",
      "Pure loss: 26.39740860973174.....Total loss: 26.39740860973174\n",
      "Pure loss: 22.38102469822399.....Total loss: 22.38102469822399\n",
      "epoch 1707 learning rate:  0.010585823081429408   Training loss:   26.39740860973174  Valing loss:   22.38102469822399\n",
      "Pure loss: 26.70911890050925.....Total loss: 26.70911890050925\n",
      "Pure loss: 22.208817512636095.....Total loss: 22.208817512636095\n",
      "epoch 1708 learning rate:  0.010585480093676816   Training loss:   26.70911890050925  Valing loss:   22.208817512636095\n",
      "Pure loss: 27.412044710559.....Total loss: 27.412044710559\n",
      "Pure loss: 22.260675109351702.....Total loss: 22.260675109351702\n",
      "epoch 1709 learning rate:  0.01058513750731422   Training loss:   27.412044710559  Valing loss:   22.260675109351702\n",
      "Pure loss: 27.72216355432469.....Total loss: 27.72216355432469\n",
      "Pure loss: 22.35210086135538.....Total loss: 22.35210086135538\n",
      "epoch 1710 learning rate:  0.010584795321637428   Training loss:   27.72216355432469  Valing loss:   22.35210086135538\n",
      "Pure loss: 27.932382422165873.....Total loss: 27.932382422165873\n",
      "Pure loss: 22.436806059233685.....Total loss: 22.436806059233685\n",
      "epoch 1711 learning rate:  0.010584453535943892   Training loss:   27.932382422165873  Valing loss:   22.436806059233685\n",
      "Pure loss: 27.339850291020557.....Total loss: 27.339850291020557\n",
      "Pure loss: 22.260567426669745.....Total loss: 22.260567426669745\n",
      "epoch 1712 learning rate:  0.01058411214953271   Training loss:   27.339850291020557  Valing loss:   22.260567426669745\n",
      "Pure loss: 27.237008464808365.....Total loss: 27.237008464808365\n",
      "Pure loss: 22.247062657323173.....Total loss: 22.247062657323173\n",
      "epoch 1713 learning rate:  0.010583771161704612   Training loss:   27.237008464808365  Valing loss:   22.247062657323173\n",
      "Pure loss: 26.599821031057147.....Total loss: 26.599821031057147\n",
      "Pure loss: 22.241210008164902.....Total loss: 22.241210008164902\n",
      "epoch 1714 learning rate:  0.01058343057176196   Training loss:   26.599821031057147  Valing loss:   22.241210008164902\n",
      "Pure loss: 26.31672348845428.....Total loss: 26.31672348845428\n",
      "Pure loss: 22.366152350846374.....Total loss: 22.366152350846374\n",
      "epoch 1715 learning rate:  0.010583090379008746   Training loss:   26.31672348845428  Valing loss:   22.366152350846374\n",
      "Pure loss: 26.36033317226422.....Total loss: 26.36033317226422\n",
      "Pure loss: 22.32694537021608.....Total loss: 22.32694537021608\n",
      "epoch 1716 learning rate:  0.010582750582750582   Training loss:   26.36033317226422  Valing loss:   22.32694537021608\n",
      "Pure loss: 26.400844103212137.....Total loss: 26.400844103212137\n",
      "Pure loss: 22.296803985062372.....Total loss: 22.296803985062372\n",
      "epoch 1717 learning rate:  0.0105824111822947   Training loss:   26.400844103212137  Valing loss:   22.296803985062372\n",
      "Pure loss: 26.108590692937874.....Total loss: 26.108590692937874\n",
      "Pure loss: 22.603191488578418.....Total loss: 22.603191488578418\n",
      "epoch 1718 learning rate:  0.010582072176949942   Training loss:   26.108590692937874  Valing loss:   22.603191488578418\n",
      "Pure loss: 26.27482179599821.....Total loss: 26.27482179599821\n",
      "Pure loss: 23.118015204571478.....Total loss: 23.118015204571478\n",
      "epoch 1719 learning rate:  0.01058173356602676   Training loss:   26.27482179599821  Valing loss:   23.118015204571478\n",
      "Pure loss: 26.296427428152125.....Total loss: 26.296427428152125\n",
      "Pure loss: 22.986856271152345.....Total loss: 22.986856271152345\n",
      "epoch 1720 learning rate:  0.01058139534883721   Training loss:   26.296427428152125  Valing loss:   22.986856271152345\n",
      "Pure loss: 26.34198990205576.....Total loss: 26.34198990205576\n",
      "Pure loss: 22.85439540740243.....Total loss: 22.85439540740243\n",
      "epoch 1721 learning rate:  0.010581057524694945   Training loss:   26.34198990205576  Valing loss:   22.85439540740243\n",
      "Pure loss: 26.341901168828066.....Total loss: 26.341901168828066\n",
      "Pure loss: 22.923044200295898.....Total loss: 22.923044200295898\n",
      "epoch 1722 learning rate:  0.010580720092915216   Training loss:   26.341901168828066  Valing loss:   22.923044200295898\n",
      "Pure loss: 26.36515332729847.....Total loss: 26.36515332729847\n",
      "Pure loss: 22.9872778998159.....Total loss: 22.9872778998159\n",
      "epoch 1723 learning rate:  0.010580383052814859   Training loss:   26.36515332729847  Valing loss:   22.9872778998159\n",
      "Pure loss: 26.42908474716604.....Total loss: 26.42908474716604\n",
      "Pure loss: 22.80751912834345.....Total loss: 22.80751912834345\n",
      "epoch 1724 learning rate:  0.010580046403712297   Training loss:   26.42908474716604  Valing loss:   22.80751912834345\n",
      "Pure loss: 26.41772349303399.....Total loss: 26.41772349303399\n",
      "Pure loss: 23.973604957300335.....Total loss: 23.973604957300335\n",
      "epoch 1725 learning rate:  0.010579710144927536   Training loss:   26.41772349303399  Valing loss:   23.973604957300335\n",
      "Pure loss: 26.41699693525989.....Total loss: 26.41699693525989\n",
      "Pure loss: 23.97128295579305.....Total loss: 23.97128295579305\n",
      "epoch 1726 learning rate:  0.010579374275782155   Training loss:   26.41699693525989  Valing loss:   23.97128295579305\n",
      "Pure loss: 26.030685242102578.....Total loss: 26.030685242102578\n",
      "Pure loss: 23.268962098920937.....Total loss: 23.268962098920937\n",
      "epoch 1727 learning rate:  0.010579038795599306   Training loss:   26.030685242102578  Valing loss:   23.268962098920937\n",
      "Pure loss: 25.956844680491734.....Total loss: 25.956844680491734\n",
      "Pure loss: 22.874010574518426.....Total loss: 22.874010574518426\n",
      "epoch 1728 learning rate:  0.010578703703703705   Training loss:   25.956844680491734  Valing loss:   22.874010574518426\n",
      "Pure loss: 25.963191112415902.....Total loss: 25.963191112415902\n",
      "Pure loss: 22.887233024866863.....Total loss: 22.887233024866863\n",
      "epoch 1729 learning rate:  0.010578368999421632   Training loss:   25.963191112415902  Valing loss:   22.887233024866863\n",
      "Pure loss: 25.977795670582182.....Total loss: 25.977795670582182\n",
      "Pure loss: 22.629595007292426.....Total loss: 22.629595007292426\n",
      "epoch 1730 learning rate:  0.010578034682080925   Training loss:   25.977795670582182  Valing loss:   22.629595007292426\n",
      "Pure loss: 26.078658180341087.....Total loss: 26.078658180341087\n",
      "Pure loss: 22.231763883377464.....Total loss: 22.231763883377464\n",
      "epoch 1731 learning rate:  0.010577700751010976   Training loss:   26.078658180341087  Valing loss:   22.231763883377464\n",
      "Pure loss: 26.62080315318573.....Total loss: 26.62080315318573\n",
      "Pure loss: 21.953913377641566.....Total loss: 21.953913377641566\n",
      "epoch 1732 learning rate:  0.010577367205542726   Training loss:   26.62080315318573  Valing loss:   21.953913377641566\n",
      "Pure loss: 26.706486590442793.....Total loss: 26.706486590442793\n",
      "Pure loss: 22.016362856424518.....Total loss: 22.016362856424518\n",
      "epoch 1733 learning rate:  0.010577034045008656   Training loss:   26.706486590442793  Valing loss:   22.016362856424518\n",
      "Pure loss: 26.61873173113707.....Total loss: 26.61873173113707\n",
      "Pure loss: 22.01945081652737.....Total loss: 22.01945081652737\n",
      "epoch 1734 learning rate:  0.010576701268742791   Training loss:   26.61873173113707  Valing loss:   22.01945081652737\n",
      "Pure loss: 26.07067545596895.....Total loss: 26.07067545596895\n",
      "Pure loss: 22.369436702426224.....Total loss: 22.369436702426224\n",
      "epoch 1735 learning rate:  0.010576368876080692   Training loss:   26.07067545596895  Valing loss:   22.369436702426224\n",
      "Pure loss: 26.141231911879416.....Total loss: 26.141231911879416\n",
      "Pure loss: 22.078110376878545.....Total loss: 22.078110376878545\n",
      "epoch 1736 learning rate:  0.010576036866359446   Training loss:   26.141231911879416  Valing loss:   22.078110376878545\n",
      "Pure loss: 26.068210219694244.....Total loss: 26.068210219694244\n",
      "Pure loss: 22.21139686045696.....Total loss: 22.21139686045696\n",
      "epoch 1737 learning rate:  0.010575705238917675   Training loss:   26.068210219694244  Valing loss:   22.21139686045696\n",
      "Pure loss: 26.385753274852817.....Total loss: 26.385753274852817\n",
      "Pure loss: 21.899679727213627.....Total loss: 21.899679727213627\n",
      "epoch 1738 learning rate:  0.010575373993095513   Training loss:   26.385753274852817  Valing loss:   21.899679727213627\n",
      "Pure loss: 26.33338861726447.....Total loss: 26.33338861726447\n",
      "Pure loss: 21.912970155393083.....Total loss: 21.912970155393083\n",
      "epoch 1739 learning rate:  0.010575043128234618   Training loss:   26.33338861726447  Valing loss:   21.912970155393083\n",
      "Pure loss: 26.33437417544399.....Total loss: 26.33437417544399\n",
      "Pure loss: 21.91249190867414.....Total loss: 21.91249190867414\n",
      "epoch 1740 learning rate:  0.010574712643678161   Training loss:   26.33437417544399  Valing loss:   21.91249190867414\n",
      "Pure loss: 26.308591981752123.....Total loss: 26.308591981752123\n",
      "Pure loss: 21.79909075010473.....Total loss: 21.79909075010473\n",
      "epoch 1741 learning rate:  0.010574382538770822   Training loss:   26.308591981752123  Valing loss:   21.79909075010473\n",
      "Pure loss: 25.80598293600221.....Total loss: 25.80598293600221\n",
      "Pure loss: 22.129817302708982.....Total loss: 22.129817302708982\n",
      "epoch 1742 learning rate:  0.010574052812858782   Training loss:   25.80598293600221  Valing loss:   22.129817302708982\n",
      "Pure loss: 25.77303497530589.....Total loss: 25.77303497530589\n",
      "Pure loss: 22.208979594533428.....Total loss: 22.208979594533428\n",
      "epoch 1743 learning rate:  0.01057372346528973   Training loss:   25.77303497530589  Valing loss:   22.208979594533428\n",
      "Pure loss: 25.78660670125003.....Total loss: 25.78660670125003\n",
      "Pure loss: 22.1566726537748.....Total loss: 22.1566726537748\n",
      "epoch 1744 learning rate:  0.010573394495412844   Training loss:   25.78660670125003  Valing loss:   22.1566726537748\n",
      "Pure loss: 25.797788280824367.....Total loss: 25.797788280824367\n",
      "Pure loss: 22.119525394865853.....Total loss: 22.119525394865853\n",
      "epoch 1745 learning rate:  0.010573065902578796   Training loss:   25.797788280824367  Valing loss:   22.119525394865853\n",
      "Pure loss: 25.91825474470598.....Total loss: 25.91825474470598\n",
      "Pure loss: 21.95607559872372.....Total loss: 21.95607559872372\n",
      "epoch 1746 learning rate:  0.010572737686139748   Training loss:   25.91825474470598  Valing loss:   21.95607559872372\n",
      "Pure loss: 25.82134370849664.....Total loss: 25.82134370849664\n",
      "Pure loss: 22.06283797445465.....Total loss: 22.06283797445465\n",
      "epoch 1747 learning rate:  0.010572409845449342   Training loss:   25.82134370849664  Valing loss:   22.06283797445465\n",
      "Pure loss: 25.881618072544516.....Total loss: 25.881618072544516\n",
      "Pure loss: 21.978855226627807.....Total loss: 21.978855226627807\n",
      "epoch 1748 learning rate:  0.0105720823798627   Training loss:   25.881618072544516  Valing loss:   21.978855226627807\n",
      "Pure loss: 25.88430724008471.....Total loss: 25.88430724008471\n",
      "Pure loss: 21.994391665971065.....Total loss: 21.994391665971065\n",
      "epoch 1749 learning rate:  0.01057175528873642   Training loss:   25.88430724008471  Valing loss:   21.994391665971065\n",
      "Pure loss: 26.078626049565987.....Total loss: 26.078626049565987\n",
      "Pure loss: 21.854790530107913.....Total loss: 21.854790530107913\n",
      "epoch 1750 learning rate:  0.010571428571428572   Training loss:   26.078626049565987  Valing loss:   21.854790530107913\n",
      "Pure loss: 26.18215101717134.....Total loss: 26.18215101717134\n",
      "Pure loss: 21.83923554254904.....Total loss: 21.83923554254904\n",
      "epoch 1751 learning rate:  0.010571102227298687   Training loss:   26.18215101717134  Valing loss:   21.83923554254904\n",
      "Pure loss: 26.32809168154675.....Total loss: 26.32809168154675\n",
      "Pure loss: 21.820826771849255.....Total loss: 21.820826771849255\n",
      "epoch 1752 learning rate:  0.010570776255707763   Training loss:   26.32809168154675  Valing loss:   21.820826771849255\n",
      "Pure loss: 26.717308043044635.....Total loss: 26.717308043044635\n",
      "Pure loss: 21.90144954049792.....Total loss: 21.90144954049792\n",
      "epoch 1753 learning rate:  0.010570450656018254   Training loss:   26.717308043044635  Valing loss:   21.90144954049792\n",
      "Pure loss: 26.932577220028044.....Total loss: 26.932577220028044\n",
      "Pure loss: 21.912193922230614.....Total loss: 21.912193922230614\n",
      "epoch 1754 learning rate:  0.010570125427594071   Training loss:   26.932577220028044  Valing loss:   21.912193922230614\n",
      "Pure loss: 26.595368922713504.....Total loss: 26.595368922713504\n",
      "Pure loss: 21.816068296082307.....Total loss: 21.816068296082307\n",
      "epoch 1755 learning rate:  0.01056980056980057   Training loss:   26.595368922713504  Valing loss:   21.816068296082307\n",
      "Pure loss: 26.091132647945305.....Total loss: 26.091132647945305\n",
      "Pure loss: 21.80482645536553.....Total loss: 21.80482645536553\n",
      "epoch 1756 learning rate:  0.010569476082004555   Training loss:   26.091132647945305  Valing loss:   21.80482645536553\n",
      "Pure loss: 25.666775726798107.....Total loss: 25.666775726798107\n",
      "Pure loss: 22.066201518520266.....Total loss: 22.066201518520266\n",
      "epoch 1757 learning rate:  0.010569151963574275   Training loss:   25.666775726798107  Valing loss:   22.066201518520266\n",
      "Pure loss: 25.655353070573117.....Total loss: 25.655353070573117\n",
      "Pure loss: 22.085247380866683.....Total loss: 22.085247380866683\n",
      "epoch 1758 learning rate:  0.010568828213879408   Training loss:   25.655353070573117  Valing loss:   22.085247380866683\n",
      "Pure loss: 25.653305938551703.....Total loss: 25.653305938551703\n",
      "Pure loss: 22.086303904825137.....Total loss: 22.086303904825137\n",
      "epoch 1759 learning rate:  0.010568504832291074   Training loss:   25.653305938551703  Valing loss:   22.086303904825137\n",
      "Pure loss: 25.56290262639852.....Total loss: 25.56290262639852\n",
      "Pure loss: 22.395072908272784.....Total loss: 22.395072908272784\n",
      "epoch 1760 learning rate:  0.010568181818181819   Training loss:   25.56290262639852  Valing loss:   22.395072908272784\n",
      "Pure loss: 25.56520278923976.....Total loss: 25.56520278923976\n",
      "Pure loss: 22.38475446727419.....Total loss: 22.38475446727419\n",
      "epoch 1761 learning rate:  0.010567859170925611   Training loss:   25.56520278923976  Valing loss:   22.38475446727419\n",
      "Pure loss: 25.56185540036566.....Total loss: 25.56185540036566\n",
      "Pure loss: 22.498089949680445.....Total loss: 22.498089949680445\n",
      "epoch 1762 learning rate:  0.010567536889897844   Training loss:   25.56185540036566  Valing loss:   22.498089949680445\n",
      "Pure loss: 25.834915409285543.....Total loss: 25.834915409285543\n",
      "Pure loss: 23.373131343739026.....Total loss: 23.373131343739026\n",
      "epoch 1763 learning rate:  0.010567214974475327   Training loss:   25.834915409285543  Valing loss:   23.373131343739026\n",
      "Pure loss: 25.65369467170603.....Total loss: 25.65369467170603\n",
      "Pure loss: 23.092150081478497.....Total loss: 23.092150081478497\n",
      "epoch 1764 learning rate:  0.01056689342403628   Training loss:   25.65369467170603  Valing loss:   23.092150081478497\n",
      "Pure loss: 25.623851130107603.....Total loss: 25.623851130107603\n",
      "Pure loss: 22.961856850234756.....Total loss: 22.961856850234756\n",
      "epoch 1765 learning rate:  0.01056657223796034   Training loss:   25.623851130107603  Valing loss:   22.961856850234756\n",
      "Pure loss: 25.64544609683678.....Total loss: 25.64544609683678\n",
      "Pure loss: 23.03479383285679.....Total loss: 23.03479383285679\n",
      "epoch 1766 learning rate:  0.010566251415628539   Training loss:   25.64544609683678  Valing loss:   23.03479383285679\n",
      "Pure loss: 25.66289255319447.....Total loss: 25.66289255319447\n",
      "Pure loss: 23.106852729743576.....Total loss: 23.106852729743576\n",
      "epoch 1767 learning rate:  0.010565930956423317   Training loss:   25.66289255319447  Valing loss:   23.106852729743576\n",
      "Pure loss: 25.607403278741007.....Total loss: 25.607403278741007\n",
      "Pure loss: 22.88324234055593.....Total loss: 22.88324234055593\n",
      "epoch 1768 learning rate:  0.010565610859728507   Training loss:   25.607403278741007  Valing loss:   22.88324234055593\n",
      "Pure loss: 25.57025301284692.....Total loss: 25.57025301284692\n",
      "Pure loss: 22.741304247906236.....Total loss: 22.741304247906236\n",
      "epoch 1769 learning rate:  0.010565291124929339   Training loss:   25.57025301284692  Valing loss:   22.741304247906236\n",
      "Pure loss: 25.54422651020766.....Total loss: 25.54422651020766\n",
      "Pure loss: 22.177777503621503.....Total loss: 22.177777503621503\n",
      "epoch 1770 learning rate:  0.01056497175141243   Training loss:   25.54422651020766  Valing loss:   22.177777503621503\n",
      "Pure loss: 25.534731345717912.....Total loss: 25.534731345717912\n",
      "Pure loss: 22.446798017784847.....Total loss: 22.446798017784847\n",
      "epoch 1771 learning rate:  0.010564652738565782   Training loss:   25.534731345717912  Valing loss:   22.446798017784847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 25.557387228783593.....Total loss: 25.557387228783593\n",
      "Pure loss: 22.188101314344337.....Total loss: 22.188101314344337\n",
      "epoch 1772 learning rate:  0.010564334085778782   Training loss:   25.557387228783593  Valing loss:   22.188101314344337\n",
      "Pure loss: 25.55495811720654.....Total loss: 25.55495811720654\n",
      "Pure loss: 22.26584635233609.....Total loss: 22.26584635233609\n",
      "epoch 1773 learning rate:  0.010564015792442188   Training loss:   25.55495811720654  Valing loss:   22.26584635233609\n",
      "Pure loss: 25.691626348421906.....Total loss: 25.691626348421906\n",
      "Pure loss: 23.32570701951776.....Total loss: 23.32570701951776\n",
      "epoch 1774 learning rate:  0.01056369785794814   Training loss:   25.691626348421906  Valing loss:   23.32570701951776\n",
      "Pure loss: 25.67589082763681.....Total loss: 25.67589082763681\n",
      "Pure loss: 23.26925068389174.....Total loss: 23.26925068389174\n",
      "epoch 1775 learning rate:  0.01056338028169014   Training loss:   25.67589082763681  Valing loss:   23.26925068389174\n",
      "Pure loss: 25.645894518460917.....Total loss: 25.645894518460917\n",
      "Pure loss: 23.180703986949442.....Total loss: 23.180703986949442\n",
      "epoch 1776 learning rate:  0.010563063063063063   Training loss:   25.645894518460917  Valing loss:   23.180703986949442\n",
      "Pure loss: 26.13175848371224.....Total loss: 26.13175848371224\n",
      "Pure loss: 24.29318459279891.....Total loss: 24.29318459279891\n",
      "epoch 1777 learning rate:  0.010562746201463141   Training loss:   26.13175848371224  Valing loss:   24.29318459279891\n",
      "Pure loss: 25.829079524226987.....Total loss: 25.829079524226987\n",
      "Pure loss: 23.762096673598357.....Total loss: 23.762096673598357\n",
      "epoch 1778 learning rate:  0.010562429696287965   Training loss:   25.829079524226987  Valing loss:   23.762096673598357\n",
      "Pure loss: 28.276513701337496.....Total loss: 28.276513701337496\n",
      "Pure loss: 28.08567453881336.....Total loss: 28.08567453881336\n",
      "epoch 1779 learning rate:  0.010562113546936482   Training loss:   28.276513701337496  Valing loss:   28.08567453881336\n",
      "Pure loss: 27.59760531160821.....Total loss: 27.59760531160821\n",
      "Pure loss: 27.151909336646717.....Total loss: 27.151909336646717\n",
      "epoch 1780 learning rate:  0.010561797752808988   Training loss:   27.59760531160821  Valing loss:   27.151909336646717\n",
      "Pure loss: 26.90256787699485.....Total loss: 26.90256787699485\n",
      "Pure loss: 26.069286810168254.....Total loss: 26.069286810168254\n",
      "epoch 1781 learning rate:  0.01056148231330713   Training loss:   26.90256787699485  Valing loss:   26.069286810168254\n",
      "Pure loss: 27.62143777956633.....Total loss: 27.62143777956633\n",
      "Pure loss: 27.080888106020492.....Total loss: 27.080888106020492\n",
      "epoch 1782 learning rate:  0.010561167227833894   Training loss:   27.62143777956633  Valing loss:   27.080888106020492\n",
      "Pure loss: 28.667942532461144.....Total loss: 28.667942532461144\n",
      "Pure loss: 28.63007166153132.....Total loss: 28.63007166153132\n",
      "epoch 1783 learning rate:  0.010560852495793607   Training loss:   28.667942532461144  Valing loss:   28.63007166153132\n",
      "Pure loss: 28.59591828814309.....Total loss: 28.59591828814309\n",
      "Pure loss: 28.528074210206373.....Total loss: 28.528074210206373\n",
      "epoch 1784 learning rate:  0.010560538116591928   Training loss:   28.59591828814309  Valing loss:   28.528074210206373\n",
      "Pure loss: 28.45784250083444.....Total loss: 28.45784250083444\n",
      "Pure loss: 28.330541607819377.....Total loss: 28.330541607819377\n",
      "epoch 1785 learning rate:  0.010560224089635854   Training loss:   28.45784250083444  Valing loss:   28.330541607819377\n",
      "Pure loss: 26.75221520342953.....Total loss: 26.75221520342953\n",
      "Pure loss: 25.643968957037128.....Total loss: 25.643968957037128\n",
      "epoch 1786 learning rate:  0.010559910414333707   Training loss:   26.75221520342953  Valing loss:   25.643968957037128\n",
      "Pure loss: 25.850809571729172.....Total loss: 25.850809571729172\n",
      "Pure loss: 24.068558783830106.....Total loss: 24.068558783830106\n",
      "epoch 1787 learning rate:  0.010559597090095131   Training loss:   25.850809571729172  Valing loss:   24.068558783830106\n",
      "Pure loss: 25.628717148179916.....Total loss: 25.628717148179916\n",
      "Pure loss: 23.749102670793555.....Total loss: 23.749102670793555\n",
      "epoch 1788 learning rate:  0.010559284116331096   Training loss:   25.628717148179916  Valing loss:   23.749102670793555\n",
      "Pure loss: 25.659949534571872.....Total loss: 25.659949534571872\n",
      "Pure loss: 23.808925213330692.....Total loss: 23.808925213330692\n",
      "epoch 1789 learning rate:  0.010558971492453885   Training loss:   25.659949534571872  Valing loss:   23.808925213330692\n",
      "Pure loss: 25.622786717459412.....Total loss: 25.622786717459412\n",
      "Pure loss: 23.73707414630796.....Total loss: 23.73707414630796\n",
      "epoch 1790 learning rate:  0.010558659217877095   Training loss:   25.622786717459412  Valing loss:   23.73707414630796\n",
      "Pure loss: 25.538760073238013.....Total loss: 25.538760073238013\n",
      "Pure loss: 23.551765819107466.....Total loss: 23.551765819107466\n",
      "epoch 1791 learning rate:  0.010558347292015633   Training loss:   25.538760073238013  Valing loss:   23.551765819107466\n",
      "Pure loss: 25.022210466196245.....Total loss: 25.022210466196245\n",
      "Pure loss: 22.04223393178981.....Total loss: 22.04223393178981\n",
      "epoch 1792 learning rate:  0.010558035714285714   Training loss:   25.022210466196245  Valing loss:   22.04223393178981\n",
      "Pure loss: 25.10798514152794.....Total loss: 25.10798514152794\n",
      "Pure loss: 21.45504109118344.....Total loss: 21.45504109118344\n",
      "epoch 1793 learning rate:  0.010557724484104853   Training loss:   25.10798514152794  Valing loss:   21.45504109118344\n",
      "Pure loss: 24.982346384240554.....Total loss: 24.982346384240554\n",
      "Pure loss: 21.704723274144673.....Total loss: 21.704723274144673\n",
      "epoch 1794 learning rate:  0.010557413600891862   Training loss:   24.982346384240554  Valing loss:   21.704723274144673\n",
      "Pure loss: 25.301966527649633.....Total loss: 25.301966527649633\n",
      "Pure loss: 21.36849982791022.....Total loss: 21.36849982791022\n",
      "epoch 1795 learning rate:  0.010557103064066852   Training loss:   25.301966527649633  Valing loss:   21.36849982791022\n",
      "Pure loss: 25.422184610107468.....Total loss: 25.422184610107468\n",
      "Pure loss: 21.340575945896738.....Total loss: 21.340575945896738\n",
      "epoch 1796 learning rate:  0.010556792873051226   Training loss:   25.422184610107468  Valing loss:   21.340575945896738\n",
      "Pure loss: 25.576563142466906.....Total loss: 25.576563142466906\n",
      "Pure loss: 21.382749235886443.....Total loss: 21.382749235886443\n",
      "epoch 1797 learning rate:  0.01055648302726767   Training loss:   25.576563142466906  Valing loss:   21.382749235886443\n",
      "Pure loss: 25.194461827750015.....Total loss: 25.194461827750015\n",
      "Pure loss: 21.32272938157008.....Total loss: 21.32272938157008\n",
      "epoch 1798 learning rate:  0.010556173526140156   Training loss:   25.194461827750015  Valing loss:   21.32272938157008\n",
      "Pure loss: 24.900925790249904.....Total loss: 24.900925790249904\n",
      "Pure loss: 22.031886009749552.....Total loss: 22.031886009749552\n",
      "epoch 1799 learning rate:  0.010555864369093941   Training loss:   24.900925790249904  Valing loss:   22.031886009749552\n",
      "Pure loss: 24.88543768787523.....Total loss: 24.88543768787523\n",
      "Pure loss: 22.11053904493458.....Total loss: 22.11053904493458\n",
      "epoch 1800 learning rate:  0.010555555555555556   Training loss:   24.88543768787523  Valing loss:   22.11053904493458\n",
      "Pure loss: 24.89283082660241.....Total loss: 24.89283082660241\n",
      "Pure loss: 22.25053848590552.....Total loss: 22.25053848590552\n",
      "epoch 1801 learning rate:  0.010555247084952804   Training loss:   24.89283082660241  Valing loss:   22.25053848590552\n",
      "Pure loss: 24.847577579179678.....Total loss: 24.847577579179678\n",
      "Pure loss: 22.055276037659297.....Total loss: 22.055276037659297\n",
      "epoch 1802 learning rate:  0.010554938956714761   Training loss:   24.847577579179678  Valing loss:   22.055276037659297\n",
      "Pure loss: 24.968429085762047.....Total loss: 24.968429085762047\n",
      "Pure loss: 22.614979853830402.....Total loss: 22.614979853830402\n",
      "epoch 1803 learning rate:  0.010554631170271769   Training loss:   24.968429085762047  Valing loss:   22.614979853830402\n",
      "Pure loss: 25.33882738698799.....Total loss: 25.33882738698799\n",
      "Pure loss: 23.46904437022365.....Total loss: 23.46904437022365\n",
      "epoch 1804 learning rate:  0.010554323725055433   Training loss:   25.33882738698799  Valing loss:   23.46904437022365\n",
      "Pure loss: 25.3113316500242.....Total loss: 25.3113316500242\n",
      "Pure loss: 23.412673506394945.....Total loss: 23.412673506394945\n",
      "epoch 1805 learning rate:  0.010554016620498615   Training loss:   25.3113316500242  Valing loss:   23.412673506394945\n",
      "Pure loss: 25.71194501474932.....Total loss: 25.71194501474932\n",
      "Pure loss: 24.081427497717076.....Total loss: 24.081427497717076\n",
      "epoch 1806 learning rate:  0.010553709856035438   Training loss:   25.71194501474932  Valing loss:   24.081427497717076\n",
      "Pure loss: 25.344076563037188.....Total loss: 25.344076563037188\n",
      "Pure loss: 23.342726493040523.....Total loss: 23.342726493040523\n",
      "epoch 1807 learning rate:  0.010553403431101272   Training loss:   25.344076563037188  Valing loss:   23.342726493040523\n",
      "Pure loss: 25.24464433965949.....Total loss: 25.24464433965949\n",
      "Pure loss: 23.150387949559182.....Total loss: 23.150387949559182\n",
      "epoch 1808 learning rate:  0.010553097345132743   Training loss:   25.24464433965949  Valing loss:   23.150387949559182\n",
      "Pure loss: 25.510512689860015.....Total loss: 25.510512689860015\n",
      "Pure loss: 23.706732753950032.....Total loss: 23.706732753950032\n",
      "epoch 1809 learning rate:  0.010552791597567716   Training loss:   25.510512689860015  Valing loss:   23.706732753950032\n",
      "Pure loss: 25.270784723048163.....Total loss: 25.270784723048163\n",
      "Pure loss: 23.153504166379015.....Total loss: 23.153504166379015\n",
      "epoch 1810 learning rate:  0.010552486187845303   Training loss:   25.270784723048163  Valing loss:   23.153504166379015\n",
      "Pure loss: 25.1960580237724.....Total loss: 25.1960580237724\n",
      "Pure loss: 22.96533690259504.....Total loss: 22.96533690259504\n",
      "epoch 1811 learning rate:  0.010552181115405854   Training loss:   25.1960580237724  Valing loss:   22.96533690259504\n",
      "Pure loss: 25.45806590345227.....Total loss: 25.45806590345227\n",
      "Pure loss: 23.44952986113214.....Total loss: 23.44952986113214\n",
      "epoch 1812 learning rate:  0.010551876379690949   Training loss:   25.45806590345227  Valing loss:   23.44952986113214\n",
      "Pure loss: 25.36082541317039.....Total loss: 25.36082541317039\n",
      "Pure loss: 23.276708167153117.....Total loss: 23.276708167153117\n",
      "epoch 1813 learning rate:  0.010551571980143408   Training loss:   25.36082541317039  Valing loss:   23.276708167153117\n",
      "Pure loss: 25.09014721272615.....Total loss: 25.09014721272615\n",
      "Pure loss: 22.61143479150188.....Total loss: 22.61143479150188\n",
      "epoch 1814 learning rate:  0.010551267916207276   Training loss:   25.09014721272615  Valing loss:   22.61143479150188\n",
      "Pure loss: 25.24113813104372.....Total loss: 25.24113813104372\n",
      "Pure loss: 22.966657247670696.....Total loss: 22.966657247670696\n",
      "epoch 1815 learning rate:  0.010550964187327825   Training loss:   25.24113813104372  Valing loss:   22.966657247670696\n",
      "Pure loss: 25.31615820085289.....Total loss: 25.31615820085289\n",
      "Pure loss: 23.12658815758288.....Total loss: 23.12658815758288\n",
      "epoch 1816 learning rate:  0.010550660792951543   Training loss:   25.31615820085289  Valing loss:   23.12658815758288\n",
      "Pure loss: 25.692666531122622.....Total loss: 25.692666531122622\n",
      "Pure loss: 23.747655334573935.....Total loss: 23.747655334573935\n",
      "epoch 1817 learning rate:  0.010550357732526142   Training loss:   25.692666531122622  Valing loss:   23.747655334573935\n",
      "Pure loss: 25.705531457824932.....Total loss: 25.705531457824932\n",
      "Pure loss: 23.776099933158513.....Total loss: 23.776099933158513\n",
      "epoch 1818 learning rate:  0.01055005500550055   Training loss:   25.705531457824932  Valing loss:   23.776099933158513\n",
      "Pure loss: 25.9234286599364.....Total loss: 25.9234286599364\n",
      "Pure loss: 24.11009405574516.....Total loss: 24.11009405574516\n",
      "epoch 1819 learning rate:  0.010549752611324905   Training loss:   25.9234286599364  Valing loss:   24.11009405574516\n",
      "Pure loss: 25.41535599480618.....Total loss: 25.41535599480618\n",
      "Pure loss: 23.32274033895477.....Total loss: 23.32274033895477\n",
      "epoch 1820 learning rate:  0.01054945054945055   Training loss:   25.41535599480618  Valing loss:   23.32274033895477\n",
      "Pure loss: 25.36338497826674.....Total loss: 25.36338497826674\n",
      "Pure loss: 23.223828569747525.....Total loss: 23.223828569747525\n",
      "epoch 1821 learning rate:  0.010549148819330039   Training loss:   25.36338497826674  Valing loss:   23.223828569747525\n",
      "Pure loss: 25.209969764580674.....Total loss: 25.209969764580674\n",
      "Pure loss: 22.945628865353036.....Total loss: 22.945628865353036\n",
      "epoch 1822 learning rate:  0.010548847420417125   Training loss:   25.209969764580674  Valing loss:   22.945628865353036\n",
      "Pure loss: 25.175567193352173.....Total loss: 25.175567193352173\n",
      "Pure loss: 22.86825399701446.....Total loss: 22.86825399701446\n",
      "epoch 1823 learning rate:  0.010548546352166758   Training loss:   25.175567193352173  Valing loss:   22.86825399701446\n",
      "Pure loss: 24.827773257661665.....Total loss: 24.827773257661665\n",
      "Pure loss: 21.837927161371702.....Total loss: 21.837927161371702\n",
      "epoch 1824 learning rate:  0.010548245614035088   Training loss:   24.827773257661665  Valing loss:   21.837927161371702\n",
      "Pure loss: 24.817686306537315.....Total loss: 24.817686306537315\n",
      "Pure loss: 21.754180655053474.....Total loss: 21.754180655053474\n",
      "epoch 1825 learning rate:  0.010547945205479452   Training loss:   24.817686306537315  Valing loss:   21.754180655053474\n",
      "Pure loss: 24.768945892805945.....Total loss: 24.768945892805945\n",
      "Pure loss: 21.668619014828657.....Total loss: 21.668619014828657\n",
      "epoch 1826 learning rate:  0.010547645125958379   Training loss:   24.768945892805945  Valing loss:   21.668619014828657\n",
      "Pure loss: 24.768917015443005.....Total loss: 24.768917015443005\n",
      "Pure loss: 21.713121097998425.....Total loss: 21.713121097998425\n",
      "epoch 1827 learning rate:  0.010547345374931582   Training loss:   24.768917015443005  Valing loss:   21.713121097998425\n",
      "Pure loss: 24.77212358753062.....Total loss: 24.77212358753062\n",
      "Pure loss: 21.74683630476718.....Total loss: 21.74683630476718\n",
      "epoch 1828 learning rate:  0.010547045951859957   Training loss:   24.77212358753062  Valing loss:   21.74683630476718\n",
      "Pure loss: 24.804360267545356.....Total loss: 24.804360267545356\n",
      "Pure loss: 21.331759411143615.....Total loss: 21.331759411143615\n",
      "epoch 1829 learning rate:  0.010546746856205578   Training loss:   24.804360267545356  Valing loss:   21.331759411143615\n",
      "Pure loss: 24.73830107473601.....Total loss: 24.73830107473601\n",
      "Pure loss: 21.626223542110978.....Total loss: 21.626223542110978\n",
      "epoch 1830 learning rate:  0.010546448087431694   Training loss:   24.73830107473601  Valing loss:   21.626223542110978\n",
      "Pure loss: 24.74583991814426.....Total loss: 24.74583991814426\n",
      "Pure loss: 21.57444888048652.....Total loss: 21.57444888048652\n",
      "epoch 1831 learning rate:  0.010546149645002731   Training loss:   24.74583991814426  Valing loss:   21.57444888048652\n",
      "Pure loss: 24.768719536522365.....Total loss: 24.768719536522365\n",
      "Pure loss: 21.84821279516898.....Total loss: 21.84821279516898\n",
      "epoch 1832 learning rate:  0.01054585152838428   Training loss:   24.768719536522365  Valing loss:   21.84821279516898\n",
      "Pure loss: 24.816622971187126.....Total loss: 24.816622971187126\n",
      "Pure loss: 21.30926986351008.....Total loss: 21.30926986351008\n",
      "epoch 1833 learning rate:  0.0105455537370431   Training loss:   24.816622971187126  Valing loss:   21.30926986351008\n",
      "Pure loss: 24.838762199959806.....Total loss: 24.838762199959806\n",
      "Pure loss: 21.258886641668422.....Total loss: 21.258886641668422\n",
      "epoch 1834 learning rate:  0.01054525627044711   Training loss:   24.838762199959806  Valing loss:   21.258886641668422\n",
      "Pure loss: 24.81645110911648.....Total loss: 24.81645110911648\n",
      "Pure loss: 21.287342629830263.....Total loss: 21.287342629830263\n",
      "epoch 1835 learning rate:  0.010544959128065395   Training loss:   24.81645110911648  Valing loss:   21.287342629830263\n",
      "Pure loss: 24.838678263381667.....Total loss: 24.838678263381667\n",
      "Pure loss: 21.238876524171502.....Total loss: 21.238876524171502\n",
      "epoch 1836 learning rate:  0.010544662309368192   Training loss:   24.838678263381667  Valing loss:   21.238876524171502\n",
      "Pure loss: 24.767837950419835.....Total loss: 24.767837950419835\n",
      "Pure loss: 21.512894589011456.....Total loss: 21.512894589011456\n",
      "epoch 1837 learning rate:  0.010544365813826891   Training loss:   24.767837950419835  Valing loss:   21.512894589011456\n",
      "Pure loss: 24.857274030399502.....Total loss: 24.857274030399502\n",
      "Pure loss: 22.47054794639177.....Total loss: 22.47054794639177\n",
      "epoch 1838 learning rate:  0.010544069640914037   Training loss:   24.857274030399502  Valing loss:   22.47054794639177\n",
      "Pure loss: 24.838517105616422.....Total loss: 24.838517105616422\n",
      "Pure loss: 22.384626906477408.....Total loss: 22.384626906477408\n",
      "epoch 1839 learning rate:  0.010543773790103317   Training loss:   24.838517105616422  Valing loss:   22.384626906477408\n",
      "Pure loss: 24.81028636697925.....Total loss: 24.81028636697925\n",
      "Pure loss: 22.28225108803225.....Total loss: 22.28225108803225\n",
      "epoch 1840 learning rate:  0.010543478260869566   Training loss:   24.81028636697925  Valing loss:   22.28225108803225\n",
      "Pure loss: 24.734379114161474.....Total loss: 24.734379114161474\n",
      "Pure loss: 21.85547656585339.....Total loss: 21.85547656585339\n",
      "epoch 1841 learning rate:  0.010543183052688756   Training loss:   24.734379114161474  Valing loss:   21.85547656585339\n",
      "Pure loss: 24.75371760535923.....Total loss: 24.75371760535923\n",
      "Pure loss: 21.639552290524293.....Total loss: 21.639552290524293\n",
      "epoch 1842 learning rate:  0.010542888165038003   Training loss:   24.75371760535923  Valing loss:   21.639552290524293\n",
      "Pure loss: 24.752765484219985.....Total loss: 24.752765484219985\n",
      "Pure loss: 21.644076542323834.....Total loss: 21.644076542323834\n",
      "epoch 1843 learning rate:  0.01054259359739555   Training loss:   24.752765484219985  Valing loss:   21.644076542323834\n",
      "Pure loss: 24.852321862607027.....Total loss: 24.852321862607027\n",
      "Pure loss: 22.554371685148453.....Total loss: 22.554371685148453\n",
      "epoch 1844 learning rate:  0.010542299349240782   Training loss:   24.852321862607027  Valing loss:   22.554371685148453\n",
      "Pure loss: 24.889214524668326.....Total loss: 24.889214524668326\n",
      "Pure loss: 22.662723006962832.....Total loss: 22.662723006962832\n",
      "epoch 1845 learning rate:  0.0105420054200542   Training loss:   24.889214524668326  Valing loss:   22.662723006962832\n",
      "Pure loss: 24.862750467564634.....Total loss: 24.862750467564634\n",
      "Pure loss: 22.581807965781664.....Total loss: 22.581807965781664\n",
      "epoch 1846 learning rate:  0.010541711809317443   Training loss:   24.862750467564634  Valing loss:   22.581807965781664\n",
      "Pure loss: 24.790137764650794.....Total loss: 24.790137764650794\n",
      "Pure loss: 22.340464360399466.....Total loss: 22.340464360399466\n",
      "epoch 1847 learning rate:  0.010541418516513265   Training loss:   24.790137764650794  Valing loss:   22.340464360399466\n",
      "Pure loss: 24.793454392080136.....Total loss: 24.793454392080136\n",
      "Pure loss: 22.35367336933745.....Total loss: 22.35367336933745\n",
      "epoch 1848 learning rate:  0.010541125541125542   Training loss:   24.793454392080136  Valing loss:   22.35367336933745\n",
      "Pure loss: 24.817601004926214.....Total loss: 24.817601004926214\n",
      "Pure loss: 22.230783849957362.....Total loss: 22.230783849957362\n",
      "epoch 1849 learning rate:  0.010540832882639265   Training loss:   24.817601004926214  Valing loss:   22.230783849957362\n",
      "Pure loss: 24.81075498442072.....Total loss: 24.81075498442072\n",
      "Pure loss: 22.147662308126417.....Total loss: 22.147662308126417\n",
      "epoch 1850 learning rate:  0.01054054054054054   Training loss:   24.81075498442072  Valing loss:   22.147662308126417\n",
      "Pure loss: 24.939544170633006.....Total loss: 24.939544170633006\n",
      "Pure loss: 22.107143944912693.....Total loss: 22.107143944912693\n",
      "epoch 1851 learning rate:  0.010540248514316586   Training loss:   24.939544170633006  Valing loss:   22.107143944912693\n",
      "Pure loss: 24.94442141315202.....Total loss: 24.94442141315202\n",
      "Pure loss: 22.213575043191245.....Total loss: 22.213575043191245\n",
      "epoch 1852 learning rate:  0.010539956803455723   Training loss:   24.94442141315202  Valing loss:   22.213575043191245\n",
      "Pure loss: 24.930737456272798.....Total loss: 24.930737456272798\n",
      "Pure loss: 22.443204290521273.....Total loss: 22.443204290521273\n",
      "epoch 1853 learning rate:  0.010539665407447383   Training loss:   24.930737456272798  Valing loss:   22.443204290521273\n",
      "Pure loss: 24.941743663252737.....Total loss: 24.941743663252737\n",
      "Pure loss: 22.588465712967174.....Total loss: 22.588465712967174\n",
      "epoch 1854 learning rate:  0.010539374325782093   Training loss:   24.941743663252737  Valing loss:   22.588465712967174\n",
      "Pure loss: 24.922921386678393.....Total loss: 24.922921386678393\n",
      "Pure loss: 22.58003147784807.....Total loss: 22.58003147784807\n",
      "epoch 1855 learning rate:  0.010539083557951483   Training loss:   24.922921386678393  Valing loss:   22.58003147784807\n",
      "Pure loss: 24.986298493892885.....Total loss: 24.986298493892885\n",
      "Pure loss: 22.53632228720021.....Total loss: 22.53632228720021\n",
      "epoch 1856 learning rate:  0.010538793103448275   Training loss:   24.986298493892885  Valing loss:   22.53632228720021\n",
      "Pure loss: 25.358519813492283.....Total loss: 25.358519813492283\n",
      "Pure loss: 23.688513035263746.....Total loss: 23.688513035263746\n",
      "epoch 1857 learning rate:  0.01053850296176629   Training loss:   25.358519813492283  Valing loss:   23.688513035263746\n",
      "Pure loss: 25.339462717607674.....Total loss: 25.339462717607674\n",
      "Pure loss: 23.614510634181908.....Total loss: 23.614510634181908\n",
      "epoch 1858 learning rate:  0.01053821313240043   Training loss:   25.339462717607674  Valing loss:   23.614510634181908\n",
      "Pure loss: 25.576355949503256.....Total loss: 25.576355949503256\n",
      "Pure loss: 24.095722344444674.....Total loss: 24.095722344444674\n",
      "epoch 1859 learning rate:  0.010537923614846692   Training loss:   25.576355949503256  Valing loss:   24.095722344444674\n",
      "Pure loss: 25.55694808475611.....Total loss: 25.55694808475611\n",
      "Pure loss: 24.05983506211658.....Total loss: 24.05983506211658\n",
      "epoch 1860 learning rate:  0.010537634408602151   Training loss:   25.55694808475611  Valing loss:   24.05983506211658\n",
      "Pure loss: 25.396546951655015.....Total loss: 25.396546951655015\n",
      "Pure loss: 23.724655608538825.....Total loss: 23.724655608538825\n",
      "epoch 1861 learning rate:  0.010537345513164965   Training loss:   25.396546951655015  Valing loss:   23.724655608538825\n",
      "Pure loss: 25.155655393871516.....Total loss: 25.155655393871516\n",
      "Pure loss: 23.084549812208724.....Total loss: 23.084549812208724\n",
      "epoch 1862 learning rate:  0.010537056928034371   Training loss:   25.155655393871516  Valing loss:   23.084549812208724\n",
      "Pure loss: 25.305165923388973.....Total loss: 25.305165923388973\n",
      "Pure loss: 23.515449360946988.....Total loss: 23.515449360946988\n",
      "epoch 1863 learning rate:  0.010536768652710683   Training loss:   25.305165923388973  Valing loss:   23.515449360946988\n",
      "Pure loss: 25.211800331491276.....Total loss: 25.211800331491276\n",
      "Pure loss: 23.309112091209233.....Total loss: 23.309112091209233\n",
      "epoch 1864 learning rate:  0.01053648068669528   Training loss:   25.211800331491276  Valing loss:   23.309112091209233\n",
      "Pure loss: 25.02236174146652.....Total loss: 25.02236174146652\n",
      "Pure loss: 22.81173797193815.....Total loss: 22.81173797193815\n",
      "epoch 1865 learning rate:  0.010536193029490618   Training loss:   25.02236174146652  Valing loss:   22.81173797193815\n",
      "Pure loss: 25.31187703568437.....Total loss: 25.31187703568437\n",
      "Pure loss: 23.508051572429476.....Total loss: 23.508051572429476\n",
      "epoch 1866 learning rate:  0.010535905680600215   Training loss:   25.31187703568437  Valing loss:   23.508051572429476\n",
      "Pure loss: 25.557513610137292.....Total loss: 25.557513610137292\n",
      "Pure loss: 24.19923620318621.....Total loss: 24.19923620318621\n",
      "epoch 1867 learning rate:  0.010535618639528656   Training loss:   25.557513610137292  Valing loss:   24.19923620318621\n",
      "Pure loss: 25.871134561481117.....Total loss: 25.871134561481117\n",
      "Pure loss: 24.74412024330282.....Total loss: 24.74412024330282\n",
      "epoch 1868 learning rate:  0.010535331905781585   Training loss:   25.871134561481117  Valing loss:   24.74412024330282\n",
      "Pure loss: 25.927531977094112.....Total loss: 25.927531977094112\n",
      "Pure loss: 24.83560302855339.....Total loss: 24.83560302855339\n",
      "epoch 1869 learning rate:  0.010535045478865704   Training loss:   25.927531977094112  Valing loss:   24.83560302855339\n",
      "Pure loss: 25.72487085241943.....Total loss: 25.72487085241943\n",
      "Pure loss: 24.470377785604818.....Total loss: 24.470377785604818\n",
      "epoch 1870 learning rate:  0.01053475935828877   Training loss:   25.72487085241943  Valing loss:   24.470377785604818\n",
      "Pure loss: 25.586899335224878.....Total loss: 25.586899335224878\n",
      "Pure loss: 24.187907139025057.....Total loss: 24.187907139025057\n",
      "epoch 1871 learning rate:  0.010534473543559594   Training loss:   25.586899335224878  Valing loss:   24.187907139025057\n",
      "Pure loss: 25.59591463899224.....Total loss: 25.59591463899224\n",
      "Pure loss: 24.202118188491518.....Total loss: 24.202118188491518\n",
      "epoch 1872 learning rate:  0.010534188034188034   Training loss:   25.59591463899224  Valing loss:   24.202118188491518\n",
      "Pure loss: 25.56976308720383.....Total loss: 25.56976308720383\n",
      "Pure loss: 24.08907772509642.....Total loss: 24.08907772509642\n",
      "epoch 1873 learning rate:  0.010533902829684997   Training loss:   25.56976308720383  Valing loss:   24.08907772509642\n",
      "Pure loss: 26.41173223065039.....Total loss: 26.41173223065039\n",
      "Pure loss: 25.62839409136965.....Total loss: 25.62839409136965\n",
      "epoch 1874 learning rate:  0.010533617929562434   Training loss:   26.41173223065039  Valing loss:   25.62839409136965\n",
      "Pure loss: 26.362889731947988.....Total loss: 26.362889731947988\n",
      "Pure loss: 25.530386560805646.....Total loss: 25.530386560805646\n",
      "epoch 1875 learning rate:  0.010533333333333334   Training loss:   26.362889731947988  Valing loss:   25.530386560805646\n",
      "Pure loss: 26.14934131793292.....Total loss: 26.14934131793292\n",
      "Pure loss: 25.161407338843382.....Total loss: 25.161407338843382\n",
      "epoch 1876 learning rate:  0.010533049040511727   Training loss:   26.14934131793292  Valing loss:   25.161407338843382\n",
      "Pure loss: 26.824755252290394.....Total loss: 26.824755252290394\n",
      "Pure loss: 26.297930488496004.....Total loss: 26.297930488496004\n",
      "epoch 1877 learning rate:  0.01053276505061268   Training loss:   26.824755252290394  Valing loss:   26.297930488496004\n",
      "Pure loss: 26.894737759356094.....Total loss: 26.894737759356094\n",
      "Pure loss: 26.4096940647235.....Total loss: 26.4096940647235\n",
      "epoch 1878 learning rate:  0.01053248136315229   Training loss:   26.894737759356094  Valing loss:   26.4096940647235\n",
      "Pure loss: 25.581571477233787.....Total loss: 25.581571477233787\n",
      "Pure loss: 24.26296469084015.....Total loss: 24.26296469084015\n",
      "epoch 1879 learning rate:  0.010532197977647685   Training loss:   25.581571477233787  Valing loss:   24.26296469084015\n",
      "Pure loss: 25.634163615845257.....Total loss: 25.634163615845257\n",
      "Pure loss: 24.370829478401266.....Total loss: 24.370829478401266\n",
      "epoch 1880 learning rate:  0.010531914893617022   Training loss:   25.634163615845257  Valing loss:   24.370829478401266\n",
      "Pure loss: 25.30936936951646.....Total loss: 25.30936936951646\n",
      "Pure loss: 23.708287397696186.....Total loss: 23.708287397696186\n",
      "epoch 1881 learning rate:  0.010531632110579479   Training loss:   25.30936936951646  Valing loss:   23.708287397696186\n",
      "Pure loss: 25.52315046824775.....Total loss: 25.52315046824775\n",
      "Pure loss: 24.176782132861554.....Total loss: 24.176782132861554\n",
      "epoch 1882 learning rate:  0.010531349628055261   Training loss:   25.52315046824775  Valing loss:   24.176782132861554\n",
      "Pure loss: 26.052193445189214.....Total loss: 26.052193445189214\n",
      "Pure loss: 25.296958343934904.....Total loss: 25.296958343934904\n",
      "epoch 1883 learning rate:  0.010531067445565588   Training loss:   26.052193445189214  Valing loss:   25.296958343934904\n",
      "Pure loss: 26.01927372273347.....Total loss: 26.01927372273347\n",
      "Pure loss: 25.24152986998535.....Total loss: 25.24152986998535\n",
      "epoch 1884 learning rate:  0.010530785562632696   Training loss:   26.01927372273347  Valing loss:   25.24152986998535\n",
      "Pure loss: 25.793228300692068.....Total loss: 25.793228300692068\n",
      "Pure loss: 24.88671241719806.....Total loss: 24.88671241719806\n",
      "epoch 1885 learning rate:  0.01053050397877984   Training loss:   25.793228300692068  Valing loss:   24.88671241719806\n",
      "Pure loss: 26.200078397560624.....Total loss: 26.200078397560624\n",
      "Pure loss: 25.535633500835548.....Total loss: 25.535633500835548\n",
      "epoch 1886 learning rate:  0.010530222693531283   Training loss:   26.200078397560624  Valing loss:   25.535633500835548\n",
      "Pure loss: 26.679734120469412.....Total loss: 26.679734120469412\n",
      "Pure loss: 26.25553980412218.....Total loss: 26.25553980412218\n",
      "epoch 1887 learning rate:  0.010529941706412294   Training loss:   26.679734120469412  Valing loss:   26.25553980412218\n",
      "Pure loss: 26.045668209771414.....Total loss: 26.045668209771414\n",
      "Pure loss: 25.26726808741683.....Total loss: 25.26726808741683\n",
      "epoch 1888 learning rate:  0.010529661016949153   Training loss:   26.045668209771414  Valing loss:   25.26726808741683\n",
      "Pure loss: 26.921116656575116.....Total loss: 26.921116656575116\n",
      "Pure loss: 26.632564755531114.....Total loss: 26.632564755531114\n",
      "epoch 1889 learning rate:  0.010529380624669137   Training loss:   26.921116656575116  Valing loss:   26.632564755531114\n",
      "Pure loss: 26.862207367825253.....Total loss: 26.862207367825253\n",
      "Pure loss: 26.54661572298651.....Total loss: 26.54661572298651\n",
      "epoch 1890 learning rate:  0.010529100529100529   Training loss:   26.862207367825253  Valing loss:   26.54661572298651\n",
      "Pure loss: 25.4845315577512.....Total loss: 25.4845315577512\n",
      "Pure loss: 24.427298584226005.....Total loss: 24.427298584226005\n",
      "epoch 1891 learning rate:  0.010528820729772608   Training loss:   25.4845315577512  Valing loss:   24.427298584226005\n",
      "Pure loss: 25.171565063128742.....Total loss: 25.171565063128742\n",
      "Pure loss: 23.88252612346891.....Total loss: 23.88252612346891\n",
      "epoch 1892 learning rate:  0.010528541226215646   Training loss:   25.171565063128742  Valing loss:   23.88252612346891\n",
      "Pure loss: 24.947135088281716.....Total loss: 24.947135088281716\n",
      "Pure loss: 23.381911734789103.....Total loss: 23.381911734789103\n",
      "epoch 1893 learning rate:  0.010528262017960909   Training loss:   24.947135088281716  Valing loss:   23.381911734789103\n",
      "Pure loss: 24.686652398651887.....Total loss: 24.686652398651887\n",
      "Pure loss: 22.775062733161036.....Total loss: 22.775062733161036\n",
      "epoch 1894 learning rate:  0.010527983104540655   Training loss:   24.686652398651887  Valing loss:   22.775062733161036\n",
      "Pure loss: 24.636732104969067.....Total loss: 24.636732104969067\n",
      "Pure loss: 22.621165629141462.....Total loss: 22.621165629141462\n",
      "epoch 1895 learning rate:  0.010527704485488128   Training loss:   24.636732104969067  Valing loss:   22.621165629141462\n",
      "Pure loss: 24.553016200754403.....Total loss: 24.553016200754403\n",
      "Pure loss: 22.322969892040653.....Total loss: 22.322969892040653\n",
      "epoch 1896 learning rate:  0.010527426160337553   Training loss:   24.553016200754403  Valing loss:   22.322969892040653\n",
      "Pure loss: 24.493918479489682.....Total loss: 24.493918479489682\n",
      "Pure loss: 22.51182607680809.....Total loss: 22.51182607680809\n",
      "epoch 1897 learning rate:  0.010527148128624144   Training loss:   24.493918479489682  Valing loss:   22.51182607680809\n",
      "Pure loss: 24.45396146142767.....Total loss: 24.45396146142767\n",
      "Pure loss: 22.376411667986655.....Total loss: 22.376411667986655\n",
      "epoch 1898 learning rate:  0.01052687038988409   Training loss:   24.45396146142767  Valing loss:   22.376411667986655\n",
      "Pure loss: 24.372271681205035.....Total loss: 24.372271681205035\n",
      "Pure loss: 22.022417038013135.....Total loss: 22.022417038013135\n",
      "epoch 1899 learning rate:  0.010526592943654555   Training loss:   24.372271681205035  Valing loss:   22.022417038013135\n",
      "Pure loss: 24.32721438989986.....Total loss: 24.32721438989986\n",
      "Pure loss: 21.77771600491224.....Total loss: 21.77771600491224\n",
      "epoch 1900 learning rate:  0.010526315789473684   Training loss:   24.32721438989986  Valing loss:   21.77771600491224\n",
      "Pure loss: 24.352459506304296.....Total loss: 24.352459506304296\n",
      "Pure loss: 21.65471866247311.....Total loss: 21.65471866247311\n",
      "epoch 1901 learning rate:  0.01052603892688059   Training loss:   24.352459506304296  Valing loss:   21.65471866247311\n",
      "Pure loss: 24.408791031062716.....Total loss: 24.408791031062716\n",
      "Pure loss: 21.678753133406634.....Total loss: 21.678753133406634\n",
      "epoch 1902 learning rate:  0.010525762355415353   Training loss:   24.408791031062716  Valing loss:   21.678753133406634\n",
      "Pure loss: 24.51034914066912.....Total loss: 24.51034914066912\n",
      "Pure loss: 22.474825165279142.....Total loss: 22.474825165279142\n",
      "epoch 1903 learning rate:  0.010525486074619023   Training loss:   24.51034914066912  Valing loss:   22.474825165279142\n",
      "Pure loss: 24.45327757536956.....Total loss: 24.45327757536956\n",
      "Pure loss: 22.0786269325337.....Total loss: 22.0786269325337\n",
      "epoch 1904 learning rate:  0.010525210084033614   Training loss:   24.45327757536956  Valing loss:   22.0786269325337\n",
      "Pure loss: 24.707495523479675.....Total loss: 24.707495523479675\n",
      "Pure loss: 23.105981173642828.....Total loss: 23.105981173642828\n",
      "epoch 1905 learning rate:  0.0105249343832021   Training loss:   24.707495523479675  Valing loss:   23.105981173642828\n",
      "Pure loss: 24.551745032001747.....Total loss: 24.551745032001747\n",
      "Pure loss: 22.64544368522838.....Total loss: 22.64544368522838\n",
      "epoch 1906 learning rate:  0.010524658971668415   Training loss:   24.551745032001747  Valing loss:   22.64544368522838\n",
      "Pure loss: 24.51420899444664.....Total loss: 24.51420899444664\n",
      "Pure loss: 22.32561277299054.....Total loss: 22.32561277299054\n",
      "epoch 1907 learning rate:  0.010524383848977453   Training loss:   24.51420899444664  Valing loss:   22.32561277299054\n",
      "Pure loss: 24.486960281225514.....Total loss: 24.486960281225514\n",
      "Pure loss: 21.799823666575914.....Total loss: 21.799823666575914\n",
      "epoch 1908 learning rate:  0.010524109014675053   Training loss:   24.486960281225514  Valing loss:   21.799823666575914\n",
      "Pure loss: 24.436440467987502.....Total loss: 24.436440467987502\n",
      "Pure loss: 21.89611511699944.....Total loss: 21.89611511699944\n",
      "epoch 1909 learning rate:  0.010523834468308015   Training loss:   24.436440467987502  Valing loss:   21.89611511699944\n",
      "Pure loss: 24.489958474006585.....Total loss: 24.489958474006585\n",
      "Pure loss: 21.8236897688487.....Total loss: 21.8236897688487\n",
      "epoch 1910 learning rate:  0.010523560209424083   Training loss:   24.489958474006585  Valing loss:   21.8236897688487\n",
      "Pure loss: 24.461042063357066.....Total loss: 24.461042063357066\n",
      "Pure loss: 21.960365091814314.....Total loss: 21.960365091814314\n",
      "epoch 1911 learning rate:  0.010523286237571951   Training loss:   24.461042063357066  Valing loss:   21.960365091814314\n",
      "Pure loss: 24.627257741309673.....Total loss: 24.627257741309673\n",
      "Pure loss: 21.56742149321886.....Total loss: 21.56742149321886\n",
      "epoch 1912 learning rate:  0.010523012552301255   Training loss:   24.627257741309673  Valing loss:   21.56742149321886\n",
      "Pure loss: 24.60332596379699.....Total loss: 24.60332596379699\n",
      "Pure loss: 21.55563394020354.....Total loss: 21.55563394020354\n",
      "epoch 1913 learning rate:  0.010522739153162573   Training loss:   24.60332596379699  Valing loss:   21.55563394020354\n",
      "Pure loss: 24.58904765915975.....Total loss: 24.58904765915975\n",
      "Pure loss: 21.576509447563435.....Total loss: 21.576509447563435\n",
      "epoch 1914 learning rate:  0.010522466039707419   Training loss:   24.58904765915975  Valing loss:   21.576509447563435\n",
      "Pure loss: 24.510184407578205.....Total loss: 24.510184407578205\n",
      "Pure loss: 21.62335300299717.....Total loss: 21.62335300299717\n",
      "epoch 1915 learning rate:  0.010522193211488251   Training loss:   24.510184407578205  Valing loss:   21.62335300299717\n",
      "Pure loss: 24.60897499464027.....Total loss: 24.60897499464027\n",
      "Pure loss: 21.463882596269453.....Total loss: 21.463882596269453\n",
      "epoch 1916 learning rate:  0.010521920668058455   Training loss:   24.60897499464027  Valing loss:   21.463882596269453\n",
      "Pure loss: 24.879633328664227.....Total loss: 24.879633328664227\n",
      "Pure loss: 21.362345561720456.....Total loss: 21.362345561720456\n",
      "epoch 1917 learning rate:  0.010521648408972352   Training loss:   24.879633328664227  Valing loss:   21.362345561720456\n",
      "Pure loss: 24.71531394857031.....Total loss: 24.71531394857031\n",
      "Pure loss: 21.27527098895003.....Total loss: 21.27527098895003\n",
      "epoch 1918 learning rate:  0.010521376433785192   Training loss:   24.71531394857031  Valing loss:   21.27527098895003\n",
      "Pure loss: 24.587840873311265.....Total loss: 24.587840873311265\n",
      "Pure loss: 21.29839277751363.....Total loss: 21.29839277751363\n",
      "epoch 1919 learning rate:  0.010521104742053153   Training loss:   24.587840873311265  Valing loss:   21.29839277751363\n",
      "Pure loss: 24.732328744059654.....Total loss: 24.732328744059654\n",
      "Pure loss: 21.23232121343137.....Total loss: 21.23232121343137\n",
      "epoch 1920 learning rate:  0.010520833333333333   Training loss:   24.732328744059654  Valing loss:   21.23232121343137\n",
      "Pure loss: 24.75764821288305.....Total loss: 24.75764821288305\n",
      "Pure loss: 21.226498038459138.....Total loss: 21.226498038459138\n",
      "epoch 1921 learning rate:  0.010520562207183759   Training loss:   24.75764821288305  Valing loss:   21.226498038459138\n",
      "Pure loss: 25.004253555525903.....Total loss: 25.004253555525903\n",
      "Pure loss: 21.226202270150164.....Total loss: 21.226202270150164\n",
      "epoch 1922 learning rate:  0.010520291363163372   Training loss:   25.004253555525903  Valing loss:   21.226202270150164\n",
      "Pure loss: 24.457196683823994.....Total loss: 24.457196683823994\n",
      "Pure loss: 20.98903237688381.....Total loss: 20.98903237688381\n",
      "epoch 1923 learning rate:  0.010520020800832034   Training loss:   24.457196683823994  Valing loss:   20.98903237688381\n",
      "Pure loss: 24.381609926725115.....Total loss: 24.381609926725115\n",
      "Pure loss: 21.02982810752024.....Total loss: 21.02982810752024\n",
      "epoch 1924 learning rate:  0.01051975051975052   Training loss:   24.381609926725115  Valing loss:   21.02982810752024\n",
      "Pure loss: 24.260820119794808.....Total loss: 24.260820119794808\n",
      "Pure loss: 21.086615106611262.....Total loss: 21.086615106611262\n",
      "epoch 1925 learning rate:  0.010519480519480519   Training loss:   24.260820119794808  Valing loss:   21.086615106611262\n",
      "Pure loss: 24.148299432227315.....Total loss: 24.148299432227315\n",
      "Pure loss: 21.314377199019308.....Total loss: 21.314377199019308\n",
      "epoch 1926 learning rate:  0.010519210799584632   Training loss:   24.148299432227315  Valing loss:   21.314377199019308\n",
      "Pure loss: 24.184704029012323.....Total loss: 24.184704029012323\n",
      "Pure loss: 21.142777903446444.....Total loss: 21.142777903446444\n",
      "epoch 1927 learning rate:  0.010518941359626362   Training loss:   24.184704029012323  Valing loss:   21.142777903446444\n",
      "Pure loss: 24.1506385592458.....Total loss: 24.1506385592458\n",
      "Pure loss: 21.3793609082936.....Total loss: 21.3793609082936\n",
      "epoch 1928 learning rate:  0.010518672199170125   Training loss:   24.1506385592458  Valing loss:   21.3793609082936\n",
      "Pure loss: 24.238893310190797.....Total loss: 24.238893310190797\n",
      "Pure loss: 22.07124157067571.....Total loss: 22.07124157067571\n",
      "epoch 1929 learning rate:  0.010518403317781234   Training loss:   24.238893310190797  Valing loss:   22.07124157067571\n",
      "Pure loss: 24.175228428451774.....Total loss: 24.175228428451774\n",
      "Pure loss: 21.838142012344438.....Total loss: 21.838142012344438\n",
      "epoch 1930 learning rate:  0.010518134715025907   Training loss:   24.175228428451774  Valing loss:   21.838142012344438\n",
      "Pure loss: 24.295059754335053.....Total loss: 24.295059754335053\n",
      "Pure loss: 22.264907271250237.....Total loss: 22.264907271250237\n",
      "epoch 1931 learning rate:  0.010517866390471258   Training loss:   24.295059754335053  Valing loss:   22.264907271250237\n",
      "Pure loss: 24.29315323204522.....Total loss: 24.29315323204522\n",
      "Pure loss: 22.27393683685661.....Total loss: 22.27393683685661\n",
      "epoch 1932 learning rate:  0.0105175983436853   Training loss:   24.29315323204522  Valing loss:   22.27393683685661\n",
      "Pure loss: 24.124556860322002.....Total loss: 24.124556860322002\n",
      "Pure loss: 21.672257098255088.....Total loss: 21.672257098255088\n",
      "epoch 1933 learning rate:  0.010517330574236938   Training loss:   24.124556860322002  Valing loss:   21.672257098255088\n",
      "Pure loss: 24.093180086697174.....Total loss: 24.093180086697174\n",
      "Pure loss: 21.351247628647833.....Total loss: 21.351247628647833\n",
      "epoch 1934 learning rate:  0.010517063081695968   Training loss:   24.093180086697174  Valing loss:   21.351247628647833\n",
      "Pure loss: 24.22857475469346.....Total loss: 24.22857475469346\n",
      "Pure loss: 21.339524181733186.....Total loss: 21.339524181733186\n",
      "epoch 1935 learning rate:  0.010516795865633075   Training loss:   24.22857475469346  Valing loss:   21.339524181733186\n",
      "Pure loss: 24.24839407328698.....Total loss: 24.24839407328698\n",
      "Pure loss: 21.28309695865781.....Total loss: 21.28309695865781\n",
      "epoch 1936 learning rate:  0.010516528925619835   Training loss:   24.24839407328698  Valing loss:   21.28309695865781\n",
      "Pure loss: 24.319413456419042.....Total loss: 24.319413456419042\n",
      "Pure loss: 21.238146127176336.....Total loss: 21.238146127176336\n",
      "epoch 1937 learning rate:  0.010516262261228704   Training loss:   24.319413456419042  Valing loss:   21.238146127176336\n",
      "Pure loss: 24.717000755229435.....Total loss: 24.717000755229435\n",
      "Pure loss: 21.102058069207278.....Total loss: 21.102058069207278\n",
      "epoch 1938 learning rate:  0.010515995872033024   Training loss:   24.717000755229435  Valing loss:   21.102058069207278\n",
      "Pure loss: 24.639606565081976.....Total loss: 24.639606565081976\n",
      "Pure loss: 21.12113416572875.....Total loss: 21.12113416572875\n",
      "epoch 1939 learning rate:  0.010515729757607013   Training loss:   24.639606565081976  Valing loss:   21.12113416572875\n",
      "Pure loss: 24.115915573073814.....Total loss: 24.115915573073814\n",
      "Pure loss: 21.545488859416206.....Total loss: 21.545488859416206\n",
      "epoch 1940 learning rate:  0.010515463917525773   Training loss:   24.115915573073814  Valing loss:   21.545488859416206\n",
      "Pure loss: 24.344455267896375.....Total loss: 24.344455267896375\n",
      "Pure loss: 22.640251564292704.....Total loss: 22.640251564292704\n",
      "epoch 1941 learning rate:  0.010515198351365275   Training loss:   24.344455267896375  Valing loss:   22.640251564292704\n",
      "Pure loss: 24.335046886591844.....Total loss: 24.335046886591844\n",
      "Pure loss: 22.5623091650997.....Total loss: 22.5623091650997\n",
      "epoch 1942 learning rate:  0.010514933058702368   Training loss:   24.335046886591844  Valing loss:   22.5623091650997\n",
      "Pure loss: 24.094461200397394.....Total loss: 24.094461200397394\n",
      "Pure loss: 21.734686180158498.....Total loss: 21.734686180158498\n",
      "epoch 1943 learning rate:  0.010514668039114772   Training loss:   24.094461200397394  Valing loss:   21.734686180158498\n",
      "Pure loss: 24.060611767518377.....Total loss: 24.060611767518377\n",
      "Pure loss: 21.75660337266866.....Total loss: 21.75660337266866\n",
      "epoch 1944 learning rate:  0.01051440329218107   Training loss:   24.060611767518377  Valing loss:   21.75660337266866\n",
      "Pure loss: 24.070987900135496.....Total loss: 24.070987900135496\n",
      "Pure loss: 21.63015092374319.....Total loss: 21.63015092374319\n",
      "epoch 1945 learning rate:  0.01051413881748072   Training loss:   24.070987900135496  Valing loss:   21.63015092374319\n",
      "Pure loss: 24.149757624045048.....Total loss: 24.149757624045048\n",
      "Pure loss: 21.56266690010757.....Total loss: 21.56266690010757\n",
      "epoch 1946 learning rate:  0.01051387461459404   Training loss:   24.149757624045048  Valing loss:   21.56266690010757\n",
      "Pure loss: 24.179872235928848.....Total loss: 24.179872235928848\n",
      "Pure loss: 21.441665838542527.....Total loss: 21.441665838542527\n",
      "epoch 1947 learning rate:  0.010513610683102208   Training loss:   24.179872235928848  Valing loss:   21.441665838542527\n",
      "Pure loss: 24.558113533271364.....Total loss: 24.558113533271364\n",
      "Pure loss: 21.189399882232447.....Total loss: 21.189399882232447\n",
      "epoch 1948 learning rate:  0.01051334702258727   Training loss:   24.558113533271364  Valing loss:   21.189399882232447\n",
      "Pure loss: 24.633997454597033.....Total loss: 24.633997454597033\n",
      "Pure loss: 21.17279366335432.....Total loss: 21.17279366335432\n",
      "epoch 1949 learning rate:  0.01051308363263212   Training loss:   24.633997454597033  Valing loss:   21.17279366335432\n",
      "Pure loss: 24.715301763032507.....Total loss: 24.715301763032507\n",
      "Pure loss: 21.192554842262734.....Total loss: 21.192554842262734\n",
      "epoch 1950 learning rate:  0.010512820512820513   Training loss:   24.715301763032507  Valing loss:   21.192554842262734\n",
      "Pure loss: 24.43804155134555.....Total loss: 24.43804155134555\n",
      "Pure loss: 21.23555438213969.....Total loss: 21.23555438213969\n",
      "epoch 1951 learning rate:  0.010512557662737058   Training loss:   24.43804155134555  Valing loss:   21.23555438213969\n",
      "Pure loss: 24.708202864686484.....Total loss: 24.708202864686484\n",
      "Pure loss: 21.283149565923125.....Total loss: 21.283149565923125\n",
      "epoch 1952 learning rate:  0.010512295081967214   Training loss:   24.708202864686484  Valing loss:   21.283149565923125\n",
      "Pure loss: 24.584761446571708.....Total loss: 24.584761446571708\n",
      "Pure loss: 21.307276679578678.....Total loss: 21.307276679578678\n",
      "epoch 1953 learning rate:  0.010512032770097286   Training loss:   24.584761446571708  Valing loss:   21.307276679578678\n",
      "Pure loss: 24.123695244728058.....Total loss: 24.123695244728058\n",
      "Pure loss: 21.102012870693464.....Total loss: 21.102012870693464\n",
      "epoch 1954 learning rate:  0.010511770726714432   Training loss:   24.123695244728058  Valing loss:   21.102012870693464\n",
      "Pure loss: 24.168273975085768.....Total loss: 24.168273975085768\n",
      "Pure loss: 21.096316965332075.....Total loss: 21.096316965332075\n",
      "epoch 1955 learning rate:  0.01051150895140665   Training loss:   24.168273975085768  Valing loss:   21.096316965332075\n",
      "Pure loss: 24.1811858472041.....Total loss: 24.1811858472041\n",
      "Pure loss: 21.10157883365186.....Total loss: 21.10157883365186\n",
      "epoch 1956 learning rate:  0.010511247443762781   Training loss:   24.1811858472041  Valing loss:   21.10157883365186\n",
      "Pure loss: 24.10945479400015.....Total loss: 24.10945479400015\n",
      "Pure loss: 21.249281156068246.....Total loss: 21.249281156068246\n",
      "epoch 1957 learning rate:  0.01051098620337251   Training loss:   24.10945479400015  Valing loss:   21.249281156068246\n",
      "Pure loss: 24.193699051944527.....Total loss: 24.193699051944527\n",
      "Pure loss: 21.15552568748073.....Total loss: 21.15552568748073\n",
      "epoch 1958 learning rate:  0.010510725229826354   Training loss:   24.193699051944527  Valing loss:   21.15552568748073\n",
      "Pure loss: 24.73321688451975.....Total loss: 24.73321688451975\n",
      "Pure loss: 21.0298790189347.....Total loss: 21.0298790189347\n",
      "epoch 1959 learning rate:  0.010510464522715672   Training loss:   24.73321688451975  Valing loss:   21.0298790189347\n",
      "Pure loss: 24.55984870926716.....Total loss: 24.55984870926716\n",
      "Pure loss: 20.89527068945562.....Total loss: 20.89527068945562\n",
      "epoch 1960 learning rate:  0.010510204081632654   Training loss:   24.55984870926716  Valing loss:   20.89527068945562\n",
      "Pure loss: 25.229251801923226.....Total loss: 25.229251801923226\n",
      "Pure loss: 21.323898301832934.....Total loss: 21.323898301832934\n",
      "epoch 1961 learning rate:  0.010509943906170322   Training loss:   25.229251801923226  Valing loss:   21.323898301832934\n",
      "Pure loss: 24.52419572141373.....Total loss: 24.52419572141373\n",
      "Pure loss: 21.087727824934642.....Total loss: 21.087727824934642\n",
      "epoch 1962 learning rate:  0.010509683995922529   Training loss:   24.52419572141373  Valing loss:   21.087727824934642\n",
      "Pure loss: 25.246882900333425.....Total loss: 25.246882900333425\n",
      "Pure loss: 21.17484350599956.....Total loss: 21.17484350599956\n",
      "epoch 1963 learning rate:  0.010509424350483954   Training loss:   25.246882900333425  Valing loss:   21.17484350599956\n",
      "Pure loss: 24.489183953235095.....Total loss: 24.489183953235095\n",
      "Pure loss: 20.722672908941977.....Total loss: 20.722672908941977\n",
      "epoch 1964 learning rate:  0.010509164969450101   Training loss:   24.489183953235095  Valing loss:   20.722672908941977\n",
      "Pure loss: 24.25965129512083.....Total loss: 24.25965129512083\n",
      "Pure loss: 20.63471819081577.....Total loss: 20.63471819081577\n",
      "epoch 1965 learning rate:  0.010508905852417302   Training loss:   24.25965129512083  Valing loss:   20.63471819081577\n",
      "Pure loss: 24.289306263297494.....Total loss: 24.289306263297494\n",
      "Pure loss: 20.6486890695872.....Total loss: 20.6486890695872\n",
      "epoch 1966 learning rate:  0.010508646998982706   Training loss:   24.289306263297494  Valing loss:   20.6486890695872\n",
      "Pure loss: 24.250313696369755.....Total loss: 24.250313696369755\n",
      "Pure loss: 20.646779340967687.....Total loss: 20.646779340967687\n",
      "epoch 1967 learning rate:  0.01050838840874428   Training loss:   24.250313696369755  Valing loss:   20.646779340967687\n",
      "Pure loss: 24.540183809742874.....Total loss: 24.540183809742874\n",
      "Pure loss: 20.688110988944615.....Total loss: 20.688110988944615\n",
      "epoch 1968 learning rate:  0.010508130081300813   Training loss:   24.540183809742874  Valing loss:   20.688110988944615\n",
      "Pure loss: 24.278457771234027.....Total loss: 24.278457771234027\n",
      "Pure loss: 20.640048360702057.....Total loss: 20.640048360702057\n",
      "epoch 1969 learning rate:  0.010507872016251904   Training loss:   24.278457771234027  Valing loss:   20.640048360702057\n",
      "Pure loss: 24.638107760587143.....Total loss: 24.638107760587143\n",
      "Pure loss: 20.662394798867123.....Total loss: 20.662394798867123\n",
      "epoch 1970 learning rate:  0.01050761421319797   Training loss:   24.638107760587143  Valing loss:   20.662394798867123\n",
      "Pure loss: 24.26301980384445.....Total loss: 24.26301980384445\n",
      "Pure loss: 20.599954744777058.....Total loss: 20.599954744777058\n",
      "epoch 1971 learning rate:  0.010507356671740234   Training loss:   24.26301980384445  Valing loss:   20.599954744777058\n",
      "Pure loss: 24.289087476503415.....Total loss: 24.289087476503415\n",
      "Pure loss: 20.594573020332508.....Total loss: 20.594573020332508\n",
      "epoch 1972 learning rate:  0.01050709939148073   Training loss:   24.289087476503415  Valing loss:   20.594573020332508\n",
      "Pure loss: 24.72334718672956.....Total loss: 24.72334718672956\n",
      "Pure loss: 20.689120551538082.....Total loss: 20.689120551538082\n",
      "epoch 1973 learning rate:  0.0105068423720223   Training loss:   24.72334718672956  Valing loss:   20.689120551538082\n",
      "Pure loss: 25.437893294381936.....Total loss: 25.437893294381936\n",
      "Pure loss: 20.93966230305009.....Total loss: 20.93966230305009\n",
      "epoch 1974 learning rate:  0.010506585612968591   Training loss:   25.437893294381936  Valing loss:   20.93966230305009\n",
      "Pure loss: 26.167605319879495.....Total loss: 26.167605319879495\n",
      "Pure loss: 21.27623068108809.....Total loss: 21.27623068108809\n",
      "epoch 1975 learning rate:  0.01050632911392405   Training loss:   26.167605319879495  Valing loss:   21.27623068108809\n",
      "Pure loss: 24.705325847964186.....Total loss: 24.705325847964186\n",
      "Pure loss: 21.033884353152843.....Total loss: 21.033884353152843\n",
      "epoch 1976 learning rate:  0.010506072874493927   Training loss:   24.705325847964186  Valing loss:   21.033884353152843\n",
      "Pure loss: 24.592773978248783.....Total loss: 24.592773978248783\n",
      "Pure loss: 20.962783461201244.....Total loss: 20.962783461201244\n",
      "epoch 1977 learning rate:  0.01050581689428427   Training loss:   24.592773978248783  Valing loss:   20.962783461201244\n",
      "Pure loss: 24.667022902623575.....Total loss: 24.667022902623575\n",
      "Pure loss: 21.009012359949036.....Total loss: 21.009012359949036\n",
      "epoch 1978 learning rate:  0.010505561172901922   Training loss:   24.667022902623575  Valing loss:   21.009012359949036\n",
      "Pure loss: 24.814187493270296.....Total loss: 24.814187493270296\n",
      "Pure loss: 21.015965822717256.....Total loss: 21.015965822717256\n",
      "epoch 1979 learning rate:  0.010505305709954523   Training loss:   24.814187493270296  Valing loss:   21.015965822717256\n",
      "Pure loss: 25.136685696412126.....Total loss: 25.136685696412126\n",
      "Pure loss: 21.270292547175547.....Total loss: 21.270292547175547\n",
      "epoch 1980 learning rate:  0.010505050505050505   Training loss:   25.136685696412126  Valing loss:   21.270292547175547\n",
      "Pure loss: 25.620329469297857.....Total loss: 25.620329469297857\n",
      "Pure loss: 21.375065589718343.....Total loss: 21.375065589718343\n",
      "epoch 1981 learning rate:  0.010504795557799092   Training loss:   25.620329469297857  Valing loss:   21.375065589718343\n",
      "Pure loss: 24.9912235795583.....Total loss: 24.9912235795583\n",
      "Pure loss: 21.160903626795438.....Total loss: 21.160903626795438\n",
      "epoch 1982 learning rate:  0.010504540867810292   Training loss:   24.9912235795583  Valing loss:   21.160903626795438\n",
      "Pure loss: 25.052716064724112.....Total loss: 25.052716064724112\n",
      "Pure loss: 21.174561364252362.....Total loss: 21.174561364252362\n",
      "epoch 1983 learning rate:  0.010504286434694907   Training loss:   25.052716064724112  Valing loss:   21.174561364252362\n",
      "Pure loss: 25.93584135113539.....Total loss: 25.93584135113539\n",
      "Pure loss: 21.566164530585255.....Total loss: 21.566164530585255\n",
      "epoch 1984 learning rate:  0.010504032258064516   Training loss:   25.93584135113539  Valing loss:   21.566164530585255\n",
      "Pure loss: 25.445572349893038.....Total loss: 25.445572349893038\n",
      "Pure loss: 21.239362439669474.....Total loss: 21.239362439669474\n",
      "epoch 1985 learning rate:  0.010503778337531487   Training loss:   25.445572349893038  Valing loss:   21.239362439669474\n",
      "Pure loss: 25.299581106590594.....Total loss: 25.299581106590594\n",
      "Pure loss: 21.171573451239162.....Total loss: 21.171573451239162\n",
      "epoch 1986 learning rate:  0.010503524672708963   Training loss:   25.299581106590594  Valing loss:   21.171573451239162\n",
      "Pure loss: 24.88653049512596.....Total loss: 24.88653049512596\n",
      "Pure loss: 21.000711956001446.....Total loss: 21.000711956001446\n",
      "epoch 1987 learning rate:  0.010503271263210871   Training loss:   24.88653049512596  Valing loss:   21.000711956001446\n",
      "Pure loss: 24.392845681113215.....Total loss: 24.392845681113215\n",
      "Pure loss: 20.887151562931667.....Total loss: 20.887151562931667\n",
      "epoch 1988 learning rate:  0.010503018108651912   Training loss:   24.392845681113215  Valing loss:   20.887151562931667\n",
      "Pure loss: 24.290858636841644.....Total loss: 24.290858636841644\n",
      "Pure loss: 20.918312324561427.....Total loss: 20.918312324561427\n",
      "epoch 1989 learning rate:  0.010502765208647562   Training loss:   24.290858636841644  Valing loss:   20.918312324561427\n",
      "Pure loss: 24.17174798701635.....Total loss: 24.17174798701635\n",
      "Pure loss: 20.84455084700645.....Total loss: 20.84455084700645\n",
      "epoch 1990 learning rate:  0.01050251256281407   Training loss:   24.17174798701635  Valing loss:   20.84455084700645\n",
      "Pure loss: 23.772079908674886.....Total loss: 23.772079908674886\n",
      "Pure loss: 21.051103161912085.....Total loss: 21.051103161912085\n",
      "epoch 1991 learning rate:  0.010502260170768458   Training loss:   23.772079908674886  Valing loss:   21.051103161912085\n",
      "Pure loss: 23.790453418667102.....Total loss: 23.790453418667102\n",
      "Pure loss: 20.788023965096006.....Total loss: 20.788023965096006\n",
      "epoch 1992 learning rate:  0.010502008032128514   Training loss:   23.790453418667102  Valing loss:   20.788023965096006\n",
      "Pure loss: 24.001991790600982.....Total loss: 24.001991790600982\n",
      "Pure loss: 20.597766009913997.....Total loss: 20.597766009913997\n",
      "epoch 1993 learning rate:  0.010501756146512794   Training loss:   24.001991790600982  Valing loss:   20.597766009913997\n",
      "Pure loss: 24.043545845298613.....Total loss: 24.043545845298613\n",
      "Pure loss: 20.592359579314.....Total loss: 20.592359579314\n",
      "epoch 1994 learning rate:  0.010501504513540622   Training loss:   24.043545845298613  Valing loss:   20.592359579314\n",
      "Pure loss: 24.539977988110998.....Total loss: 24.539977988110998\n",
      "Pure loss: 20.630257604241343.....Total loss: 20.630257604241343\n",
      "epoch 1995 learning rate:  0.01050125313283208   Training loss:   24.539977988110998  Valing loss:   20.630257604241343\n",
      "Pure loss: 25.256125349777566.....Total loss: 25.256125349777566\n",
      "Pure loss: 20.90678221513928.....Total loss: 20.90678221513928\n",
      "epoch 1996 learning rate:  0.010501002004008017   Training loss:   25.256125349777566  Valing loss:   20.90678221513928\n",
      "Pure loss: 24.98764088331139.....Total loss: 24.98764088331139\n",
      "Pure loss: 20.88125931004406.....Total loss: 20.88125931004406\n",
      "epoch 1997 learning rate:  0.010500751126690036   Training loss:   24.98764088331139  Valing loss:   20.88125931004406\n",
      "Pure loss: 24.20221408294817.....Total loss: 24.20221408294817\n",
      "Pure loss: 20.760280410610516.....Total loss: 20.760280410610516\n",
      "epoch 1998 learning rate:  0.010500500500500502   Training loss:   24.20221408294817  Valing loss:   20.760280410610516\n",
      "Pure loss: 23.835814479176122.....Total loss: 23.835814479176122\n",
      "Pure loss: 20.59436031627028.....Total loss: 20.59436031627028\n",
      "epoch 1999 learning rate:  0.01050025012506253   Training loss:   23.835814479176122  Valing loss:   20.59436031627028\n",
      "Pure loss: 24.141402967075663.....Total loss: 24.141402967075663\n",
      "Pure loss: 20.63030078533446.....Total loss: 20.63030078533446\n",
      "epoch 2000 learning rate:  0.0105   Training loss:   24.141402967075663  Valing loss:   20.63030078533446\n",
      "Pure loss: 23.952964295265218.....Total loss: 23.952964295265218\n",
      "Pure loss: 20.48882566347108.....Total loss: 20.48882566347108\n",
      "epoch 2001 learning rate:  0.010499750124937531   Training loss:   23.952964295265218  Valing loss:   20.48882566347108\n",
      "Pure loss: 24.049857883677202.....Total loss: 24.049857883677202\n",
      "Pure loss: 20.561713365338147.....Total loss: 20.561713365338147\n",
      "epoch 2002 learning rate:  0.0104995004995005   Training loss:   24.049857883677202  Valing loss:   20.561713365338147\n",
      "Pure loss: 24.036211362323574.....Total loss: 24.036211362323574\n",
      "Pure loss: 20.55295010701734.....Total loss: 20.55295010701734\n",
      "epoch 2003 learning rate:  0.010499251123315028   Training loss:   24.036211362323574  Valing loss:   20.55295010701734\n",
      "Pure loss: 23.751582236993276.....Total loss: 23.751582236993276\n",
      "Pure loss: 20.403442967224343.....Total loss: 20.403442967224343\n",
      "epoch 2004 learning rate:  0.010499001996007984   Training loss:   23.751582236993276  Valing loss:   20.403442967224343\n",
      "Pure loss: 23.69440040228954.....Total loss: 23.69440040228954\n",
      "Pure loss: 20.381107265718985.....Total loss: 20.381107265718985\n",
      "epoch 2005 learning rate:  0.010498753117206983   Training loss:   23.69440040228954  Valing loss:   20.381107265718985\n",
      "Pure loss: 23.85185029851475.....Total loss: 23.85185029851475\n",
      "Pure loss: 20.40033591023735.....Total loss: 20.40033591023735\n",
      "epoch 2006 learning rate:  0.01049850448654038   Training loss:   23.85185029851475  Valing loss:   20.40033591023735\n",
      "Pure loss: 23.774472497601536.....Total loss: 23.774472497601536\n",
      "Pure loss: 20.377461654143744.....Total loss: 20.377461654143744\n",
      "epoch 2007 learning rate:  0.01049825610363727   Training loss:   23.774472497601536  Valing loss:   20.377461654143744\n",
      "Pure loss: 23.71871546707907.....Total loss: 23.71871546707907\n",
      "Pure loss: 20.360076309003002.....Total loss: 20.360076309003002\n",
      "epoch 2008 learning rate:  0.01049800796812749   Training loss:   23.71871546707907  Valing loss:   20.360076309003002\n",
      "Pure loss: 23.660884893391025.....Total loss: 23.660884893391025\n",
      "Pure loss: 20.3604226932471.....Total loss: 20.3604226932471\n",
      "epoch 2009 learning rate:  0.010497760079641613   Training loss:   23.660884893391025  Valing loss:   20.3604226932471\n",
      "Pure loss: 23.325321457747382.....Total loss: 23.325321457747382\n",
      "Pure loss: 20.415455379939356.....Total loss: 20.415455379939356\n",
      "epoch 2010 learning rate:  0.010497512437810946   Training loss:   23.325321457747382  Valing loss:   20.415455379939356\n",
      "Pure loss: 23.36511067597607.....Total loss: 23.36511067597607\n",
      "Pure loss: 20.398363079404483.....Total loss: 20.398363079404483\n",
      "epoch 2011 learning rate:  0.010497265042267528   Training loss:   23.36511067597607  Valing loss:   20.398363079404483\n",
      "Pure loss: 23.22523236507521.....Total loss: 23.22523236507521\n",
      "Pure loss: 20.60960543567869.....Total loss: 20.60960543567869\n",
      "epoch 2012 learning rate:  0.010497017892644135   Training loss:   23.22523236507521  Valing loss:   20.60960543567869\n",
      "Pure loss: 23.308185437202628.....Total loss: 23.308185437202628\n",
      "Pure loss: 20.68919382454295.....Total loss: 20.68919382454295\n",
      "epoch 2013 learning rate:  0.010496770988574267   Training loss:   23.308185437202628  Valing loss:   20.68919382454295\n",
      "Pure loss: 23.751104776153372.....Total loss: 23.751104776153372\n",
      "Pure loss: 21.674959067325105.....Total loss: 21.674959067325105\n",
      "epoch 2014 learning rate:  0.010496524329692156   Training loss:   23.751104776153372  Valing loss:   21.674959067325105\n",
      "Pure loss: 23.489071961680562.....Total loss: 23.489071961680562\n",
      "Pure loss: 21.17383859109735.....Total loss: 21.17383859109735\n",
      "epoch 2015 learning rate:  0.010496277915632755   Training loss:   23.489071961680562  Valing loss:   21.17383859109735\n",
      "Pure loss: 23.53796757693574.....Total loss: 23.53796757693574\n",
      "Pure loss: 21.255392980422815.....Total loss: 21.255392980422815\n",
      "epoch 2016 learning rate:  0.010496031746031746   Training loss:   23.53796757693574  Valing loss:   21.255392980422815\n",
      "Pure loss: 23.66188968831286.....Total loss: 23.66188968831286\n",
      "Pure loss: 21.602190857031793.....Total loss: 21.602190857031793\n",
      "epoch 2017 learning rate:  0.010495785820525533   Training loss:   23.66188968831286  Valing loss:   21.602190857031793\n",
      "Pure loss: 23.620459984149903.....Total loss: 23.620459984149903\n",
      "Pure loss: 21.52475041668151.....Total loss: 21.52475041668151\n",
      "epoch 2018 learning rate:  0.010495540138751239   Training loss:   23.620459984149903  Valing loss:   21.52475041668151\n",
      "Pure loss: 23.563867046109277.....Total loss: 23.563867046109277\n",
      "Pure loss: 21.326124619415253.....Total loss: 21.326124619415253\n",
      "epoch 2019 learning rate:  0.010495294700346706   Training loss:   23.563867046109277  Valing loss:   21.326124619415253\n",
      "Pure loss: 23.40557557958218.....Total loss: 23.40557557958218\n",
      "Pure loss: 20.385747223328384.....Total loss: 20.385747223328384\n",
      "epoch 2020 learning rate:  0.010495049504950496   Training loss:   23.40557557958218  Valing loss:   20.385747223328384\n",
      "Pure loss: 23.325581205170472.....Total loss: 23.325581205170472\n",
      "Pure loss: 20.699277139858967.....Total loss: 20.699277139858967\n",
      "epoch 2021 learning rate:  0.010494804552201881   Training loss:   23.325581205170472  Valing loss:   20.699277139858967\n",
      "Pure loss: 23.576469572713574.....Total loss: 23.576469572713574\n",
      "Pure loss: 21.603964497759982.....Total loss: 21.603964497759982\n",
      "epoch 2022 learning rate:  0.010494559841740851   Training loss:   23.576469572713574  Valing loss:   21.603964497759982\n",
      "Pure loss: 23.493809091429227.....Total loss: 23.493809091429227\n",
      "Pure loss: 21.342056905110677.....Total loss: 21.342056905110677\n",
      "epoch 2023 learning rate:  0.010494315373208107   Training loss:   23.493809091429227  Valing loss:   21.342056905110677\n",
      "Pure loss: 23.91102218891594.....Total loss: 23.91102218891594\n",
      "Pure loss: 21.967088137056273.....Total loss: 21.967088137056273\n",
      "epoch 2024 learning rate:  0.01049407114624506   Training loss:   23.91102218891594  Valing loss:   21.967088137056273\n",
      "Pure loss: 24.182165251883745.....Total loss: 24.182165251883745\n",
      "Pure loss: 22.50319639634597.....Total loss: 22.50319639634597\n",
      "epoch 2025 learning rate:  0.010493827160493827   Training loss:   24.182165251883745  Valing loss:   22.50319639634597\n",
      "Pure loss: 24.14878938631256.....Total loss: 24.14878938631256\n",
      "Pure loss: 22.455212807557007.....Total loss: 22.455212807557007\n",
      "epoch 2026 learning rate:  0.010493583415597236   Training loss:   24.14878938631256  Valing loss:   22.455212807557007\n",
      "Pure loss: 23.921838379153105.....Total loss: 23.921838379153105\n",
      "Pure loss: 21.932304297469617.....Total loss: 21.932304297469617\n",
      "epoch 2027 learning rate:  0.010493339911198817   Training loss:   23.921838379153105  Valing loss:   21.932304297469617\n",
      "Pure loss: 24.839524078804356.....Total loss: 24.839524078804356\n",
      "Pure loss: 23.286075074333784.....Total loss: 23.286075074333784\n",
      "epoch 2028 learning rate:  0.010493096646942802   Training loss:   24.839524078804356  Valing loss:   23.286075074333784\n",
      "Pure loss: 25.99655971548566.....Total loss: 25.99655971548566\n",
      "Pure loss: 24.848614022932136.....Total loss: 24.848614022932136\n",
      "epoch 2029 learning rate:  0.010492853622474126   Training loss:   25.99655971548566  Valing loss:   24.848614022932136\n",
      "Pure loss: 24.758618195806335.....Total loss: 24.758618195806335\n",
      "Pure loss: 23.119990016525595.....Total loss: 23.119990016525595\n",
      "epoch 2030 learning rate:  0.010492610837438424   Training loss:   24.758618195806335  Valing loss:   23.119990016525595\n",
      "Pure loss: 24.492848866251215.....Total loss: 24.492848866251215\n",
      "Pure loss: 22.740029304360142.....Total loss: 22.740029304360142\n",
      "epoch 2031 learning rate:  0.010492368291482028   Training loss:   24.492848866251215  Valing loss:   22.740029304360142\n",
      "Pure loss: 24.295099471763955.....Total loss: 24.295099471763955\n",
      "Pure loss: 22.343757789710363.....Total loss: 22.343757789710363\n",
      "epoch 2032 learning rate:  0.01049212598425197   Training loss:   24.295099471763955  Valing loss:   22.343757789710363\n",
      "Pure loss: 23.861612976283812.....Total loss: 23.861612976283812\n",
      "Pure loss: 21.009819436026564.....Total loss: 21.009819436026564\n",
      "epoch 2033 learning rate:  0.010491883915395966   Training loss:   23.861612976283812  Valing loss:   21.009819436026564\n",
      "Pure loss: 24.004521166898744.....Total loss: 24.004521166898744\n",
      "Pure loss: 22.652790090945157.....Total loss: 22.652790090945157\n",
      "epoch 2034 learning rate:  0.01049164208456244   Training loss:   24.004521166898744  Valing loss:   22.652790090945157\n",
      "Pure loss: 24.310610036627605.....Total loss: 24.310610036627605\n",
      "Pure loss: 23.173311701937376.....Total loss: 23.173311701937376\n",
      "epoch 2035 learning rate:  0.010491400491400491   Training loss:   24.310610036627605  Valing loss:   23.173311701937376\n",
      "Pure loss: 23.63322212978339.....Total loss: 23.63322212978339\n",
      "Pure loss: 21.86886508968476.....Total loss: 21.86886508968476\n",
      "epoch 2036 learning rate:  0.01049115913555992   Training loss:   23.63322212978339  Valing loss:   21.86886508968476\n",
      "Pure loss: 23.1863367499054.....Total loss: 23.1863367499054\n",
      "Pure loss: 21.201448970544355.....Total loss: 21.201448970544355\n",
      "epoch 2037 learning rate:  0.010490918016691213   Training loss:   23.1863367499054  Valing loss:   21.201448970544355\n",
      "Pure loss: 23.052934877166976.....Total loss: 23.052934877166976\n",
      "Pure loss: 20.905926141072307.....Total loss: 20.905926141072307\n",
      "epoch 2038 learning rate:  0.010490677134445535   Training loss:   23.052934877166976  Valing loss:   20.905926141072307\n",
      "Pure loss: 22.914824443987907.....Total loss: 22.914824443987907\n",
      "Pure loss: 20.34899799934441.....Total loss: 20.34899799934441\n",
      "epoch 2039 learning rate:  0.010490436488474742   Training loss:   22.914824443987907  Valing loss:   20.34899799934441\n",
      "Pure loss: 22.857792530582202.....Total loss: 22.857792530582202\n",
      "Pure loss: 20.174975332104005.....Total loss: 20.174975332104005\n",
      "epoch 2040 learning rate:  0.010490196078431373   Training loss:   22.857792530582202  Valing loss:   20.174975332104005\n",
      "Pure loss: 22.87201098573459.....Total loss: 22.87201098573459\n",
      "Pure loss: 20.212032231419226.....Total loss: 20.212032231419226\n",
      "epoch 2041 learning rate:  0.010489955903968643   Training loss:   22.87201098573459  Valing loss:   20.212032231419226\n",
      "Pure loss: 22.988522606070077.....Total loss: 22.988522606070077\n",
      "Pure loss: 20.600221162544074.....Total loss: 20.600221162544074\n",
      "epoch 2042 learning rate:  0.010489715964740451   Training loss:   22.988522606070077  Valing loss:   20.600221162544074\n",
      "Pure loss: 23.543408029627567.....Total loss: 23.543408029627567\n",
      "Pure loss: 21.994531734964088.....Total loss: 21.994531734964088\n",
      "epoch 2043 learning rate:  0.010489476260401372   Training loss:   23.543408029627567  Valing loss:   21.994531734964088\n",
      "Pure loss: 23.161569166443215.....Total loss: 23.161569166443215\n",
      "Pure loss: 21.25725566276815.....Total loss: 21.25725566276815\n",
      "epoch 2044 learning rate:  0.010489236790606654   Training loss:   23.161569166443215  Valing loss:   21.25725566276815\n",
      "Pure loss: 23.50269106806523.....Total loss: 23.50269106806523\n",
      "Pure loss: 21.962181678085475.....Total loss: 21.962181678085475\n",
      "epoch 2045 learning rate:  0.010488997555012225   Training loss:   23.50269106806523  Valing loss:   21.962181678085475\n",
      "Pure loss: 22.9105467610175.....Total loss: 22.9105467610175\n",
      "Pure loss: 20.650844076202453.....Total loss: 20.650844076202453\n",
      "epoch 2046 learning rate:  0.010488758553274682   Training loss:   22.9105467610175  Valing loss:   20.650844076202453\n",
      "Pure loss: 23.1493866712601.....Total loss: 23.1493866712601\n",
      "Pure loss: 21.333683882784285.....Total loss: 21.333683882784285\n",
      "epoch 2047 learning rate:  0.010488519785051295   Training loss:   23.1493866712601  Valing loss:   21.333683882784285\n",
      "Pure loss: 23.125965657324265.....Total loss: 23.125965657324265\n",
      "Pure loss: 21.27715906413748.....Total loss: 21.27715906413748\n",
      "epoch 2048 learning rate:  0.01048828125   Training loss:   23.125965657324265  Valing loss:   21.27715906413748\n",
      "Pure loss: 23.081416266779012.....Total loss: 23.081416266779012\n",
      "Pure loss: 21.2003958327017.....Total loss: 21.2003958327017\n",
      "epoch 2049 learning rate:  0.010488042947779405   Training loss:   23.081416266779012  Valing loss:   21.2003958327017\n",
      "Pure loss: 22.841467273271128.....Total loss: 22.841467273271128\n",
      "Pure loss: 20.554135671471656.....Total loss: 20.554135671471656\n",
      "epoch 2050 learning rate:  0.010487804878048781   Training loss:   22.841467273271128  Valing loss:   20.554135671471656\n",
      "Pure loss: 22.75747164457309.....Total loss: 22.75747164457309\n",
      "Pure loss: 20.191937314576002.....Total loss: 20.191937314576002\n",
      "epoch 2051 learning rate:  0.010487567040468064   Training loss:   22.75747164457309  Valing loss:   20.191937314576002\n",
      "Pure loss: 22.801062537809308.....Total loss: 22.801062537809308\n",
      "Pure loss: 20.379829928256807.....Total loss: 20.379829928256807\n",
      "epoch 2052 learning rate:  0.010487329434697856   Training loss:   22.801062537809308  Valing loss:   20.379829928256807\n",
      "Pure loss: 22.78340946968422.....Total loss: 22.78340946968422\n",
      "Pure loss: 20.16073040683356.....Total loss: 20.16073040683356\n",
      "epoch 2053 learning rate:  0.010487092060399416   Training loss:   22.78340946968422  Valing loss:   20.16073040683356\n",
      "Pure loss: 22.892654476386525.....Total loss: 22.892654476386525\n",
      "Pure loss: 20.743542071029722.....Total loss: 20.743542071029722\n",
      "epoch 2054 learning rate:  0.010486854917234664   Training loss:   22.892654476386525  Valing loss:   20.743542071029722\n",
      "Pure loss: 22.92005127457914.....Total loss: 22.92005127457914\n",
      "Pure loss: 20.812617485804086.....Total loss: 20.812617485804086\n",
      "epoch 2055 learning rate:  0.01048661800486618   Training loss:   22.92005127457914  Valing loss:   20.812617485804086\n",
      "Pure loss: 22.778517068133095.....Total loss: 22.778517068133095\n",
      "Pure loss: 20.593105794941156.....Total loss: 20.593105794941156\n",
      "epoch 2056 learning rate:  0.010486381322957199   Training loss:   22.778517068133095  Valing loss:   20.593105794941156\n",
      "Pure loss: 22.942203407183126.....Total loss: 22.942203407183126\n",
      "Pure loss: 21.03304494729916.....Total loss: 21.03304494729916\n",
      "epoch 2057 learning rate:  0.01048614487117161   Training loss:   22.942203407183126  Valing loss:   21.03304494729916\n",
      "Pure loss: 22.903735754901543.....Total loss: 22.903735754901543\n",
      "Pure loss: 20.923806759220703.....Total loss: 20.923806759220703\n",
      "epoch 2058 learning rate:  0.010485908649173956   Training loss:   22.903735754901543  Valing loss:   20.923806759220703\n",
      "Pure loss: 23.000055851161537.....Total loss: 23.000055851161537\n",
      "Pure loss: 21.17559016780783.....Total loss: 21.17559016780783\n",
      "epoch 2059 learning rate:  0.010485672656629433   Training loss:   23.000055851161537  Valing loss:   21.17559016780783\n",
      "Pure loss: 23.00229584438198.....Total loss: 23.00229584438198\n",
      "Pure loss: 21.178859467157853.....Total loss: 21.178859467157853\n",
      "epoch 2060 learning rate:  0.010485436893203883   Training loss:   23.00229584438198  Valing loss:   21.178859467157853\n",
      "Pure loss: 23.169077221529257.....Total loss: 23.169077221529257\n",
      "Pure loss: 21.5989593646898.....Total loss: 21.5989593646898\n",
      "epoch 2061 learning rate:  0.010485201358563805   Training loss:   23.169077221529257  Valing loss:   21.5989593646898\n",
      "Pure loss: 22.89181551989932.....Total loss: 22.89181551989932\n",
      "Pure loss: 21.025016516662756.....Total loss: 21.025016516662756\n",
      "epoch 2062 learning rate:  0.010484966052376334   Training loss:   22.89181551989932  Valing loss:   21.025016516662756\n",
      "Pure loss: 22.767978427834482.....Total loss: 22.767978427834482\n",
      "Pure loss: 20.688572714555814.....Total loss: 20.688572714555814\n",
      "epoch 2063 learning rate:  0.01048473097430926   Training loss:   22.767978427834482  Valing loss:   20.688572714555814\n",
      "Pure loss: 23.15817625268876.....Total loss: 23.15817625268876\n",
      "Pure loss: 21.753466329709536.....Total loss: 21.753466329709536\n",
      "epoch 2064 learning rate:  0.010484496124031007   Training loss:   23.15817625268876  Valing loss:   21.753466329709536\n",
      "Pure loss: 22.89700354294167.....Total loss: 22.89700354294167\n",
      "Pure loss: 21.194262413335974.....Total loss: 21.194262413335974\n",
      "epoch 2065 learning rate:  0.010484261501210655   Training loss:   22.89700354294167  Valing loss:   21.194262413335974\n",
      "Pure loss: 23.129990740906116.....Total loss: 23.129990740906116\n",
      "Pure loss: 21.670758897687833.....Total loss: 21.670758897687833\n",
      "epoch 2066 learning rate:  0.010484027105517909   Training loss:   23.129990740906116  Valing loss:   21.670758897687833\n",
      "Pure loss: 22.91323993065376.....Total loss: 22.91323993065376\n",
      "Pure loss: 21.177882953505325.....Total loss: 21.177882953505325\n",
      "epoch 2067 learning rate:  0.010483792936623125   Training loss:   22.91323993065376  Valing loss:   21.177882953505325\n",
      "Pure loss: 22.818024863524748.....Total loss: 22.818024863524748\n",
      "Pure loss: 20.92942104522017.....Total loss: 20.92942104522017\n",
      "epoch 2068 learning rate:  0.010483558994197293   Training loss:   22.818024863524748  Valing loss:   20.92942104522017\n",
      "Pure loss: 22.704003903680444.....Total loss: 22.704003903680444\n",
      "Pure loss: 20.389409468807454.....Total loss: 20.389409468807454\n",
      "epoch 2069 learning rate:  0.010483325277912035   Training loss:   22.704003903680444  Valing loss:   20.389409468807454\n",
      "Pure loss: 22.701243759381303.....Total loss: 22.701243759381303\n",
      "Pure loss: 20.405439963576466.....Total loss: 20.405439963576466\n",
      "epoch 2070 learning rate:  0.010483091787439613   Training loss:   22.701243759381303  Valing loss:   20.405439963576466\n",
      "Pure loss: 22.754986292604073.....Total loss: 22.754986292604073\n",
      "Pure loss: 20.40838600993766.....Total loss: 20.40838600993766\n",
      "epoch 2071 learning rate:  0.010482858522452922   Training loss:   22.754986292604073  Valing loss:   20.40838600993766\n",
      "Pure loss: 22.827139537366325.....Total loss: 22.827139537366325\n",
      "Pure loss: 20.33514973362154.....Total loss: 20.33514973362154\n",
      "epoch 2072 learning rate:  0.010482625482625484   Training loss:   22.827139537366325  Valing loss:   20.33514973362154\n",
      "Pure loss: 22.97238206811914.....Total loss: 22.97238206811914\n",
      "Pure loss: 20.12823979888669.....Total loss: 20.12823979888669\n",
      "epoch 2073 learning rate:  0.010482392667631452   Training loss:   22.97238206811914  Valing loss:   20.12823979888669\n",
      "Pure loss: 23.249471744348536.....Total loss: 23.249471744348536\n",
      "Pure loss: 20.32344305654926.....Total loss: 20.32344305654926\n",
      "epoch 2074 learning rate:  0.010482160077145612   Training loss:   23.249471744348536  Valing loss:   20.32344305654926\n",
      "Pure loss: 23.766012637763932.....Total loss: 23.766012637763932\n",
      "Pure loss: 20.66445583150024.....Total loss: 20.66445583150024\n",
      "epoch 2075 learning rate:  0.010481927710843374   Training loss:   23.766012637763932  Valing loss:   20.66445583150024\n",
      "Pure loss: 23.95729452535817.....Total loss: 23.95729452535817\n",
      "Pure loss: 20.64804615197754.....Total loss: 20.64804615197754\n",
      "epoch 2076 learning rate:  0.01048169556840077   Training loss:   23.95729452535817  Valing loss:   20.64804615197754\n",
      "Pure loss: 23.328526947148276.....Total loss: 23.328526947148276\n",
      "Pure loss: 20.78191471611416.....Total loss: 20.78191471611416\n",
      "epoch 2077 learning rate:  0.010481463649494463   Training loss:   23.328526947148276  Valing loss:   20.78191471611416\n",
      "Pure loss: 23.1008943939472.....Total loss: 23.1008943939472\n",
      "Pure loss: 20.64107050707688.....Total loss: 20.64107050707688\n",
      "epoch 2078 learning rate:  0.010481231953801733   Training loss:   23.1008943939472  Valing loss:   20.64107050707688\n",
      "Pure loss: 23.05975683576555.....Total loss: 23.05975683576555\n",
      "Pure loss: 20.813862706133254.....Total loss: 20.813862706133254\n",
      "epoch 2079 learning rate:  0.010481000481000481   Training loss:   23.05975683576555  Valing loss:   20.813862706133254\n",
      "Pure loss: 23.070198937231602.....Total loss: 23.070198937231602\n",
      "Pure loss: 20.769211100406352.....Total loss: 20.769211100406352\n",
      "epoch 2080 learning rate:  0.010480769230769231   Training loss:   23.070198937231602  Valing loss:   20.769211100406352\n",
      "Pure loss: 23.127566357821244.....Total loss: 23.127566357821244\n",
      "Pure loss: 20.77483502510015.....Total loss: 20.77483502510015\n",
      "epoch 2081 learning rate:  0.010480538202787121   Training loss:   23.127566357821244  Valing loss:   20.77483502510015\n",
      "Pure loss: 23.137951580705195.....Total loss: 23.137951580705195\n",
      "Pure loss: 20.647438442641782.....Total loss: 20.647438442641782\n",
      "epoch 2082 learning rate:  0.010480307396733909   Training loss:   23.137951580705195  Valing loss:   20.647438442641782\n",
      "Pure loss: 23.0448101968053.....Total loss: 23.0448101968053\n",
      "Pure loss: 20.717373893208915.....Total loss: 20.717373893208915\n",
      "epoch 2083 learning rate:  0.010480076812289966   Training loss:   23.0448101968053  Valing loss:   20.717373893208915\n",
      "Pure loss: 23.43675218106789.....Total loss: 23.43675218106789\n",
      "Pure loss: 20.927268605020096.....Total loss: 20.927268605020096\n",
      "epoch 2084 learning rate:  0.010479846449136276   Training loss:   23.43675218106789  Valing loss:   20.927268605020096\n",
      "Pure loss: 23.71823528789983.....Total loss: 23.71823528789983\n",
      "Pure loss: 21.124196931483546.....Total loss: 21.124196931483546\n",
      "epoch 2085 learning rate:  0.010479616306954436   Training loss:   23.71823528789983  Valing loss:   21.124196931483546\n",
      "Pure loss: 23.605957338469675.....Total loss: 23.605957338469675\n",
      "Pure loss: 21.44411445661641.....Total loss: 21.44411445661641\n",
      "epoch 2086 learning rate:  0.010479386385426654   Training loss:   23.605957338469675  Valing loss:   21.44411445661641\n",
      "Pure loss: 23.576968190136594.....Total loss: 23.576968190136594\n",
      "Pure loss: 21.20852351067271.....Total loss: 21.20852351067271\n",
      "epoch 2087 learning rate:  0.010479156684235746   Training loss:   23.576968190136594  Valing loss:   21.20852351067271\n",
      "Pure loss: 23.59586810367475.....Total loss: 23.59586810367475\n",
      "Pure loss: 21.18488667932671.....Total loss: 21.18488667932671\n",
      "epoch 2088 learning rate:  0.010478927203065135   Training loss:   23.59586810367475  Valing loss:   21.18488667932671\n",
      "Pure loss: 23.96120379546413.....Total loss: 23.96120379546413\n",
      "Pure loss: 21.097729948705695.....Total loss: 21.097729948705695\n",
      "epoch 2089 learning rate:  0.010478697941598851   Training loss:   23.96120379546413  Valing loss:   21.097729948705695\n",
      "Pure loss: 23.855578717503832.....Total loss: 23.855578717503832\n",
      "Pure loss: 21.082257197091113.....Total loss: 21.082257197091113\n",
      "epoch 2090 learning rate:  0.010478468899521531   Training loss:   23.855578717503832  Valing loss:   21.082257197091113\n",
      "Pure loss: 23.329812513106692.....Total loss: 23.329812513106692\n",
      "Pure loss: 20.87004741401289.....Total loss: 20.87004741401289\n",
      "epoch 2091 learning rate:  0.010478240076518413   Training loss:   23.329812513106692  Valing loss:   20.87004741401289\n",
      "Pure loss: 23.332266631860545.....Total loss: 23.332266631860545\n",
      "Pure loss: 20.865159127311472.....Total loss: 20.865159127311472\n",
      "epoch 2092 learning rate:  0.010478011472275334   Training loss:   23.332266631860545  Valing loss:   20.865159127311472\n",
      "Pure loss: 23.331257777352.....Total loss: 23.331257777352\n",
      "Pure loss: 20.866140498719727.....Total loss: 20.866140498719727\n",
      "epoch 2093 learning rate:  0.010477783086478738   Training loss:   23.331257777352  Valing loss:   20.866140498719727\n",
      "Pure loss: 23.176422136416658.....Total loss: 23.176422136416658\n",
      "Pure loss: 20.735605791962872.....Total loss: 20.735605791962872\n",
      "epoch 2094 learning rate:  0.010477554918815664   Training loss:   23.176422136416658  Valing loss:   20.735605791962872\n",
      "Pure loss: 22.734388486455614.....Total loss: 22.734388486455614\n",
      "Pure loss: 20.81234885087141.....Total loss: 20.81234885087141\n",
      "epoch 2095 learning rate:  0.010477326968973747   Training loss:   22.734388486455614  Valing loss:   20.81234885087141\n",
      "Pure loss: 22.7668337789804.....Total loss: 22.7668337789804\n",
      "Pure loss: 20.93619015954193.....Total loss: 20.93619015954193\n",
      "epoch 2096 learning rate:  0.010477099236641222   Training loss:   22.7668337789804  Valing loss:   20.93619015954193\n",
      "Pure loss: 22.729524647182398.....Total loss: 22.729524647182398\n",
      "Pure loss: 21.034694116632263.....Total loss: 21.034694116632263\n",
      "epoch 2097 learning rate:  0.010476871721506915   Training loss:   22.729524647182398  Valing loss:   21.034694116632263\n",
      "Pure loss: 22.68505123524347.....Total loss: 22.68505123524347\n",
      "Pure loss: 20.91763214214147.....Total loss: 20.91763214214147\n",
      "epoch 2098 learning rate:  0.010476644423260248   Training loss:   22.68505123524347  Valing loss:   20.91763214214147\n",
      "Pure loss: 22.546716305468554.....Total loss: 22.546716305468554\n",
      "Pure loss: 20.560841521687372.....Total loss: 20.560841521687372\n",
      "epoch 2099 learning rate:  0.010476417341591234   Training loss:   22.546716305468554  Valing loss:   20.560841521687372\n",
      "Pure loss: 22.550187929201964.....Total loss: 22.550187929201964\n",
      "Pure loss: 20.515784985036785.....Total loss: 20.515784985036785\n",
      "epoch 2100 learning rate:  0.010476190476190476   Training loss:   22.550187929201964  Valing loss:   20.515784985036785\n",
      "Pure loss: 22.58484923099189.....Total loss: 22.58484923099189\n",
      "Pure loss: 20.44147171088469.....Total loss: 20.44147171088469\n",
      "epoch 2101 learning rate:  0.010475963826749167   Training loss:   22.58484923099189  Valing loss:   20.44147171088469\n",
      "Pure loss: 22.590992722354002.....Total loss: 22.590992722354002\n",
      "Pure loss: 20.514777570663135.....Total loss: 20.514777570663135\n",
      "epoch 2102 learning rate:  0.010475737392959086   Training loss:   22.590992722354002  Valing loss:   20.514777570663135\n",
      "Pure loss: 22.557602630352836.....Total loss: 22.557602630352836\n",
      "Pure loss: 20.398147191244348.....Total loss: 20.398147191244348\n",
      "epoch 2103 learning rate:  0.010475511174512602   Training loss:   22.557602630352836  Valing loss:   20.398147191244348\n",
      "Pure loss: 22.479754236854372.....Total loss: 22.479754236854372\n",
      "Pure loss: 20.041613921305107.....Total loss: 20.041613921305107\n",
      "epoch 2104 learning rate:  0.010475285171102662   Training loss:   22.479754236854372  Valing loss:   20.041613921305107\n",
      "Pure loss: 22.478218606226047.....Total loss: 22.478218606226047\n",
      "Pure loss: 20.24372895964391.....Total loss: 20.24372895964391\n",
      "epoch 2105 learning rate:  0.010475059382422804   Training loss:   22.478218606226047  Valing loss:   20.24372895964391\n",
      "Pure loss: 22.476248029077848.....Total loss: 22.476248029077848\n",
      "Pure loss: 20.23041157564414.....Total loss: 20.23041157564414\n",
      "epoch 2106 learning rate:  0.010474833808167142   Training loss:   22.476248029077848  Valing loss:   20.23041157564414\n",
      "Pure loss: 22.483393566099696.....Total loss: 22.483393566099696\n",
      "Pure loss: 20.06516119681728.....Total loss: 20.06516119681728\n",
      "epoch 2107 learning rate:  0.010474608448030375   Training loss:   22.483393566099696  Valing loss:   20.06516119681728\n",
      "Pure loss: 22.48593070108853.....Total loss: 22.48593070108853\n",
      "Pure loss: 20.26728560388895.....Total loss: 20.26728560388895\n",
      "epoch 2108 learning rate:  0.01047438330170778   Training loss:   22.48593070108853  Valing loss:   20.26728560388895\n",
      "Pure loss: 22.48052220367156.....Total loss: 22.48052220367156\n",
      "Pure loss: 20.2765978260759.....Total loss: 20.2765978260759\n",
      "epoch 2109 learning rate:  0.010474158368895212   Training loss:   22.48052220367156  Valing loss:   20.2765978260759\n",
      "Pure loss: 22.46602827034258.....Total loss: 22.46602827034258\n",
      "Pure loss: 20.417211034678406.....Total loss: 20.417211034678406\n",
      "epoch 2110 learning rate:  0.0104739336492891   Training loss:   22.46602827034258  Valing loss:   20.417211034678406\n",
      "Pure loss: 22.392460390156124.....Total loss: 22.392460390156124\n",
      "Pure loss: 20.156375012168084.....Total loss: 20.156375012168084\n",
      "epoch 2111 learning rate:  0.010473709142586453   Training loss:   22.392460390156124  Valing loss:   20.156375012168084\n",
      "Pure loss: 22.689415818889568.....Total loss: 22.689415818889568\n",
      "Pure loss: 21.146385804922712.....Total loss: 21.146385804922712\n",
      "epoch 2112 learning rate:  0.010473484848484848   Training loss:   22.689415818889568  Valing loss:   21.146385804922712\n",
      "Pure loss: 22.492343386261037.....Total loss: 22.492343386261037\n",
      "Pure loss: 20.699480162271307.....Total loss: 20.699480162271307\n",
      "epoch 2113 learning rate:  0.010473260766682441   Training loss:   22.492343386261037  Valing loss:   20.699480162271307\n",
      "Pure loss: 22.49504067208127.....Total loss: 22.49504067208127\n",
      "Pure loss: 20.707919667828737.....Total loss: 20.707919667828737\n",
      "epoch 2114 learning rate:  0.010473036896877957   Training loss:   22.49504067208127  Valing loss:   20.707919667828737\n",
      "Pure loss: 22.456652373842918.....Total loss: 22.456652373842918\n",
      "Pure loss: 20.523158905927687.....Total loss: 20.523158905927687\n",
      "epoch 2115 learning rate:  0.010472813238770686   Training loss:   22.456652373842918  Valing loss:   20.523158905927687\n",
      "Pure loss: 22.4105292926941.....Total loss: 22.4105292926941\n",
      "Pure loss: 20.339505298393643.....Total loss: 20.339505298393643\n",
      "epoch 2116 learning rate:  0.010472589792060491   Training loss:   22.4105292926941  Valing loss:   20.339505298393643\n",
      "Pure loss: 22.524136791916387.....Total loss: 22.524136791916387\n",
      "Pure loss: 20.386906676162848.....Total loss: 20.386906676162848\n",
      "epoch 2117 learning rate:  0.010472366556447803   Training loss:   22.524136791916387  Valing loss:   20.386906676162848\n",
      "Pure loss: 22.4271149852978.....Total loss: 22.4271149852978\n",
      "Pure loss: 20.123582962608943.....Total loss: 20.123582962608943\n",
      "epoch 2118 learning rate:  0.010472143531633617   Training loss:   22.4271149852978  Valing loss:   20.123582962608943\n",
      "Pure loss: 22.408768726868892.....Total loss: 22.408768726868892\n",
      "Pure loss: 20.299261493488043.....Total loss: 20.299261493488043\n",
      "epoch 2119 learning rate:  0.010471920717319491   Training loss:   22.408768726868892  Valing loss:   20.299261493488043\n",
      "Pure loss: 22.409989265185676.....Total loss: 22.409989265185676\n",
      "Pure loss: 20.309369635640625.....Total loss: 20.309369635640625\n",
      "epoch 2120 learning rate:  0.010471698113207547   Training loss:   22.409989265185676  Valing loss:   20.309369635640625\n",
      "Pure loss: 22.364762585046762.....Total loss: 22.364762585046762\n",
      "Pure loss: 20.382923749192628.....Total loss: 20.382923749192628\n",
      "epoch 2121 learning rate:  0.010471475719000471   Training loss:   22.364762585046762  Valing loss:   20.382923749192628\n",
      "Pure loss: 22.356918162097013.....Total loss: 22.356918162097013\n",
      "Pure loss: 20.391548850798753.....Total loss: 20.391548850798753\n",
      "epoch 2122 learning rate:  0.010471253534401509   Training loss:   22.356918162097013  Valing loss:   20.391548850798753\n",
      "Pure loss: 22.332118381594814.....Total loss: 22.332118381594814\n",
      "Pure loss: 20.25047601707276.....Total loss: 20.25047601707276\n",
      "epoch 2123 learning rate:  0.010471031559114462   Training loss:   22.332118381594814  Valing loss:   20.25047601707276\n",
      "Pure loss: 22.276925151375377.....Total loss: 22.276925151375377\n",
      "Pure loss: 19.98848531849444.....Total loss: 19.98848531849444\n",
      "epoch 2124 learning rate:  0.010470809792843691   Training loss:   22.276925151375377  Valing loss:   19.98848531849444\n",
      "Pure loss: 22.964226370376764.....Total loss: 22.964226370376764\n",
      "Pure loss: 21.866780708831563.....Total loss: 21.866780708831563\n",
      "epoch 2125 learning rate:  0.010470588235294119   Training loss:   22.964226370376764  Valing loss:   21.866780708831563\n",
      "Pure loss: 22.802386324713336.....Total loss: 22.802386324713336\n",
      "Pure loss: 21.50846200395001.....Total loss: 21.50846200395001\n",
      "epoch 2126 learning rate:  0.010470366886171215   Training loss:   22.802386324713336  Valing loss:   21.50846200395001\n",
      "Pure loss: 22.85674498883956.....Total loss: 22.85674498883956\n",
      "Pure loss: 21.789868470160606.....Total loss: 21.789868470160606\n",
      "epoch 2127 learning rate:  0.010470145745181006   Training loss:   22.85674498883956  Valing loss:   21.789868470160606\n",
      "Pure loss: 22.561232487310626.....Total loss: 22.561232487310626\n",
      "Pure loss: 21.18627253961893.....Total loss: 21.18627253961893\n",
      "epoch 2128 learning rate:  0.010469924812030075   Training loss:   22.561232487310626  Valing loss:   21.18627253961893\n",
      "Pure loss: 22.354022631727165.....Total loss: 22.354022631727165\n",
      "Pure loss: 20.72485903772043.....Total loss: 20.72485903772043\n",
      "epoch 2129 learning rate:  0.010469704086425552   Training loss:   22.354022631727165  Valing loss:   20.72485903772043\n",
      "Pure loss: 22.09260158112675.....Total loss: 22.09260158112675\n",
      "Pure loss: 19.76626474807847.....Total loss: 19.76626474807847\n",
      "epoch 2130 learning rate:  0.010469483568075118   Training loss:   22.09260158112675  Valing loss:   19.76626474807847\n",
      "Pure loss: 22.11603378523656.....Total loss: 22.11603378523656\n",
      "Pure loss: 20.015773046001716.....Total loss: 20.015773046001716\n",
      "epoch 2131 learning rate:  0.010469263256687001   Training loss:   22.11603378523656  Valing loss:   20.015773046001716\n",
      "Pure loss: 22.179309903247358.....Total loss: 22.179309903247358\n",
      "Pure loss: 20.435705292992527.....Total loss: 20.435705292992527\n",
      "epoch 2132 learning rate:  0.01046904315196998   Training loss:   22.179309903247358  Valing loss:   20.435705292992527\n",
      "Pure loss: 23.86518691897503.....Total loss: 23.86518691897503\n",
      "Pure loss: 23.61429271670497.....Total loss: 23.61429271670497\n",
      "epoch 2133 learning rate:  0.01046882325363338   Training loss:   23.86518691897503  Valing loss:   23.61429271670497\n",
      "Pure loss: 23.302180519159066.....Total loss: 23.302180519159066\n",
      "Pure loss: 22.759810377239255.....Total loss: 22.759810377239255\n",
      "epoch 2134 learning rate:  0.010468603561387067   Training loss:   23.302180519159066  Valing loss:   22.759810377239255\n",
      "Pure loss: 23.35697170702203.....Total loss: 23.35697170702203\n",
      "Pure loss: 22.85630851359476.....Total loss: 22.85630851359476\n",
      "epoch 2135 learning rate:  0.010468384074941453   Training loss:   23.35697170702203  Valing loss:   22.85630851359476\n",
      "Pure loss: 23.226534122728424.....Total loss: 23.226534122728424\n",
      "Pure loss: 22.647986179280366.....Total loss: 22.647986179280366\n",
      "epoch 2136 learning rate:  0.01046816479400749   Training loss:   23.226534122728424  Valing loss:   22.647986179280366\n",
      "Pure loss: 23.146417919795937.....Total loss: 23.146417919795937\n",
      "Pure loss: 22.504576873594328.....Total loss: 22.504576873594328\n",
      "epoch 2137 learning rate:  0.010467945718296678   Training loss:   23.146417919795937  Valing loss:   22.504576873594328\n",
      "Pure loss: 22.600265869951407.....Total loss: 22.600265869951407\n",
      "Pure loss: 21.51058150267476.....Total loss: 21.51058150267476\n",
      "epoch 2138 learning rate:  0.010467726847521048   Training loss:   22.600265869951407  Valing loss:   21.51058150267476\n",
      "Pure loss: 22.61639517006936.....Total loss: 22.61639517006936\n",
      "Pure loss: 21.511351944076722.....Total loss: 21.511351944076722\n",
      "epoch 2139 learning rate:  0.010467508181393175   Training loss:   22.61639517006936  Valing loss:   21.511351944076722\n",
      "Pure loss: 22.610848798284646.....Total loss: 22.610848798284646\n",
      "Pure loss: 21.52690465914347.....Total loss: 21.52690465914347\n",
      "epoch 2140 learning rate:  0.010467289719626169   Training loss:   22.610848798284646  Valing loss:   21.52690465914347\n",
      "Pure loss: 22.29237861011788.....Total loss: 22.29237861011788\n",
      "Pure loss: 20.892948348361873.....Total loss: 20.892948348361873\n",
      "epoch 2141 learning rate:  0.010467071461933677   Training loss:   22.29237861011788  Valing loss:   20.892948348361873\n",
      "Pure loss: 22.196362998325636.....Total loss: 22.196362998325636\n",
      "Pure loss: 20.631708916987762.....Total loss: 20.631708916987762\n",
      "epoch 2142 learning rate:  0.010466853408029879   Training loss:   22.196362998325636  Valing loss:   20.631708916987762\n",
      "Pure loss: 22.095347541431934.....Total loss: 22.095347541431934\n",
      "Pure loss: 20.29156871014691.....Total loss: 20.29156871014691\n",
      "epoch 2143 learning rate:  0.010466635557629491   Training loss:   22.095347541431934  Valing loss:   20.29156871014691\n",
      "Pure loss: 22.01635228053585.....Total loss: 22.01635228053585\n",
      "Pure loss: 19.24964311479508.....Total loss: 19.24964311479508\n",
      "epoch 2144 learning rate:  0.010466417910447761   Training loss:   22.01635228053585  Valing loss:   19.24964311479508\n",
      "Pure loss: 22.01780298415578.....Total loss: 22.01780298415578\n",
      "Pure loss: 19.245329124179026.....Total loss: 19.245329124179026\n",
      "epoch 2145 learning rate:  0.010466200466200466   Training loss:   22.01780298415578  Valing loss:   19.245329124179026\n",
      "Pure loss: 21.862580051418984.....Total loss: 21.862580051418984\n",
      "Pure loss: 19.367567236632297.....Total loss: 19.367567236632297\n",
      "epoch 2146 learning rate:  0.010465983224603915   Training loss:   21.862580051418984  Valing loss:   19.367567236632297\n",
      "Pure loss: 21.88777061482752.....Total loss: 21.88777061482752\n",
      "Pure loss: 19.347592124081487.....Total loss: 19.347592124081487\n",
      "epoch 2147 learning rate:  0.010465766185374942   Training loss:   21.88777061482752  Valing loss:   19.347592124081487\n",
      "Pure loss: 21.92105489177495.....Total loss: 21.92105489177495\n",
      "Pure loss: 19.349523180093115.....Total loss: 19.349523180093115\n",
      "epoch 2148 learning rate:  0.010465549348230912   Training loss:   21.92105489177495  Valing loss:   19.349523180093115\n",
      "Pure loss: 21.929528376355993.....Total loss: 21.929528376355993\n",
      "Pure loss: 19.35324362225769.....Total loss: 19.35324362225769\n",
      "epoch 2149 learning rate:  0.010465332712889717   Training loss:   21.929528376355993  Valing loss:   19.35324362225769\n",
      "Pure loss: 22.103505711875783.....Total loss: 22.103505711875783\n",
      "Pure loss: 19.284788735720987.....Total loss: 19.284788735720987\n",
      "epoch 2150 learning rate:  0.010465116279069767   Training loss:   22.103505711875783  Valing loss:   19.284788735720987\n",
      "Pure loss: 22.263578356871985.....Total loss: 22.263578356871985\n",
      "Pure loss: 21.02953548796244.....Total loss: 21.02953548796244\n",
      "epoch 2151 learning rate:  0.010464900046490005   Training loss:   22.263578356871985  Valing loss:   21.02953548796244\n",
      "Pure loss: 22.982713974108155.....Total loss: 22.982713974108155\n",
      "Pure loss: 22.357870755391957.....Total loss: 22.357870755391957\n",
      "epoch 2152 learning rate:  0.010464684014869888   Training loss:   22.982713974108155  Valing loss:   22.357870755391957\n",
      "Pure loss: 22.5874723634213.....Total loss: 22.5874723634213\n",
      "Pure loss: 21.703439076139883.....Total loss: 21.703439076139883\n",
      "epoch 2153 learning rate:  0.0104644681839294   Training loss:   22.5874723634213  Valing loss:   21.703439076139883\n",
      "Pure loss: 22.519662843105426.....Total loss: 22.519662843105426\n",
      "Pure loss: 21.586708526769936.....Total loss: 21.586708526769936\n",
      "epoch 2154 learning rate:  0.010464252553389043   Training loss:   22.519662843105426  Valing loss:   21.586708526769936\n",
      "Pure loss: 22.245226515604838.....Total loss: 22.245226515604838\n",
      "Pure loss: 21.063557280030565.....Total loss: 21.063557280030565\n",
      "epoch 2155 learning rate:  0.010464037122969838   Training loss:   22.245226515604838  Valing loss:   21.063557280030565\n",
      "Pure loss: 22.371160895874514.....Total loss: 22.371160895874514\n",
      "Pure loss: 21.41118678035433.....Total loss: 21.41118678035433\n",
      "epoch 2156 learning rate:  0.01046382189239332   Training loss:   22.371160895874514  Valing loss:   21.41118678035433\n",
      "Pure loss: 22.00416036609046.....Total loss: 22.00416036609046\n",
      "Pure loss: 20.643710299759242.....Total loss: 20.643710299759242\n",
      "epoch 2157 learning rate:  0.010463606861381548   Training loss:   22.00416036609046  Valing loss:   20.643710299759242\n",
      "Pure loss: 21.7392683487303.....Total loss: 21.7392683487303\n",
      "Pure loss: 20.01175356874725.....Total loss: 20.01175356874725\n",
      "epoch 2158 learning rate:  0.01046339202965709   Training loss:   21.7392683487303  Valing loss:   20.01175356874725\n",
      "Pure loss: 21.68672247229248.....Total loss: 21.68672247229248\n",
      "Pure loss: 19.7917278214984.....Total loss: 19.7917278214984\n",
      "epoch 2159 learning rate:  0.01046317739694303   Training loss:   21.68672247229248  Valing loss:   19.7917278214984\n",
      "Pure loss: 21.692585213975836.....Total loss: 21.692585213975836\n",
      "Pure loss: 19.743761343011876.....Total loss: 19.743761343011876\n",
      "epoch 2160 learning rate:  0.010462962962962964   Training loss:   21.692585213975836  Valing loss:   19.743761343011876\n",
      "Pure loss: 21.87716654674475.....Total loss: 21.87716654674475\n",
      "Pure loss: 20.353647501983072.....Total loss: 20.353647501983072\n",
      "epoch 2161 learning rate:  0.010462748727440999   Training loss:   21.87716654674475  Valing loss:   20.353647501983072\n",
      "Pure loss: 22.16430729628712.....Total loss: 22.16430729628712\n",
      "Pure loss: 21.031121771064075.....Total loss: 21.031121771064075\n",
      "epoch 2162 learning rate:  0.010462534690101758   Training loss:   22.16430729628712  Valing loss:   21.031121771064075\n",
      "Pure loss: 22.293466732853602.....Total loss: 22.293466732853602\n",
      "Pure loss: 21.284093016476046.....Total loss: 21.284093016476046\n",
      "epoch 2163 learning rate:  0.010462320850670365   Training loss:   22.293466732853602  Valing loss:   21.284093016476046\n",
      "Pure loss: 22.236817729298163.....Total loss: 22.236817729298163\n",
      "Pure loss: 21.154809881071355.....Total loss: 21.154809881071355\n",
      "epoch 2164 learning rate:  0.01046210720887246   Training loss:   22.236817729298163  Valing loss:   21.154809881071355\n",
      "Pure loss: 22.20624352881432.....Total loss: 22.20624352881432\n",
      "Pure loss: 21.09631175329253.....Total loss: 21.09631175329253\n",
      "epoch 2165 learning rate:  0.01046189376443418   Training loss:   22.20624352881432  Valing loss:   21.09631175329253\n",
      "Pure loss: 22.20593313808043.....Total loss: 22.20593313808043\n",
      "Pure loss: 21.094519561502075.....Total loss: 21.094519561502075\n",
      "epoch 2166 learning rate:  0.01046168051708218   Training loss:   22.20593313808043  Valing loss:   21.094519561502075\n",
      "Pure loss: 22.194976113418004.....Total loss: 22.194976113418004\n",
      "Pure loss: 21.063342163987944.....Total loss: 21.063342163987944\n",
      "epoch 2167 learning rate:  0.01046146746654361   Training loss:   22.194976113418004  Valing loss:   21.063342163987944\n",
      "Pure loss: 22.208209012593805.....Total loss: 22.208209012593805\n",
      "Pure loss: 21.076038907639504.....Total loss: 21.076038907639504\n",
      "epoch 2168 learning rate:  0.010461254612546126   Training loss:   22.208209012593805  Valing loss:   21.076038907639504\n",
      "Pure loss: 22.093861545242792.....Total loss: 22.093861545242792\n",
      "Pure loss: 20.684006621136547.....Total loss: 20.684006621136547\n",
      "epoch 2169 learning rate:  0.010461041954817889   Training loss:   22.093861545242792  Valing loss:   20.684006621136547\n",
      "Pure loss: 22.108440695156798.....Total loss: 22.108440695156798\n",
      "Pure loss: 20.714758842664942.....Total loss: 20.714758842664942\n",
      "epoch 2170 learning rate:  0.010460829493087558   Training loss:   22.108440695156798  Valing loss:   20.714758842664942\n",
      "Pure loss: 21.842917605239087.....Total loss: 21.842917605239087\n",
      "Pure loss: 20.073646521372112.....Total loss: 20.073646521372112\n",
      "epoch 2171 learning rate:  0.010460617227084293   Training loss:   21.842917605239087  Valing loss:   20.073646521372112\n",
      "Pure loss: 23.099961747572326.....Total loss: 23.099961747572326\n",
      "Pure loss: 22.624005626492526.....Total loss: 22.624005626492526\n",
      "epoch 2172 learning rate:  0.010460405156537754   Training loss:   23.099961747572326  Valing loss:   22.624005626492526\n",
      "Pure loss: 23.296749136568135.....Total loss: 23.296749136568135\n",
      "Pure loss: 22.91945195126228.....Total loss: 22.91945195126228\n",
      "epoch 2173 learning rate:  0.010460193281178094   Training loss:   23.296749136568135  Valing loss:   22.91945195126228\n",
      "Pure loss: 23.24182063920558.....Total loss: 23.24182063920558\n",
      "Pure loss: 22.83407983715629.....Total loss: 22.83407983715629\n",
      "epoch 2174 learning rate:  0.010459981600735972   Training loss:   23.24182063920558  Valing loss:   22.83407983715629\n",
      "Pure loss: 23.21302929080439.....Total loss: 23.21302929080439\n",
      "Pure loss: 22.787730337769606.....Total loss: 22.787730337769606\n",
      "epoch 2175 learning rate:  0.01045977011494253   Training loss:   23.21302929080439  Valing loss:   22.787730337769606\n",
      "Pure loss: 23.92006059169536.....Total loss: 23.92006059169536\n",
      "Pure loss: 23.824176042634715.....Total loss: 23.824176042634715\n",
      "epoch 2176 learning rate:  0.010459558823529412   Training loss:   23.92006059169536  Valing loss:   23.824176042634715\n",
      "Pure loss: 23.607255182374388.....Total loss: 23.607255182374388\n",
      "Pure loss: 23.380514431760304.....Total loss: 23.380514431760304\n",
      "epoch 2177 learning rate:  0.010459347726228755   Training loss:   23.607255182374388  Valing loss:   23.380514431760304\n",
      "Pure loss: 23.246400847097476.....Total loss: 23.246400847097476\n",
      "Pure loss: 22.816769693744877.....Total loss: 22.816769693744877\n",
      "epoch 2178 learning rate:  0.010459136822773187   Training loss:   23.246400847097476  Valing loss:   22.816769693744877\n",
      "Pure loss: 23.0446652124725.....Total loss: 23.0446652124725\n",
      "Pure loss: 22.499215068572028.....Total loss: 22.499215068572028\n",
      "epoch 2179 learning rate:  0.010458926112895823   Training loss:   23.0446652124725  Valing loss:   22.499215068572028\n",
      "Pure loss: 22.98783220884885.....Total loss: 22.98783220884885\n",
      "Pure loss: 22.391239080084866.....Total loss: 22.391239080084866\n",
      "epoch 2180 learning rate:  0.010458715596330275   Training loss:   22.98783220884885  Valing loss:   22.391239080084866\n",
      "Pure loss: 22.50464620103722.....Total loss: 22.50464620103722\n",
      "Pure loss: 21.628777996768232.....Total loss: 21.628777996768232\n",
      "epoch 2181 learning rate:  0.010458505272810637   Training loss:   22.50464620103722  Valing loss:   21.628777996768232\n",
      "Pure loss: 22.44542547859088.....Total loss: 22.44542547859088\n",
      "Pure loss: 21.524851096597814.....Total loss: 21.524851096597814\n",
      "epoch 2182 learning rate:  0.010458295142071495   Training loss:   22.44542547859088  Valing loss:   21.524851096597814\n",
      "Pure loss: 22.451406437368696.....Total loss: 22.451406437368696\n",
      "Pure loss: 21.53828583608172.....Total loss: 21.53828583608172\n",
      "epoch 2183 learning rate:  0.010458085203847917   Training loss:   22.451406437368696  Valing loss:   21.53828583608172\n",
      "Pure loss: 22.005336092031023.....Total loss: 22.005336092031023\n",
      "Pure loss: 20.680026644970596.....Total loss: 20.680026644970596\n",
      "epoch 2184 learning rate:  0.010457875457875458   Training loss:   22.005336092031023  Valing loss:   20.680026644970596\n",
      "Pure loss: 21.85257971256068.....Total loss: 21.85257971256068\n",
      "Pure loss: 20.256126667238778.....Total loss: 20.256126667238778\n",
      "epoch 2185 learning rate:  0.01045766590389016   Training loss:   21.85257971256068  Valing loss:   20.256126667238778\n",
      "Pure loss: 21.762684076246785.....Total loss: 21.762684076246785\n",
      "Pure loss: 19.661527807311415.....Total loss: 19.661527807311415\n",
      "epoch 2186 learning rate:  0.010457456541628546   Training loss:   21.762684076246785  Valing loss:   19.661527807311415\n",
      "Pure loss: 21.677082072658333.....Total loss: 21.677082072658333\n",
      "Pure loss: 19.619777712204566.....Total loss: 19.619777712204566\n",
      "epoch 2187 learning rate:  0.010457247370827618   Training loss:   21.677082072658333  Valing loss:   19.619777712204566\n",
      "Pure loss: 21.655132181275118.....Total loss: 21.655132181275118\n",
      "Pure loss: 19.344751238351517.....Total loss: 19.344751238351517\n",
      "epoch 2188 learning rate:  0.010457038391224863   Training loss:   21.655132181275118  Valing loss:   19.344751238351517\n",
      "Pure loss: 21.722449212177004.....Total loss: 21.722449212177004\n",
      "Pure loss: 19.40080046149185.....Total loss: 19.40080046149185\n",
      "epoch 2189 learning rate:  0.010456829602558245   Training loss:   21.722449212177004  Valing loss:   19.40080046149185\n",
      "Pure loss: 21.7540360550343.....Total loss: 21.7540360550343\n",
      "Pure loss: 19.38726001568254.....Total loss: 19.38726001568254\n",
      "epoch 2190 learning rate:  0.01045662100456621   Training loss:   21.7540360550343  Valing loss:   19.38726001568254\n",
      "Pure loss: 21.712965075322636.....Total loss: 21.712965075322636\n",
      "Pure loss: 19.507280801553268.....Total loss: 19.507280801553268\n",
      "epoch 2191 learning rate:  0.010456412596987676   Training loss:   21.712965075322636  Valing loss:   19.507280801553268\n",
      "Pure loss: 21.788760623254877.....Total loss: 21.788760623254877\n",
      "Pure loss: 20.229842441691556.....Total loss: 20.229842441691556\n",
      "epoch 2192 learning rate:  0.010456204379562043   Training loss:   21.788760623254877  Valing loss:   20.229842441691556\n",
      "Pure loss: 22.20180036557419.....Total loss: 22.20180036557419\n",
      "Pure loss: 21.227194041426376.....Total loss: 21.227194041426376\n",
      "epoch 2193 learning rate:  0.010455996352029184   Training loss:   22.20180036557419  Valing loss:   21.227194041426376\n",
      "Pure loss: 22.032871860986273.....Total loss: 22.032871860986273\n",
      "Pure loss: 20.84433348017512.....Total loss: 20.84433348017512\n",
      "epoch 2194 learning rate:  0.010455788514129444   Training loss:   22.032871860986273  Valing loss:   20.84433348017512\n",
      "Pure loss: 22.03835785784212.....Total loss: 22.03835785784212\n",
      "Pure loss: 20.812858591806936.....Total loss: 20.812858591806936\n",
      "epoch 2195 learning rate:  0.010455580865603645   Training loss:   22.03835785784212  Valing loss:   20.812858591806936\n",
      "Pure loss: 21.886041875938776.....Total loss: 21.886041875938776\n",
      "Pure loss: 20.443068031916827.....Total loss: 20.443068031916827\n",
      "epoch 2196 learning rate:  0.010455373406193078   Training loss:   21.886041875938776  Valing loss:   20.443068031916827\n",
      "Pure loss: 21.910305281805183.....Total loss: 21.910305281805183\n",
      "Pure loss: 20.532046583754802.....Total loss: 20.532046583754802\n",
      "epoch 2197 learning rate:  0.010455166135639508   Training loss:   21.910305281805183  Valing loss:   20.532046583754802\n",
      "Pure loss: 21.845994904716143.....Total loss: 21.845994904716143\n",
      "Pure loss: 20.287659603479735.....Total loss: 20.287659603479735\n",
      "epoch 2198 learning rate:  0.010454959053685169   Training loss:   21.845994904716143  Valing loss:   20.287659603479735\n",
      "Pure loss: 21.820168318119464.....Total loss: 21.820168318119464\n",
      "Pure loss: 20.64413897365471.....Total loss: 20.64413897365471\n",
      "epoch 2199 learning rate:  0.01045475216007276   Training loss:   21.820168318119464  Valing loss:   20.64413897365471\n",
      "Pure loss: 21.80435752702211.....Total loss: 21.80435752702211\n",
      "Pure loss: 20.600320054177804.....Total loss: 20.600320054177804\n",
      "epoch 2200 learning rate:  0.010454545454545454   Training loss:   21.80435752702211  Valing loss:   20.600320054177804\n",
      "Pure loss: 21.574716728234165.....Total loss: 21.574716728234165\n",
      "Pure loss: 19.9944920289417.....Total loss: 19.9944920289417\n",
      "epoch 2201 learning rate:  0.010454338936846889   Training loss:   21.574716728234165  Valing loss:   19.9944920289417\n",
      "Pure loss: 21.497584522896158.....Total loss: 21.497584522896158\n",
      "Pure loss: 19.565339685497342.....Total loss: 19.565339685497342\n",
      "epoch 2202 learning rate:  0.010454132606721163   Training loss:   21.497584522896158  Valing loss:   19.565339685497342\n",
      "Pure loss: 21.4957216370179.....Total loss: 21.4957216370179\n",
      "Pure loss: 19.480382480510883.....Total loss: 19.480382480510883\n",
      "epoch 2203 learning rate:  0.010453926463912846   Training loss:   21.4957216370179  Valing loss:   19.480382480510883\n",
      "Pure loss: 21.51668478913774.....Total loss: 21.51668478913774\n",
      "Pure loss: 19.807113163786116.....Total loss: 19.807113163786116\n",
      "epoch 2204 learning rate:  0.01045372050816697   Training loss:   21.51668478913774  Valing loss:   19.807113163786116\n",
      "Pure loss: 21.56391289737984.....Total loss: 21.56391289737984\n",
      "Pure loss: 19.989159742333925.....Total loss: 19.989159742333925\n",
      "epoch 2205 learning rate:  0.010453514739229026   Training loss:   21.56391289737984  Valing loss:   19.989159742333925\n",
      "Pure loss: 21.57835567121308.....Total loss: 21.57835567121308\n",
      "Pure loss: 20.060480849577573.....Total loss: 20.060480849577573\n",
      "epoch 2206 learning rate:  0.010453309156844968   Training loss:   21.57835567121308  Valing loss:   20.060480849577573\n",
      "Pure loss: 21.589214030696766.....Total loss: 21.589214030696766\n",
      "Pure loss: 20.103790695506124.....Total loss: 20.103790695506124\n",
      "epoch 2207 learning rate:  0.010453103760761214   Training loss:   21.589214030696766  Valing loss:   20.103790695506124\n",
      "Pure loss: 21.737439375481973.....Total loss: 21.737439375481973\n",
      "Pure loss: 20.456338892433447.....Total loss: 20.456338892433447\n",
      "epoch 2208 learning rate:  0.010452898550724638   Training loss:   21.737439375481973  Valing loss:   20.456338892433447\n",
      "Pure loss: 22.28914460301166.....Total loss: 22.28914460301166\n",
      "Pure loss: 21.556946450823755.....Total loss: 21.556946450823755\n",
      "epoch 2209 learning rate:  0.010452693526482572   Training loss:   22.28914460301166  Valing loss:   21.556946450823755\n",
      "Pure loss: 21.467079766212354.....Total loss: 21.467079766212354\n",
      "Pure loss: 19.76757084217263.....Total loss: 19.76757084217263\n",
      "epoch 2210 learning rate:  0.010452488687782806   Training loss:   21.467079766212354  Valing loss:   19.76757084217263\n",
      "Pure loss: 21.561509488645676.....Total loss: 21.561509488645676\n",
      "Pure loss: 20.006942258745173.....Total loss: 20.006942258745173\n",
      "epoch 2211 learning rate:  0.010452284034373586   Training loss:   21.561509488645676  Valing loss:   20.006942258745173\n",
      "Pure loss: 21.527603670004844.....Total loss: 21.527603670004844\n",
      "Pure loss: 19.958787703099617.....Total loss: 19.958787703099617\n",
      "epoch 2212 learning rate:  0.010452079566003617   Training loss:   21.527603670004844  Valing loss:   19.958787703099617\n",
      "Pure loss: 21.79641134549414.....Total loss: 21.79641134549414\n",
      "Pure loss: 20.476953586178674.....Total loss: 20.476953586178674\n",
      "epoch 2213 learning rate:  0.010451875282422052   Training loss:   21.79641134549414  Valing loss:   20.476953586178674\n",
      "Pure loss: 22.945543754347085.....Total loss: 22.945543754347085\n",
      "Pure loss: 22.36460936203548.....Total loss: 22.36460936203548\n",
      "epoch 2214 learning rate:  0.0104516711833785   Training loss:   22.945543754347085  Valing loss:   22.36460936203548\n",
      "Pure loss: 22.15915701859133.....Total loss: 22.15915701859133\n",
      "Pure loss: 20.986233733323914.....Total loss: 20.986233733323914\n",
      "epoch 2215 learning rate:  0.010451467268623026   Training loss:   22.15915701859133  Valing loss:   20.986233733323914\n",
      "Pure loss: 22.300652766467874.....Total loss: 22.300652766467874\n",
      "Pure loss: 21.231735760474404.....Total loss: 21.231735760474404\n",
      "epoch 2216 learning rate:  0.010451263537906137   Training loss:   22.300652766467874  Valing loss:   21.231735760474404\n",
      "Pure loss: 22.24886787770544.....Total loss: 22.24886787770544\n",
      "Pure loss: 21.142608805848774.....Total loss: 21.142608805848774\n",
      "epoch 2217 learning rate:  0.010451059990978801   Training loss:   22.24886787770544  Valing loss:   21.142608805848774\n",
      "Pure loss: 21.786496728938484.....Total loss: 21.786496728938484\n",
      "Pure loss: 20.303915220078473.....Total loss: 20.303915220078473\n",
      "epoch 2218 learning rate:  0.010450856627592426   Training loss:   21.786496728938484  Valing loss:   20.303915220078473\n",
      "Pure loss: 21.700235794085156.....Total loss: 21.700235794085156\n",
      "Pure loss: 20.176596213318646.....Total loss: 20.176596213318646\n",
      "epoch 2219 learning rate:  0.010450653447498874   Training loss:   21.700235794085156  Valing loss:   20.176596213318646\n",
      "Pure loss: 21.741241429764845.....Total loss: 21.741241429764845\n",
      "Pure loss: 20.26911930421222.....Total loss: 20.26911930421222\n",
      "epoch 2220 learning rate:  0.010450450450450451   Training loss:   21.741241429764845  Valing loss:   20.26911930421222\n",
      "Pure loss: 21.769602492591794.....Total loss: 21.769602492591794\n",
      "Pure loss: 20.320302364573784.....Total loss: 20.320302364573784\n",
      "epoch 2221 learning rate:  0.01045024763619991   Training loss:   21.769602492591794  Valing loss:   20.320302364573784\n",
      "Pure loss: 21.579269159280862.....Total loss: 21.579269159280862\n",
      "Pure loss: 20.059532018494792.....Total loss: 20.059532018494792\n",
      "epoch 2222 learning rate:  0.01045004500450045   Training loss:   21.579269159280862  Valing loss:   20.059532018494792\n",
      "Pure loss: 21.41511811245316.....Total loss: 21.41511811245316\n",
      "Pure loss: 19.70017550478561.....Total loss: 19.70017550478561\n",
      "epoch 2223 learning rate:  0.010449842555105714   Training loss:   21.41511811245316  Valing loss:   19.70017550478561\n",
      "Pure loss: 21.49267102173049.....Total loss: 21.49267102173049\n",
      "Pure loss: 19.87976015689238.....Total loss: 19.87976015689238\n",
      "epoch 2224 learning rate:  0.010449640287769785   Training loss:   21.49267102173049  Valing loss:   19.87976015689238\n",
      "Pure loss: 21.510084020176606.....Total loss: 21.510084020176606\n",
      "Pure loss: 19.92343700687781.....Total loss: 19.92343700687781\n",
      "epoch 2225 learning rate:  0.010449438202247191   Training loss:   21.510084020176606  Valing loss:   19.92343700687781\n",
      "Pure loss: 21.440650218799735.....Total loss: 21.440650218799735\n",
      "Pure loss: 19.69491696773911.....Total loss: 19.69491696773911\n",
      "epoch 2226 learning rate:  0.010449236298292901   Training loss:   21.440650218799735  Valing loss:   19.69491696773911\n",
      "Pure loss: 21.305600277902915.....Total loss: 21.305600277902915\n",
      "Pure loss: 19.19066315484168.....Total loss: 19.19066315484168\n",
      "epoch 2227 learning rate:  0.010449034575662326   Training loss:   21.305600277902915  Valing loss:   19.19066315484168\n",
      "Pure loss: 21.263546719485383.....Total loss: 21.263546719485383\n",
      "Pure loss: 19.065827261853407.....Total loss: 19.065827261853407\n",
      "epoch 2228 learning rate:  0.01044883303411131   Training loss:   21.263546719485383  Valing loss:   19.065827261853407\n",
      "Pure loss: 21.44365677616721.....Total loss: 21.44365677616721\n",
      "Pure loss: 19.900838731609124.....Total loss: 19.900838731609124\n",
      "epoch 2229 learning rate:  0.010448631673396143   Training loss:   21.44365677616721  Valing loss:   19.900838731609124\n",
      "Pure loss: 21.375048931379318.....Total loss: 21.375048931379318\n",
      "Pure loss: 19.663860636456754.....Total loss: 19.663860636456754\n",
      "epoch 2230 learning rate:  0.010448430493273543   Training loss:   21.375048931379318  Valing loss:   19.663860636456754\n",
      "Pure loss: 21.294883131407666.....Total loss: 21.294883131407666\n",
      "Pure loss: 19.433549566217607.....Total loss: 19.433549566217607\n",
      "epoch 2231 learning rate:  0.010448229493500672   Training loss:   21.294883131407666  Valing loss:   19.433549566217607\n",
      "Pure loss: 21.250179536956782.....Total loss: 21.250179536956782\n",
      "Pure loss: 18.865386685845763.....Total loss: 18.865386685845763\n",
      "epoch 2232 learning rate:  0.010448028673835126   Training loss:   21.250179536956782  Valing loss:   18.865386685845763\n",
      "Pure loss: 21.318379514653316.....Total loss: 21.318379514653316\n",
      "Pure loss: 18.658993917286075.....Total loss: 18.658993917286075\n",
      "epoch 2233 learning rate:  0.01044782803403493   Training loss:   21.318379514653316  Valing loss:   18.658993917286075\n",
      "Pure loss: 21.317216194443922.....Total loss: 21.317216194443922\n",
      "Pure loss: 18.662010376619367.....Total loss: 18.662010376619367\n",
      "epoch 2234 learning rate:  0.01044762757385855   Training loss:   21.317216194443922  Valing loss:   18.662010376619367\n",
      "Pure loss: 21.473226915593703.....Total loss: 21.473226915593703\n",
      "Pure loss: 18.48500802010949.....Total loss: 18.48500802010949\n",
      "epoch 2235 learning rate:  0.010447427293064877   Training loss:   21.473226915593703  Valing loss:   18.48500802010949\n",
      "Pure loss: 21.755585878245856.....Total loss: 21.755585878245856\n",
      "Pure loss: 18.386920558611425.....Total loss: 18.386920558611425\n",
      "epoch 2236 learning rate:  0.010447227191413238   Training loss:   21.755585878245856  Valing loss:   18.386920558611425\n",
      "Pure loss: 21.307245504149794.....Total loss: 21.307245504149794\n",
      "Pure loss: 18.55198487219358.....Total loss: 18.55198487219358\n",
      "epoch 2237 learning rate:  0.01044702726866339   Training loss:   21.307245504149794  Valing loss:   18.55198487219358\n",
      "Pure loss: 21.204974454555508.....Total loss: 21.204974454555508\n",
      "Pure loss: 18.834346808214864.....Total loss: 18.834346808214864\n",
      "epoch 2238 learning rate:  0.010446827524575514   Training loss:   21.204974454555508  Valing loss:   18.834346808214864\n",
      "Pure loss: 21.20625151098529.....Total loss: 21.20625151098529\n",
      "Pure loss: 18.805149803445143.....Total loss: 18.805149803445143\n",
      "epoch 2239 learning rate:  0.010446627958910227   Training loss:   21.20625151098529  Valing loss:   18.805149803445143\n",
      "Pure loss: 21.236157204004154.....Total loss: 21.236157204004154\n",
      "Pure loss: 18.687195290708473.....Total loss: 18.687195290708473\n",
      "epoch 2240 learning rate:  0.010446428571428572   Training loss:   21.236157204004154  Valing loss:   18.687195290708473\n",
      "Pure loss: 21.216491837269096.....Total loss: 21.216491837269096\n",
      "Pure loss: 18.796853546280914.....Total loss: 18.796853546280914\n",
      "epoch 2241 learning rate:  0.010446229361892012   Training loss:   21.216491837269096  Valing loss:   18.796853546280914\n",
      "Pure loss: 21.24538246653453.....Total loss: 21.24538246653453\n",
      "Pure loss: 18.853836353516655.....Total loss: 18.853836353516655\n",
      "epoch 2242 learning rate:  0.010446030330062445   Training loss:   21.24538246653453  Valing loss:   18.853836353516655\n",
      "Pure loss: 21.29740905140655.....Total loss: 21.29740905140655\n",
      "Pure loss: 18.509698974972146.....Total loss: 18.509698974972146\n",
      "epoch 2243 learning rate:  0.010445831475702185   Training loss:   21.29740905140655  Valing loss:   18.509698974972146\n",
      "Pure loss: 21.29724460230155.....Total loss: 21.29724460230155\n",
      "Pure loss: 18.509984502744913.....Total loss: 18.509984502744913\n",
      "epoch 2244 learning rate:  0.010445632798573976   Training loss:   21.29724460230155  Valing loss:   18.509984502744913\n",
      "Pure loss: 21.540759029119542.....Total loss: 21.540759029119542\n",
      "Pure loss: 18.35515158604733.....Total loss: 18.35515158604733\n",
      "epoch 2245 learning rate:  0.01044543429844098   Training loss:   21.540759029119542  Valing loss:   18.35515158604733\n",
      "Pure loss: 21.636018827925042.....Total loss: 21.636018827925042\n",
      "Pure loss: 18.33231543676143.....Total loss: 18.33231543676143\n",
      "epoch 2246 learning rate:  0.010445235975066785   Training loss:   21.636018827925042  Valing loss:   18.33231543676143\n",
      "Pure loss: 21.922834276550578.....Total loss: 21.922834276550578\n",
      "Pure loss: 18.300223465983848.....Total loss: 18.300223465983848\n",
      "epoch 2247 learning rate:  0.010445037828215398   Training loss:   21.922834276550578  Valing loss:   18.300223465983848\n",
      "Pure loss: 22.74854693320873.....Total loss: 22.74854693320873\n",
      "Pure loss: 18.443665253374792.....Total loss: 18.443665253374792\n",
      "epoch 2248 learning rate:  0.010444839857651246   Training loss:   22.74854693320873  Valing loss:   18.443665253374792\n",
      "Pure loss: 22.779851010619748.....Total loss: 22.779851010619748\n",
      "Pure loss: 18.45768835026065.....Total loss: 18.45768835026065\n",
      "epoch 2249 learning rate:  0.010444642063139173   Training loss:   22.779851010619748  Valing loss:   18.45768835026065\n",
      "Pure loss: 22.81133929867751.....Total loss: 22.81133929867751\n",
      "Pure loss: 18.46984074975511.....Total loss: 18.46984074975511\n",
      "epoch 2250 learning rate:  0.010444444444444445   Training loss:   22.81133929867751  Valing loss:   18.46984074975511\n",
      "Pure loss: 22.866721182734423.....Total loss: 22.866721182734423\n",
      "Pure loss: 18.49140724247731.....Total loss: 18.49140724247731\n",
      "epoch 2251 learning rate:  0.010444247001332741   Training loss:   22.866721182734423  Valing loss:   18.49140724247731\n",
      "Pure loss: 23.745322493345235.....Total loss: 23.745322493345235\n",
      "Pure loss: 18.89585853122726.....Total loss: 18.89585853122726\n",
      "epoch 2252 learning rate:  0.01044404973357016   Training loss:   23.745322493345235  Valing loss:   18.89585853122726\n",
      "Pure loss: 22.979343315577143.....Total loss: 22.979343315577143\n",
      "Pure loss: 18.510485843976795.....Total loss: 18.510485843976795\n",
      "epoch 2253 learning rate:  0.010443852640923214   Training loss:   22.979343315577143  Valing loss:   18.510485843976795\n",
      "Pure loss: 23.12918476589453.....Total loss: 23.12918476589453\n",
      "Pure loss: 18.585725466646903.....Total loss: 18.585725466646903\n",
      "epoch 2254 learning rate:  0.01044365572315883   Training loss:   23.12918476589453  Valing loss:   18.585725466646903\n",
      "Pure loss: 23.111340968029783.....Total loss: 23.111340968029783\n",
      "Pure loss: 18.57818318387383.....Total loss: 18.57818318387383\n",
      "epoch 2255 learning rate:  0.010443458980044347   Training loss:   23.111340968029783  Valing loss:   18.57818318387383\n",
      "Pure loss: 23.007218570840823.....Total loss: 23.007218570840823\n",
      "Pure loss: 18.525994323397736.....Total loss: 18.525994323397736\n",
      "epoch 2256 learning rate:  0.010443262411347519   Training loss:   23.007218570840823  Valing loss:   18.525994323397736\n",
      "Pure loss: 23.223645813777857.....Total loss: 23.223645813777857\n",
      "Pure loss: 18.625325199356844.....Total loss: 18.625325199356844\n",
      "epoch 2257 learning rate:  0.010443066016836509   Training loss:   23.223645813777857  Valing loss:   18.625325199356844\n",
      "Pure loss: 24.091937616098978.....Total loss: 24.091937616098978\n",
      "Pure loss: 19.09423496825102.....Total loss: 19.09423496825102\n",
      "epoch 2258 learning rate:  0.010442869796279893   Training loss:   24.091937616098978  Valing loss:   19.09423496825102\n",
      "Pure loss: 24.563277085832436.....Total loss: 24.563277085832436\n",
      "Pure loss: 19.41308634261509.....Total loss: 19.41308634261509\n",
      "epoch 2259 learning rate:  0.010442673749446659   Training loss:   24.563277085832436  Valing loss:   19.41308634261509\n",
      "Pure loss: 23.95947358185325.....Total loss: 23.95947358185325\n",
      "Pure loss: 19.02647883183914.....Total loss: 19.02647883183914\n",
      "epoch 2260 learning rate:  0.010442477876106195   Training loss:   23.95947358185325  Valing loss:   19.02647883183914\n",
      "Pure loss: 22.832351440194895.....Total loss: 22.832351440194895\n",
      "Pure loss: 18.786761517859084.....Total loss: 18.786761517859084\n",
      "epoch 2261 learning rate:  0.010442282176028307   Training loss:   22.832351440194895  Valing loss:   18.786761517859084\n",
      "Pure loss: 22.354263444699225.....Total loss: 22.354263444699225\n",
      "Pure loss: 18.767746067835155.....Total loss: 18.767746067835155\n",
      "epoch 2262 learning rate:  0.0104420866489832   Training loss:   22.354263444699225  Valing loss:   18.767746067835155\n",
      "Pure loss: 22.544075824498062.....Total loss: 22.544075824498062\n",
      "Pure loss: 18.733039679148632.....Total loss: 18.733039679148632\n",
      "epoch 2263 learning rate:  0.010441891294741493   Training loss:   22.544075824498062  Valing loss:   18.733039679148632\n",
      "Pure loss: 22.606116749369153.....Total loss: 22.606116749369153\n",
      "Pure loss: 18.551099359203096.....Total loss: 18.551099359203096\n",
      "epoch 2264 learning rate:  0.010441696113074205   Training loss:   22.606116749369153  Valing loss:   18.551099359203096\n",
      "Pure loss: 22.613895983273856.....Total loss: 22.613895983273856\n",
      "Pure loss: 18.409088571324737.....Total loss: 18.409088571324737\n",
      "epoch 2265 learning rate:  0.010441501103752759   Training loss:   22.613895983273856  Valing loss:   18.409088571324737\n",
      "Pure loss: 22.69052468320743.....Total loss: 22.69052468320743\n",
      "Pure loss: 18.42265133774559.....Total loss: 18.42265133774559\n",
      "epoch 2266 learning rate:  0.010441306266548985   Training loss:   22.69052468320743  Valing loss:   18.42265133774559\n",
      "Pure loss: 22.719536787057237.....Total loss: 22.719536787057237\n",
      "Pure loss: 18.420161454756023.....Total loss: 18.420161454756023\n",
      "epoch 2267 learning rate:  0.010441111601235113   Training loss:   22.719536787057237  Valing loss:   18.420161454756023\n",
      "Pure loss: 23.407116444994383.....Total loss: 23.407116444994383\n",
      "Pure loss: 18.7372364400545.....Total loss: 18.7372364400545\n",
      "epoch 2268 learning rate:  0.010440917107583774   Training loss:   23.407116444994383  Valing loss:   18.7372364400545\n",
      "Pure loss: 22.88381847139029.....Total loss: 22.88381847139029\n",
      "Pure loss: 18.494564408697524.....Total loss: 18.494564408697524\n",
      "epoch 2269 learning rate:  0.010440722785368005   Training loss:   22.88381847139029  Valing loss:   18.494564408697524\n",
      "Pure loss: 22.974981784980397.....Total loss: 22.974981784980397\n",
      "Pure loss: 18.532906334519627.....Total loss: 18.532906334519627\n",
      "epoch 2270 learning rate:  0.010440528634361233   Training loss:   22.974981784980397  Valing loss:   18.532906334519627\n",
      "Pure loss: 22.268477616608937.....Total loss: 22.268477616608937\n",
      "Pure loss: 18.179060446708533.....Total loss: 18.179060446708533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2271 learning rate:  0.010440334654337297   Training loss:   22.268477616608937  Valing loss:   18.179060446708533\n",
      "Pure loss: 22.48240694779705.....Total loss: 22.48240694779705\n",
      "Pure loss: 18.22707288403392.....Total loss: 18.22707288403392\n",
      "epoch 2272 learning rate:  0.010440140845070422   Training loss:   22.48240694779705  Valing loss:   18.22707288403392\n",
      "Pure loss: 21.862024657433082.....Total loss: 21.862024657433082\n",
      "Pure loss: 18.09548886624894.....Total loss: 18.09548886624894\n",
      "epoch 2273 learning rate:  0.01043994720633524   Training loss:   21.862024657433082  Valing loss:   18.09548886624894\n",
      "Pure loss: 22.00596799542269.....Total loss: 22.00596799542269\n",
      "Pure loss: 18.0998399007421.....Total loss: 18.0998399007421\n",
      "epoch 2274 learning rate:  0.010439753737906772   Training loss:   22.00596799542269  Valing loss:   18.0998399007421\n",
      "Pure loss: 22.702029631859297.....Total loss: 22.702029631859297\n",
      "Pure loss: 18.309750477191333.....Total loss: 18.309750477191333\n",
      "epoch 2275 learning rate:  0.01043956043956044   Training loss:   22.702029631859297  Valing loss:   18.309750477191333\n",
      "Pure loss: 22.734886040281047.....Total loss: 22.734886040281047\n",
      "Pure loss: 18.327011036724194.....Total loss: 18.327011036724194\n",
      "epoch 2276 learning rate:  0.010439367311072057   Training loss:   22.734886040281047  Valing loss:   18.327011036724194\n",
      "Pure loss: 22.306591770402427.....Total loss: 22.306591770402427\n",
      "Pure loss: 18.192157437485523.....Total loss: 18.192157437485523\n",
      "epoch 2277 learning rate:  0.01043917435221783   Training loss:   22.306591770402427  Valing loss:   18.192157437485523\n",
      "Pure loss: 22.326258793197063.....Total loss: 22.326258793197063\n",
      "Pure loss: 18.195113077349756.....Total loss: 18.195113077349756\n",
      "epoch 2278 learning rate:  0.010438981562774364   Training loss:   22.326258793197063  Valing loss:   18.195113077349756\n",
      "Pure loss: 21.979890022280404.....Total loss: 21.979890022280404\n",
      "Pure loss: 18.1168116934144.....Total loss: 18.1168116934144\n",
      "epoch 2279 learning rate:  0.010438788942518649   Training loss:   21.979890022280404  Valing loss:   18.1168116934144\n",
      "Pure loss: 22.43324294905719.....Total loss: 22.43324294905719\n",
      "Pure loss: 18.206800777487416.....Total loss: 18.206800777487416\n",
      "epoch 2280 learning rate:  0.01043859649122807   Training loss:   22.43324294905719  Valing loss:   18.206800777487416\n",
      "Pure loss: 22.30583520623249.....Total loss: 22.30583520623249\n",
      "Pure loss: 18.172063052363153.....Total loss: 18.172063052363153\n",
      "epoch 2281 learning rate:  0.010438404208680403   Training loss:   22.30583520623249  Valing loss:   18.172063052363153\n",
      "Pure loss: 22.47784806708451.....Total loss: 22.47784806708451\n",
      "Pure loss: 18.233982470336006.....Total loss: 18.233982470336006\n",
      "epoch 2282 learning rate:  0.010438212094653813   Training loss:   22.47784806708451  Valing loss:   18.233982470336006\n",
      "Pure loss: 22.572070203417024.....Total loss: 22.572070203417024\n",
      "Pure loss: 18.264650678374945.....Total loss: 18.264650678374945\n",
      "epoch 2283 learning rate:  0.010438020148926851   Training loss:   22.572070203417024  Valing loss:   18.264650678374945\n",
      "Pure loss: 21.992373139388917.....Total loss: 21.992373139388917\n",
      "Pure loss: 18.082684781434207.....Total loss: 18.082684781434207\n",
      "epoch 2284 learning rate:  0.010437828371278459   Training loss:   21.992373139388917  Valing loss:   18.082684781434207\n",
      "Pure loss: 21.728823602927783.....Total loss: 21.728823602927783\n",
      "Pure loss: 18.067831426622355.....Total loss: 18.067831426622355\n",
      "epoch 2285 learning rate:  0.010437636761487965   Training loss:   21.728823602927783  Valing loss:   18.067831426622355\n",
      "Pure loss: 21.631350101498665.....Total loss: 21.631350101498665\n",
      "Pure loss: 18.12194013536134.....Total loss: 18.12194013536134\n",
      "epoch 2286 learning rate:  0.010437445319335083   Training loss:   21.631350101498665  Valing loss:   18.12194013536134\n",
      "Pure loss: 21.598175431923803.....Total loss: 21.598175431923803\n",
      "Pure loss: 18.029996856019924.....Total loss: 18.029996856019924\n",
      "epoch 2287 learning rate:  0.010437254044599912   Training loss:   21.598175431923803  Valing loss:   18.029996856019924\n",
      "Pure loss: 21.876979295918986.....Total loss: 21.876979295918986\n",
      "Pure loss: 18.05018382638689.....Total loss: 18.05018382638689\n",
      "epoch 2288 learning rate:  0.010437062937062938   Training loss:   21.876979295918986  Valing loss:   18.05018382638689\n",
      "Pure loss: 21.896575100357516.....Total loss: 21.896575100357516\n",
      "Pure loss: 18.045370942232328.....Total loss: 18.045370942232328\n",
      "epoch 2289 learning rate:  0.010436871996505025   Training loss:   21.896575100357516  Valing loss:   18.045370942232328\n",
      "Pure loss: 21.74372189278085.....Total loss: 21.74372189278085\n",
      "Pure loss: 18.03333231529389.....Total loss: 18.03333231529389\n",
      "epoch 2290 learning rate:  0.010436681222707423   Training loss:   21.74372189278085  Valing loss:   18.03333231529389\n",
      "Pure loss: 21.75339520653542.....Total loss: 21.75339520653542\n",
      "Pure loss: 18.024657480090212.....Total loss: 18.024657480090212\n",
      "epoch 2291 learning rate:  0.010436490615451767   Training loss:   21.75339520653542  Valing loss:   18.024657480090212\n",
      "Pure loss: 22.25167512227821.....Total loss: 22.25167512227821\n",
      "Pure loss: 18.118950536707256.....Total loss: 18.118950536707256\n",
      "epoch 2292 learning rate:  0.01043630017452007   Training loss:   22.25167512227821  Valing loss:   18.118950536707256\n",
      "Pure loss: 22.036039983068044.....Total loss: 22.036039983068044\n",
      "Pure loss: 18.273200315636736.....Total loss: 18.273200315636736\n",
      "epoch 2293 learning rate:  0.010436109899694724   Training loss:   22.036039983068044  Valing loss:   18.273200315636736\n",
      "Pure loss: 21.686322058133047.....Total loss: 21.686322058133047\n",
      "Pure loss: 18.251437047210693.....Total loss: 18.251437047210693\n",
      "epoch 2294 learning rate:  0.0104359197907585   Training loss:   21.686322058133047  Valing loss:   18.251437047210693\n",
      "Pure loss: 21.683917186240244.....Total loss: 21.683917186240244\n",
      "Pure loss: 18.3116863554607.....Total loss: 18.3116863554607\n",
      "epoch 2295 learning rate:  0.010435729847494553   Training loss:   21.683917186240244  Valing loss:   18.3116863554607\n",
      "Pure loss: 21.507892671983956.....Total loss: 21.507892671983956\n",
      "Pure loss: 18.432716974459176.....Total loss: 18.432716974459176\n",
      "epoch 2296 learning rate:  0.010435540069686411   Training loss:   21.507892671983956  Valing loss:   18.432716974459176\n",
      "Pure loss: 21.393426047701226.....Total loss: 21.393426047701226\n",
      "Pure loss: 18.63474674732947.....Total loss: 18.63474674732947\n",
      "epoch 2297 learning rate:  0.01043535045711798   Training loss:   21.393426047701226  Valing loss:   18.63474674732947\n",
      "Pure loss: 21.229846057237253.....Total loss: 21.229846057237253\n",
      "Pure loss: 18.33705022272598.....Total loss: 18.33705022272598\n",
      "epoch 2298 learning rate:  0.010435161009573542   Training loss:   21.229846057237253  Valing loss:   18.33705022272598\n",
      "Pure loss: 21.37904460243738.....Total loss: 21.37904460243738\n",
      "Pure loss: 18.24922151843413.....Total loss: 18.24922151843413\n",
      "epoch 2299 learning rate:  0.010434971726837756   Training loss:   21.37904460243738  Valing loss:   18.24922151843413\n",
      "Pure loss: 21.37836449594608.....Total loss: 21.37836449594608\n",
      "Pure loss: 18.202010410573973.....Total loss: 18.202010410573973\n",
      "epoch 2300 learning rate:  0.010434782608695653   Training loss:   21.37836449594608  Valing loss:   18.202010410573973\n",
      "Pure loss: 21.538158913708724.....Total loss: 21.538158913708724\n",
      "Pure loss: 18.132257563052953.....Total loss: 18.132257563052953\n",
      "epoch 2301 learning rate:  0.010434593654932638   Training loss:   21.538158913708724  Valing loss:   18.132257563052953\n",
      "Pure loss: 21.616579968371553.....Total loss: 21.616579968371553\n",
      "Pure loss: 18.09759339729684.....Total loss: 18.09759339729684\n",
      "epoch 2302 learning rate:  0.010434404865334491   Training loss:   21.616579968371553  Valing loss:   18.09759339729684\n",
      "Pure loss: 21.426493977574022.....Total loss: 21.426493977574022\n",
      "Pure loss: 18.12513981399908.....Total loss: 18.12513981399908\n",
      "epoch 2303 learning rate:  0.010434216239687365   Training loss:   21.426493977574022  Valing loss:   18.12513981399908\n",
      "Pure loss: 21.447722649159623.....Total loss: 21.447722649159623\n",
      "Pure loss: 18.083999383084322.....Total loss: 18.083999383084322\n",
      "epoch 2304 learning rate:  0.010434027777777778   Training loss:   21.447722649159623  Valing loss:   18.083999383084322\n",
      "Pure loss: 21.417956737254176.....Total loss: 21.417956737254176\n",
      "Pure loss: 18.09113872343585.....Total loss: 18.09113872343585\n",
      "epoch 2305 learning rate:  0.010433839479392625   Training loss:   21.417956737254176  Valing loss:   18.09113872343585\n",
      "Pure loss: 21.539648050808896.....Total loss: 21.539648050808896\n",
      "Pure loss: 18.056101098151856.....Total loss: 18.056101098151856\n",
      "epoch 2306 learning rate:  0.010433651344319167   Training loss:   21.539648050808896  Valing loss:   18.056101098151856\n",
      "Pure loss: 22.165352303643502.....Total loss: 22.165352303643502\n",
      "Pure loss: 18.119733507459863.....Total loss: 18.119733507459863\n",
      "epoch 2307 learning rate:  0.010433463372345037   Training loss:   22.165352303643502  Valing loss:   18.119733507459863\n",
      "Pure loss: 22.417624461862296.....Total loss: 22.417624461862296\n",
      "Pure loss: 18.191336004598725.....Total loss: 18.191336004598725\n",
      "epoch 2308 learning rate:  0.010433275563258232   Training loss:   22.417624461862296  Valing loss:   18.191336004598725\n",
      "Pure loss: 22.503895102671848.....Total loss: 22.503895102671848\n",
      "Pure loss: 18.222029721946527.....Total loss: 18.222029721946527\n",
      "epoch 2309 learning rate:  0.01043308791684712   Training loss:   22.503895102671848  Valing loss:   18.222029721946527\n",
      "Pure loss: 22.346531298680862.....Total loss: 22.346531298680862\n",
      "Pure loss: 18.176209580284905.....Total loss: 18.176209580284905\n",
      "epoch 2310 learning rate:  0.010432900432900433   Training loss:   22.346531298680862  Valing loss:   18.176209580284905\n",
      "Pure loss: 22.299740068635437.....Total loss: 22.299740068635437\n",
      "Pure loss: 18.1796640725508.....Total loss: 18.1796640725508\n",
      "epoch 2311 learning rate:  0.01043271311120727   Training loss:   22.299740068635437  Valing loss:   18.1796640725508\n",
      "Pure loss: 21.859910956234305.....Total loss: 21.859910956234305\n",
      "Pure loss: 18.079225507930243.....Total loss: 18.079225507930243\n",
      "epoch 2312 learning rate:  0.010432525951557093   Training loss:   21.859910956234305  Valing loss:   18.079225507930243\n",
      "Pure loss: 21.815991464082177.....Total loss: 21.815991464082177\n",
      "Pure loss: 18.117656636668748.....Total loss: 18.117656636668748\n",
      "epoch 2313 learning rate:  0.010432338953739732   Training loss:   21.815991464082177  Valing loss:   18.117656636668748\n",
      "Pure loss: 21.68642403831884.....Total loss: 21.68642403831884\n",
      "Pure loss: 18.121824036974157.....Total loss: 18.121824036974157\n",
      "epoch 2314 learning rate:  0.010432152117545376   Training loss:   21.68642403831884  Valing loss:   18.121824036974157\n",
      "Pure loss: 21.67954126473812.....Total loss: 21.67954126473812\n",
      "Pure loss: 18.061778248016648.....Total loss: 18.061778248016648\n",
      "epoch 2315 learning rate:  0.01043196544276458   Training loss:   21.67954126473812  Valing loss:   18.061778248016648\n",
      "Pure loss: 22.161797558097337.....Total loss: 22.161797558097337\n",
      "Pure loss: 18.14005595551405.....Total loss: 18.14005595551405\n",
      "epoch 2316 learning rate:  0.010431778929188255   Training loss:   22.161797558097337  Valing loss:   18.14005595551405\n",
      "Pure loss: 21.74433855027864.....Total loss: 21.74433855027864\n",
      "Pure loss: 18.07378735201992.....Total loss: 18.07378735201992\n",
      "epoch 2317 learning rate:  0.010431592576607682   Training loss:   21.74433855027864  Valing loss:   18.07378735201992\n",
      "Pure loss: 21.60779650168946.....Total loss: 21.60779650168946\n",
      "Pure loss: 18.07596515572054.....Total loss: 18.07596515572054\n",
      "epoch 2318 learning rate:  0.010431406384814495   Training loss:   21.60779650168946  Valing loss:   18.07596515572054\n",
      "Pure loss: 21.73312291498296.....Total loss: 21.73312291498296\n",
      "Pure loss: 18.082164267366267.....Total loss: 18.082164267366267\n",
      "epoch 2319 learning rate:  0.01043122035360069   Training loss:   21.73312291498296  Valing loss:   18.082164267366267\n",
      "Pure loss: 21.53663380732079.....Total loss: 21.53663380732079\n",
      "Pure loss: 18.081426124076316.....Total loss: 18.081426124076316\n",
      "epoch 2320 learning rate:  0.01043103448275862   Training loss:   21.53663380732079  Valing loss:   18.081426124076316\n",
      "Pure loss: 21.92720372783763.....Total loss: 21.92720372783763\n",
      "Pure loss: 18.091734277726516.....Total loss: 18.091734277726516\n",
      "epoch 2321 learning rate:  0.010430848772081   Training loss:   21.92720372783763  Valing loss:   18.091734277726516\n",
      "Pure loss: 21.76694195563676.....Total loss: 21.76694195563676\n",
      "Pure loss: 18.080823823689983.....Total loss: 18.080823823689983\n",
      "epoch 2322 learning rate:  0.010430663221360896   Training loss:   21.76694195563676  Valing loss:   18.080823823689983\n",
      "Pure loss: 21.876016314388576.....Total loss: 21.876016314388576\n",
      "Pure loss: 18.104106142962728.....Total loss: 18.104106142962728\n",
      "epoch 2323 learning rate:  0.010430477830391735   Training loss:   21.876016314388576  Valing loss:   18.104106142962728\n",
      "Pure loss: 22.519556177195874.....Total loss: 22.519556177195874\n",
      "Pure loss: 18.25674378711243.....Total loss: 18.25674378711243\n",
      "epoch 2324 learning rate:  0.010430292598967298   Training loss:   22.519556177195874  Valing loss:   18.25674378711243\n",
      "Pure loss: 22.4827817241901.....Total loss: 22.4827817241901\n",
      "Pure loss: 18.237663422708522.....Total loss: 18.237663422708522\n",
      "epoch 2325 learning rate:  0.01043010752688172   Training loss:   22.4827817241901  Valing loss:   18.237663422708522\n",
      "Pure loss: 21.56549987609678.....Total loss: 21.56549987609678\n",
      "Pure loss: 18.015834675525753.....Total loss: 18.015834675525753\n",
      "epoch 2326 learning rate:  0.010429922613929494   Training loss:   21.56549987609678  Valing loss:   18.015834675525753\n",
      "Pure loss: 21.69304698657549.....Total loss: 21.69304698657549\n",
      "Pure loss: 18.017369930999415.....Total loss: 18.017369930999415\n",
      "epoch 2327 learning rate:  0.010429737859905457   Training loss:   21.69304698657549  Valing loss:   18.017369930999415\n",
      "Pure loss: 21.67536906600941.....Total loss: 21.67536906600941\n",
      "Pure loss: 18.032299267763378.....Total loss: 18.032299267763378\n",
      "epoch 2328 learning rate:  0.010429553264604811   Training loss:   21.67536906600941  Valing loss:   18.032299267763378\n",
      "Pure loss: 21.53974729245575.....Total loss: 21.53974729245575\n",
      "Pure loss: 18.03753554242243.....Total loss: 18.03753554242243\n",
      "epoch 2329 learning rate:  0.0104293688278231   Training loss:   21.53974729245575  Valing loss:   18.03753554242243\n",
      "Pure loss: 21.577994333611326.....Total loss: 21.577994333611326\n",
      "Pure loss: 18.03514326400641.....Total loss: 18.03514326400641\n",
      "epoch 2330 learning rate:  0.010429184549356223   Training loss:   21.577994333611326  Valing loss:   18.03514326400641\n",
      "Pure loss: 20.846655759430142.....Total loss: 20.846655759430142\n",
      "Pure loss: 18.86987188274703.....Total loss: 18.86987188274703\n",
      "epoch 2331 learning rate:  0.01042900042900043   Training loss:   20.846655759430142  Valing loss:   18.86987188274703\n",
      "Pure loss: 20.74302126396798.....Total loss: 20.74302126396798\n",
      "Pure loss: 18.624000453775796.....Total loss: 18.624000453775796\n",
      "epoch 2332 learning rate:  0.010428816466552316   Training loss:   20.74302126396798  Valing loss:   18.624000453775796\n",
      "Pure loss: 20.74513846047188.....Total loss: 20.74513846047188\n",
      "Pure loss: 18.78984109103387.....Total loss: 18.78984109103387\n",
      "epoch 2333 learning rate:  0.01042863266180883   Training loss:   20.74513846047188  Valing loss:   18.78984109103387\n",
      "Pure loss: 20.75681817398418.....Total loss: 20.75681817398418\n",
      "Pure loss: 18.80846555224674.....Total loss: 18.80846555224674\n",
      "epoch 2334 learning rate:  0.010428449014567267   Training loss:   20.75681817398418  Valing loss:   18.80846555224674\n",
      "Pure loss: 20.74842764301003.....Total loss: 20.74842764301003\n",
      "Pure loss: 18.771248704138035.....Total loss: 18.771248704138035\n",
      "epoch 2335 learning rate:  0.010428265524625268   Training loss:   20.74842764301003  Valing loss:   18.771248704138035\n",
      "Pure loss: 20.742785083556583.....Total loss: 20.742785083556583\n",
      "Pure loss: 18.557199596479542.....Total loss: 18.557199596479542\n",
      "epoch 2336 learning rate:  0.010428082191780822   Training loss:   20.742785083556583  Valing loss:   18.557199596479542\n",
      "Pure loss: 20.751928616798025.....Total loss: 20.751928616798025\n",
      "Pure loss: 18.62140423504932.....Total loss: 18.62140423504932\n",
      "epoch 2337 learning rate:  0.010427899015832263   Training loss:   20.751928616798025  Valing loss:   18.62140423504932\n",
      "Pure loss: 20.749837296957065.....Total loss: 20.749837296957065\n",
      "Pure loss: 18.739974200740228.....Total loss: 18.739974200740228\n",
      "epoch 2338 learning rate:  0.010427715996578272   Training loss:   20.749837296957065  Valing loss:   18.739974200740228\n",
      "Pure loss: 20.750635657000835.....Total loss: 20.750635657000835\n",
      "Pure loss: 18.760218185872702.....Total loss: 18.760218185872702\n",
      "epoch 2339 learning rate:  0.010427533133817871   Training loss:   20.750635657000835  Valing loss:   18.760218185872702\n",
      "Pure loss: 20.748352800364398.....Total loss: 20.748352800364398\n",
      "Pure loss: 18.69680534010217.....Total loss: 18.69680534010217\n",
      "epoch 2340 learning rate:  0.010427350427350428   Training loss:   20.748352800364398  Valing loss:   18.69680534010217\n",
      "Pure loss: 20.80652017326735.....Total loss: 20.80652017326735\n",
      "Pure loss: 18.35916367547954.....Total loss: 18.35916367547954\n",
      "epoch 2341 learning rate:  0.010427167876975651   Training loss:   20.80652017326735  Valing loss:   18.35916367547954\n",
      "Pure loss: 20.81831383684005.....Total loss: 20.81831383684005\n",
      "Pure loss: 18.311924524460007.....Total loss: 18.311924524460007\n",
      "epoch 2342 learning rate:  0.010426985482493595   Training loss:   20.81831383684005  Valing loss:   18.311924524460007\n",
      "Pure loss: 20.824712685364016.....Total loss: 20.824712685364016\n",
      "Pure loss: 18.29685109287481.....Total loss: 18.29685109287481\n",
      "epoch 2343 learning rate:  0.010426803243704652   Training loss:   20.824712685364016  Valing loss:   18.29685109287481\n",
      "Pure loss: 20.738317619866542.....Total loss: 20.738317619866542\n",
      "Pure loss: 18.476558461720106.....Total loss: 18.476558461720106\n",
      "epoch 2344 learning rate:  0.010426621160409556   Training loss:   20.738317619866542  Valing loss:   18.476558461720106\n",
      "Pure loss: 20.7072575677409.....Total loss: 20.7072575677409\n",
      "Pure loss: 18.674764362607156.....Total loss: 18.674764362607156\n",
      "epoch 2345 learning rate:  0.010426439232409383   Training loss:   20.7072575677409  Valing loss:   18.674764362607156\n",
      "Pure loss: 20.71409240875134.....Total loss: 20.71409240875134\n",
      "Pure loss: 18.570330009429412.....Total loss: 18.570330009429412\n",
      "epoch 2346 learning rate:  0.01042625745950554   Training loss:   20.71409240875134  Valing loss:   18.570330009429412\n",
      "Pure loss: 20.731278548646902.....Total loss: 20.731278548646902\n",
      "Pure loss: 18.46351002801188.....Total loss: 18.46351002801188\n",
      "epoch 2347 learning rate:  0.010426075841499786   Training loss:   20.731278548646902  Valing loss:   18.46351002801188\n",
      "Pure loss: 20.69983269886745.....Total loss: 20.69983269886745\n",
      "Pure loss: 18.536985185859514.....Total loss: 18.536985185859514\n",
      "epoch 2348 learning rate:  0.010425894378194208   Training loss:   20.69983269886745  Valing loss:   18.536985185859514\n",
      "Pure loss: 20.662222906863942.....Total loss: 20.662222906863942\n",
      "Pure loss: 18.897307987841387.....Total loss: 18.897307987841387\n",
      "epoch 2349 learning rate:  0.01042571306939123   Training loss:   20.662222906863942  Valing loss:   18.897307987841387\n",
      "Pure loss: 20.68210764417017.....Total loss: 20.68210764417017\n",
      "Pure loss: 18.996749384801006.....Total loss: 18.996749384801006\n",
      "epoch 2350 learning rate:  0.010425531914893617   Training loss:   20.68210764417017  Valing loss:   18.996749384801006\n",
      "Pure loss: 20.667551154555966.....Total loss: 20.667551154555966\n",
      "Pure loss: 18.933669613662136.....Total loss: 18.933669613662136\n",
      "epoch 2351 learning rate:  0.010425350914504467   Training loss:   20.667551154555966  Valing loss:   18.933669613662136\n",
      "Pure loss: 20.69279188391765.....Total loss: 20.69279188391765\n",
      "Pure loss: 19.06334896515874.....Total loss: 19.06334896515874\n",
      "epoch 2352 learning rate:  0.010425170068027212   Training loss:   20.69279188391765  Valing loss:   19.06334896515874\n",
      "Pure loss: 20.640642723604394.....Total loss: 20.640642723604394\n",
      "Pure loss: 18.796177333547003.....Total loss: 18.796177333547003\n",
      "epoch 2353 learning rate:  0.010424989375265618   Training loss:   20.640642723604394  Valing loss:   18.796177333547003\n",
      "Pure loss: 20.654785663834677.....Total loss: 20.654785663834677\n",
      "Pure loss: 18.345069740050988.....Total loss: 18.345069740050988\n",
      "epoch 2354 learning rate:  0.010424808836023789   Training loss:   20.654785663834677  Valing loss:   18.345069740050988\n",
      "Pure loss: 20.730606618002895.....Total loss: 20.730606618002895\n",
      "Pure loss: 18.258818837373482.....Total loss: 18.258818837373482\n",
      "epoch 2355 learning rate:  0.010424628450106157   Training loss:   20.730606618002895  Valing loss:   18.258818837373482\n",
      "Pure loss: 20.8760327509969.....Total loss: 20.8760327509969\n",
      "Pure loss: 18.184386596207634.....Total loss: 18.184386596207634\n",
      "epoch 2356 learning rate:  0.010424448217317488   Training loss:   20.8760327509969  Valing loss:   18.184386596207634\n",
      "Pure loss: 20.777629539168053.....Total loss: 20.777629539168053\n",
      "Pure loss: 18.228662080586616.....Total loss: 18.228662080586616\n",
      "epoch 2357 learning rate:  0.010424268137462877   Training loss:   20.777629539168053  Valing loss:   18.228662080586616\n",
      "Pure loss: 20.838235852682253.....Total loss: 20.838235852682253\n",
      "Pure loss: 18.171336810499678.....Total loss: 18.171336810499678\n",
      "epoch 2358 learning rate:  0.010424088210347753   Training loss:   20.838235852682253  Valing loss:   18.171336810499678\n",
      "Pure loss: 20.85156333960188.....Total loss: 20.85156333960188\n",
      "Pure loss: 18.172195912681705.....Total loss: 18.172195912681705\n",
      "epoch 2359 learning rate:  0.010423908435777873   Training loss:   20.85156333960188  Valing loss:   18.172195912681705\n",
      "Pure loss: 20.888424376562515.....Total loss: 20.888424376562515\n",
      "Pure loss: 18.152978957779823.....Total loss: 18.152978957779823\n",
      "epoch 2360 learning rate:  0.010423728813559323   Training loss:   20.888424376562515  Valing loss:   18.152978957779823\n",
      "Pure loss: 20.873298227862858.....Total loss: 20.873298227862858\n",
      "Pure loss: 18.159366355919154.....Total loss: 18.159366355919154\n",
      "epoch 2361 learning rate:  0.010423549343498519   Training loss:   20.873298227862858  Valing loss:   18.159366355919154\n",
      "Pure loss: 21.04596944700663.....Total loss: 21.04596944700663\n",
      "Pure loss: 18.153004139632642.....Total loss: 18.153004139632642\n",
      "epoch 2362 learning rate:  0.010423370025402202   Training loss:   21.04596944700663  Valing loss:   18.153004139632642\n",
      "Pure loss: 20.905930886683155.....Total loss: 20.905930886683155\n",
      "Pure loss: 18.116351105280387.....Total loss: 18.116351105280387\n",
      "epoch 2363 learning rate:  0.010423190859077443   Training loss:   20.905930886683155  Valing loss:   18.116351105280387\n",
      "Pure loss: 21.144797211311325.....Total loss: 21.144797211311325\n",
      "Pure loss: 18.037148444350258.....Total loss: 18.037148444350258\n",
      "epoch 2364 learning rate:  0.010423011844331642   Training loss:   21.144797211311325  Valing loss:   18.037148444350258\n",
      "Pure loss: 20.654148606427338.....Total loss: 20.654148606427338\n",
      "Pure loss: 18.319136541920617.....Total loss: 18.319136541920617\n",
      "epoch 2365 learning rate:  0.010422832980972517   Training loss:   20.654148606427338  Valing loss:   18.319136541920617\n",
      "Pure loss: 20.655513689515434.....Total loss: 20.655513689515434\n",
      "Pure loss: 18.31680798843509.....Total loss: 18.31680798843509\n",
      "epoch 2366 learning rate:  0.010422654268808115   Training loss:   20.655513689515434  Valing loss:   18.31680798843509\n",
      "Pure loss: 20.645656266410473.....Total loss: 20.645656266410473\n",
      "Pure loss: 18.33569235847638.....Total loss: 18.33569235847638\n",
      "epoch 2367 learning rate:  0.01042247570764681   Training loss:   20.645656266410473  Valing loss:   18.33569235847638\n",
      "Pure loss: 20.63216212390448.....Total loss: 20.63216212390448\n",
      "Pure loss: 18.38393598986928.....Total loss: 18.38393598986928\n",
      "epoch 2368 learning rate:  0.010422297297297298   Training loss:   20.63216212390448  Valing loss:   18.38393598986928\n",
      "Pure loss: 20.624956936219554.....Total loss: 20.624956936219554\n",
      "Pure loss: 18.41229948802244.....Total loss: 18.41229948802244\n",
      "epoch 2369 learning rate:  0.010422119037568595   Training loss:   20.624956936219554  Valing loss:   18.41229948802244\n",
      "Pure loss: 20.631431485600004.....Total loss: 20.631431485600004\n",
      "Pure loss: 19.09246761980055.....Total loss: 19.09246761980055\n",
      "epoch 2370 learning rate:  0.010421940928270042   Training loss:   20.631431485600004  Valing loss:   19.09246761980055\n",
      "Pure loss: 20.709138218518262.....Total loss: 20.709138218518262\n",
      "Pure loss: 19.347503326333058.....Total loss: 19.347503326333058\n",
      "epoch 2371 learning rate:  0.010421762969211303   Training loss:   20.709138218518262  Valing loss:   19.347503326333058\n",
      "Pure loss: 20.620262224213278.....Total loss: 20.620262224213278\n",
      "Pure loss: 18.994945875983028.....Total loss: 18.994945875983028\n",
      "epoch 2372 learning rate:  0.01042158516020236   Training loss:   20.620262224213278  Valing loss:   18.994945875983028\n",
      "Pure loss: 20.8864268371192.....Total loss: 20.8864268371192\n",
      "Pure loss: 19.86828522019276.....Total loss: 19.86828522019276\n",
      "epoch 2373 learning rate:  0.010421407501053519   Training loss:   20.8864268371192  Valing loss:   19.86828522019276\n",
      "Pure loss: 20.966306772345234.....Total loss: 20.966306772345234\n",
      "Pure loss: 20.039611842397058.....Total loss: 20.039611842397058\n",
      "epoch 2374 learning rate:  0.0104212299915754   Training loss:   20.966306772345234  Valing loss:   20.039611842397058\n",
      "Pure loss: 21.089818494265735.....Total loss: 21.089818494265735\n",
      "Pure loss: 20.25837408640044.....Total loss: 20.25837408640044\n",
      "epoch 2375 learning rate:  0.010421052631578947   Training loss:   21.089818494265735  Valing loss:   20.25837408640044\n",
      "Pure loss: 21.57968279711043.....Total loss: 21.57968279711043\n",
      "Pure loss: 21.128848322172274.....Total loss: 21.128848322172274\n",
      "epoch 2376 learning rate:  0.010420875420875421   Training loss:   21.57968279711043  Valing loss:   21.128848322172274\n",
      "Pure loss: 21.46035294413678.....Total loss: 21.46035294413678\n",
      "Pure loss: 20.923467409497196.....Total loss: 20.923467409497196\n",
      "epoch 2377 learning rate:  0.0104206983592764   Training loss:   21.46035294413678  Valing loss:   20.923467409497196\n",
      "Pure loss: 21.663962203233353.....Total loss: 21.663962203233353\n",
      "Pure loss: 21.26180654193647.....Total loss: 21.26180654193647\n",
      "epoch 2378 learning rate:  0.010420521446593776   Training loss:   21.663962203233353  Valing loss:   21.26180654193647\n",
      "Pure loss: 21.700032874425933.....Total loss: 21.700032874425933\n",
      "Pure loss: 21.315095818339874.....Total loss: 21.315095818339874\n",
      "epoch 2379 learning rate:  0.010420344682639766   Training loss:   21.700032874425933  Valing loss:   21.315095818339874\n",
      "Pure loss: 22.256938385936998.....Total loss: 22.256938385936998\n",
      "Pure loss: 22.118540998329276.....Total loss: 22.118540998329276\n",
      "epoch 2380 learning rate:  0.010420168067226891   Training loss:   22.256938385936998  Valing loss:   22.118540998329276\n",
      "Pure loss: 22.37387110757157.....Total loss: 22.37387110757157\n",
      "Pure loss: 22.27173767216487.....Total loss: 22.27173767216487\n",
      "epoch 2381 learning rate:  0.010419991600167997   Training loss:   22.37387110757157  Valing loss:   22.27173767216487\n",
      "Pure loss: 22.81966103294203.....Total loss: 22.81966103294203\n",
      "Pure loss: 22.928012396858648.....Total loss: 22.928012396858648\n",
      "epoch 2382 learning rate:  0.010419815281276239   Training loss:   22.81966103294203  Valing loss:   22.928012396858648\n",
      "Pure loss: 21.77734168080443.....Total loss: 21.77734168080443\n",
      "Pure loss: 21.346917905202265.....Total loss: 21.346917905202265\n",
      "epoch 2383 learning rate:  0.010419639110365086   Training loss:   21.77734168080443  Valing loss:   21.346917905202265\n",
      "Pure loss: 21.98205482955737.....Total loss: 21.98205482955737\n",
      "Pure loss: 21.666004208379633.....Total loss: 21.666004208379633\n",
      "epoch 2384 learning rate:  0.010419463087248322   Training loss:   21.98205482955737  Valing loss:   21.666004208379633\n",
      "Pure loss: 21.954880378513.....Total loss: 21.954880378513\n",
      "Pure loss: 21.62323875956062.....Total loss: 21.62323875956062\n",
      "epoch 2385 learning rate:  0.010419287211740041   Training loss:   21.954880378513  Valing loss:   21.62323875956062\n",
      "Pure loss: 21.70626907412475.....Total loss: 21.70626907412475\n",
      "Pure loss: 21.218976804862027.....Total loss: 21.218976804862027\n",
      "epoch 2386 learning rate:  0.010419111483654653   Training loss:   21.70626907412475  Valing loss:   21.218976804862027\n",
      "Pure loss: 22.194832168794232.....Total loss: 22.194832168794232\n",
      "Pure loss: 22.015461501284207.....Total loss: 22.015461501284207\n",
      "epoch 2387 learning rate:  0.010418935902806872   Training loss:   22.194832168794232  Valing loss:   22.015461501284207\n",
      "Pure loss: 21.450866327622247.....Total loss: 21.450866327622247\n",
      "Pure loss: 20.840548172210983.....Total loss: 20.840548172210983\n",
      "epoch 2388 learning rate:  0.010418760469011726   Training loss:   21.450866327622247  Valing loss:   20.840548172210983\n",
      "Pure loss: 21.361553635317268.....Total loss: 21.361553635317268\n",
      "Pure loss: 20.698466882320798.....Total loss: 20.698466882320798\n",
      "epoch 2389 learning rate:  0.010418585182084555   Training loss:   21.361553635317268  Valing loss:   20.698466882320798\n",
      "Pure loss: 21.15103568185589.....Total loss: 21.15103568185589\n",
      "Pure loss: 20.346986817489814.....Total loss: 20.346986817489814\n",
      "epoch 2390 learning rate:  0.010418410041841004   Training loss:   21.15103568185589  Valing loss:   20.346986817489814\n",
      "Pure loss: 20.88845087137167.....Total loss: 20.88845087137167\n",
      "Pure loss: 19.899714272791694.....Total loss: 19.899714272791694\n",
      "epoch 2391 learning rate:  0.010418235048097031   Training loss:   20.88845087137167  Valing loss:   19.899714272791694\n",
      "Pure loss: 20.485617780222203.....Total loss: 20.485617780222203\n",
      "Pure loss: 18.875256119120742.....Total loss: 18.875256119120742\n",
      "epoch 2392 learning rate:  0.010418060200668896   Training loss:   20.485617780222203  Valing loss:   18.875256119120742\n",
      "Pure loss: 20.435190685658586.....Total loss: 20.435190685658586\n",
      "Pure loss: 18.179402322773736.....Total loss: 18.179402322773736\n",
      "epoch 2393 learning rate:  0.010417885499373171   Training loss:   20.435190685658586  Valing loss:   18.179402322773736\n",
      "Pure loss: 20.46785648095308.....Total loss: 20.46785648095308\n",
      "Pure loss: 18.10572986123183.....Total loss: 18.10572986123183\n",
      "epoch 2394 learning rate:  0.010417710944026734   Training loss:   20.46785648095308  Valing loss:   18.10572986123183\n",
      "Pure loss: 20.538503731417208.....Total loss: 20.538503731417208\n",
      "Pure loss: 18.06215645548498.....Total loss: 18.06215645548498\n",
      "epoch 2395 learning rate:  0.010417536534446764   Training loss:   20.538503731417208  Valing loss:   18.06215645548498\n",
      "Pure loss: 20.62609054293959.....Total loss: 20.62609054293959\n",
      "Pure loss: 18.03455428691246.....Total loss: 18.03455428691246\n",
      "epoch 2396 learning rate:  0.010417362270450752   Training loss:   20.62609054293959  Valing loss:   18.03455428691246\n",
      "Pure loss: 20.33981516039615.....Total loss: 20.33981516039615\n",
      "Pure loss: 18.315006904761006.....Total loss: 18.315006904761006\n",
      "epoch 2397 learning rate:  0.010417188151856488   Training loss:   20.33981516039615  Valing loss:   18.315006904761006\n",
      "Pure loss: 20.42213837468076.....Total loss: 20.42213837468076\n",
      "Pure loss: 18.180055267452097.....Total loss: 18.180055267452097\n",
      "epoch 2398 learning rate:  0.010417014178482068   Training loss:   20.42213837468076  Valing loss:   18.180055267452097\n",
      "Pure loss: 20.39561561753469.....Total loss: 20.39561561753469\n",
      "Pure loss: 18.28210269191296.....Total loss: 18.28210269191296\n",
      "epoch 2399 learning rate:  0.010416840350145895   Training loss:   20.39561561753469  Valing loss:   18.28210269191296\n",
      "Pure loss: 20.365087332222224.....Total loss: 20.365087332222224\n",
      "Pure loss: 18.691223976474337.....Total loss: 18.691223976474337\n",
      "epoch 2400 learning rate:  0.010416666666666666   Training loss:   20.365087332222224  Valing loss:   18.691223976474337\n",
      "Pure loss: 20.367436411136755.....Total loss: 20.367436411136755\n",
      "Pure loss: 18.758201797007214.....Total loss: 18.758201797007214\n",
      "epoch 2401 learning rate:  0.01041649312786339   Training loss:   20.367436411136755  Valing loss:   18.758201797007214\n",
      "Pure loss: 20.429252815494053.....Total loss: 20.429252815494053\n",
      "Pure loss: 18.99167532258185.....Total loss: 18.99167532258185\n",
      "epoch 2402 learning rate:  0.010416319733555371   Training loss:   20.429252815494053  Valing loss:   18.99167532258185\n",
      "Pure loss: 20.408621762469284.....Total loss: 20.408621762469284\n",
      "Pure loss: 18.925882296875923.....Total loss: 18.925882296875923\n",
      "epoch 2403 learning rate:  0.010416146483562214   Training loss:   20.408621762469284  Valing loss:   18.925882296875923\n",
      "Pure loss: 20.50311794588206.....Total loss: 20.50311794588206\n",
      "Pure loss: 19.025991656081516.....Total loss: 19.025991656081516\n",
      "epoch 2404 learning rate:  0.010415973377703827   Training loss:   20.50311794588206  Valing loss:   19.025991656081516\n",
      "Pure loss: 22.100395590074655.....Total loss: 22.100395590074655\n",
      "Pure loss: 22.020902587280666.....Total loss: 22.020902587280666\n",
      "epoch 2405 learning rate:  0.010415800415800416   Training loss:   22.100395590074655  Valing loss:   22.020902587280666\n",
      "Pure loss: 22.711009198291105.....Total loss: 22.711009198291105\n",
      "Pure loss: 22.908251775209227.....Total loss: 22.908251775209227\n",
      "epoch 2406 learning rate:  0.010415627597672486   Training loss:   22.711009198291105  Valing loss:   22.908251775209227\n",
      "Pure loss: 21.80636476718349.....Total loss: 21.80636476718349\n",
      "Pure loss: 21.529986636246466.....Total loss: 21.529986636246466\n",
      "epoch 2407 learning rate:  0.01041545492314084   Training loss:   21.80636476718349  Valing loss:   21.529986636246466\n",
      "Pure loss: 21.6181415188149.....Total loss: 21.6181415188149\n",
      "Pure loss: 21.26414887921915.....Total loss: 21.26414887921915\n",
      "epoch 2408 learning rate:  0.010415282392026578   Training loss:   21.6181415188149  Valing loss:   21.26414887921915\n",
      "Pure loss: 22.58245613125068.....Total loss: 22.58245613125068\n",
      "Pure loss: 22.599213029844698.....Total loss: 22.599213029844698\n",
      "epoch 2409 learning rate:  0.0104151100041511   Training loss:   22.58245613125068  Valing loss:   22.599213029844698\n",
      "Pure loss: 22.63745105140428.....Total loss: 22.63745105140428\n",
      "Pure loss: 22.66471999656413.....Total loss: 22.66471999656413\n",
      "epoch 2410 learning rate:  0.0104149377593361   Training loss:   22.63745105140428  Valing loss:   22.66471999656413\n",
      "Pure loss: 22.511946376604207.....Total loss: 22.511946376604207\n",
      "Pure loss: 22.49543383412886.....Total loss: 22.49543383412886\n",
      "epoch 2411 learning rate:  0.010414765657403567   Training loss:   22.511946376604207  Valing loss:   22.49543383412886\n",
      "Pure loss: 24.05146238648166.....Total loss: 24.05146238648166\n",
      "Pure loss: 24.440429179675263.....Total loss: 24.440429179675263\n",
      "epoch 2412 learning rate:  0.010414593698175788   Training loss:   24.05146238648166  Valing loss:   24.440429179675263\n",
      "Pure loss: 23.876393258234096.....Total loss: 23.876393258234096\n",
      "Pure loss: 24.203370396387363.....Total loss: 24.203370396387363\n",
      "epoch 2413 learning rate:  0.010414421881475342   Training loss:   23.876393258234096  Valing loss:   24.203370396387363\n",
      "Pure loss: 22.986093248470254.....Total loss: 22.986093248470254\n",
      "Pure loss: 23.041057062788987.....Total loss: 23.041057062788987\n",
      "epoch 2414 learning rate:  0.010414250207125103   Training loss:   22.986093248470254  Valing loss:   23.041057062788987\n",
      "Pure loss: 22.446278762597647.....Total loss: 22.446278762597647\n",
      "Pure loss: 22.23306665396704.....Total loss: 22.23306665396704\n",
      "epoch 2415 learning rate:  0.01041407867494824   Training loss:   22.446278762597647  Valing loss:   22.23306665396704\n",
      "Pure loss: 22.0010237233148.....Total loss: 22.0010237233148\n",
      "Pure loss: 21.53181403927353.....Total loss: 21.53181403927353\n",
      "epoch 2416 learning rate:  0.010413907284768213   Training loss:   22.0010237233148  Valing loss:   21.53181403927353\n",
      "Pure loss: 21.592223006332066.....Total loss: 21.592223006332066\n",
      "Pure loss: 20.919888860624297.....Total loss: 20.919888860624297\n",
      "epoch 2417 learning rate:  0.010413736036408772   Training loss:   21.592223006332066  Valing loss:   20.919888860624297\n",
      "Pure loss: 21.327511176717174.....Total loss: 21.327511176717174\n",
      "Pure loss: 20.423448935447997.....Total loss: 20.423448935447997\n",
      "epoch 2418 learning rate:  0.010413564929693963   Training loss:   21.327511176717174  Valing loss:   20.423448935447997\n",
      "Pure loss: 20.995115134175578.....Total loss: 20.995115134175578\n",
      "Pure loss: 19.81768270313902.....Total loss: 19.81768270313902\n",
      "epoch 2419 learning rate:  0.010413393964448119   Training loss:   20.995115134175578  Valing loss:   19.81768270313902\n",
      "Pure loss: 21.015254328537175.....Total loss: 21.015254328537175\n",
      "Pure loss: 19.862241383272657.....Total loss: 19.862241383272657\n",
      "epoch 2420 learning rate:  0.010413223140495868   Training loss:   21.015254328537175  Valing loss:   19.862241383272657\n",
      "Pure loss: 20.895209602802367.....Total loss: 20.895209602802367\n",
      "Pure loss: 19.650243417339478.....Total loss: 19.650243417339478\n",
      "epoch 2421 learning rate:  0.010413052457662123   Training loss:   20.895209602802367  Valing loss:   19.650243417339478\n",
      "Pure loss: 20.681844207462415.....Total loss: 20.681844207462415\n",
      "Pure loss: 19.29951610592558.....Total loss: 19.29951610592558\n",
      "epoch 2422 learning rate:  0.010412881915772089   Training loss:   20.681844207462415  Valing loss:   19.29951610592558\n",
      "Pure loss: 20.586802474402106.....Total loss: 20.586802474402106\n",
      "Pure loss: 19.0097624297681.....Total loss: 19.0097624297681\n",
      "epoch 2423 learning rate:  0.010412711514651259   Training loss:   20.586802474402106  Valing loss:   19.0097624297681\n",
      "Pure loss: 22.054616200405103.....Total loss: 22.054616200405103\n",
      "Pure loss: 21.909226218576915.....Total loss: 21.909226218576915\n",
      "epoch 2424 learning rate:  0.010412541254125413   Training loss:   22.054616200405103  Valing loss:   21.909226218576915\n",
      "Pure loss: 21.955326324083366.....Total loss: 21.955326324083366\n",
      "Pure loss: 21.76141852395725.....Total loss: 21.76141852395725\n",
      "epoch 2425 learning rate:  0.01041237113402062   Training loss:   21.955326324083366  Valing loss:   21.76141852395725\n",
      "Pure loss: 22.503252039158266.....Total loss: 22.503252039158266\n",
      "Pure loss: 22.573237737935806.....Total loss: 22.573237737935806\n",
      "epoch 2426 learning rate:  0.010412201154163232   Training loss:   22.503252039158266  Valing loss:   22.573237737935806\n",
      "Pure loss: 21.517743326997234.....Total loss: 21.517743326997234\n",
      "Pure loss: 21.047835633986086.....Total loss: 21.047835633986086\n",
      "epoch 2427 learning rate:  0.010412031314379892   Training loss:   21.517743326997234  Valing loss:   21.047835633986086\n",
      "Pure loss: 20.76816367487186.....Total loss: 20.76816367487186\n",
      "Pure loss: 19.698926324101375.....Total loss: 19.698926324101375\n",
      "epoch 2428 learning rate:  0.010411861614497529   Training loss:   20.76816367487186  Valing loss:   19.698926324101375\n",
      "Pure loss: 20.51418548406448.....Total loss: 20.51418548406448\n",
      "Pure loss: 19.146160188678422.....Total loss: 19.146160188678422\n",
      "epoch 2429 learning rate:  0.010411692054343351   Training loss:   20.51418548406448  Valing loss:   19.146160188678422\n",
      "Pure loss: 20.69249146482786.....Total loss: 20.69249146482786\n",
      "Pure loss: 19.557828609055427.....Total loss: 19.557828609055427\n",
      "epoch 2430 learning rate:  0.010411522633744856   Training loss:   20.69249146482786  Valing loss:   19.557828609055427\n",
      "Pure loss: 20.534804783735165.....Total loss: 20.534804783735165\n",
      "Pure loss: 19.27332967685416.....Total loss: 19.27332967685416\n",
      "epoch 2431 learning rate:  0.010411353352529823   Training loss:   20.534804783735165  Valing loss:   19.27332967685416\n",
      "Pure loss: 20.219156954574853.....Total loss: 20.219156954574853\n",
      "Pure loss: 18.418862064603346.....Total loss: 18.418862064603346\n",
      "epoch 2432 learning rate:  0.010411184210526316   Training loss:   20.219156954574853  Valing loss:   18.418862064603346\n",
      "Pure loss: 20.876122752783008.....Total loss: 20.876122752783008\n",
      "Pure loss: 19.823349965160354.....Total loss: 19.823349965160354\n",
      "epoch 2433 learning rate:  0.01041101520756268   Training loss:   20.876122752783008  Valing loss:   19.823349965160354\n",
      "Pure loss: 20.35561158548687.....Total loss: 20.35561158548687\n",
      "Pure loss: 19.055349868146706.....Total loss: 19.055349868146706\n",
      "epoch 2434 learning rate:  0.010410846343467544   Training loss:   20.35561158548687  Valing loss:   19.055349868146706\n",
      "Pure loss: 20.209365390026253.....Total loss: 20.209365390026253\n",
      "Pure loss: 18.6046663495188.....Total loss: 18.6046663495188\n",
      "epoch 2435 learning rate:  0.010410677618069816   Training loss:   20.209365390026253  Valing loss:   18.6046663495188\n",
      "Pure loss: 20.346140676569213.....Total loss: 20.346140676569213\n",
      "Pure loss: 18.969487195610846.....Total loss: 18.969487195610846\n",
      "epoch 2436 learning rate:  0.010410509031198686   Training loss:   20.346140676569213  Valing loss:   18.969487195610846\n",
      "Pure loss: 20.736194924217962.....Total loss: 20.736194924217962\n",
      "Pure loss: 19.960809252585.....Total loss: 19.960809252585\n",
      "epoch 2437 learning rate:  0.010410340582683628   Training loss:   20.736194924217962  Valing loss:   19.960809252585\n",
      "Pure loss: 20.61660696031464.....Total loss: 20.61660696031464\n",
      "Pure loss: 19.756756898103692.....Total loss: 19.756756898103692\n",
      "epoch 2438 learning rate:  0.01041017227235439   Training loss:   20.61660696031464  Valing loss:   19.756756898103692\n",
      "Pure loss: 20.45453266815745.....Total loss: 20.45453266815745\n",
      "Pure loss: 19.47630875516045.....Total loss: 19.47630875516045\n",
      "epoch 2439 learning rate:  0.010410004100041   Training loss:   20.45453266815745  Valing loss:   19.47630875516045\n",
      "Pure loss: 21.098934309503868.....Total loss: 21.098934309503868\n",
      "Pure loss: 20.65011824301706.....Total loss: 20.65011824301706\n",
      "epoch 2440 learning rate:  0.01040983606557377   Training loss:   21.098934309503868  Valing loss:   20.65011824301706\n",
      "Pure loss: 20.98788273721967.....Total loss: 20.98788273721967\n",
      "Pure loss: 20.463040626365434.....Total loss: 20.463040626365434\n",
      "epoch 2441 learning rate:  0.010409668168783286   Training loss:   20.98788273721967  Valing loss:   20.463040626365434\n",
      "Pure loss: 20.77234995975775.....Total loss: 20.77234995975775\n",
      "Pure loss: 20.122139189106388.....Total loss: 20.122139189106388\n",
      "epoch 2442 learning rate:  0.01040950040950041   Training loss:   20.77234995975775  Valing loss:   20.122139189106388\n",
      "Pure loss: 20.509314954004.....Total loss: 20.509314954004\n",
      "Pure loss: 19.693244440053828.....Total loss: 19.693244440053828\n",
      "epoch 2443 learning rate:  0.010409332787556283   Training loss:   20.509314954004  Valing loss:   19.693244440053828\n",
      "Pure loss: 20.582390279445534.....Total loss: 20.582390279445534\n",
      "Pure loss: 19.821196000880324.....Total loss: 19.821196000880324\n",
      "epoch 2444 learning rate:  0.010409165302782325   Training loss:   20.582390279445534  Valing loss:   19.821196000880324\n",
      "Pure loss: 20.42705114756509.....Total loss: 20.42705114756509\n",
      "Pure loss: 19.53595617309657.....Total loss: 19.53595617309657\n",
      "epoch 2445 learning rate:  0.010408997955010225   Training loss:   20.42705114756509  Valing loss:   19.53595617309657\n",
      "Pure loss: 20.515882710771315.....Total loss: 20.515882710771315\n",
      "Pure loss: 19.71611142570762.....Total loss: 19.71611142570762\n",
      "epoch 2446 learning rate:  0.010408830744071954   Training loss:   20.515882710771315  Valing loss:   19.71611142570762\n",
      "Pure loss: 20.3218846194839.....Total loss: 20.3218846194839\n",
      "Pure loss: 19.359675501922403.....Total loss: 19.359675501922403\n",
      "epoch 2447 learning rate:  0.010408663669799756   Training loss:   20.3218846194839  Valing loss:   19.359675501922403\n",
      "Pure loss: 20.00793561895019.....Total loss: 20.00793561895019\n",
      "Pure loss: 18.591197216267883.....Total loss: 18.591197216267883\n",
      "epoch 2448 learning rate:  0.010408496732026144   Training loss:   20.00793561895019  Valing loss:   18.591197216267883\n",
      "Pure loss: 19.958118315521688.....Total loss: 19.958118315521688\n",
      "Pure loss: 18.349965686417086.....Total loss: 18.349965686417086\n",
      "epoch 2449 learning rate:  0.010408329930583913   Training loss:   19.958118315521688  Valing loss:   18.349965686417086\n",
      "Pure loss: 19.95717095760844.....Total loss: 19.95717095760844\n",
      "Pure loss: 18.413954064118183.....Total loss: 18.413954064118183\n",
      "epoch 2450 learning rate:  0.010408163265306122   Training loss:   19.95717095760844  Valing loss:   18.413954064118183\n",
      "Pure loss: 19.92600141265654.....Total loss: 19.92600141265654\n",
      "Pure loss: 18.196398273227356.....Total loss: 18.196398273227356\n",
      "epoch 2451 learning rate:  0.010407996736026113   Training loss:   19.92600141265654  Valing loss:   18.196398273227356\n",
      "Pure loss: 19.95566277274291.....Total loss: 19.95566277274291\n",
      "Pure loss: 18.005192796024378.....Total loss: 18.005192796024378\n",
      "epoch 2452 learning rate:  0.010407830342577489   Training loss:   19.95566277274291  Valing loss:   18.005192796024378\n",
      "Pure loss: 19.97857867527986.....Total loss: 19.97857867527986\n",
      "Pure loss: 17.929116263546724.....Total loss: 17.929116263546724\n",
      "epoch 2453 learning rate:  0.01040766408479413   Training loss:   19.97857867527986  Valing loss:   17.929116263546724\n",
      "Pure loss: 19.909565255443496.....Total loss: 19.909565255443496\n",
      "Pure loss: 18.36448646913005.....Total loss: 18.36448646913005\n",
      "epoch 2454 learning rate:  0.010407497962510188   Training loss:   19.909565255443496  Valing loss:   18.36448646913005\n",
      "Pure loss: 19.944824611235408.....Total loss: 19.944824611235408\n",
      "Pure loss: 18.50481149807059.....Total loss: 18.50481149807059\n",
      "epoch 2455 learning rate:  0.010407331975560081   Training loss:   19.944824611235408  Valing loss:   18.50481149807059\n",
      "Pure loss: 19.92141365516302.....Total loss: 19.92141365516302\n",
      "Pure loss: 18.42068971229077.....Total loss: 18.42068971229077\n",
      "epoch 2456 learning rate:  0.010407166123778502   Training loss:   19.92141365516302  Valing loss:   18.42068971229077\n",
      "Pure loss: 20.04482337789572.....Total loss: 20.04482337789572\n",
      "Pure loss: 18.738323468425907.....Total loss: 18.738323468425907\n",
      "epoch 2457 learning rate:  0.010407000407000407   Training loss:   20.04482337789572  Valing loss:   18.738323468425907\n",
      "Pure loss: 20.31842976061634.....Total loss: 20.31842976061634\n",
      "Pure loss: 19.268269158820083.....Total loss: 19.268269158820083\n",
      "epoch 2458 learning rate:  0.010406834825061025   Training loss:   20.31842976061634  Valing loss:   19.268269158820083\n",
      "Pure loss: 20.130524503891245.....Total loss: 20.130524503891245\n",
      "Pure loss: 18.77485249871182.....Total loss: 18.77485249871182\n",
      "epoch 2459 learning rate:  0.010406669377795852   Training loss:   20.130524503891245  Valing loss:   18.77485249871182\n",
      "Pure loss: 20.0350954676344.....Total loss: 20.0350954676344\n",
      "Pure loss: 18.227725257295813.....Total loss: 18.227725257295813\n",
      "epoch 2460 learning rate:  0.01040650406504065   Training loss:   20.0350954676344  Valing loss:   18.227725257295813\n",
      "Pure loss: 20.034643391593605.....Total loss: 20.034643391593605\n",
      "Pure loss: 18.23561192142617.....Total loss: 18.23561192142617\n",
      "epoch 2461 learning rate:  0.010406338886631452   Training loss:   20.034643391593605  Valing loss:   18.23561192142617\n",
      "Pure loss: 20.152900747599144.....Total loss: 20.152900747599144\n",
      "Pure loss: 18.870278184447745.....Total loss: 18.870278184447745\n",
      "epoch 2462 learning rate:  0.01040617384240455   Training loss:   20.152900747599144  Valing loss:   18.870278184447745\n",
      "Pure loss: 19.91129122566037.....Total loss: 19.91129122566037\n",
      "Pure loss: 18.13960826159758.....Total loss: 18.13960826159758\n",
      "epoch 2463 learning rate:  0.010406008932196509   Training loss:   19.91129122566037  Valing loss:   18.13960826159758\n",
      "Pure loss: 19.97123938201216.....Total loss: 19.97123938201216\n",
      "Pure loss: 17.748082614010574.....Total loss: 17.748082614010574\n",
      "epoch 2464 learning rate:  0.010405844155844156   Training loss:   19.97123938201216  Valing loss:   17.748082614010574\n",
      "Pure loss: 20.112959528190803.....Total loss: 20.112959528190803\n",
      "Pure loss: 17.564508651887667.....Total loss: 17.564508651887667\n",
      "epoch 2465 learning rate:  0.010405679513184584   Training loss:   20.112959528190803  Valing loss:   17.564508651887667\n",
      "Pure loss: 20.05640491435106.....Total loss: 20.05640491435106\n",
      "Pure loss: 17.567030552338068.....Total loss: 17.567030552338068\n",
      "epoch 2466 learning rate:  0.010405515004055151   Training loss:   20.05640491435106  Valing loss:   17.567030552338068\n",
      "Pure loss: 20.050088931741822.....Total loss: 20.050088931741822\n",
      "Pure loss: 17.570457354775787.....Total loss: 17.570457354775787\n",
      "epoch 2467 learning rate:  0.010405350628293474   Training loss:   20.050088931741822  Valing loss:   17.570457354775787\n",
      "Pure loss: 19.913907340348945.....Total loss: 19.913907340348945\n",
      "Pure loss: 17.658899839361524.....Total loss: 17.658899839361524\n",
      "epoch 2468 learning rate:  0.010405186385737439   Training loss:   19.913907340348945  Valing loss:   17.658899839361524\n",
      "Pure loss: 19.946016348542944.....Total loss: 19.946016348542944\n",
      "Pure loss: 17.6015767951442.....Total loss: 17.6015767951442\n",
      "epoch 2469 learning rate:  0.010405022276225192   Training loss:   19.946016348542944  Valing loss:   17.6015767951442\n",
      "Pure loss: 20.052708070318104.....Total loss: 20.052708070318104\n",
      "Pure loss: 17.497960519572228.....Total loss: 17.497960519572228\n",
      "epoch 2470 learning rate:  0.010404858299595142   Training loss:   20.052708070318104  Valing loss:   17.497960519572228\n",
      "Pure loss: 19.87190116222002.....Total loss: 19.87190116222002\n",
      "Pure loss: 17.94084514213577.....Total loss: 17.94084514213577\n",
      "epoch 2471 learning rate:  0.010404694455685957   Training loss:   19.87190116222002  Valing loss:   17.94084514213577\n",
      "Pure loss: 19.79717132682578.....Total loss: 19.79717132682578\n",
      "Pure loss: 17.90917909191769.....Total loss: 17.90917909191769\n",
      "epoch 2472 learning rate:  0.01040453074433657   Training loss:   19.79717132682578  Valing loss:   17.90917909191769\n",
      "Pure loss: 19.764298682421753.....Total loss: 19.764298682421753\n",
      "Pure loss: 18.12337751585778.....Total loss: 18.12337751585778\n",
      "epoch 2473 learning rate:  0.010404367165386171   Training loss:   19.764298682421753  Valing loss:   18.12337751585778\n",
      "Pure loss: 19.80770409279659.....Total loss: 19.80770409279659\n",
      "Pure loss: 18.283402265170093.....Total loss: 18.283402265170093\n",
      "epoch 2474 learning rate:  0.010404203718674211   Training loss:   19.80770409279659  Valing loss:   18.283402265170093\n",
      "Pure loss: 19.735660082432453.....Total loss: 19.735660082432453\n",
      "Pure loss: 17.943197321437033.....Total loss: 17.943197321437033\n",
      "epoch 2475 learning rate:  0.010404040404040405   Training loss:   19.735660082432453  Valing loss:   17.943197321437033\n",
      "Pure loss: 19.941016266735662.....Total loss: 19.941016266735662\n",
      "Pure loss: 18.705783754290337.....Total loss: 18.705783754290337\n",
      "epoch 2476 learning rate:  0.010403877221324718   Training loss:   19.941016266735662  Valing loss:   18.705783754290337\n",
      "Pure loss: 20.07930864554663.....Total loss: 20.07930864554663\n",
      "Pure loss: 19.030926237717296.....Total loss: 19.030926237717296\n",
      "epoch 2477 learning rate:  0.01040371417036738   Training loss:   20.07930864554663  Valing loss:   19.030926237717296\n",
      "Pure loss: 20.042673372628013.....Total loss: 20.042673372628013\n",
      "Pure loss: 18.954275207157135.....Total loss: 18.954275207157135\n",
      "epoch 2478 learning rate:  0.010403551251008879   Training loss:   20.042673372628013  Valing loss:   18.954275207157135\n",
      "Pure loss: 20.00988833018747.....Total loss: 20.00988833018747\n",
      "Pure loss: 18.878908861636273.....Total loss: 18.878908861636273\n",
      "epoch 2479 learning rate:  0.010403388463089957   Training loss:   20.00988833018747  Valing loss:   18.878908861636273\n",
      "Pure loss: 20.075045714874992.....Total loss: 20.075045714874992\n",
      "Pure loss: 19.019964622811123.....Total loss: 19.019964622811123\n",
      "epoch 2480 learning rate:  0.010403225806451612   Training loss:   20.075045714874992  Valing loss:   19.019964622811123\n",
      "Pure loss: 20.152692161638065.....Total loss: 20.152692161638065\n",
      "Pure loss: 19.173015076681665.....Total loss: 19.173015076681665\n",
      "epoch 2481 learning rate:  0.010403063280935107   Training loss:   20.152692161638065  Valing loss:   19.173015076681665\n",
      "Pure loss: 20.233083691821744.....Total loss: 20.233083691821744\n",
      "Pure loss: 19.325279633495494.....Total loss: 19.325279633495494\n",
      "epoch 2482 learning rate:  0.01040290088638195   Training loss:   20.233083691821744  Valing loss:   19.325279633495494\n",
      "Pure loss: 20.110599934329258.....Total loss: 20.110599934329258\n",
      "Pure loss: 19.069916641341194.....Total loss: 19.069916641341194\n",
      "epoch 2483 learning rate:  0.010402738622633911   Training loss:   20.110599934329258  Valing loss:   19.069916641341194\n",
      "Pure loss: 20.089057496636133.....Total loss: 20.089057496636133\n",
      "Pure loss: 19.022271211278262.....Total loss: 19.022271211278262\n",
      "epoch 2484 learning rate:  0.010402576489533012   Training loss:   20.089057496636133  Valing loss:   19.022271211278262\n",
      "Pure loss: 19.994569318744368.....Total loss: 19.994569318744368\n",
      "Pure loss: 18.832050423829514.....Total loss: 18.832050423829514\n",
      "epoch 2485 learning rate:  0.01040241448692153   Training loss:   19.994569318744368  Valing loss:   18.832050423829514\n",
      "Pure loss: 19.74713259990509.....Total loss: 19.74713259990509\n",
      "Pure loss: 18.091574237578953.....Total loss: 18.091574237578953\n",
      "epoch 2486 learning rate:  0.010402252614641996   Training loss:   19.74713259990509  Valing loss:   18.091574237578953\n",
      "Pure loss: 19.74166345733885.....Total loss: 19.74166345733885\n",
      "Pure loss: 18.092207728684507.....Total loss: 18.092207728684507\n",
      "epoch 2487 learning rate:  0.010402090872537193   Training loss:   19.74166345733885  Valing loss:   18.092207728684507\n",
      "Pure loss: 19.74314314595816.....Total loss: 19.74314314595816\n",
      "Pure loss: 18.2046125718995.....Total loss: 18.2046125718995\n",
      "epoch 2488 learning rate:  0.01040192926045016   Training loss:   19.74314314595816  Valing loss:   18.2046125718995\n",
      "Pure loss: 19.76295923450246.....Total loss: 19.76295923450246\n",
      "Pure loss: 18.30488627862394.....Total loss: 18.30488627862394\n",
      "epoch 2489 learning rate:  0.010401767778224186   Training loss:   19.76295923450246  Valing loss:   18.30488627862394\n",
      "Pure loss: 19.777583731050555.....Total loss: 19.777583731050555\n",
      "Pure loss: 18.341089412678695.....Total loss: 18.341089412678695\n",
      "epoch 2490 learning rate:  0.010401606425702812   Training loss:   19.777583731050555  Valing loss:   18.341089412678695\n",
      "Pure loss: 19.723097562533507.....Total loss: 19.723097562533507\n",
      "Pure loss: 18.161958173800556.....Total loss: 18.161958173800556\n",
      "epoch 2491 learning rate:  0.010401445202729828   Training loss:   19.723097562533507  Valing loss:   18.161958173800556\n",
      "Pure loss: 19.705635837858658.....Total loss: 19.705635837858658\n",
      "Pure loss: 18.09556867539331.....Total loss: 18.09556867539331\n",
      "epoch 2492 learning rate:  0.010401284109149278   Training loss:   19.705635837858658  Valing loss:   18.09556867539331\n",
      "Pure loss: 19.66350410730489.....Total loss: 19.66350410730489\n",
      "Pure loss: 17.73766247492891.....Total loss: 17.73766247492891\n",
      "epoch 2493 learning rate:  0.010401123144805456   Training loss:   19.66350410730489  Valing loss:   17.73766247492891\n",
      "Pure loss: 19.720541599921166.....Total loss: 19.720541599921166\n",
      "Pure loss: 17.487625353302196.....Total loss: 17.487625353302196\n",
      "epoch 2494 learning rate:  0.010400962309542903   Training loss:   19.720541599921166  Valing loss:   17.487625353302196\n",
      "Pure loss: 19.715333379694748.....Total loss: 19.715333379694748\n",
      "Pure loss: 17.495128349009146.....Total loss: 17.495128349009146\n",
      "epoch 2495 learning rate:  0.010400801603206413   Training loss:   19.715333379694748  Valing loss:   17.495128349009146\n",
      "Pure loss: 19.713654369865566.....Total loss: 19.713654369865566\n",
      "Pure loss: 18.181017614562926.....Total loss: 18.181017614562926\n",
      "epoch 2496 learning rate:  0.010400641025641026   Training loss:   19.713654369865566  Valing loss:   18.181017614562926\n",
      "Pure loss: 19.770622189358843.....Total loss: 19.770622189358843\n",
      "Pure loss: 18.350121551992604.....Total loss: 18.350121551992604\n",
      "epoch 2497 learning rate:  0.010400480576692031   Training loss:   19.770622189358843  Valing loss:   18.350121551992604\n",
      "Pure loss: 19.61701200973812.....Total loss: 19.61701200973812\n",
      "Pure loss: 17.865554655461835.....Total loss: 17.865554655461835\n",
      "epoch 2498 learning rate:  0.010400320256204965   Training loss:   19.61701200973812  Valing loss:   17.865554655461835\n",
      "Pure loss: 19.717605665564193.....Total loss: 19.717605665564193\n",
      "Pure loss: 18.339630890584463.....Total loss: 18.339630890584463\n",
      "epoch 2499 learning rate:  0.01040016006402561   Training loss:   19.717605665564193  Valing loss:   18.339630890584463\n",
      "Pure loss: 19.72064239801443.....Total loss: 19.72064239801443\n",
      "Pure loss: 18.34791311034146.....Total loss: 18.34791311034146\n",
      "epoch 2500 learning rate:  0.0104   Training loss:   19.72064239801443  Valing loss:   18.34791311034146\n",
      "Pure loss: 19.706847935226527.....Total loss: 19.706847935226527\n",
      "Pure loss: 18.30711262100011.....Total loss: 18.30711262100011\n",
      "epoch 2501 learning rate:  0.01039984006397441   Training loss:   19.706847935226527  Valing loss:   18.30711262100011\n",
      "Pure loss: 19.881688511696026.....Total loss: 19.881688511696026\n",
      "Pure loss: 18.691224543340468.....Total loss: 18.691224543340468\n",
      "epoch 2502 learning rate:  0.010399680255795364   Training loss:   19.881688511696026  Valing loss:   18.691224543340468\n",
      "Pure loss: 19.626862908240636.....Total loss: 19.626862908240636\n",
      "Pure loss: 17.937500473411358.....Total loss: 17.937500473411358\n",
      "epoch 2503 learning rate:  0.010399520575309629   Training loss:   19.626862908240636  Valing loss:   17.937500473411358\n",
      "Pure loss: 19.989958917785305.....Total loss: 19.989958917785305\n",
      "Pure loss: 19.01431184837606.....Total loss: 19.01431184837606\n",
      "epoch 2504 learning rate:  0.010399361022364217   Training loss:   19.989958917785305  Valing loss:   19.01431184837606\n",
      "Pure loss: 19.938112373965843.....Total loss: 19.938112373965843\n",
      "Pure loss: 18.933930624174256.....Total loss: 18.933930624174256\n",
      "epoch 2505 learning rate:  0.010399201596806388   Training loss:   19.938112373965843  Valing loss:   18.933930624174256\n",
      "Pure loss: 19.911202049682142.....Total loss: 19.911202049682142\n",
      "Pure loss: 18.87179404075247.....Total loss: 18.87179404075247\n",
      "epoch 2506 learning rate:  0.01039904229848364   Training loss:   19.911202049682142  Valing loss:   18.87179404075247\n",
      "Pure loss: 19.918639844506018.....Total loss: 19.918639844506018\n",
      "Pure loss: 18.88468921426138.....Total loss: 18.88468921426138\n",
      "epoch 2507 learning rate:  0.010398883127243718   Training loss:   19.918639844506018  Valing loss:   18.88468921426138\n",
      "Pure loss: 20.021731662064184.....Total loss: 20.021731662064184\n",
      "Pure loss: 19.087702369317473.....Total loss: 19.087702369317473\n",
      "epoch 2508 learning rate:  0.010398724082934609   Training loss:   20.021731662064184  Valing loss:   19.087702369317473\n",
      "Pure loss: 19.982470405058578.....Total loss: 19.982470405058578\n",
      "Pure loss: 19.01490416001354.....Total loss: 19.01490416001354\n",
      "epoch 2509 learning rate:  0.010398565165404544   Training loss:   19.982470405058578  Valing loss:   19.01490416001354\n",
      "Pure loss: 19.61098827030369.....Total loss: 19.61098827030369\n",
      "Pure loss: 18.129507770121464.....Total loss: 18.129507770121464\n",
      "epoch 2510 learning rate:  0.010398406374501993   Training loss:   19.61098827030369  Valing loss:   18.129507770121464\n",
      "Pure loss: 19.615099531504328.....Total loss: 19.615099531504328\n",
      "Pure loss: 18.14156040400592.....Total loss: 18.14156040400592\n",
      "epoch 2511 learning rate:  0.010398247710075668   Training loss:   19.615099531504328  Valing loss:   18.14156040400592\n",
      "Pure loss: 19.792172817974354.....Total loss: 19.792172817974354\n",
      "Pure loss: 18.619574179303914.....Total loss: 18.619574179303914\n",
      "epoch 2512 learning rate:  0.010398089171974522   Training loss:   19.792172817974354  Valing loss:   18.619574179303914\n",
      "Pure loss: 20.01315416663256.....Total loss: 20.01315416663256\n",
      "Pure loss: 18.857952936010857.....Total loss: 18.857952936010857\n",
      "epoch 2513 learning rate:  0.010397930760047752   Training loss:   20.01315416663256  Valing loss:   18.857952936010857\n",
      "Pure loss: 19.732490555709546.....Total loss: 19.732490555709546\n",
      "Pure loss: 18.20508960616899.....Total loss: 18.20508960616899\n",
      "epoch 2514 learning rate:  0.010397772474144789   Training loss:   19.732490555709546  Valing loss:   18.20508960616899\n",
      "Pure loss: 19.518759499802314.....Total loss: 19.518759499802314\n",
      "Pure loss: 17.732729770601217.....Total loss: 17.732729770601217\n",
      "epoch 2515 learning rate:  0.010397614314115308   Training loss:   19.518759499802314  Valing loss:   17.732729770601217\n",
      "Pure loss: 19.55636260125779.....Total loss: 19.55636260125779\n",
      "Pure loss: 17.317573113629216.....Total loss: 17.317573113629216\n",
      "epoch 2516 learning rate:  0.010397456279809222   Training loss:   19.55636260125779  Valing loss:   17.317573113629216\n",
      "Pure loss: 19.568123055946714.....Total loss: 19.568123055946714\n",
      "Pure loss: 17.29585475225854.....Total loss: 17.29585475225854\n",
      "epoch 2517 learning rate:  0.010397298371076679   Training loss:   19.568123055946714  Valing loss:   17.29585475225854\n",
      "Pure loss: 19.689614158803224.....Total loss: 19.689614158803224\n",
      "Pure loss: 17.165502520287564.....Total loss: 17.165502520287564\n",
      "epoch 2518 learning rate:  0.01039714058776807   Training loss:   19.689614158803224  Valing loss:   17.165502520287564\n",
      "Pure loss: 19.95080563091571.....Total loss: 19.95080563091571\n",
      "Pure loss: 17.02957198548184.....Total loss: 17.02957198548184\n",
      "epoch 2519 learning rate:  0.010396982929734021   Training loss:   19.95080563091571  Valing loss:   17.02957198548184\n",
      "Pure loss: 19.959273017603756.....Total loss: 19.959273017603756\n",
      "Pure loss: 18.04108585965679.....Total loss: 18.04108585965679\n",
      "epoch 2520 learning rate:  0.010396825396825397   Training loss:   19.959273017603756  Valing loss:   18.04108585965679\n",
      "Pure loss: 19.675990538619573.....Total loss: 19.675990538619573\n",
      "Pure loss: 17.415624861351073.....Total loss: 17.415624861351073\n",
      "epoch 2521 learning rate:  0.010396667988893297   Training loss:   19.675990538619573  Valing loss:   17.415624861351073\n",
      "Pure loss: 19.692557474815406.....Total loss: 19.692557474815406\n",
      "Pure loss: 18.02756321939085.....Total loss: 18.02756321939085\n",
      "epoch 2522 learning rate:  0.010396510705789057   Training loss:   19.692557474815406  Valing loss:   18.02756321939085\n",
      "Pure loss: 19.62449901074324.....Total loss: 19.62449901074324\n",
      "Pure loss: 17.88946838174497.....Total loss: 17.88946838174497\n",
      "epoch 2523 learning rate:  0.01039635354736425   Training loss:   19.62449901074324  Valing loss:   17.88946838174497\n",
      "Pure loss: 20.72350986394864.....Total loss: 20.72350986394864\n",
      "Pure loss: 20.33266129683651.....Total loss: 20.33266129683651\n",
      "epoch 2524 learning rate:  0.010396196513470683   Training loss:   20.72350986394864  Valing loss:   20.33266129683651\n",
      "Pure loss: 20.741944637334026.....Total loss: 20.741944637334026\n",
      "Pure loss: 20.361191774373417.....Total loss: 20.361191774373417\n",
      "epoch 2525 learning rate:  0.010396039603960397   Training loss:   20.741944637334026  Valing loss:   20.361191774373417\n",
      "Pure loss: 21.07206612657338.....Total loss: 21.07206612657338\n",
      "Pure loss: 20.865554288560833.....Total loss: 20.865554288560833\n",
      "epoch 2526 learning rate:  0.01039588281868567   Training loss:   21.07206612657338  Valing loss:   20.865554288560833\n",
      "Pure loss: 20.627798936640833.....Total loss: 20.627798936640833\n",
      "Pure loss: 20.23707130650531.....Total loss: 20.23707130650531\n",
      "epoch 2527 learning rate:  0.010395726157499011   Training loss:   20.627798936640833  Valing loss:   20.23707130650531\n",
      "Pure loss: 20.36818603877745.....Total loss: 20.36818603877745\n",
      "Pure loss: 19.840640859487458.....Total loss: 19.840640859487458\n",
      "epoch 2528 learning rate:  0.010395569620253165   Training loss:   20.36818603877745  Valing loss:   19.840640859487458\n",
      "Pure loss: 20.474422608743353.....Total loss: 20.474422608743353\n",
      "Pure loss: 19.98901927874067.....Total loss: 19.98901927874067\n",
      "epoch 2529 learning rate:  0.010395413206801107   Training loss:   20.474422608743353  Valing loss:   19.98901927874067\n",
      "Pure loss: 20.78363540243339.....Total loss: 20.78363540243339\n",
      "Pure loss: 20.463488777484766.....Total loss: 20.463488777484766\n",
      "epoch 2530 learning rate:  0.010395256916996047   Training loss:   20.78363540243339  Valing loss:   20.463488777484766\n",
      "Pure loss: 20.008949384576244.....Total loss: 20.008949384576244\n",
      "Pure loss: 19.12260685802028.....Total loss: 19.12260685802028\n",
      "epoch 2531 learning rate:  0.010395100750691427   Training loss:   20.008949384576244  Valing loss:   19.12260685802028\n",
      "Pure loss: 19.615843456204395.....Total loss: 19.615843456204395\n",
      "Pure loss: 18.384875692878683.....Total loss: 18.384875692878683\n",
      "epoch 2532 learning rate:  0.010394944707740916   Training loss:   19.615843456204395  Valing loss:   18.384875692878683\n",
      "Pure loss: 19.54399579585182.....Total loss: 19.54399579585182\n",
      "Pure loss: 18.176906959797694.....Total loss: 18.176906959797694\n",
      "epoch 2533 learning rate:  0.01039478878799842   Training loss:   19.54399579585182  Valing loss:   18.176906959797694\n",
      "Pure loss: 19.507100904619666.....Total loss: 19.507100904619666\n",
      "Pure loss: 18.096493277629357.....Total loss: 18.096493277629357\n",
      "epoch 2534 learning rate:  0.010394632991318074   Training loss:   19.507100904619666  Valing loss:   18.096493277629357\n",
      "Pure loss: 19.73616984929985.....Total loss: 19.73616984929985\n",
      "Pure loss: 18.660055766217052.....Total loss: 18.660055766217052\n",
      "epoch 2535 learning rate:  0.01039447731755424   Training loss:   19.73616984929985  Valing loss:   18.660055766217052\n",
      "Pure loss: 20.237934217287084.....Total loss: 20.237934217287084\n",
      "Pure loss: 19.605927850943893.....Total loss: 19.605927850943893\n",
      "epoch 2536 learning rate:  0.010394321766561514   Training loss:   20.237934217287084  Valing loss:   19.605927850943893\n",
      "Pure loss: 19.741398769487116.....Total loss: 19.741398769487116\n",
      "Pure loss: 18.702329560789074.....Total loss: 18.702329560789074\n",
      "epoch 2537 learning rate:  0.010394166338194719   Training loss:   19.741398769487116  Valing loss:   18.702329560789074\n",
      "Pure loss: 19.631617240976222.....Total loss: 19.631617240976222\n",
      "Pure loss: 18.513832555398004.....Total loss: 18.513832555398004\n",
      "epoch 2538 learning rate:  0.010394011032308905   Training loss:   19.631617240976222  Valing loss:   18.513832555398004\n",
      "Pure loss: 19.53000123498593.....Total loss: 19.53000123498593\n",
      "Pure loss: 18.29518670785597.....Total loss: 18.29518670785597\n",
      "epoch 2539 learning rate:  0.010393855848759355   Training loss:   19.53000123498593  Valing loss:   18.29518670785597\n",
      "Pure loss: 19.33977509619878.....Total loss: 19.33977509619878\n",
      "Pure loss: 17.810773166365756.....Total loss: 17.810773166365756\n",
      "epoch 2540 learning rate:  0.010393700787401575   Training loss:   19.33977509619878  Valing loss:   17.810773166365756\n",
      "Pure loss: 19.347469994325376.....Total loss: 19.347469994325376\n",
      "Pure loss: 17.83400449191017.....Total loss: 17.83400449191017\n",
      "epoch 2541 learning rate:  0.010393545848091303   Training loss:   19.347469994325376  Valing loss:   17.83400449191017\n",
      "Pure loss: 19.400707712367744.....Total loss: 19.400707712367744\n",
      "Pure loss: 17.995731715948462.....Total loss: 17.995731715948462\n",
      "epoch 2542 learning rate:  0.0103933910306845   Training loss:   19.400707712367744  Valing loss:   17.995731715948462\n",
      "Pure loss: 19.307433134882956.....Total loss: 19.307433134882956\n",
      "Pure loss: 17.64936271089364.....Total loss: 17.64936271089364\n",
      "epoch 2543 learning rate:  0.010393236335037357   Training loss:   19.307433134882956  Valing loss:   17.64936271089364\n",
      "Pure loss: 19.30676223676621.....Total loss: 19.30676223676621\n",
      "Pure loss: 17.644471701637993.....Total loss: 17.644471701637993\n",
      "epoch 2544 learning rate:  0.01039308176100629   Training loss:   19.30676223676621  Valing loss:   17.644471701637993\n",
      "Pure loss: 19.343566682022182.....Total loss: 19.343566682022182\n",
      "Pure loss: 17.51808408103245.....Total loss: 17.51808408103245\n",
      "epoch 2545 learning rate:  0.010392927308447937   Training loss:   19.343566682022182  Valing loss:   17.51808408103245\n",
      "Pure loss: 19.388426609259827.....Total loss: 19.388426609259827\n",
      "Pure loss: 17.21597578864909.....Total loss: 17.21597578864909\n",
      "epoch 2546 learning rate:  0.010392772977219168   Training loss:   19.388426609259827  Valing loss:   17.21597578864909\n",
      "Pure loss: 19.40650616710926.....Total loss: 19.40650616710926\n",
      "Pure loss: 17.20879367907191.....Total loss: 17.20879367907191\n",
      "epoch 2547 learning rate:  0.010392618767177071   Training loss:   19.40650616710926  Valing loss:   17.20879367907191\n",
      "Pure loss: 19.520511808250962.....Total loss: 19.520511808250962\n",
      "Pure loss: 17.115761557961683.....Total loss: 17.115761557961683\n",
      "epoch 2548 learning rate:  0.010392464678178965   Training loss:   19.520511808250962  Valing loss:   17.115761557961683\n",
      "Pure loss: 19.48679602089091.....Total loss: 19.48679602089091\n",
      "Pure loss: 17.139229733164612.....Total loss: 17.139229733164612\n",
      "epoch 2549 learning rate:  0.010392310710082385   Training loss:   19.48679602089091  Valing loss:   17.139229733164612\n",
      "Pure loss: 19.64262234543097.....Total loss: 19.64262234543097\n",
      "Pure loss: 17.119474380034994.....Total loss: 17.119474380034994\n",
      "epoch 2550 learning rate:  0.010392156862745099   Training loss:   19.64262234543097  Valing loss:   17.119474380034994\n",
      "Pure loss: 19.94195423710701.....Total loss: 19.94195423710701\n",
      "Pure loss: 17.143363454472464.....Total loss: 17.143363454472464\n",
      "epoch 2551 learning rate:  0.010392003136025089   Training loss:   19.94195423710701  Valing loss:   17.143363454472464\n",
      "Pure loss: 20.157784573286538.....Total loss: 20.157784573286538\n",
      "Pure loss: 17.26787852720062.....Total loss: 17.26787852720062\n",
      "epoch 2552 learning rate:  0.010391849529780565   Training loss:   20.157784573286538  Valing loss:   17.26787852720062\n",
      "Pure loss: 20.526387934984935.....Total loss: 20.526387934984935\n",
      "Pure loss: 17.403223790575726.....Total loss: 17.403223790575726\n",
      "epoch 2553 learning rate:  0.010391696043869957   Training loss:   20.526387934984935  Valing loss:   17.403223790575726\n",
      "Pure loss: 20.4911827136082.....Total loss: 20.4911827136082\n",
      "Pure loss: 17.388801927472674.....Total loss: 17.388801927472674\n",
      "epoch 2554 learning rate:  0.010391542678151918   Training loss:   20.4911827136082  Valing loss:   17.388801927472674\n",
      "Pure loss: 19.855140978525874.....Total loss: 19.855140978525874\n",
      "Pure loss: 17.035522040826304.....Total loss: 17.035522040826304\n",
      "epoch 2555 learning rate:  0.010391389432485323   Training loss:   19.855140978525874  Valing loss:   17.035522040826304\n",
      "Pure loss: 20.243791465698806.....Total loss: 20.243791465698806\n",
      "Pure loss: 17.09939109535325.....Total loss: 17.09939109535325\n",
      "epoch 2556 learning rate:  0.010391236306729265   Training loss:   20.243791465698806  Valing loss:   17.09939109535325\n",
      "Pure loss: 20.24619723559469.....Total loss: 20.24619723559469\n",
      "Pure loss: 17.100098672486137.....Total loss: 17.100098672486137\n",
      "epoch 2557 learning rate:  0.010391083300743058   Training loss:   20.24619723559469  Valing loss:   17.100098672486137\n",
      "Pure loss: 20.119893189087612.....Total loss: 20.119893189087612\n",
      "Pure loss: 17.039491604022604.....Total loss: 17.039491604022604\n",
      "epoch 2558 learning rate:  0.01039093041438624   Training loss:   20.119893189087612  Valing loss:   17.039491604022604\n",
      "Pure loss: 20.648847670228395.....Total loss: 20.648847670228395\n",
      "Pure loss: 17.224416551754615.....Total loss: 17.224416551754615\n",
      "epoch 2559 learning rate:  0.010390777647518562   Training loss:   20.648847670228395  Valing loss:   17.224416551754615\n",
      "Pure loss: 20.35735792573779.....Total loss: 20.35735792573779\n",
      "Pure loss: 17.095633403252794.....Total loss: 17.095633403252794\n",
      "epoch 2560 learning rate:  0.010390625   Training loss:   20.35735792573779  Valing loss:   17.095633403252794\n",
      "Pure loss: 19.462630554941803.....Total loss: 19.462630554941803\n",
      "Pure loss: 17.083671634417104.....Total loss: 17.083671634417104\n",
      "epoch 2561 learning rate:  0.010390472471690747   Training loss:   19.462630554941803  Valing loss:   17.083671634417104\n",
      "Pure loss: 19.141335823579432.....Total loss: 19.141335823579432\n",
      "Pure loss: 17.046052683480724.....Total loss: 17.046052683480724\n",
      "epoch 2562 learning rate:  0.01039032006245121   Training loss:   19.141335823579432  Valing loss:   17.046052683480724\n",
      "Pure loss: 19.093540575655553.....Total loss: 19.093540575655553\n",
      "Pure loss: 17.380489528276183.....Total loss: 17.380489528276183\n",
      "epoch 2563 learning rate:  0.010390167772142021   Training loss:   19.093540575655553  Valing loss:   17.380489528276183\n",
      "Pure loss: 19.241696362438176.....Total loss: 19.241696362438176\n",
      "Pure loss: 18.067733846216846.....Total loss: 18.067733846216846\n",
      "epoch 2564 learning rate:  0.010390015600624025   Training loss:   19.241696362438176  Valing loss:   18.067733846216846\n",
      "Pure loss: 19.218058444514604.....Total loss: 19.218058444514604\n",
      "Pure loss: 18.005731282696406.....Total loss: 18.005731282696406\n",
      "epoch 2565 learning rate:  0.010389863547758284   Training loss:   19.218058444514604  Valing loss:   18.005731282696406\n",
      "Pure loss: 19.292536989575538.....Total loss: 19.292536989575538\n",
      "Pure loss: 18.1973696672611.....Total loss: 18.1973696672611\n",
      "epoch 2566 learning rate:  0.01038971161340608   Training loss:   19.292536989575538  Valing loss:   18.1973696672611\n",
      "Pure loss: 19.118257874574688.....Total loss: 19.118257874574688\n",
      "Pure loss: 17.764267823039724.....Total loss: 17.764267823039724\n",
      "epoch 2567 learning rate:  0.010389559797428905   Training loss:   19.118257874574688  Valing loss:   17.764267823039724\n",
      "Pure loss: 19.250694716666132.....Total loss: 19.250694716666132\n",
      "Pure loss: 18.077397426037752.....Total loss: 18.077397426037752\n",
      "epoch 2568 learning rate:  0.010389408099688474   Training loss:   19.250694716666132  Valing loss:   18.077397426037752\n",
      "Pure loss: 19.248857508853458.....Total loss: 19.248857508853458\n",
      "Pure loss: 18.0739846134292.....Total loss: 18.0739846134292\n",
      "epoch 2569 learning rate:  0.010389256520046712   Training loss:   19.248857508853458  Valing loss:   18.0739846134292\n",
      "Pure loss: 19.68322675162889.....Total loss: 19.68322675162889\n",
      "Pure loss: 19.021158684029576.....Total loss: 19.021158684029576\n",
      "epoch 2570 learning rate:  0.01038910505836576   Training loss:   19.68322675162889  Valing loss:   19.021158684029576\n",
      "Pure loss: 19.64813759430172.....Total loss: 19.64813759430172\n",
      "Pure loss: 18.96112795329452.....Total loss: 18.96112795329452\n",
      "epoch 2571 learning rate:  0.010388953714507973   Training loss:   19.64813759430172  Valing loss:   18.96112795329452\n",
      "Pure loss: 19.21369509916211.....Total loss: 19.21369509916211\n",
      "Pure loss: 18.040242978857652.....Total loss: 18.040242978857652\n",
      "epoch 2572 learning rate:  0.010388802488335926   Training loss:   19.21369509916211  Valing loss:   18.040242978857652\n",
      "Pure loss: 19.122700402650846.....Total loss: 19.122700402650846\n",
      "Pure loss: 17.812209358183413.....Total loss: 17.812209358183413\n",
      "epoch 2573 learning rate:  0.010388651379712397   Training loss:   19.122700402650846  Valing loss:   17.812209358183413\n",
      "Pure loss: 19.060360645551782.....Total loss: 19.060360645551782\n",
      "Pure loss: 17.62454205449373.....Total loss: 17.62454205449373\n",
      "epoch 2574 learning rate:  0.010388500388500388   Training loss:   19.060360645551782  Valing loss:   17.62454205449373\n",
      "Pure loss: 19.27461227155184.....Total loss: 19.27461227155184\n",
      "Pure loss: 18.165853016015323.....Total loss: 18.165853016015323\n",
      "epoch 2575 learning rate:  0.010388349514563107   Training loss:   19.27461227155184  Valing loss:   18.165853016015323\n",
      "Pure loss: 19.15107711135842.....Total loss: 19.15107711135842\n",
      "Pure loss: 17.845925266885054.....Total loss: 17.845925266885054\n",
      "epoch 2576 learning rate:  0.010388198757763976   Training loss:   19.15107711135842  Valing loss:   17.845925266885054\n",
      "Pure loss: 19.194386567910172.....Total loss: 19.194386567910172\n",
      "Pure loss: 17.942742758141332.....Total loss: 17.942742758141332\n",
      "epoch 2577 learning rate:  0.010388048117966628   Training loss:   19.194386567910172  Valing loss:   17.942742758141332\n",
      "Pure loss: 19.60875223383424.....Total loss: 19.60875223383424\n",
      "Pure loss: 18.838744610397892.....Total loss: 18.838744610397892\n",
      "epoch 2578 learning rate:  0.01038789759503491   Training loss:   19.60875223383424  Valing loss:   18.838744610397892\n",
      "Pure loss: 19.389435465718684.....Total loss: 19.389435465718684\n",
      "Pure loss: 18.499328202044328.....Total loss: 18.499328202044328\n",
      "epoch 2579 learning rate:  0.010387747188832881   Training loss:   19.389435465718684  Valing loss:   18.499328202044328\n",
      "Pure loss: 19.35880454747311.....Total loss: 19.35880454747311\n",
      "Pure loss: 18.439238717060213.....Total loss: 18.439238717060213\n",
      "epoch 2580 learning rate:  0.010387596899224806   Training loss:   19.35880454747311  Valing loss:   18.439238717060213\n",
      "Pure loss: 19.27297367742194.....Total loss: 19.27297367742194\n",
      "Pure loss: 18.26550672461239.....Total loss: 18.26550672461239\n",
      "epoch 2581 learning rate:  0.010387446726075165   Training loss:   19.27297367742194  Valing loss:   18.26550672461239\n",
      "Pure loss: 19.231233363939708.....Total loss: 19.231233363939708\n",
      "Pure loss: 18.18113496316731.....Total loss: 18.18113496316731\n",
      "epoch 2582 learning rate:  0.010387296669248645   Training loss:   19.231233363939708  Valing loss:   18.18113496316731\n",
      "Pure loss: 19.056328741596854.....Total loss: 19.056328741596854\n",
      "Pure loss: 17.66994477029565.....Total loss: 17.66994477029565\n",
      "epoch 2583 learning rate:  0.010387146728610144   Training loss:   19.056328741596854  Valing loss:   17.66994477029565\n",
      "Pure loss: 19.043305075224612.....Total loss: 19.043305075224612\n",
      "Pure loss: 17.598724485336714.....Total loss: 17.598724485336714\n",
      "epoch 2584 learning rate:  0.010386996904024767   Training loss:   19.043305075224612  Valing loss:   17.598724485336714\n",
      "Pure loss: 19.044171562581596.....Total loss: 19.044171562581596\n",
      "Pure loss: 17.61614840516771.....Total loss: 17.61614840516771\n",
      "epoch 2585 learning rate:  0.010386847195357833   Training loss:   19.044171562581596  Valing loss:   17.61614840516771\n",
      "Pure loss: 19.029819869388582.....Total loss: 19.029819869388582\n",
      "Pure loss: 17.526056490778863.....Total loss: 17.526056490778863\n",
      "epoch 2586 learning rate:  0.010386697602474865   Training loss:   19.029819869388582  Valing loss:   17.526056490778863\n",
      "Pure loss: 19.0750960410124.....Total loss: 19.0750960410124\n",
      "Pure loss: 17.781977896151847.....Total loss: 17.781977896151847\n",
      "epoch 2587 learning rate:  0.010386548125241592   Training loss:   19.0750960410124  Valing loss:   17.781977896151847\n",
      "Pure loss: 19.441721342980948.....Total loss: 19.441721342980948\n",
      "Pure loss: 18.5538926878892.....Total loss: 18.5538926878892\n",
      "epoch 2588 learning rate:  0.010386398763523957   Training loss:   19.441721342980948  Valing loss:   18.5538926878892\n",
      "Pure loss: 19.73652546201193.....Total loss: 19.73652546201193\n",
      "Pure loss: 19.07419683662182.....Total loss: 19.07419683662182\n",
      "epoch 2589 learning rate:  0.010386249517188103   Training loss:   19.73652546201193  Valing loss:   19.07419683662182\n",
      "Pure loss: 19.499119810035165.....Total loss: 19.499119810035165\n",
      "Pure loss: 18.69062861148168.....Total loss: 18.69062861148168\n",
      "epoch 2590 learning rate:  0.010386100386100387   Training loss:   19.499119810035165  Valing loss:   18.69062861148168\n",
      "Pure loss: 20.055816829897662.....Total loss: 20.055816829897662\n",
      "Pure loss: 19.641677124575246.....Total loss: 19.641677124575246\n",
      "epoch 2591 learning rate:  0.010385951370127364   Training loss:   20.055816829897662  Valing loss:   19.641677124575246\n",
      "Pure loss: 19.776292603327267.....Total loss: 19.776292603327267\n",
      "Pure loss: 19.20289515415926.....Total loss: 19.20289515415926\n",
      "epoch 2592 learning rate:  0.010385802469135803   Training loss:   19.776292603327267  Valing loss:   19.20289515415926\n",
      "Pure loss: 19.552699456451087.....Total loss: 19.552699456451087\n",
      "Pure loss: 18.80333694091051.....Total loss: 18.80333694091051\n",
      "epoch 2593 learning rate:  0.010385653682992673   Training loss:   19.552699456451087  Valing loss:   18.80333694091051\n",
      "Pure loss: 19.30914250809189.....Total loss: 19.30914250809189\n",
      "Pure loss: 18.335073463832455.....Total loss: 18.335073463832455\n",
      "epoch 2594 learning rate:  0.010385505011565151   Training loss:   19.30914250809189  Valing loss:   18.335073463832455\n",
      "Pure loss: 19.23902550069392.....Total loss: 19.23902550069392\n",
      "Pure loss: 18.177764182608605.....Total loss: 18.177764182608605\n",
      "epoch 2595 learning rate:  0.010385356454720617   Training loss:   19.23902550069392  Valing loss:   18.177764182608605\n",
      "Pure loss: 19.699584058565005.....Total loss: 19.699584058565005\n",
      "Pure loss: 19.112549944062657.....Total loss: 19.112549944062657\n",
      "epoch 2596 learning rate:  0.010385208012326657   Training loss:   19.699584058565005  Valing loss:   19.112549944062657\n",
      "Pure loss: 19.608415967804365.....Total loss: 19.608415967804365\n",
      "Pure loss: 18.95494899145639.....Total loss: 18.95494899145639\n",
      "epoch 2597 learning rate:  0.010385059684251059   Training loss:   19.608415967804365  Valing loss:   18.95494899145639\n",
      "Pure loss: 19.9256754095793.....Total loss: 19.9256754095793\n",
      "Pure loss: 19.489265412230402.....Total loss: 19.489265412230402\n",
      "epoch 2598 learning rate:  0.010384911470361817   Training loss:   19.9256754095793  Valing loss:   19.489265412230402\n",
      "Pure loss: 19.304446433190787.....Total loss: 19.304446433190787\n",
      "Pure loss: 18.477753952401248.....Total loss: 18.477753952401248\n",
      "epoch 2599 learning rate:  0.010384763370527126   Training loss:   19.304446433190787  Valing loss:   18.477753952401248\n",
      "Pure loss: 19.304912795802522.....Total loss: 19.304912795802522\n",
      "Pure loss: 18.46709682885458.....Total loss: 18.46709682885458\n",
      "epoch 2600 learning rate:  0.010384615384615384   Training loss:   19.304912795802522  Valing loss:   18.46709682885458\n",
      "Pure loss: 19.817914850828124.....Total loss: 19.817914850828124\n",
      "Pure loss: 19.40355659557046.....Total loss: 19.40355659557046\n",
      "epoch 2601 learning rate:  0.010384467512495194   Training loss:   19.817914850828124  Valing loss:   19.40355659557046\n",
      "Pure loss: 20.091893976273045.....Total loss: 20.091893976273045\n",
      "Pure loss: 19.88326872334053.....Total loss: 19.88326872334053\n",
      "epoch 2602 learning rate:  0.010384319754035358   Training loss:   20.091893976273045  Valing loss:   19.88326872334053\n",
      "Pure loss: 19.758545813830935.....Total loss: 19.758545813830935\n",
      "Pure loss: 19.261907855941015.....Total loss: 19.261907855941015\n",
      "epoch 2603 learning rate:  0.010384172109104878   Training loss:   19.758545813830935  Valing loss:   19.261907855941015\n",
      "Pure loss: 19.538221642101814.....Total loss: 19.538221642101814\n",
      "Pure loss: 18.88769529470846.....Total loss: 18.88769529470846\n",
      "epoch 2604 learning rate:  0.010384024577572966   Training loss:   19.538221642101814  Valing loss:   18.88769529470846\n",
      "Pure loss: 19.384287291703.....Total loss: 19.384287291703\n",
      "Pure loss: 18.58350131055555.....Total loss: 18.58350131055555\n",
      "epoch 2605 learning rate:  0.01038387715930902   Training loss:   19.384287291703  Valing loss:   18.58350131055555\n",
      "Pure loss: 19.44068953699772.....Total loss: 19.44068953699772\n",
      "Pure loss: 18.699082875376607.....Total loss: 18.699082875376607\n",
      "epoch 2606 learning rate:  0.010383729854182656   Training loss:   19.44068953699772  Valing loss:   18.699082875376607\n",
      "Pure loss: 19.420694370352496.....Total loss: 19.420694370352496\n",
      "Pure loss: 18.635365484750974.....Total loss: 18.635365484750974\n",
      "epoch 2607 learning rate:  0.010383582662063675   Training loss:   19.420694370352496  Valing loss:   18.635365484750974\n",
      "Pure loss: 19.617074928634004.....Total loss: 19.617074928634004\n",
      "Pure loss: 19.019581369622664.....Total loss: 19.019581369622664\n",
      "epoch 2608 learning rate:  0.010383435582822086   Training loss:   19.617074928634004  Valing loss:   19.019581369622664\n",
      "Pure loss: 19.604625504858184.....Total loss: 19.604625504858184\n",
      "Pure loss: 18.842277402687554.....Total loss: 18.842277402687554\n",
      "epoch 2609 learning rate:  0.010383288616328095   Training loss:   19.604625504858184  Valing loss:   18.842277402687554\n",
      "Pure loss: 19.557821062161928.....Total loss: 19.557821062161928\n",
      "Pure loss: 18.84164174978613.....Total loss: 18.84164174978613\n",
      "epoch 2610 learning rate:  0.010383141762452107   Training loss:   19.557821062161928  Valing loss:   18.84164174978613\n",
      "Pure loss: 19.627462195107164.....Total loss: 19.627462195107164\n",
      "Pure loss: 18.98904569872641.....Total loss: 18.98904569872641\n",
      "epoch 2611 learning rate:  0.010382995021064727   Training loss:   19.627462195107164  Valing loss:   18.98904569872641\n",
      "Pure loss: 19.34111992760812.....Total loss: 19.34111992760812\n",
      "Pure loss: 18.496682688394927.....Total loss: 18.496682688394927\n",
      "epoch 2612 learning rate:  0.010382848392036754   Training loss:   19.34111992760812  Valing loss:   18.496682688394927\n",
      "Pure loss: 19.321892970821857.....Total loss: 19.321892970821857\n",
      "Pure loss: 18.51939181466385.....Total loss: 18.51939181466385\n",
      "epoch 2613 learning rate:  0.01038270187523919   Training loss:   19.321892970821857  Valing loss:   18.51939181466385\n",
      "Pure loss: 19.094255234376607.....Total loss: 19.094255234376607\n",
      "Pure loss: 18.01319315855439.....Total loss: 18.01319315855439\n",
      "epoch 2614 learning rate:  0.01038255547054323   Training loss:   19.094255234376607  Valing loss:   18.01319315855439\n",
      "Pure loss: 19.045889184631807.....Total loss: 19.045889184631807\n",
      "Pure loss: 17.887034575043447.....Total loss: 17.887034575043447\n",
      "epoch 2615 learning rate:  0.010382409177820268   Training loss:   19.045889184631807  Valing loss:   17.887034575043447\n",
      "Pure loss: 19.105585312933012.....Total loss: 19.105585312933012\n",
      "Pure loss: 18.034971500754246.....Total loss: 18.034971500754246\n",
      "epoch 2616 learning rate:  0.010382262996941897   Training loss:   19.105585312933012  Valing loss:   18.034971500754246\n",
      "Pure loss: 19.097276777428476.....Total loss: 19.097276777428476\n",
      "Pure loss: 18.034502159563843.....Total loss: 18.034502159563843\n",
      "epoch 2617 learning rate:  0.0103821169277799   Training loss:   19.097276777428476  Valing loss:   18.034502159563843\n",
      "Pure loss: 18.9766250853013.....Total loss: 18.9766250853013\n",
      "Pure loss: 17.601317528789366.....Total loss: 17.601317528789366\n",
      "epoch 2618 learning rate:  0.010381970970206265   Training loss:   18.9766250853013  Valing loss:   17.601317528789366\n",
      "Pure loss: 18.907377117506048.....Total loss: 18.907377117506048\n",
      "Pure loss: 17.199326583873283.....Total loss: 17.199326583873283\n",
      "epoch 2619 learning rate:  0.010381825124093165   Training loss:   18.907377117506048  Valing loss:   17.199326583873283\n",
      "Pure loss: 18.971530426181342.....Total loss: 18.971530426181342\n",
      "Pure loss: 17.154949372493636.....Total loss: 17.154949372493636\n",
      "epoch 2620 learning rate:  0.010381679389312977   Training loss:   18.971530426181342  Valing loss:   17.154949372493636\n",
      "Pure loss: 19.129472815454655.....Total loss: 19.129472815454655\n",
      "Pure loss: 16.799495800986584.....Total loss: 16.799495800986584\n",
      "epoch 2621 learning rate:  0.010381533765738268   Training loss:   19.129472815454655  Valing loss:   16.799495800986584\n",
      "Pure loss: 18.885152746554834.....Total loss: 18.885152746554834\n",
      "Pure loss: 16.98936558161606.....Total loss: 16.98936558161606\n",
      "epoch 2622 learning rate:  0.010381388253241801   Training loss:   18.885152746554834  Valing loss:   16.98936558161606\n",
      "Pure loss: 18.83919113276939.....Total loss: 18.83919113276939\n",
      "Pure loss: 17.048838147679056.....Total loss: 17.048838147679056\n",
      "epoch 2623 learning rate:  0.010381242851696532   Training loss:   18.83919113276939  Valing loss:   17.048838147679056\n",
      "Pure loss: 18.888186194559406.....Total loss: 18.888186194559406\n",
      "Pure loss: 17.028238411399986.....Total loss: 17.028238411399986\n",
      "epoch 2624 learning rate:  0.01038109756097561   Training loss:   18.888186194559406  Valing loss:   17.028238411399986\n",
      "Pure loss: 18.893077612835267.....Total loss: 18.893077612835267\n",
      "Pure loss: 17.022931616138088.....Total loss: 17.022931616138088\n",
      "epoch 2625 learning rate:  0.010380952380952381   Training loss:   18.893077612835267  Valing loss:   17.022931616138088\n",
      "Pure loss: 18.786942998850957.....Total loss: 18.786942998850957\n",
      "Pure loss: 17.035673101130353.....Total loss: 17.035673101130353\n",
      "epoch 2626 learning rate:  0.010380807311500382   Training loss:   18.786942998850957  Valing loss:   17.035673101130353\n",
      "Pure loss: 18.87911396263676.....Total loss: 18.87911396263676\n",
      "Pure loss: 16.868808843133664.....Total loss: 16.868808843133664\n",
      "epoch 2627 learning rate:  0.010380662352493339   Training loss:   18.87911396263676  Valing loss:   16.868808843133664\n",
      "Pure loss: 18.94315782036357.....Total loss: 18.94315782036357\n",
      "Pure loss: 16.867379487719056.....Total loss: 16.867379487719056\n",
      "epoch 2628 learning rate:  0.010380517503805175   Training loss:   18.94315782036357  Valing loss:   16.867379487719056\n",
      "Pure loss: 18.87670821631524.....Total loss: 18.87670821631524\n",
      "Pure loss: 16.902346994062466.....Total loss: 16.902346994062466\n",
      "epoch 2629 learning rate:  0.010380372765310003   Training loss:   18.87670821631524  Valing loss:   16.902346994062466\n",
      "Pure loss: 18.907538390678646.....Total loss: 18.907538390678646\n",
      "Pure loss: 16.924275852064326.....Total loss: 16.924275852064326\n",
      "epoch 2630 learning rate:  0.010380228136882129   Training loss:   18.907538390678646  Valing loss:   16.924275852064326\n",
      "Pure loss: 19.07732870923197.....Total loss: 19.07732870923197\n",
      "Pure loss: 16.974316313976956.....Total loss: 16.974316313976956\n",
      "epoch 2631 learning rate:  0.010380083618396048   Training loss:   19.07732870923197  Valing loss:   16.974316313976956\n",
      "Pure loss: 18.99922945124762.....Total loss: 18.99922945124762\n",
      "Pure loss: 17.080330207898445.....Total loss: 17.080330207898445\n",
      "epoch 2632 learning rate:  0.010379939209726444   Training loss:   18.99922945124762  Valing loss:   17.080330207898445\n",
      "Pure loss: 18.740293870309653.....Total loss: 18.740293870309653\n",
      "Pure loss: 17.133542420842645.....Total loss: 17.133542420842645\n",
      "epoch 2633 learning rate:  0.010379794910748197   Training loss:   18.740293870309653  Valing loss:   17.133542420842645\n",
      "Pure loss: 18.844677050778692.....Total loss: 18.844677050778692\n",
      "Pure loss: 17.539124087905016.....Total loss: 17.539124087905016\n",
      "epoch 2634 learning rate:  0.010379650721336372   Training loss:   18.844677050778692  Valing loss:   17.539124087905016\n",
      "Pure loss: 18.853073094415173.....Total loss: 18.853073094415173\n",
      "Pure loss: 17.560506879225443.....Total loss: 17.560506879225443\n",
      "epoch 2635 learning rate:  0.010379506641366224   Training loss:   18.853073094415173  Valing loss:   17.560506879225443\n",
      "Pure loss: 18.83876941281426.....Total loss: 18.83876941281426\n",
      "Pure loss: 17.51956081942576.....Total loss: 17.51956081942576\n",
      "epoch 2636 learning rate:  0.010379362670713202   Training loss:   18.83876941281426  Valing loss:   17.51956081942576\n",
      "Pure loss: 18.74281015322895.....Total loss: 18.74281015322895\n",
      "Pure loss: 17.25835242092481.....Total loss: 17.25835242092481\n",
      "epoch 2637 learning rate:  0.01037921880925294   Training loss:   18.74281015322895  Valing loss:   17.25835242092481\n",
      "Pure loss: 18.785562754308298.....Total loss: 18.785562754308298\n",
      "Pure loss: 17.41647181781401.....Total loss: 17.41647181781401\n",
      "epoch 2638 learning rate:  0.010379075056861259   Training loss:   18.785562754308298  Valing loss:   17.41647181781401\n",
      "Pure loss: 18.83670389778178.....Total loss: 18.83670389778178\n",
      "Pure loss: 17.562285171587984.....Total loss: 17.562285171587984\n",
      "epoch 2639 learning rate:  0.010378931413414172   Training loss:   18.83670389778178  Valing loss:   17.562285171587984\n",
      "Pure loss: 18.718718758387077.....Total loss: 18.718718758387077\n",
      "Pure loss: 17.123464851019403.....Total loss: 17.123464851019403\n",
      "epoch 2640 learning rate:  0.01037878787878788   Training loss:   18.718718758387077  Valing loss:   17.123464851019403\n",
      "Pure loss: 18.72073823135141.....Total loss: 18.72073823135141\n",
      "Pure loss: 17.030975294344938.....Total loss: 17.030975294344938\n",
      "epoch 2641 learning rate:  0.010378644452858765   Training loss:   18.72073823135141  Valing loss:   17.030975294344938\n",
      "Pure loss: 18.72955282208493.....Total loss: 18.72955282208493\n",
      "Pure loss: 17.131158634066942.....Total loss: 17.131158634066942\n",
      "epoch 2642 learning rate:  0.010378501135503407   Training loss:   18.72955282208493  Valing loss:   17.131158634066942\n",
      "Pure loss: 18.733380968091343.....Total loss: 18.733380968091343\n",
      "Pure loss: 17.003014898721947.....Total loss: 17.003014898721947\n",
      "epoch 2643 learning rate:  0.010378357926598563   Training loss:   18.733380968091343  Valing loss:   17.003014898721947\n",
      "Pure loss: 18.69439847967824.....Total loss: 18.69439847967824\n",
      "Pure loss: 17.152075836011473.....Total loss: 17.152075836011473\n",
      "epoch 2644 learning rate:  0.01037821482602118   Training loss:   18.69439847967824  Valing loss:   17.152075836011473\n",
      "Pure loss: 18.69139695009259.....Total loss: 18.69139695009259\n",
      "Pure loss: 17.13368111164438.....Total loss: 17.13368111164438\n",
      "epoch 2645 learning rate:  0.010378071833648394   Training loss:   18.69139695009259  Valing loss:   17.13368111164438\n",
      "Pure loss: 18.739284747099592.....Total loss: 18.739284747099592\n",
      "Pure loss: 16.99046708375039.....Total loss: 16.99046708375039\n",
      "epoch 2646 learning rate:  0.010377928949357521   Training loss:   18.739284747099592  Valing loss:   16.99046708375039\n",
      "Pure loss: 18.723903713219617.....Total loss: 18.723903713219617\n",
      "Pure loss: 17.134811451931476.....Total loss: 17.134811451931476\n",
      "epoch 2647 learning rate:  0.010377786173026067   Training loss:   18.723903713219617  Valing loss:   17.134811451931476\n",
      "Pure loss: 18.749195839474933.....Total loss: 18.749195839474933\n",
      "Pure loss: 17.260239977967466.....Total loss: 17.260239977967466\n",
      "epoch 2648 learning rate:  0.010377643504531723   Training loss:   18.749195839474933  Valing loss:   17.260239977967466\n",
      "Pure loss: 18.75646853392176.....Total loss: 18.75646853392176\n",
      "Pure loss: 17.306934361761805.....Total loss: 17.306934361761805\n",
      "epoch 2649 learning rate:  0.010377500943752359   Training loss:   18.75646853392176  Valing loss:   17.306934361761805\n",
      "Pure loss: 18.74597596808614.....Total loss: 18.74597596808614\n",
      "Pure loss: 17.043448651343898.....Total loss: 17.043448651343898\n",
      "epoch 2650 learning rate:  0.010377358490566037   Training loss:   18.74597596808614  Valing loss:   17.043448651343898\n",
      "Pure loss: 18.829140089442806.....Total loss: 18.829140089442806\n",
      "Pure loss: 17.04125676435083.....Total loss: 17.04125676435083\n",
      "epoch 2651 learning rate:  0.010377216144850999   Training loss:   18.829140089442806  Valing loss:   17.04125676435083\n",
      "Pure loss: 18.83570201851731.....Total loss: 18.83570201851731\n",
      "Pure loss: 17.037204320103115.....Total loss: 17.037204320103115\n",
      "epoch 2652 learning rate:  0.010377073906485672   Training loss:   18.83570201851731  Valing loss:   17.037204320103115\n",
      "Pure loss: 18.84792374584602.....Total loss: 18.84792374584602\n",
      "Pure loss: 17.038636286463916.....Total loss: 17.038636286463916\n",
      "epoch 2653 learning rate:  0.010376931775348663   Training loss:   18.84792374584602  Valing loss:   17.038636286463916\n",
      "Pure loss: 18.79319178441049.....Total loss: 18.79319178441049\n",
      "Pure loss: 17.070550428217366.....Total loss: 17.070550428217366\n",
      "epoch 2654 learning rate:  0.010376789751318765   Training loss:   18.79319178441049  Valing loss:   17.070550428217366\n",
      "Pure loss: 18.796091041421843.....Total loss: 18.796091041421843\n",
      "Pure loss: 17.057068326285567.....Total loss: 17.057068326285567\n",
      "epoch 2655 learning rate:  0.010376647834274954   Training loss:   18.796091041421843  Valing loss:   17.057068326285567\n",
      "Pure loss: 18.798655435510607.....Total loss: 18.798655435510607\n",
      "Pure loss: 17.092665188051186.....Total loss: 17.092665188051186\n",
      "epoch 2656 learning rate:  0.010376506024096385   Training loss:   18.798655435510607  Valing loss:   17.092665188051186\n",
      "Pure loss: 18.863051178277725.....Total loss: 18.863051178277725\n",
      "Pure loss: 17.132104096679793.....Total loss: 17.132104096679793\n",
      "epoch 2657 learning rate:  0.010376364320662402   Training loss:   18.863051178277725  Valing loss:   17.132104096679793\n",
      "Pure loss: 18.808047970696393.....Total loss: 18.808047970696393\n",
      "Pure loss: 17.303595113706216.....Total loss: 17.303595113706216\n",
      "epoch 2658 learning rate:  0.01037622272385252   Training loss:   18.808047970696393  Valing loss:   17.303595113706216\n",
      "Pure loss: 18.934167020418865.....Total loss: 18.934167020418865\n",
      "Pure loss: 17.741876118174467.....Total loss: 17.741876118174467\n",
      "epoch 2659 learning rate:  0.010376081233546447   Training loss:   18.934167020418865  Valing loss:   17.741876118174467\n",
      "Pure loss: 18.800628948242036.....Total loss: 18.800628948242036\n",
      "Pure loss: 17.170657623439347.....Total loss: 17.170657623439347\n",
      "epoch 2660 learning rate:  0.010375939849624061   Training loss:   18.800628948242036  Valing loss:   17.170657623439347\n",
      "Pure loss: 18.89010901124054.....Total loss: 18.89010901124054\n",
      "Pure loss: 17.184777465411823.....Total loss: 17.184777465411823\n",
      "epoch 2661 learning rate:  0.010375798571965427   Training loss:   18.89010901124054  Valing loss:   17.184777465411823\n",
      "Pure loss: 18.904637613037018.....Total loss: 18.904637613037018\n",
      "Pure loss: 17.143952770306292.....Total loss: 17.143952770306292\n",
      "epoch 2662 learning rate:  0.010375657400450789   Training loss:   18.904637613037018  Valing loss:   17.143952770306292\n",
      "Pure loss: 18.717680384752175.....Total loss: 18.717680384752175\n",
      "Pure loss: 17.15103513699829.....Total loss: 17.15103513699829\n",
      "epoch 2663 learning rate:  0.010375516334960571   Training loss:   18.717680384752175  Valing loss:   17.15103513699829\n",
      "Pure loss: 18.735864460303574.....Total loss: 18.735864460303574\n",
      "Pure loss: 17.02797196800707.....Total loss: 17.02797196800707\n",
      "epoch 2664 learning rate:  0.010375375375375376   Training loss:   18.735864460303574  Valing loss:   17.02797196800707\n",
      "Pure loss: 18.73143766380144.....Total loss: 18.73143766380144\n",
      "Pure loss: 17.117524006187192.....Total loss: 17.117524006187192\n",
      "epoch 2665 learning rate:  0.010375234521575984   Training loss:   18.73143766380144  Valing loss:   17.117524006187192\n",
      "Pure loss: 18.831765370054104.....Total loss: 18.831765370054104\n",
      "Pure loss: 17.68573883293028.....Total loss: 17.68573883293028\n",
      "epoch 2666 learning rate:  0.010375093773443362   Training loss:   18.831765370054104  Valing loss:   17.68573883293028\n",
      "Pure loss: 18.826970095465942.....Total loss: 18.826970095465942\n",
      "Pure loss: 17.67261381008348.....Total loss: 17.67261381008348\n",
      "epoch 2667 learning rate:  0.010374953130858643   Training loss:   18.826970095465942  Valing loss:   17.67261381008348\n",
      "Pure loss: 18.821884452335944.....Total loss: 18.821884452335944\n",
      "Pure loss: 17.657549181242846.....Total loss: 17.657549181242846\n",
      "epoch 2668 learning rate:  0.010374812593703148   Training loss:   18.821884452335944  Valing loss:   17.657549181242846\n",
      "Pure loss: 18.82264006007571.....Total loss: 18.82264006007571\n",
      "Pure loss: 17.590168033913418.....Total loss: 17.590168033913418\n",
      "epoch 2669 learning rate:  0.010374672161858374   Training loss:   18.82264006007571  Valing loss:   17.590168033913418\n",
      "Pure loss: 18.666757987549733.....Total loss: 18.666757987549733\n",
      "Pure loss: 17.07404576447717.....Total loss: 17.07404576447717\n",
      "epoch 2670 learning rate:  0.010374531835205992   Training loss:   18.666757987549733  Valing loss:   17.07404576447717\n",
      "Pure loss: 18.69529133347244.....Total loss: 18.69529133347244\n",
      "Pure loss: 17.058681836166258.....Total loss: 17.058681836166258\n",
      "epoch 2671 learning rate:  0.010374391613627855   Training loss:   18.69529133347244  Valing loss:   17.058681836166258\n",
      "Pure loss: 18.724710823060008.....Total loss: 18.724710823060008\n",
      "Pure loss: 16.790759476334138.....Total loss: 16.790759476334138\n",
      "epoch 2672 learning rate:  0.010374251497005988   Training loss:   18.724710823060008  Valing loss:   16.790759476334138\n",
      "Pure loss: 18.834686836655496.....Total loss: 18.834686836655496\n",
      "Pure loss: 16.82632854810243.....Total loss: 16.82632854810243\n",
      "epoch 2673 learning rate:  0.010374111485222597   Training loss:   18.834686836655496  Valing loss:   16.82632854810243\n",
      "Pure loss: 18.72153594538675.....Total loss: 18.72153594538675\n",
      "Pure loss: 16.74540879980024.....Total loss: 16.74540879980024\n",
      "epoch 2674 learning rate:  0.01037397157816006   Training loss:   18.72153594538675  Valing loss:   16.74540879980024\n",
      "Pure loss: 18.676899307433658.....Total loss: 18.676899307433658\n",
      "Pure loss: 16.853955633902814.....Total loss: 16.853955633902814\n",
      "epoch 2675 learning rate:  0.010373831775700934   Training loss:   18.676899307433658  Valing loss:   16.853955633902814\n",
      "Pure loss: 18.733049529007904.....Total loss: 18.733049529007904\n",
      "Pure loss: 16.781356540113574.....Total loss: 16.781356540113574\n",
      "epoch 2676 learning rate:  0.010373692077727953   Training loss:   18.733049529007904  Valing loss:   16.781356540113574\n",
      "Pure loss: 18.958629085371353.....Total loss: 18.958629085371353\n",
      "Pure loss: 16.625084950585695.....Total loss: 16.625084950585695\n",
      "epoch 2677 learning rate:  0.01037355248412402   Training loss:   18.958629085371353  Valing loss:   16.625084950585695\n",
      "Pure loss: 18.808713968924394.....Total loss: 18.808713968924394\n",
      "Pure loss: 16.558197520694787.....Total loss: 16.558197520694787\n",
      "epoch 2678 learning rate:  0.010373412994772218   Training loss:   18.808713968924394  Valing loss:   16.558197520694787\n",
      "Pure loss: 18.66541215115229.....Total loss: 18.66541215115229\n",
      "Pure loss: 16.579505617810344.....Total loss: 16.579505617810344\n",
      "epoch 2679 learning rate:  0.010373273609555804   Training loss:   18.66541215115229  Valing loss:   16.579505617810344\n",
      "Pure loss: 18.6114574417161.....Total loss: 18.6114574417161\n",
      "Pure loss: 16.72016353198339.....Total loss: 16.72016353198339\n",
      "epoch 2680 learning rate:  0.01037313432835821   Training loss:   18.6114574417161  Valing loss:   16.72016353198339\n",
      "Pure loss: 18.609647282584937.....Total loss: 18.609647282584937\n",
      "Pure loss: 17.199860859522857.....Total loss: 17.199860859522857\n",
      "epoch 2681 learning rate:  0.010372995151063037   Training loss:   18.609647282584937  Valing loss:   17.199860859522857\n",
      "Pure loss: 18.618089421175355.....Total loss: 18.618089421175355\n",
      "Pure loss: 17.234805522588612.....Total loss: 17.234805522588612\n",
      "epoch 2682 learning rate:  0.010372856077554064   Training loss:   18.618089421175355  Valing loss:   17.234805522588612\n",
      "Pure loss: 18.673386719225476.....Total loss: 18.673386719225476\n",
      "Pure loss: 17.410517552806947.....Total loss: 17.410517552806947\n",
      "epoch 2683 learning rate:  0.010372717107715245   Training loss:   18.673386719225476  Valing loss:   17.410517552806947\n",
      "Pure loss: 18.55611431303394.....Total loss: 18.55611431303394\n",
      "Pure loss: 17.03749620480726.....Total loss: 17.03749620480726\n",
      "epoch 2684 learning rate:  0.0103725782414307   Training loss:   18.55611431303394  Valing loss:   17.03749620480726\n",
      "Pure loss: 18.561821114251835.....Total loss: 18.561821114251835\n",
      "Pure loss: 17.061758419096115.....Total loss: 17.061758419096115\n",
      "epoch 2685 learning rate:  0.01037243947858473   Training loss:   18.561821114251835  Valing loss:   17.061758419096115\n",
      "Pure loss: 18.609629541778794.....Total loss: 18.609629541778794\n",
      "Pure loss: 16.909446344163023.....Total loss: 16.909446344163023\n",
      "epoch 2686 learning rate:  0.010372300819061802   Training loss:   18.609629541778794  Valing loss:   16.909446344163023\n",
      "Pure loss: 18.645105923499923.....Total loss: 18.645105923499923\n",
      "Pure loss: 17.12579179673135.....Total loss: 17.12579179673135\n",
      "epoch 2687 learning rate:  0.010372162262746558   Training loss:   18.645105923499923  Valing loss:   17.12579179673135\n",
      "Pure loss: 18.64532165634445.....Total loss: 18.64532165634445\n",
      "Pure loss: 17.126967690766286.....Total loss: 17.126967690766286\n",
      "epoch 2688 learning rate:  0.01037202380952381   Training loss:   18.64532165634445  Valing loss:   17.126967690766286\n",
      "Pure loss: 18.648941068953473.....Total loss: 18.648941068953473\n",
      "Pure loss: 17.160562093809116.....Total loss: 17.160562093809116\n",
      "epoch 2689 learning rate:  0.010371885459278543   Training loss:   18.648941068953473  Valing loss:   17.160562093809116\n",
      "Pure loss: 18.62467859254307.....Total loss: 18.62467859254307\n",
      "Pure loss: 16.913515587907007.....Total loss: 16.913515587907007\n",
      "epoch 2690 learning rate:  0.010371747211895911   Training loss:   18.62467859254307  Valing loss:   16.913515587907007\n",
      "Pure loss: 18.624894580266492.....Total loss: 18.624894580266492\n",
      "Pure loss: 16.922713097786836.....Total loss: 16.922713097786836\n",
      "epoch 2691 learning rate:  0.010371609067261242   Training loss:   18.624894580266492  Valing loss:   16.922713097786836\n",
      "Pure loss: 18.632192158771694.....Total loss: 18.632192158771694\n",
      "Pure loss: 16.770380082550453.....Total loss: 16.770380082550453\n",
      "epoch 2692 learning rate:  0.01037147102526003   Training loss:   18.632192158771694  Valing loss:   16.770380082550453\n",
      "Pure loss: 18.633961481297543.....Total loss: 18.633961481297543\n",
      "Pure loss: 16.762713706414026.....Total loss: 16.762713706414026\n",
      "epoch 2693 learning rate:  0.010371333085777944   Training loss:   18.633961481297543  Valing loss:   16.762713706414026\n",
      "Pure loss: 18.63328360395156.....Total loss: 18.63328360395156\n",
      "Pure loss: 16.85076970496209.....Total loss: 16.85076970496209\n",
      "epoch 2694 learning rate:  0.010371195248700817   Training loss:   18.63328360395156  Valing loss:   16.85076970496209\n",
      "Pure loss: 18.729908973898702.....Total loss: 18.729908973898702\n",
      "Pure loss: 16.59173852930289.....Total loss: 16.59173852930289\n",
      "epoch 2695 learning rate:  0.010371057513914657   Training loss:   18.729908973898702  Valing loss:   16.59173852930289\n",
      "Pure loss: 18.557108156605807.....Total loss: 18.557108156605807\n",
      "Pure loss: 16.835180076247447.....Total loss: 16.835180076247447\n",
      "epoch 2696 learning rate:  0.010370919881305639   Training loss:   18.557108156605807  Valing loss:   16.835180076247447\n",
      "Pure loss: 18.533167858712744.....Total loss: 18.533167858712744\n",
      "Pure loss: 16.856105045045624.....Total loss: 16.856105045045624\n",
      "epoch 2697 learning rate:  0.010370782350760104   Training loss:   18.533167858712744  Valing loss:   16.856105045045624\n",
      "Pure loss: 18.525556413113335.....Total loss: 18.525556413113335\n",
      "Pure loss: 16.696175120722124.....Total loss: 16.696175120722124\n",
      "epoch 2698 learning rate:  0.010370644922164567   Training loss:   18.525556413113335  Valing loss:   16.696175120722124\n",
      "Pure loss: 18.60512168541333.....Total loss: 18.60512168541333\n",
      "Pure loss: 16.66054060880114.....Total loss: 16.66054060880114\n",
      "epoch 2699 learning rate:  0.010370507595405707   Training loss:   18.60512168541333  Valing loss:   16.66054060880114\n",
      "Pure loss: 18.588166945481056.....Total loss: 18.588166945481056\n",
      "Pure loss: 16.782926766729457.....Total loss: 16.782926766729457\n",
      "epoch 2700 learning rate:  0.01037037037037037   Training loss:   18.588166945481056  Valing loss:   16.782926766729457\n",
      "Pure loss: 18.596720998244145.....Total loss: 18.596720998244145\n",
      "Pure loss: 16.785100092732733.....Total loss: 16.785100092732733\n",
      "epoch 2701 learning rate:  0.010370233246945576   Training loss:   18.596720998244145  Valing loss:   16.785100092732733\n",
      "Pure loss: 18.58445391823383.....Total loss: 18.58445391823383\n",
      "Pure loss: 16.791811190414926.....Total loss: 16.791811190414926\n",
      "epoch 2702 learning rate:  0.010370096225018504   Training loss:   18.58445391823383  Valing loss:   16.791811190414926\n",
      "Pure loss: 18.588453007724727.....Total loss: 18.588453007724727\n",
      "Pure loss: 16.479219210869864.....Total loss: 16.479219210869864\n",
      "epoch 2703 learning rate:  0.010369959304476508   Training loss:   18.588453007724727  Valing loss:   16.479219210869864\n",
      "Pure loss: 18.63865584825339.....Total loss: 18.63865584825339\n",
      "Pure loss: 16.455375511185878.....Total loss: 16.455375511185878\n",
      "epoch 2704 learning rate:  0.010369822485207101   Training loss:   18.63865584825339  Valing loss:   16.455375511185878\n",
      "Pure loss: 18.609558589207133.....Total loss: 18.609558589207133\n",
      "Pure loss: 16.648420073208772.....Total loss: 16.648420073208772\n",
      "epoch 2705 learning rate:  0.010369685767097967   Training loss:   18.609558589207133  Valing loss:   16.648420073208772\n",
      "Pure loss: 18.716763397362797.....Total loss: 18.716763397362797\n",
      "Pure loss: 16.545988479715547.....Total loss: 16.545988479715547\n",
      "epoch 2706 learning rate:  0.010369549150036956   Training loss:   18.716763397362797  Valing loss:   16.545988479715547\n",
      "Pure loss: 18.602239298948216.....Total loss: 18.602239298948216\n",
      "Pure loss: 16.736738388124483.....Total loss: 16.736738388124483\n",
      "epoch 2707 learning rate:  0.01036941263391208   Training loss:   18.602239298948216  Valing loss:   16.736738388124483\n",
      "Pure loss: 18.579213451662437.....Total loss: 18.579213451662437\n",
      "Pure loss: 16.747576457041294.....Total loss: 16.747576457041294\n",
      "epoch 2708 learning rate:  0.010369276218611521   Training loss:   18.579213451662437  Valing loss:   16.747576457041294\n",
      "Pure loss: 18.533794062756332.....Total loss: 18.533794062756332\n",
      "Pure loss: 16.81140879397031.....Total loss: 16.81140879397031\n",
      "epoch 2709 learning rate:  0.010369139904023624   Training loss:   18.533794062756332  Valing loss:   16.81140879397031\n",
      "Pure loss: 18.528401859029493.....Total loss: 18.528401859029493\n",
      "Pure loss: 16.714847802324975.....Total loss: 16.714847802324975\n",
      "epoch 2710 learning rate:  0.0103690036900369   Training loss:   18.528401859029493  Valing loss:   16.714847802324975\n",
      "Pure loss: 18.55421154836955.....Total loss: 18.55421154836955\n",
      "Pure loss: 16.60112373594155.....Total loss: 16.60112373594155\n",
      "epoch 2711 learning rate:  0.010368867576540023   Training loss:   18.55421154836955  Valing loss:   16.60112373594155\n",
      "Pure loss: 18.53888717028311.....Total loss: 18.53888717028311\n",
      "Pure loss: 16.88423510920253.....Total loss: 16.88423510920253\n",
      "epoch 2712 learning rate:  0.010368731563421829   Training loss:   18.53888717028311  Valing loss:   16.88423510920253\n",
      "Pure loss: 18.532911012043083.....Total loss: 18.532911012043083\n",
      "Pure loss: 16.835032005315004.....Total loss: 16.835032005315004\n",
      "epoch 2713 learning rate:  0.010368595650571324   Training loss:   18.532911012043083  Valing loss:   16.835032005315004\n",
      "Pure loss: 18.51348909069388.....Total loss: 18.51348909069388\n",
      "Pure loss: 16.830603674629938.....Total loss: 16.830603674629938\n",
      "epoch 2714 learning rate:  0.010368459837877672   Training loss:   18.51348909069388  Valing loss:   16.830603674629938\n",
      "Pure loss: 18.48045387563434.....Total loss: 18.48045387563434\n",
      "Pure loss: 16.419893796366274.....Total loss: 16.419893796366274\n",
      "epoch 2715 learning rate:  0.010368324125230203   Training loss:   18.48045387563434  Valing loss:   16.419893796366274\n",
      "Pure loss: 18.70537581566377.....Total loss: 18.70537581566377\n",
      "Pure loss: 16.955471091326537.....Total loss: 16.955471091326537\n",
      "epoch 2716 learning rate:  0.01036818851251841   Training loss:   18.70537581566377  Valing loss:   16.955471091326537\n",
      "Pure loss: 18.729020227005716.....Total loss: 18.729020227005716\n",
      "Pure loss: 17.130720687974907.....Total loss: 17.130720687974907\n",
      "epoch 2717 learning rate:  0.010368052999631948   Training loss:   18.729020227005716  Valing loss:   17.130720687974907\n",
      "Pure loss: 18.71077637226364.....Total loss: 18.71077637226364\n",
      "Pure loss: 17.0780530810017.....Total loss: 17.0780530810017\n",
      "epoch 2718 learning rate:  0.010367917586460633   Training loss:   18.71077637226364  Valing loss:   17.0780530810017\n",
      "Pure loss: 18.592913013848005.....Total loss: 18.592913013848005\n",
      "Pure loss: 16.617727565878504.....Total loss: 16.617727565878504\n",
      "epoch 2719 learning rate:  0.010367782272894447   Training loss:   18.592913013848005  Valing loss:   16.617727565878504\n",
      "Pure loss: 18.69062011364329.....Total loss: 18.69062011364329\n",
      "Pure loss: 17.25973090470661.....Total loss: 17.25973090470661\n",
      "epoch 2720 learning rate:  0.01036764705882353   Training loss:   18.69062011364329  Valing loss:   17.25973090470661\n",
      "Pure loss: 18.610662089549482.....Total loss: 18.610662089549482\n",
      "Pure loss: 16.977566824477748.....Total loss: 16.977566824477748\n",
      "epoch 2721 learning rate:  0.010367511944138185   Training loss:   18.610662089549482  Valing loss:   16.977566824477748\n",
      "Pure loss: 18.612641359289253.....Total loss: 18.612641359289253\n",
      "Pure loss: 16.98646549974864.....Total loss: 16.98646549974864\n",
      "epoch 2722 learning rate:  0.010367376928728876   Training loss:   18.612641359289253  Valing loss:   16.98646549974864\n",
      "Pure loss: 18.573772906118432.....Total loss: 18.573772906118432\n",
      "Pure loss: 16.820755037865055.....Total loss: 16.820755037865055\n",
      "epoch 2723 learning rate:  0.010367242012486228   Training loss:   18.573772906118432  Valing loss:   16.820755037865055\n",
      "Pure loss: 18.694689773283816.....Total loss: 18.694689773283816\n",
      "Pure loss: 17.223593047210976.....Total loss: 17.223593047210976\n",
      "epoch 2724 learning rate:  0.010367107195301027   Training loss:   18.694689773283816  Valing loss:   17.223593047210976\n",
      "Pure loss: 18.527968514814035.....Total loss: 18.527968514814035\n",
      "Pure loss: 16.681338747299076.....Total loss: 16.681338747299076\n",
      "epoch 2725 learning rate:  0.01036697247706422   Training loss:   18.527968514814035  Valing loss:   16.681338747299076\n",
      "Pure loss: 18.521813084704178.....Total loss: 18.521813084704178\n",
      "Pure loss: 16.487879716593284.....Total loss: 16.487879716593284\n",
      "epoch 2726 learning rate:  0.010366837857666912   Training loss:   18.521813084704178  Valing loss:   16.487879716593284\n",
      "Pure loss: 18.762504254262694.....Total loss: 18.762504254262694\n",
      "Pure loss: 16.166909175946014.....Total loss: 16.166909175946014\n",
      "epoch 2727 learning rate:  0.010366703337000367   Training loss:   18.762504254262694  Valing loss:   16.166909175946014\n",
      "Pure loss: 18.60033371982015.....Total loss: 18.60033371982015\n",
      "Pure loss: 16.300389646153537.....Total loss: 16.300389646153537\n",
      "epoch 2728 learning rate:  0.010366568914956012   Training loss:   18.60033371982015  Valing loss:   16.300389646153537\n",
      "Pure loss: 18.646494440751372.....Total loss: 18.646494440751372\n",
      "Pure loss: 16.163872594290034.....Total loss: 16.163872594290034\n",
      "epoch 2729 learning rate:  0.01036643459142543   Training loss:   18.646494440751372  Valing loss:   16.163872594290034\n",
      "Pure loss: 18.580204833547825.....Total loss: 18.580204833547825\n",
      "Pure loss: 16.28658774191191.....Total loss: 16.28658774191191\n",
      "epoch 2730 learning rate:  0.010366300366300367   Training loss:   18.580204833547825  Valing loss:   16.28658774191191\n",
      "Pure loss: 18.565606762018533.....Total loss: 18.565606762018533\n",
      "Pure loss: 16.398430439947514.....Total loss: 16.398430439947514\n",
      "epoch 2731 learning rate:  0.01036616623947272   Training loss:   18.565606762018533  Valing loss:   16.398430439947514\n",
      "Pure loss: 18.56622624608145.....Total loss: 18.56622624608145\n",
      "Pure loss: 16.388314805487845.....Total loss: 16.388314805487845\n",
      "epoch 2732 learning rate:  0.010366032210834554   Training loss:   18.56622624608145  Valing loss:   16.388314805487845\n",
      "Pure loss: 18.912415357653554.....Total loss: 18.912415357653554\n",
      "Pure loss: 17.005875610508774.....Total loss: 17.005875610508774\n",
      "epoch 2733 learning rate:  0.010365898280278082   Training loss:   18.912415357653554  Valing loss:   17.005875610508774\n",
      "Pure loss: 18.961577332348472.....Total loss: 18.961577332348472\n",
      "Pure loss: 17.06538951246549.....Total loss: 17.06538951246549\n",
      "epoch 2734 learning rate:  0.010365764447695684   Training loss:   18.961577332348472  Valing loss:   17.06538951246549\n",
      "Pure loss: 18.957417980029238.....Total loss: 18.957417980029238\n",
      "Pure loss: 17.04687562722159.....Total loss: 17.04687562722159\n",
      "epoch 2735 learning rate:  0.010365630712979891   Training loss:   18.957417980029238  Valing loss:   17.04687562722159\n",
      "Pure loss: 19.011675410426548.....Total loss: 19.011675410426548\n",
      "Pure loss: 17.153207892758655.....Total loss: 17.153207892758655\n",
      "epoch 2736 learning rate:  0.010365497076023392   Training loss:   19.011675410426548  Valing loss:   17.153207892758655\n",
      "Pure loss: 19.0709951920061.....Total loss: 19.0709951920061\n",
      "Pure loss: 17.310280271643933.....Total loss: 17.310280271643933\n",
      "epoch 2737 learning rate:  0.010365363536719036   Training loss:   19.0709951920061  Valing loss:   17.310280271643933\n",
      "Pure loss: 18.954312682059776.....Total loss: 18.954312682059776\n",
      "Pure loss: 16.656795549145173.....Total loss: 16.656795549145173\n",
      "epoch 2738 learning rate:  0.010365230094959824   Training loss:   18.954312682059776  Valing loss:   16.656795549145173\n",
      "Pure loss: 18.848094945256666.....Total loss: 18.848094945256666\n",
      "Pure loss: 17.638365100664792.....Total loss: 17.638365100664792\n",
      "epoch 2739 learning rate:  0.01036509675063892   Training loss:   18.848094945256666  Valing loss:   17.638365100664792\n",
      "Pure loss: 18.691638723700333.....Total loss: 18.691638723700333\n",
      "Pure loss: 17.38461512597037.....Total loss: 17.38461512597037\n",
      "epoch 2740 learning rate:  0.010364963503649635   Training loss:   18.691638723700333  Valing loss:   17.38461512597037\n",
      "Pure loss: 18.783967223060767.....Total loss: 18.783967223060767\n",
      "Pure loss: 17.576661910704672.....Total loss: 17.576661910704672\n",
      "epoch 2741 learning rate:  0.010364830353885444   Training loss:   18.783967223060767  Valing loss:   17.576661910704672\n",
      "Pure loss: 18.541866650139625.....Total loss: 18.541866650139625\n",
      "Pure loss: 17.066049701134784.....Total loss: 17.066049701134784\n",
      "epoch 2742 learning rate:  0.010364697301239971   Training loss:   18.541866650139625  Valing loss:   17.066049701134784\n",
      "Pure loss: 18.46093835453778.....Total loss: 18.46093835453778\n",
      "Pure loss: 16.766397394003647.....Total loss: 16.766397394003647\n",
      "epoch 2743 learning rate:  0.010364564345607   Training loss:   18.46093835453778  Valing loss:   16.766397394003647\n",
      "Pure loss: 18.388943440309415.....Total loss: 18.388943440309415\n",
      "Pure loss: 16.633194445167437.....Total loss: 16.633194445167437\n",
      "epoch 2744 learning rate:  0.010364431486880467   Training loss:   18.388943440309415  Valing loss:   16.633194445167437\n",
      "Pure loss: 18.354466731504683.....Total loss: 18.354466731504683\n",
      "Pure loss: 16.4185104474801.....Total loss: 16.4185104474801\n",
      "epoch 2745 learning rate:  0.010364298724954462   Training loss:   18.354466731504683  Valing loss:   16.4185104474801\n",
      "Pure loss: 18.40900188747829.....Total loss: 18.40900188747829\n",
      "Pure loss: 16.303285900364127.....Total loss: 16.303285900364127\n",
      "epoch 2746 learning rate:  0.010364166059723234   Training loss:   18.40900188747829  Valing loss:   16.303285900364127\n",
      "Pure loss: 18.44306338565211.....Total loss: 18.44306338565211\n",
      "Pure loss: 16.26098064845311.....Total loss: 16.26098064845311\n",
      "epoch 2747 learning rate:  0.010364033491081179   Training loss:   18.44306338565211  Valing loss:   16.26098064845311\n",
      "Pure loss: 18.410842166682812.....Total loss: 18.410842166682812\n",
      "Pure loss: 16.255632993116077.....Total loss: 16.255632993116077\n",
      "epoch 2748 learning rate:  0.010363901018922853   Training loss:   18.410842166682812  Valing loss:   16.255632993116077\n",
      "Pure loss: 18.470960741219933.....Total loss: 18.470960741219933\n",
      "Pure loss: 16.298219250368906.....Total loss: 16.298219250368906\n",
      "epoch 2749 learning rate:  0.010363768643142961   Training loss:   18.470960741219933  Valing loss:   16.298219250368906\n",
      "Pure loss: 18.556873671440073.....Total loss: 18.556873671440073\n",
      "Pure loss: 16.15418281732395.....Total loss: 16.15418281732395\n",
      "epoch 2750 learning rate:  0.010363636363636363   Training loss:   18.556873671440073  Valing loss:   16.15418281732395\n",
      "Pure loss: 18.600984845363406.....Total loss: 18.600984845363406\n",
      "Pure loss: 16.134669022487678.....Total loss: 16.134669022487678\n",
      "epoch 2751 learning rate:  0.010363504180298074   Training loss:   18.600984845363406  Valing loss:   16.134669022487678\n",
      "Pure loss: 18.808319218297026.....Total loss: 18.808319218297026\n",
      "Pure loss: 16.075530122831143.....Total loss: 16.075530122831143\n",
      "epoch 2752 learning rate:  0.010363372093023257   Training loss:   18.808319218297026  Valing loss:   16.075530122831143\n",
      "Pure loss: 19.08029297162853.....Total loss: 19.08029297162853\n",
      "Pure loss: 16.10081492955747.....Total loss: 16.10081492955747\n",
      "epoch 2753 learning rate:  0.010363240101707229   Training loss:   19.08029297162853  Valing loss:   16.10081492955747\n",
      "Pure loss: 19.448497821153833.....Total loss: 19.448497821153833\n",
      "Pure loss: 16.214780536097223.....Total loss: 16.214780536097223\n",
      "epoch 2754 learning rate:  0.010363108206245461   Training loss:   19.448497821153833  Valing loss:   16.214780536097223\n",
      "Pure loss: 19.034507702135887.....Total loss: 19.034507702135887\n",
      "Pure loss: 16.089962276358122.....Total loss: 16.089962276358122\n",
      "epoch 2755 learning rate:  0.010362976406533575   Training loss:   19.034507702135887  Valing loss:   16.089962276358122\n",
      "Pure loss: 19.276401192635568.....Total loss: 19.276401192635568\n",
      "Pure loss: 16.153132182011696.....Total loss: 16.153132182011696\n",
      "epoch 2756 learning rate:  0.010362844702467344   Training loss:   19.276401192635568  Valing loss:   16.153132182011696\n",
      "Pure loss: 19.09764572364585.....Total loss: 19.09764572364585\n",
      "Pure loss: 16.09910520595372.....Total loss: 16.09910520595372\n",
      "epoch 2757 learning rate:  0.010362713093942691   Training loss:   19.09764572364585  Valing loss:   16.09910520595372\n",
      "Pure loss: 19.625939878665672.....Total loss: 19.625939878665672\n",
      "Pure loss: 16.24897498177774.....Total loss: 16.24897498177774\n",
      "epoch 2758 learning rate:  0.010362581580855693   Training loss:   19.625939878665672  Valing loss:   16.24897498177774\n",
      "Pure loss: 19.510852621758946.....Total loss: 19.510852621758946\n",
      "Pure loss: 16.199461774438266.....Total loss: 16.199461774438266\n",
      "epoch 2759 learning rate:  0.010362450163102574   Training loss:   19.510852621758946  Valing loss:   16.199461774438266\n",
      "Pure loss: 19.736071955232426.....Total loss: 19.736071955232426\n",
      "Pure loss: 16.278995229502517.....Total loss: 16.278995229502517\n",
      "epoch 2760 learning rate:  0.01036231884057971   Training loss:   19.736071955232426  Valing loss:   16.278995229502517\n",
      "Pure loss: 19.88730733720382.....Total loss: 19.88730733720382\n",
      "Pure loss: 16.36048321302666.....Total loss: 16.36048321302666\n",
      "epoch 2761 learning rate:  0.010362187613183629   Training loss:   19.88730733720382  Valing loss:   16.36048321302666\n",
      "Pure loss: 19.38810936143402.....Total loss: 19.38810936143402\n",
      "Pure loss: 16.13450244127435.....Total loss: 16.13450244127435\n",
      "epoch 2762 learning rate:  0.010362056480811007   Training loss:   19.38810936143402  Valing loss:   16.13450244127435\n",
      "Pure loss: 19.268133235439567.....Total loss: 19.268133235439567\n",
      "Pure loss: 16.094073221774863.....Total loss: 16.094073221774863\n",
      "epoch 2763 learning rate:  0.010361925443358668   Training loss:   19.268133235439567  Valing loss:   16.094073221774863\n",
      "Pure loss: 18.999002652925682.....Total loss: 18.999002652925682\n",
      "Pure loss: 15.952552993355926.....Total loss: 15.952552993355926\n",
      "epoch 2764 learning rate:  0.01036179450072359   Training loss:   18.999002652925682  Valing loss:   15.952552993355926\n",
      "Pure loss: 19.055465859411136.....Total loss: 19.055465859411136\n",
      "Pure loss: 15.958231408222272.....Total loss: 15.958231408222272\n",
      "epoch 2765 learning rate:  0.010361663652802893   Training loss:   19.055465859411136  Valing loss:   15.958231408222272\n",
      "Pure loss: 18.85652176461753.....Total loss: 18.85652176461753\n",
      "Pure loss: 15.959851434810965.....Total loss: 15.959851434810965\n",
      "epoch 2766 learning rate:  0.010361532899493854   Training loss:   18.85652176461753  Valing loss:   15.959851434810965\n",
      "Pure loss: 18.747409504299824.....Total loss: 18.747409504299824\n",
      "Pure loss: 15.993910625261845.....Total loss: 15.993910625261845\n",
      "epoch 2767 learning rate:  0.010361402240693892   Training loss:   18.747409504299824  Valing loss:   15.993910625261845\n",
      "Pure loss: 18.756291123464667.....Total loss: 18.756291123464667\n",
      "Pure loss: 15.951496907948938.....Total loss: 15.951496907948938\n",
      "epoch 2768 learning rate:  0.010361271676300578   Training loss:   18.756291123464667  Valing loss:   15.951496907948938\n",
      "Pure loss: 18.433912853738676.....Total loss: 18.433912853738676\n",
      "Pure loss: 16.25116778123544.....Total loss: 16.25116778123544\n",
      "epoch 2769 learning rate:  0.010361141206211628   Training loss:   18.433912853738676  Valing loss:   16.25116778123544\n",
      "Pure loss: 18.478564866195228.....Total loss: 18.478564866195228\n",
      "Pure loss: 16.06439209394715.....Total loss: 16.06439209394715\n",
      "epoch 2770 learning rate:  0.01036101083032491   Training loss:   18.478564866195228  Valing loss:   16.06439209394715\n",
      "Pure loss: 18.35527460087968.....Total loss: 18.35527460087968\n",
      "Pure loss: 16.485205261125184.....Total loss: 16.485205261125184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2771 learning rate:  0.010360880548538435   Training loss:   18.35527460087968  Valing loss:   16.485205261125184\n",
      "Pure loss: 18.46227291457238.....Total loss: 18.46227291457238\n",
      "Pure loss: 16.65919639757441.....Total loss: 16.65919639757441\n",
      "epoch 2772 learning rate:  0.01036075036075036   Training loss:   18.46227291457238  Valing loss:   16.65919639757441\n",
      "Pure loss: 18.396170633401564.....Total loss: 18.396170633401564\n",
      "Pure loss: 16.34392255357732.....Total loss: 16.34392255357732\n",
      "epoch 2773 learning rate:  0.010360620266858998   Training loss:   18.396170633401564  Valing loss:   16.34392255357732\n",
      "Pure loss: 18.366104219362892.....Total loss: 18.366104219362892\n",
      "Pure loss: 16.234572755238876.....Total loss: 16.234572755238876\n",
      "epoch 2774 learning rate:  0.010360490266762798   Training loss:   18.366104219362892  Valing loss:   16.234572755238876\n",
      "Pure loss: 18.347523311854815.....Total loss: 18.347523311854815\n",
      "Pure loss: 16.167040512262673.....Total loss: 16.167040512262673\n",
      "epoch 2775 learning rate:  0.01036036036036036   Training loss:   18.347523311854815  Valing loss:   16.167040512262673\n",
      "Pure loss: 18.34801268597624.....Total loss: 18.34801268597624\n",
      "Pure loss: 16.16364030251394.....Total loss: 16.16364030251394\n",
      "epoch 2776 learning rate:  0.010360230547550432   Training loss:   18.34801268597624  Valing loss:   16.16364030251394\n",
      "Pure loss: 18.300660447329548.....Total loss: 18.300660447329548\n",
      "Pure loss: 16.84821364012574.....Total loss: 16.84821364012574\n",
      "epoch 2777 learning rate:  0.010360100828231905   Training loss:   18.300660447329548  Valing loss:   16.84821364012574\n",
      "Pure loss: 18.355716908799547.....Total loss: 18.355716908799547\n",
      "Pure loss: 17.003420795741103.....Total loss: 17.003420795741103\n",
      "epoch 2778 learning rate:  0.010359971202303815   Training loss:   18.355716908799547  Valing loss:   17.003420795741103\n",
      "Pure loss: 18.230556327501063.....Total loss: 18.230556327501063\n",
      "Pure loss: 16.67856130038534.....Total loss: 16.67856130038534\n",
      "epoch 2779 learning rate:  0.010359841669665347   Training loss:   18.230556327501063  Valing loss:   16.67856130038534\n",
      "Pure loss: 18.402245714991615.....Total loss: 18.402245714991615\n",
      "Pure loss: 17.06666448641713.....Total loss: 17.06666448641713\n",
      "epoch 2780 learning rate:  0.010359712230215827   Training loss:   18.402245714991615  Valing loss:   17.06666448641713\n",
      "Pure loss: 18.533273184746943.....Total loss: 18.533273184746943\n",
      "Pure loss: 17.26634816349218.....Total loss: 17.26634816349218\n",
      "epoch 2781 learning rate:  0.010359582883854728   Training loss:   18.533273184746943  Valing loss:   17.26634816349218\n",
      "Pure loss: 18.39910313698544.....Total loss: 18.39910313698544\n",
      "Pure loss: 17.043100063435745.....Total loss: 17.043100063435745\n",
      "epoch 2782 learning rate:  0.010359453630481668   Training loss:   18.39910313698544  Valing loss:   17.043100063435745\n",
      "Pure loss: 18.214805059701845.....Total loss: 18.214805059701845\n",
      "Pure loss: 16.37710230469881.....Total loss: 16.37710230469881\n",
      "epoch 2783 learning rate:  0.010359324469996408   Training loss:   18.214805059701845  Valing loss:   16.37710230469881\n",
      "Pure loss: 18.221534778979862.....Total loss: 18.221534778979862\n",
      "Pure loss: 16.258306026324778.....Total loss: 16.258306026324778\n",
      "epoch 2784 learning rate:  0.01035919540229885   Training loss:   18.221534778979862  Valing loss:   16.258306026324778\n",
      "Pure loss: 18.212281003055434.....Total loss: 18.212281003055434\n",
      "Pure loss: 16.291979204553265.....Total loss: 16.291979204553265\n",
      "epoch 2785 learning rate:  0.010359066427289048   Training loss:   18.212281003055434  Valing loss:   16.291979204553265\n",
      "Pure loss: 18.227974718707298.....Total loss: 18.227974718707298\n",
      "Pure loss: 16.820942366344507.....Total loss: 16.820942366344507\n",
      "epoch 2786 learning rate:  0.010358937544867193   Training loss:   18.227974718707298  Valing loss:   16.820942366344507\n",
      "Pure loss: 18.390889639144863.....Total loss: 18.390889639144863\n",
      "Pure loss: 17.266606184634938.....Total loss: 17.266606184634938\n",
      "epoch 2787 learning rate:  0.010358808754933621   Training loss:   18.390889639144863  Valing loss:   17.266606184634938\n",
      "Pure loss: 18.220189976142468.....Total loss: 18.220189976142468\n",
      "Pure loss: 16.781168703610938.....Total loss: 16.781168703610938\n",
      "epoch 2788 learning rate:  0.01035868005738881   Training loss:   18.220189976142468  Valing loss:   16.781168703610938\n",
      "Pure loss: 18.26884114656289.....Total loss: 18.26884114656289\n",
      "Pure loss: 16.93354092180259.....Total loss: 16.93354092180259\n",
      "epoch 2789 learning rate:  0.010358551452133382   Training loss:   18.26884114656289  Valing loss:   16.93354092180259\n",
      "Pure loss: 18.158903736403303.....Total loss: 18.158903736403303\n",
      "Pure loss: 16.603213155358794.....Total loss: 16.603213155358794\n",
      "epoch 2790 learning rate:  0.0103584229390681   Training loss:   18.158903736403303  Valing loss:   16.603213155358794\n",
      "Pure loss: 18.174297947405154.....Total loss: 18.174297947405154\n",
      "Pure loss: 16.4682617570796.....Total loss: 16.4682617570796\n",
      "epoch 2791 learning rate:  0.010358294518093874   Training loss:   18.174297947405154  Valing loss:   16.4682617570796\n",
      "Pure loss: 18.170146282373523.....Total loss: 18.170146282373523\n",
      "Pure loss: 16.472809230677573.....Total loss: 16.472809230677573\n",
      "epoch 2792 learning rate:  0.010358166189111747   Training loss:   18.170146282373523  Valing loss:   16.472809230677573\n",
      "Pure loss: 18.278358918158514.....Total loss: 18.278358918158514\n",
      "Pure loss: 16.20081805286517.....Total loss: 16.20081805286517\n",
      "epoch 2793 learning rate:  0.010358037952022914   Training loss:   18.278358918158514  Valing loss:   16.20081805286517\n",
      "Pure loss: 18.164633472255197.....Total loss: 18.164633472255197\n",
      "Pure loss: 16.667975227067842.....Total loss: 16.667975227067842\n",
      "epoch 2794 learning rate:  0.010357909806728705   Training loss:   18.164633472255197  Valing loss:   16.667975227067842\n",
      "Pure loss: 18.16362188842105.....Total loss: 18.16362188842105\n",
      "Pure loss: 16.63678353324584.....Total loss: 16.63678353324584\n",
      "epoch 2795 learning rate:  0.01035778175313059   Training loss:   18.16362188842105  Valing loss:   16.63678353324584\n",
      "Pure loss: 18.185199834157423.....Total loss: 18.185199834157423\n",
      "Pure loss: 16.54281965444286.....Total loss: 16.54281965444286\n",
      "epoch 2796 learning rate:  0.010357653791130187   Training loss:   18.185199834157423  Valing loss:   16.54281965444286\n",
      "Pure loss: 18.176275143193163.....Total loss: 18.176275143193163\n",
      "Pure loss: 16.561288459094655.....Total loss: 16.561288459094655\n",
      "epoch 2797 learning rate:  0.010357525920629246   Training loss:   18.176275143193163  Valing loss:   16.561288459094655\n",
      "Pure loss: 18.242054309024947.....Total loss: 18.242054309024947\n",
      "Pure loss: 16.379891589074465.....Total loss: 16.379891589074465\n",
      "epoch 2798 learning rate:  0.010357398141529665   Training loss:   18.242054309024947  Valing loss:   16.379891589074465\n",
      "Pure loss: 18.414658430194578.....Total loss: 18.414658430194578\n",
      "Pure loss: 16.301318693941308.....Total loss: 16.301318693941308\n",
      "epoch 2799 learning rate:  0.010357270453733476   Training loss:   18.414658430194578  Valing loss:   16.301318693941308\n",
      "Pure loss: 18.375896144903788.....Total loss: 18.375896144903788\n",
      "Pure loss: 16.28838374563556.....Total loss: 16.28838374563556\n",
      "epoch 2800 learning rate:  0.010357142857142858   Training loss:   18.375896144903788  Valing loss:   16.28838374563556\n",
      "Pure loss: 18.52394690214939.....Total loss: 18.52394690214939\n",
      "Pure loss: 16.24829294196518.....Total loss: 16.24829294196518\n",
      "epoch 2801 learning rate:  0.010357015351660122   Training loss:   18.52394690214939  Valing loss:   16.24829294196518\n",
      "Pure loss: 18.492350361551487.....Total loss: 18.492350361551487\n",
      "Pure loss: 16.238481884011907.....Total loss: 16.238481884011907\n",
      "epoch 2802 learning rate:  0.010356887937187724   Training loss:   18.492350361551487  Valing loss:   16.238481884011907\n",
      "Pure loss: 18.327926974817924.....Total loss: 18.327926974817924\n",
      "Pure loss: 16.23460811856572.....Total loss: 16.23460811856572\n",
      "epoch 2803 learning rate:  0.010356760613628255   Training loss:   18.327926974817924  Valing loss:   16.23460811856572\n",
      "Pure loss: 18.758805285475823.....Total loss: 18.758805285475823\n",
      "Pure loss: 16.112664900556283.....Total loss: 16.112664900556283\n",
      "epoch 2804 learning rate:  0.010356633380884451   Training loss:   18.758805285475823  Valing loss:   16.112664900556283\n",
      "Pure loss: 18.737077407501545.....Total loss: 18.737077407501545\n",
      "Pure loss: 16.10484965899809.....Total loss: 16.10484965899809\n",
      "epoch 2805 learning rate:  0.01035650623885918   Training loss:   18.737077407501545  Valing loss:   16.10484965899809\n",
      "Pure loss: 18.283563701087893.....Total loss: 18.283563701087893\n",
      "Pure loss: 16.22347078355324.....Total loss: 16.22347078355324\n",
      "epoch 2806 learning rate:  0.010356379187455453   Training loss:   18.283563701087893  Valing loss:   16.22347078355324\n",
      "Pure loss: 18.915869699076936.....Total loss: 18.915869699076936\n",
      "Pure loss: 17.71261439024459.....Total loss: 17.71261439024459\n",
      "epoch 2807 learning rate:  0.010356252226576417   Training loss:   18.915869699076936  Valing loss:   17.71261439024459\n",
      "Pure loss: 18.997491659505187.....Total loss: 18.997491659505187\n",
      "Pure loss: 17.89303172364877.....Total loss: 17.89303172364877\n",
      "epoch 2808 learning rate:  0.010356125356125356   Training loss:   18.997491659505187  Valing loss:   17.89303172364877\n",
      "Pure loss: 19.10825864320421.....Total loss: 19.10825864320421\n",
      "Pure loss: 18.091985302610773.....Total loss: 18.091985302610773\n",
      "epoch 2809 learning rate:  0.010355998576005696   Training loss:   19.10825864320421  Valing loss:   18.091985302610773\n",
      "Pure loss: 19.425245407382235.....Total loss: 19.425245407382235\n",
      "Pure loss: 18.55506571571939.....Total loss: 18.55506571571939\n",
      "epoch 2810 learning rate:  0.010355871886120997   Training loss:   19.425245407382235  Valing loss:   18.55506571571939\n",
      "Pure loss: 19.234362510952742.....Total loss: 19.234362510952742\n",
      "Pure loss: 18.15046089903145.....Total loss: 18.15046089903145\n",
      "epoch 2811 learning rate:  0.010355745286374956   Training loss:   19.234362510952742  Valing loss:   18.15046089903145\n",
      "Pure loss: 19.204510948473875.....Total loss: 19.204510948473875\n",
      "Pure loss: 18.09083825469534.....Total loss: 18.09083825469534\n",
      "epoch 2812 learning rate:  0.010355618776671408   Training loss:   19.204510948473875  Valing loss:   18.09083825469534\n",
      "Pure loss: 18.46883669182064.....Total loss: 18.46883669182064\n",
      "Pure loss: 17.02940601651794.....Total loss: 17.02940601651794\n",
      "epoch 2813 learning rate:  0.010355492356914327   Training loss:   18.46883669182064  Valing loss:   17.02940601651794\n",
      "Pure loss: 18.593377449791884.....Total loss: 18.593377449791884\n",
      "Pure loss: 17.392620232009254.....Total loss: 17.392620232009254\n",
      "epoch 2814 learning rate:  0.010355366027007817   Training loss:   18.593377449791884  Valing loss:   17.392620232009254\n",
      "Pure loss: 18.422105147281425.....Total loss: 18.422105147281425\n",
      "Pure loss: 17.104823862503054.....Total loss: 17.104823862503054\n",
      "epoch 2815 learning rate:  0.010355239786856128   Training loss:   18.422105147281425  Valing loss:   17.104823862503054\n",
      "Pure loss: 18.348080451631354.....Total loss: 18.348080451631354\n",
      "Pure loss: 16.9467105137691.....Total loss: 16.9467105137691\n",
      "epoch 2816 learning rate:  0.010355113636363636   Training loss:   18.348080451631354  Valing loss:   16.9467105137691\n",
      "Pure loss: 18.321982802714462.....Total loss: 18.321982802714462\n",
      "Pure loss: 16.88657027029107.....Total loss: 16.88657027029107\n",
      "epoch 2817 learning rate:  0.01035498757543486   Training loss:   18.321982802714462  Valing loss:   16.88657027029107\n",
      "Pure loss: 18.311756547959163.....Total loss: 18.311756547959163\n",
      "Pure loss: 16.85351569928116.....Total loss: 16.85351569928116\n",
      "epoch 2818 learning rate:  0.01035486160397445   Training loss:   18.311756547959163  Valing loss:   16.85351569928116\n",
      "Pure loss: 18.368115043171585.....Total loss: 18.368115043171585\n",
      "Pure loss: 17.05357948339736.....Total loss: 17.05357948339736\n",
      "epoch 2819 learning rate:  0.010354735721887194   Training loss:   18.368115043171585  Valing loss:   17.05357948339736\n",
      "Pure loss: 18.2279538359658.....Total loss: 18.2279538359658\n",
      "Pure loss: 16.482575172187424.....Total loss: 16.482575172187424\n",
      "epoch 2820 learning rate:  0.010354609929078015   Training loss:   18.2279538359658  Valing loss:   16.482575172187424\n",
      "Pure loss: 18.235263446997976.....Total loss: 18.235263446997976\n",
      "Pure loss: 16.40060654351601.....Total loss: 16.40060654351601\n",
      "epoch 2821 learning rate:  0.010354484225451967   Training loss:   18.235263446997976  Valing loss:   16.40060654351601\n",
      "Pure loss: 18.238842101767595.....Total loss: 18.238842101767595\n",
      "Pure loss: 16.524720143749406.....Total loss: 16.524720143749406\n",
      "epoch 2822 learning rate:  0.010354358610914246   Training loss:   18.238842101767595  Valing loss:   16.524720143749406\n",
      "Pure loss: 18.23478515081358.....Total loss: 18.23478515081358\n",
      "Pure loss: 16.483786153418784.....Total loss: 16.483786153418784\n",
      "epoch 2823 learning rate:  0.010354233085370174   Training loss:   18.23478515081358  Valing loss:   16.483786153418784\n",
      "Pure loss: 18.17023668700556.....Total loss: 18.17023668700556\n",
      "Pure loss: 16.33105672740359.....Total loss: 16.33105672740359\n",
      "epoch 2824 learning rate:  0.010354107648725212   Training loss:   18.17023668700556  Valing loss:   16.33105672740359\n",
      "Pure loss: 18.205151299278313.....Total loss: 18.205151299278313\n",
      "Pure loss: 16.676815206403695.....Total loss: 16.676815206403695\n",
      "epoch 2825 learning rate:  0.010353982300884955   Training loss:   18.205151299278313  Valing loss:   16.676815206403695\n",
      "Pure loss: 18.221797544447377.....Total loss: 18.221797544447377\n",
      "Pure loss: 16.760755060684946.....Total loss: 16.760755060684946\n",
      "epoch 2826 learning rate:  0.010353857041755132   Training loss:   18.221797544447377  Valing loss:   16.760755060684946\n",
      "Pure loss: 18.21596372815806.....Total loss: 18.21596372815806\n",
      "Pure loss: 16.672461078280442.....Total loss: 16.672461078280442\n",
      "epoch 2827 learning rate:  0.0103537318712416   Training loss:   18.21596372815806  Valing loss:   16.672461078280442\n",
      "Pure loss: 18.487686170402842.....Total loss: 18.487686170402842\n",
      "Pure loss: 17.264074160064208.....Total loss: 17.264074160064208\n",
      "epoch 2828 learning rate:  0.010353606789250354   Training loss:   18.487686170402842  Valing loss:   17.264074160064208\n",
      "Pure loss: 18.57095821263369.....Total loss: 18.57095821263369\n",
      "Pure loss: 17.439200347663665.....Total loss: 17.439200347663665\n",
      "epoch 2829 learning rate:  0.010353481795687522   Training loss:   18.57095821263369  Valing loss:   17.439200347663665\n",
      "Pure loss: 18.25462420895265.....Total loss: 18.25462420895265\n",
      "Pure loss: 16.666519538526092.....Total loss: 16.666519538526092\n",
      "epoch 2830 learning rate:  0.010353356890459364   Training loss:   18.25462420895265  Valing loss:   16.666519538526092\n",
      "Pure loss: 18.19900408148172.....Total loss: 18.19900408148172\n",
      "Pure loss: 16.46940445592234.....Total loss: 16.46940445592234\n",
      "epoch 2831 learning rate:  0.010353232073472272   Training loss:   18.19900408148172  Valing loss:   16.46940445592234\n",
      "Pure loss: 18.17450665595132.....Total loss: 18.17450665595132\n",
      "Pure loss: 16.25636887270317.....Total loss: 16.25636887270317\n",
      "epoch 2832 learning rate:  0.010353107344632769   Training loss:   18.17450665595132  Valing loss:   16.25636887270317\n",
      "Pure loss: 18.226660077206123.....Total loss: 18.226660077206123\n",
      "Pure loss: 16.099812266912267.....Total loss: 16.099812266912267\n",
      "epoch 2833 learning rate:  0.010352982703847512   Training loss:   18.226660077206123  Valing loss:   16.099812266912267\n",
      "Pure loss: 18.357571761188943.....Total loss: 18.357571761188943\n",
      "Pure loss: 15.952989556247363.....Total loss: 15.952989556247363\n",
      "epoch 2834 learning rate:  0.010352858151023289   Training loss:   18.357571761188943  Valing loss:   15.952989556247363\n",
      "Pure loss: 18.551427953221662.....Total loss: 18.551427953221662\n",
      "Pure loss: 15.941367319794308.....Total loss: 15.941367319794308\n",
      "epoch 2835 learning rate:  0.01035273368606702   Training loss:   18.551427953221662  Valing loss:   15.941367319794308\n",
      "Pure loss: 18.462758562635873.....Total loss: 18.462758562635873\n",
      "Pure loss: 15.924477872491531.....Total loss: 15.924477872491531\n",
      "epoch 2836 learning rate:  0.010352609308885754   Training loss:   18.462758562635873  Valing loss:   15.924477872491531\n",
      "Pure loss: 18.39677732213493.....Total loss: 18.39677732213493\n",
      "Pure loss: 15.931861951236252.....Total loss: 15.931861951236252\n",
      "epoch 2837 learning rate:  0.010352485019386676   Training loss:   18.39677732213493  Valing loss:   15.931861951236252\n",
      "Pure loss: 18.56508223089873.....Total loss: 18.56508223089873\n",
      "Pure loss: 15.956130983153022.....Total loss: 15.956130983153022\n",
      "epoch 2838 learning rate:  0.010352360817477096   Training loss:   18.56508223089873  Valing loss:   15.956130983153022\n",
      "Pure loss: 18.52701838723308.....Total loss: 18.52701838723308\n",
      "Pure loss: 15.95199472257804.....Total loss: 15.95199472257804\n",
      "epoch 2839 learning rate:  0.01035223670306446   Training loss:   18.52701838723308  Valing loss:   15.95199472257804\n",
      "Pure loss: 19.026514380824736.....Total loss: 19.026514380824736\n",
      "Pure loss: 16.054766660509102.....Total loss: 16.054766660509102\n",
      "epoch 2840 learning rate:  0.010352112676056338   Training loss:   19.026514380824736  Valing loss:   16.054766660509102\n",
      "Pure loss: 18.77026036712717.....Total loss: 18.77026036712717\n",
      "Pure loss: 15.959171760612056.....Total loss: 15.959171760612056\n",
      "epoch 2841 learning rate:  0.010351988736360437   Training loss:   18.77026036712717  Valing loss:   15.959171760612056\n",
      "Pure loss: 18.65638627391389.....Total loss: 18.65638627391389\n",
      "Pure loss: 15.944471179562045.....Total loss: 15.944471179562045\n",
      "epoch 2842 learning rate:  0.01035186488388459   Training loss:   18.65638627391389  Valing loss:   15.944471179562045\n",
      "Pure loss: 18.922202749127376.....Total loss: 18.922202749127376\n",
      "Pure loss: 15.992449504582568.....Total loss: 15.992449504582568\n",
      "epoch 2843 learning rate:  0.010351741118536758   Training loss:   18.922202749127376  Valing loss:   15.992449504582568\n",
      "Pure loss: 18.93901242326177.....Total loss: 18.93901242326177\n",
      "Pure loss: 15.997089822636228.....Total loss: 15.997089822636228\n",
      "epoch 2844 learning rate:  0.010351617440225036   Training loss:   18.93901242326177  Valing loss:   15.997089822636228\n",
      "Pure loss: 18.88953837743404.....Total loss: 18.88953837743404\n",
      "Pure loss: 15.966402204210368.....Total loss: 15.966402204210368\n",
      "epoch 2845 learning rate:  0.010351493848857645   Training loss:   18.88953837743404  Valing loss:   15.966402204210368\n",
      "Pure loss: 19.597245701040578.....Total loss: 19.597245701040578\n",
      "Pure loss: 16.276777946652746.....Total loss: 16.276777946652746\n",
      "epoch 2846 learning rate:  0.010351370344342938   Training loss:   19.597245701040578  Valing loss:   16.276777946652746\n",
      "Pure loss: 18.908724905075207.....Total loss: 18.908724905075207\n",
      "Pure loss: 16.009132864062305.....Total loss: 16.009132864062305\n",
      "epoch 2847 learning rate:  0.010351246926589392   Training loss:   18.908724905075207  Valing loss:   16.009132864062305\n",
      "Pure loss: 19.38652696368038.....Total loss: 19.38652696368038\n",
      "Pure loss: 16.269245389482332.....Total loss: 16.269245389482332\n",
      "epoch 2848 learning rate:  0.010351123595505617   Training loss:   19.38652696368038  Valing loss:   16.269245389482332\n",
      "Pure loss: 19.190095106617207.....Total loss: 19.190095106617207\n",
      "Pure loss: 16.209813251295504.....Total loss: 16.209813251295504\n",
      "epoch 2849 learning rate:  0.010351000351000351   Training loss:   19.190095106617207  Valing loss:   16.209813251295504\n",
      "Pure loss: 19.074408773673014.....Total loss: 19.074408773673014\n",
      "Pure loss: 16.121773503076643.....Total loss: 16.121773503076643\n",
      "epoch 2850 learning rate:  0.010350877192982456   Training loss:   19.074408773673014  Valing loss:   16.121773503076643\n",
      "Pure loss: 19.101732024801958.....Total loss: 19.101732024801958\n",
      "Pure loss: 16.132387546955563.....Total loss: 16.132387546955563\n",
      "epoch 2851 learning rate:  0.010350754121360927   Training loss:   19.101732024801958  Valing loss:   16.132387546955563\n",
      "Pure loss: 19.383867107026145.....Total loss: 19.383867107026145\n",
      "Pure loss: 16.322786198358223.....Total loss: 16.322786198358223\n",
      "epoch 2852 learning rate:  0.010350631136044881   Training loss:   19.383867107026145  Valing loss:   16.322786198358223\n",
      "Pure loss: 19.23908285117883.....Total loss: 19.23908285117883\n",
      "Pure loss: 16.249821843951178.....Total loss: 16.249821843951178\n",
      "epoch 2853 learning rate:  0.010350508236943569   Training loss:   19.23908285117883  Valing loss:   16.249821843951178\n",
      "Pure loss: 18.993026358888223.....Total loss: 18.993026358888223\n",
      "Pure loss: 16.187602750840032.....Total loss: 16.187602750840032\n",
      "epoch 2854 learning rate:  0.010350385423966364   Training loss:   18.993026358888223  Valing loss:   16.187602750840032\n",
      "Pure loss: 18.949136943064925.....Total loss: 18.949136943064925\n",
      "Pure loss: 16.168012350168244.....Total loss: 16.168012350168244\n",
      "epoch 2855 learning rate:  0.010350262697022767   Training loss:   18.949136943064925  Valing loss:   16.168012350168244\n",
      "Pure loss: 18.858840171225193.....Total loss: 18.858840171225193\n",
      "Pure loss: 16.125405974940502.....Total loss: 16.125405974940502\n",
      "epoch 2856 learning rate:  0.01035014005602241   Training loss:   18.858840171225193  Valing loss:   16.125405974940502\n",
      "Pure loss: 19.49375414473285.....Total loss: 19.49375414473285\n",
      "Pure loss: 16.312788753790965.....Total loss: 16.312788753790965\n",
      "epoch 2857 learning rate:  0.010350017500875044   Training loss:   19.49375414473285  Valing loss:   16.312788753790965\n",
      "Pure loss: 19.419116147035183.....Total loss: 19.419116147035183\n",
      "Pure loss: 16.276533862264237.....Total loss: 16.276533862264237\n",
      "epoch 2858 learning rate:  0.010349895031490553   Training loss:   19.419116147035183  Valing loss:   16.276533862264237\n",
      "Pure loss: 19.302426493464584.....Total loss: 19.302426493464584\n",
      "Pure loss: 16.219996530659685.....Total loss: 16.219996530659685\n",
      "epoch 2859 learning rate:  0.010349772647778943   Training loss:   19.302426493464584  Valing loss:   16.219996530659685\n",
      "Pure loss: 19.35679975917284.....Total loss: 19.35679975917284\n",
      "Pure loss: 16.258985421701716.....Total loss: 16.258985421701716\n",
      "epoch 2860 learning rate:  0.01034965034965035   Training loss:   19.35679975917284  Valing loss:   16.258985421701716\n",
      "Pure loss: 19.212151347229895.....Total loss: 19.212151347229895\n",
      "Pure loss: 16.14751573395067.....Total loss: 16.14751573395067\n",
      "epoch 2861 learning rate:  0.01034952813701503   Training loss:   19.212151347229895  Valing loss:   16.14751573395067\n",
      "Pure loss: 19.134611650982354.....Total loss: 19.134611650982354\n",
      "Pure loss: 16.107643915773913.....Total loss: 16.107643915773913\n",
      "epoch 2862 learning rate:  0.010349406009783369   Training loss:   19.134611650982354  Valing loss:   16.107643915773913\n",
      "Pure loss: 19.813825222581684.....Total loss: 19.813825222581684\n",
      "Pure loss: 16.45988700916593.....Total loss: 16.45988700916593\n",
      "epoch 2863 learning rate:  0.010349283967865875   Training loss:   19.813825222581684  Valing loss:   16.45988700916593\n",
      "Pure loss: 19.76400031164268.....Total loss: 19.76400031164268\n",
      "Pure loss: 16.445862615498715.....Total loss: 16.445862615498715\n",
      "epoch 2864 learning rate:  0.010349162011173184   Training loss:   19.76400031164268  Valing loss:   16.445862615498715\n",
      "Pure loss: 19.575691883209192.....Total loss: 19.575691883209192\n",
      "Pure loss: 16.342215408222966.....Total loss: 16.342215408222966\n",
      "epoch 2865 learning rate:  0.010349040139616057   Training loss:   19.575691883209192  Valing loss:   16.342215408222966\n",
      "Pure loss: 18.17663748518687.....Total loss: 18.17663748518687\n",
      "Pure loss: 16.196762060416845.....Total loss: 16.196762060416845\n",
      "epoch 2866 learning rate:  0.010348918353105373   Training loss:   18.17663748518687  Valing loss:   16.196762060416845\n",
      "Pure loss: 18.136191372673995.....Total loss: 18.136191372673995\n",
      "Pure loss: 16.21076745913492.....Total loss: 16.21076745913492\n",
      "epoch 2867 learning rate:  0.010348796651552146   Training loss:   18.136191372673995  Valing loss:   16.21076745913492\n",
      "Pure loss: 18.20539826826601.....Total loss: 18.20539826826601\n",
      "Pure loss: 16.18549824230248.....Total loss: 16.18549824230248\n",
      "epoch 2868 learning rate:  0.010348675034867503   Training loss:   18.20539826826601  Valing loss:   16.18549824230248\n",
      "Pure loss: 18.260402791575217.....Total loss: 18.260402791575217\n",
      "Pure loss: 16.211876339362504.....Total loss: 16.211876339362504\n",
      "epoch 2869 learning rate:  0.010348553502962704   Training loss:   18.260402791575217  Valing loss:   16.211876339362504\n",
      "Pure loss: 18.317238961301022.....Total loss: 18.317238961301022\n",
      "Pure loss: 16.188114634499442.....Total loss: 16.188114634499442\n",
      "epoch 2870 learning rate:  0.010348432055749129   Training loss:   18.317238961301022  Valing loss:   16.188114634499442\n",
      "Pure loss: 18.267031125649602.....Total loss: 18.267031125649602\n",
      "Pure loss: 16.16401358770644.....Total loss: 16.16401358770644\n",
      "epoch 2871 learning rate:  0.010348310693138279   Training loss:   18.267031125649602  Valing loss:   16.16401358770644\n",
      "Pure loss: 18.20230370048847.....Total loss: 18.20230370048847\n",
      "Pure loss: 16.184487796806675.....Total loss: 16.184487796806675\n",
      "epoch 2872 learning rate:  0.010348189415041783   Training loss:   18.20230370048847  Valing loss:   16.184487796806675\n",
      "Pure loss: 18.14116257157993.....Total loss: 18.14116257157993\n",
      "Pure loss: 16.24705744996559.....Total loss: 16.24705744996559\n",
      "epoch 2873 learning rate:  0.010348068221371388   Training loss:   18.14116257157993  Valing loss:   16.24705744996559\n",
      "Pure loss: 18.30229308366298.....Total loss: 18.30229308366298\n",
      "Pure loss: 16.206502978375163.....Total loss: 16.206502978375163\n",
      "epoch 2874 learning rate:  0.01034794711203897   Training loss:   18.30229308366298  Valing loss:   16.206502978375163\n",
      "Pure loss: 18.168124040469763.....Total loss: 18.168124040469763\n",
      "Pure loss: 16.125863481990457.....Total loss: 16.125863481990457\n",
      "epoch 2875 learning rate:  0.010347826086956523   Training loss:   18.168124040469763  Valing loss:   16.125863481990457\n",
      "Pure loss: 18.277802958977496.....Total loss: 18.277802958977496\n",
      "Pure loss: 16.138002776748426.....Total loss: 16.138002776748426\n",
      "epoch 2876 learning rate:  0.010347705146036161   Training loss:   18.277802958977496  Valing loss:   16.138002776748426\n",
      "Pure loss: 18.470912275243638.....Total loss: 18.470912275243638\n",
      "Pure loss: 16.174098671816672.....Total loss: 16.174098671816672\n",
      "epoch 2877 learning rate:  0.010347584289190128   Training loss:   18.470912275243638  Valing loss:   16.174098671816672\n",
      "Pure loss: 18.52940909988638.....Total loss: 18.52940909988638\n",
      "Pure loss: 16.17873194496588.....Total loss: 16.17873194496588\n",
      "epoch 2878 learning rate:  0.010347463516330785   Training loss:   18.52940909988638  Valing loss:   16.17873194496588\n",
      "Pure loss: 18.632136080313103.....Total loss: 18.632136080313103\n",
      "Pure loss: 16.238077665468694.....Total loss: 16.238077665468694\n",
      "epoch 2879 learning rate:  0.010347342827370615   Training loss:   18.632136080313103  Valing loss:   16.238077665468694\n",
      "Pure loss: 18.47409048680696.....Total loss: 18.47409048680696\n",
      "Pure loss: 16.16278829392642.....Total loss: 16.16278829392642\n",
      "epoch 2880 learning rate:  0.010347222222222223   Training loss:   18.47409048680696  Valing loss:   16.16278829392642\n",
      "Pure loss: 18.352570703707574.....Total loss: 18.352570703707574\n",
      "Pure loss: 16.118386034846655.....Total loss: 16.118386034846655\n",
      "epoch 2881 learning rate:  0.010347101700798333   Training loss:   18.352570703707574  Valing loss:   16.118386034846655\n",
      "Pure loss: 18.49832076732521.....Total loss: 18.49832076732521\n",
      "Pure loss: 16.11127893940958.....Total loss: 16.11127893940958\n",
      "epoch 2882 learning rate:  0.010346981263011798   Training loss:   18.49832076732521  Valing loss:   16.11127893940958\n",
      "Pure loss: 18.51084035081585.....Total loss: 18.51084035081585\n",
      "Pure loss: 16.11315774093968.....Total loss: 16.11315774093968\n",
      "epoch 2883 learning rate:  0.010346860908775582   Training loss:   18.51084035081585  Valing loss:   16.11315774093968\n",
      "Pure loss: 18.32869433502786.....Total loss: 18.32869433502786\n",
      "Pure loss: 16.089879152701606.....Total loss: 16.089879152701606\n",
      "epoch 2884 learning rate:  0.010346740638002774   Training loss:   18.32869433502786  Valing loss:   16.089879152701606\n",
      "Pure loss: 18.579692237330175.....Total loss: 18.579692237330175\n",
      "Pure loss: 16.107682080962224.....Total loss: 16.107682080962224\n",
      "epoch 2885 learning rate:  0.010346620450606586   Training loss:   18.579692237330175  Valing loss:   16.107682080962224\n",
      "Pure loss: 18.110196577925656.....Total loss: 18.110196577925656\n",
      "Pure loss: 16.185100279432902.....Total loss: 16.185100279432902\n",
      "epoch 2886 learning rate:  0.010346500346500347   Training loss:   18.110196577925656  Valing loss:   16.185100279432902\n",
      "Pure loss: 18.364711343475996.....Total loss: 18.364711343475996\n",
      "Pure loss: 16.184175581856746.....Total loss: 16.184175581856746\n",
      "epoch 2887 learning rate:  0.010346380325597506   Training loss:   18.364711343475996  Valing loss:   16.184175581856746\n",
      "Pure loss: 18.54424780960342.....Total loss: 18.54424780960342\n",
      "Pure loss: 16.189227902173112.....Total loss: 16.189227902173112\n",
      "epoch 2888 learning rate:  0.010346260387811635   Training loss:   18.54424780960342  Valing loss:   16.189227902173112\n",
      "Pure loss: 18.636230620852626.....Total loss: 18.636230620852626\n",
      "Pure loss: 16.196217396034847.....Total loss: 16.196217396034847\n",
      "epoch 2889 learning rate:  0.01034614053305642   Training loss:   18.636230620852626  Valing loss:   16.196217396034847\n",
      "Pure loss: 18.50850880443575.....Total loss: 18.50850880443575\n",
      "Pure loss: 16.163450418202604.....Total loss: 16.163450418202604\n",
      "epoch 2890 learning rate:  0.010346020761245675   Training loss:   18.50850880443575  Valing loss:   16.163450418202604\n",
      "Pure loss: 18.39204157214009.....Total loss: 18.39204157214009\n",
      "Pure loss: 16.121459367604086.....Total loss: 16.121459367604086\n",
      "epoch 2891 learning rate:  0.010345901072293325   Training loss:   18.39204157214009  Valing loss:   16.121459367604086\n",
      "Pure loss: 18.615280923490005.....Total loss: 18.615280923490005\n",
      "Pure loss: 16.234508984377342.....Total loss: 16.234508984377342\n",
      "epoch 2892 learning rate:  0.010345781466113417   Training loss:   18.615280923490005  Valing loss:   16.234508984377342\n",
      "Pure loss: 18.647772449144508.....Total loss: 18.647772449144508\n",
      "Pure loss: 16.257162635975135.....Total loss: 16.257162635975135\n",
      "epoch 2893 learning rate:  0.010345661942620118   Training loss:   18.647772449144508  Valing loss:   16.257162635975135\n",
      "Pure loss: 18.15867407676561.....Total loss: 18.15867407676561\n",
      "Pure loss: 16.267860099147907.....Total loss: 16.267860099147907\n",
      "epoch 2894 learning rate:  0.010345542501727712   Training loss:   18.15867407676561  Valing loss:   16.267860099147907\n",
      "Pure loss: 18.592167250247808.....Total loss: 18.592167250247808\n",
      "Pure loss: 16.309609213741865.....Total loss: 16.309609213741865\n",
      "epoch 2895 learning rate:  0.010345423143350605   Training loss:   18.592167250247808  Valing loss:   16.309609213741865\n",
      "Pure loss: 18.54416019439006.....Total loss: 18.54416019439006\n",
      "Pure loss: 16.283943512013096.....Total loss: 16.283943512013096\n",
      "epoch 2896 learning rate:  0.010345303867403316   Training loss:   18.54416019439006  Valing loss:   16.283943512013096\n",
      "Pure loss: 18.507579805918613.....Total loss: 18.507579805918613\n",
      "Pure loss: 16.252024613748663.....Total loss: 16.252024613748663\n",
      "epoch 2897 learning rate:  0.010345184673800483   Training loss:   18.507579805918613  Valing loss:   16.252024613748663\n",
      "Pure loss: 18.321100059187497.....Total loss: 18.321100059187497\n",
      "Pure loss: 16.294285833659625.....Total loss: 16.294285833659625\n",
      "epoch 2898 learning rate:  0.010345065562456866   Training loss:   18.321100059187497  Valing loss:   16.294285833659625\n",
      "Pure loss: 18.299965851203222.....Total loss: 18.299965851203222\n",
      "Pure loss: 16.28420770702818.....Total loss: 16.28420770702818\n",
      "epoch 2899 learning rate:  0.01034494653328734   Training loss:   18.299965851203222  Valing loss:   16.28420770702818\n",
      "Pure loss: 18.149497777647454.....Total loss: 18.149497777647454\n",
      "Pure loss: 16.30571969365996.....Total loss: 16.30571969365996\n",
      "epoch 2900 learning rate:  0.010344827586206896   Training loss:   18.149497777647454  Valing loss:   16.30571969365996\n",
      "Pure loss: 17.875048076101653.....Total loss: 17.875048076101653\n",
      "Pure loss: 16.51465584902723.....Total loss: 16.51465584902723\n",
      "epoch 2901 learning rate:  0.010344708721130644   Training loss:   17.875048076101653  Valing loss:   16.51465584902723\n",
      "Pure loss: 17.874700836061603.....Total loss: 17.874700836061603\n",
      "Pure loss: 16.51602426010701.....Total loss: 16.51602426010701\n",
      "epoch 2902 learning rate:  0.010344589937973811   Training loss:   17.874700836061603  Valing loss:   16.51602426010701\n",
      "Pure loss: 17.956555339934273.....Total loss: 17.956555339934273\n",
      "Pure loss: 16.113216289299285.....Total loss: 16.113216289299285\n",
      "epoch 2903 learning rate:  0.010344471236651739   Training loss:   17.956555339934273  Valing loss:   16.113216289299285\n",
      "Pure loss: 18.237246839639027.....Total loss: 18.237246839639027\n",
      "Pure loss: 16.031085380733106.....Total loss: 16.031085380733106\n",
      "epoch 2904 learning rate:  0.01034435261707989   Training loss:   18.237246839639027  Valing loss:   16.031085380733106\n",
      "Pure loss: 18.334734786540043.....Total loss: 18.334734786540043\n",
      "Pure loss: 16.036197870345738.....Total loss: 16.036197870345738\n",
      "epoch 2905 learning rate:  0.010344234079173838   Training loss:   18.334734786540043  Valing loss:   16.036197870345738\n",
      "Pure loss: 18.16788624963844.....Total loss: 18.16788624963844\n",
      "Pure loss: 16.01149050823719.....Total loss: 16.01149050823719\n",
      "epoch 2906 learning rate:  0.010344115622849278   Training loss:   18.16788624963844  Valing loss:   16.01149050823719\n",
      "Pure loss: 18.222373087239276.....Total loss: 18.222373087239276\n",
      "Pure loss: 16.002200436195064.....Total loss: 16.002200436195064\n",
      "epoch 2907 learning rate:  0.010343997248022017   Training loss:   18.222373087239276  Valing loss:   16.002200436195064\n",
      "Pure loss: 18.38865200755359.....Total loss: 18.38865200755359\n",
      "Pure loss: 16.00500343485498.....Total loss: 16.00500343485498\n",
      "epoch 2908 learning rate:  0.010343878954607979   Training loss:   18.38865200755359  Valing loss:   16.00500343485498\n",
      "Pure loss: 18.849935704662894.....Total loss: 18.849935704662894\n",
      "Pure loss: 16.177255205899556.....Total loss: 16.177255205899556\n",
      "epoch 2909 learning rate:  0.010343760742523205   Training loss:   18.849935704662894  Valing loss:   16.177255205899556\n",
      "Pure loss: 17.79661362885369.....Total loss: 17.79661362885369\n",
      "Pure loss: 16.381426639447252.....Total loss: 16.381426639447252\n",
      "epoch 2910 learning rate:  0.01034364261168385   Training loss:   17.79661362885369  Valing loss:   16.381426639447252\n",
      "Pure loss: 17.797613365237233.....Total loss: 17.797613365237233\n",
      "Pure loss: 16.37512653266241.....Total loss: 16.37512653266241\n",
      "epoch 2911 learning rate:  0.010343524562006184   Training loss:   17.797613365237233  Valing loss:   16.37512653266241\n",
      "Pure loss: 17.785579070737427.....Total loss: 17.785579070737427\n",
      "Pure loss: 16.505709949378527.....Total loss: 16.505709949378527\n",
      "epoch 2912 learning rate:  0.010343406593406594   Training loss:   17.785579070737427  Valing loss:   16.505709949378527\n",
      "Pure loss: 17.81288339717169.....Total loss: 17.81288339717169\n",
      "Pure loss: 16.34694461884788.....Total loss: 16.34694461884788\n",
      "epoch 2913 learning rate:  0.01034328870580158   Training loss:   17.81288339717169  Valing loss:   16.34694461884788\n",
      "Pure loss: 17.797704811126497.....Total loss: 17.797704811126497\n",
      "Pure loss: 16.383671880003533.....Total loss: 16.383671880003533\n",
      "epoch 2914 learning rate:  0.010343170899107756   Training loss:   17.797704811126497  Valing loss:   16.383671880003533\n",
      "Pure loss: 17.89255326189268.....Total loss: 17.89255326189268\n",
      "Pure loss: 16.221196676770674.....Total loss: 16.221196676770674\n",
      "epoch 2915 learning rate:  0.010343053173241853   Training loss:   17.89255326189268  Valing loss:   16.221196676770674\n",
      "Pure loss: 17.852749363608282.....Total loss: 17.852749363608282\n",
      "Pure loss: 16.265515983918828.....Total loss: 16.265515983918828\n",
      "epoch 2916 learning rate:  0.010342935528120713   Training loss:   17.852749363608282  Valing loss:   16.265515983918828\n",
      "Pure loss: 17.817777577810915.....Total loss: 17.817777577810915\n",
      "Pure loss: 16.299085444028094.....Total loss: 16.299085444028094\n",
      "epoch 2917 learning rate:  0.010342817963661296   Training loss:   17.817777577810915  Valing loss:   16.299085444028094\n",
      "Pure loss: 17.884445427670776.....Total loss: 17.884445427670776\n",
      "Pure loss: 16.167125914534587.....Total loss: 16.167125914534587\n",
      "epoch 2918 learning rate:  0.010342700479780672   Training loss:   17.884445427670776  Valing loss:   16.167125914534587\n",
      "Pure loss: 18.168925334461893.....Total loss: 18.168925334461893\n",
      "Pure loss: 15.988625016461492.....Total loss: 15.988625016461492\n",
      "epoch 2919 learning rate:  0.010342583076396026   Training loss:   18.168925334461893  Valing loss:   15.988625016461492\n",
      "Pure loss: 18.285376631257527.....Total loss: 18.285376631257527\n",
      "Pure loss: 16.015397950337096.....Total loss: 16.015397950337096\n",
      "epoch 2920 learning rate:  0.010342465753424658   Training loss:   18.285376631257527  Valing loss:   16.015397950337096\n",
      "Pure loss: 18.170340986226396.....Total loss: 18.170340986226396\n",
      "Pure loss: 16.580557690925033.....Total loss: 16.580557690925033\n",
      "epoch 2921 learning rate:  0.010342348510783979   Training loss:   18.170340986226396  Valing loss:   16.580557690925033\n",
      "Pure loss: 18.148059286014817.....Total loss: 18.148059286014817\n",
      "Pure loss: 16.52929335364757.....Total loss: 16.52929335364757\n",
      "epoch 2922 learning rate:  0.010342231348391512   Training loss:   18.148059286014817  Valing loss:   16.52929335364757\n",
      "Pure loss: 18.146154363815203.....Total loss: 18.146154363815203\n",
      "Pure loss: 16.50068164091799.....Total loss: 16.50068164091799\n",
      "epoch 2923 learning rate:  0.010342114266164899   Training loss:   18.146154363815203  Valing loss:   16.50068164091799\n",
      "Pure loss: 18.1405922475861.....Total loss: 18.1405922475861\n",
      "Pure loss: 16.539526533254925.....Total loss: 16.539526533254925\n",
      "epoch 2924 learning rate:  0.010341997264021888   Training loss:   18.1405922475861  Valing loss:   16.539526533254925\n",
      "Pure loss: 18.188148616466915.....Total loss: 18.188148616466915\n",
      "Pure loss: 17.542271382118393.....Total loss: 17.542271382118393\n",
      "epoch 2925 learning rate:  0.010341880341880341   Training loss:   18.188148616466915  Valing loss:   17.542271382118393\n",
      "Pure loss: 17.96631703970658.....Total loss: 17.96631703970658\n",
      "Pure loss: 17.05212051840368.....Total loss: 17.05212051840368\n",
      "epoch 2926 learning rate:  0.010341763499658237   Training loss:   17.96631703970658  Valing loss:   17.05212051840368\n",
      "Pure loss: 17.93157688057542.....Total loss: 17.93157688057542\n",
      "Pure loss: 16.969909622955957.....Total loss: 16.969909622955957\n",
      "epoch 2927 learning rate:  0.01034164673727366   Training loss:   17.93157688057542  Valing loss:   16.969909622955957\n",
      "Pure loss: 17.775921147375946.....Total loss: 17.775921147375946\n",
      "Pure loss: 16.593551552363785.....Total loss: 16.593551552363785\n",
      "epoch 2928 learning rate:  0.01034153005464481   Training loss:   17.775921147375946  Valing loss:   16.593551552363785\n",
      "Pure loss: 17.836255918494636.....Total loss: 17.836255918494636\n",
      "Pure loss: 16.28415740891203.....Total loss: 16.28415740891203\n",
      "epoch 2929 learning rate:  0.010341413451689997   Training loss:   17.836255918494636  Valing loss:   16.28415740891203\n",
      "Pure loss: 17.916533638348486.....Total loss: 17.916533638348486\n",
      "Pure loss: 16.283488354325055.....Total loss: 16.283488354325055\n",
      "epoch 2930 learning rate:  0.010341296928327645   Training loss:   17.916533638348486  Valing loss:   16.283488354325055\n",
      "Pure loss: 18.234772055036682.....Total loss: 18.234772055036682\n",
      "Pure loss: 16.257458110375556.....Total loss: 16.257458110375556\n",
      "epoch 2931 learning rate:  0.010341180484476287   Training loss:   18.234772055036682  Valing loss:   16.257458110375556\n",
      "Pure loss: 17.908000704403914.....Total loss: 17.908000704403914\n",
      "Pure loss: 16.42726673846136.....Total loss: 16.42726673846136\n",
      "epoch 2932 learning rate:  0.010341064120054571   Training loss:   17.908000704403914  Valing loss:   16.42726673846136\n",
      "Pure loss: 17.87483723752073.....Total loss: 17.87483723752073\n",
      "Pure loss: 16.46041220471192.....Total loss: 16.46041220471192\n",
      "epoch 2933 learning rate:  0.010340947834981249   Training loss:   17.87483723752073  Valing loss:   16.46041220471192\n",
      "Pure loss: 17.83068337321531.....Total loss: 17.83068337321531\n",
      "Pure loss: 16.82345130921857.....Total loss: 16.82345130921857\n",
      "epoch 2934 learning rate:  0.010340831629175188   Training loss:   17.83068337321531  Valing loss:   16.82345130921857\n",
      "Pure loss: 17.82495989669061.....Total loss: 17.82495989669061\n",
      "Pure loss: 16.803624347236767.....Total loss: 16.803624347236767\n",
      "epoch 2935 learning rate:  0.010340715502555366   Training loss:   17.82495989669061  Valing loss:   16.803624347236767\n",
      "Pure loss: 17.781698245096365.....Total loss: 17.781698245096365\n",
      "Pure loss: 16.612861267559378.....Total loss: 16.612861267559378\n",
      "epoch 2936 learning rate:  0.010340599455040872   Training loss:   17.781698245096365  Valing loss:   16.612861267559378\n",
      "Pure loss: 17.78143160424114.....Total loss: 17.78143160424114\n",
      "Pure loss: 16.606578285763.....Total loss: 16.606578285763\n",
      "epoch 2937 learning rate:  0.010340483486550903   Training loss:   17.78143160424114  Valing loss:   16.606578285763\n",
      "Pure loss: 17.758764981479253.....Total loss: 17.758764981479253\n",
      "Pure loss: 16.674474508474606.....Total loss: 16.674474508474606\n",
      "epoch 2938 learning rate:  0.010340367597004765   Training loss:   17.758764981479253  Valing loss:   16.674474508474606\n",
      "Pure loss: 17.76020130072307.....Total loss: 17.76020130072307\n",
      "Pure loss: 16.523692229554694.....Total loss: 16.523692229554694\n",
      "epoch 2939 learning rate:  0.010340251786321879   Training loss:   17.76020130072307  Valing loss:   16.523692229554694\n",
      "Pure loss: 17.748427092999723.....Total loss: 17.748427092999723\n",
      "Pure loss: 16.55394597716009.....Total loss: 16.55394597716009\n",
      "epoch 2940 learning rate:  0.010340136054421769   Training loss:   17.748427092999723  Valing loss:   16.55394597716009\n",
      "Pure loss: 17.74873046257743.....Total loss: 17.74873046257743\n",
      "Pure loss: 16.528678958652453.....Total loss: 16.528678958652453\n",
      "epoch 2941 learning rate:  0.010340020401224073   Training loss:   17.74873046257743  Valing loss:   16.528678958652453\n",
      "Pure loss: 17.754170143065835.....Total loss: 17.754170143065835\n",
      "Pure loss: 16.643154714002176.....Total loss: 16.643154714002176\n",
      "epoch 2942 learning rate:  0.010339904826648538   Training loss:   17.754170143065835  Valing loss:   16.643154714002176\n",
      "Pure loss: 17.893825316724527.....Total loss: 17.893825316724527\n",
      "Pure loss: 17.24051713808871.....Total loss: 17.24051713808871\n",
      "epoch 2943 learning rate:  0.010339789330615018   Training loss:   17.893825316724527  Valing loss:   17.24051713808871\n",
      "Pure loss: 17.778131659319747.....Total loss: 17.778131659319747\n",
      "Pure loss: 16.851498885196133.....Total loss: 16.851498885196133\n",
      "epoch 2944 learning rate:  0.010339673913043478   Training loss:   17.778131659319747  Valing loss:   16.851498885196133\n",
      "Pure loss: 17.74581758061879.....Total loss: 17.74581758061879\n",
      "Pure loss: 16.85352889053373.....Total loss: 16.85352889053373\n",
      "epoch 2945 learning rate:  0.01033955857385399   Training loss:   17.74581758061879  Valing loss:   16.85352889053373\n",
      "Pure loss: 17.72305529058594.....Total loss: 17.72305529058594\n",
      "Pure loss: 16.763481820796947.....Total loss: 16.763481820796947\n",
      "epoch 2946 learning rate:  0.010339443312966734   Training loss:   17.72305529058594  Valing loss:   16.763481820796947\n",
      "Pure loss: 17.736058933524752.....Total loss: 17.736058933524752\n",
      "Pure loss: 16.670961992459972.....Total loss: 16.670961992459972\n",
      "epoch 2947 learning rate:  0.010339328130302002   Training loss:   17.736058933524752  Valing loss:   16.670961992459972\n",
      "Pure loss: 17.73345753960757.....Total loss: 17.73345753960757\n",
      "Pure loss: 16.65477834129857.....Total loss: 16.65477834129857\n",
      "epoch 2948 learning rate:  0.01033921302578019   Training loss:   17.73345753960757  Valing loss:   16.65477834129857\n",
      "Pure loss: 17.827817494350743.....Total loss: 17.827817494350743\n",
      "Pure loss: 17.13471263811638.....Total loss: 17.13471263811638\n",
      "epoch 2949 learning rate:  0.010339097999321804   Training loss:   17.827817494350743  Valing loss:   17.13471263811638\n",
      "Pure loss: 17.706210699864222.....Total loss: 17.706210699864222\n",
      "Pure loss: 16.728069653030282.....Total loss: 16.728069653030282\n",
      "epoch 2950 learning rate:  0.010338983050847458   Training loss:   17.706210699864222  Valing loss:   16.728069653030282\n",
      "Pure loss: 17.705879017855686.....Total loss: 17.705879017855686\n",
      "Pure loss: 16.742388566172515.....Total loss: 16.742388566172515\n",
      "epoch 2951 learning rate:  0.010338868180277872   Training loss:   17.705879017855686  Valing loss:   16.742388566172515\n",
      "Pure loss: 17.707500407958374.....Total loss: 17.707500407958374\n",
      "Pure loss: 16.731740842998047.....Total loss: 16.731740842998047\n",
      "epoch 2952 learning rate:  0.010338753387533876   Training loss:   17.707500407958374  Valing loss:   16.731740842998047\n",
      "Pure loss: 17.83202426416551.....Total loss: 17.83202426416551\n",
      "Pure loss: 17.17894789790027.....Total loss: 17.17894789790027\n",
      "epoch 2953 learning rate:  0.010338638672536404   Training loss:   17.83202426416551  Valing loss:   17.17894789790027\n",
      "Pure loss: 17.73100711940556.....Total loss: 17.73100711940556\n",
      "Pure loss: 16.844209288371847.....Total loss: 16.844209288371847\n",
      "epoch 2954 learning rate:  0.0103385240352065   Training loss:   17.73100711940556  Valing loss:   16.844209288371847\n",
      "Pure loss: 17.748427899740044.....Total loss: 17.748427899740044\n",
      "Pure loss: 16.763441226823463.....Total loss: 16.763441226823463\n",
      "epoch 2955 learning rate:  0.010338409475465313   Training loss:   17.748427899740044  Valing loss:   16.763441226823463\n",
      "Pure loss: 17.736203523956277.....Total loss: 17.736203523956277\n",
      "Pure loss: 16.78188417766215.....Total loss: 16.78188417766215\n",
      "epoch 2956 learning rate:  0.0103382949932341   Training loss:   17.736203523956277  Valing loss:   16.78188417766215\n",
      "Pure loss: 17.72021418915042.....Total loss: 17.72021418915042\n",
      "Pure loss: 16.612406184746423.....Total loss: 16.612406184746423\n",
      "epoch 2957 learning rate:  0.010338180588434224   Training loss:   17.72021418915042  Valing loss:   16.612406184746423\n",
      "Pure loss: 17.80941709231153.....Total loss: 17.80941709231153\n",
      "Pure loss: 16.60135418184024.....Total loss: 16.60135418184024\n",
      "epoch 2958 learning rate:  0.010338066260987154   Training loss:   17.80941709231153  Valing loss:   16.60135418184024\n",
      "Pure loss: 17.823699503842924.....Total loss: 17.823699503842924\n",
      "Pure loss: 16.598563287662753.....Total loss: 16.598563287662753\n",
      "epoch 2959 learning rate:  0.010337952010814465   Training loss:   17.823699503842924  Valing loss:   16.598563287662753\n",
      "Pure loss: 17.88809809065778.....Total loss: 17.88809809065778\n",
      "Pure loss: 16.6215318131912.....Total loss: 16.6215318131912\n",
      "epoch 2960 learning rate:  0.010337837837837838   Training loss:   17.88809809065778  Valing loss:   16.6215318131912\n",
      "Pure loss: 17.9180862080861.....Total loss: 17.9180862080861\n",
      "Pure loss: 17.105285245313983.....Total loss: 17.105285245313983\n",
      "epoch 2961 learning rate:  0.010337723741979062   Training loss:   17.9180862080861  Valing loss:   17.105285245313983\n",
      "Pure loss: 17.90611665907283.....Total loss: 17.90611665907283\n",
      "Pure loss: 17.10335530740233.....Total loss: 17.10335530740233\n",
      "epoch 2962 learning rate:  0.010337609723160028   Training loss:   17.90611665907283  Valing loss:   17.10335530740233\n",
      "Pure loss: 17.854966406793764.....Total loss: 17.854966406793764\n",
      "Pure loss: 16.836781724925235.....Total loss: 16.836781724925235\n",
      "epoch 2963 learning rate:  0.010337495781302735   Training loss:   17.854966406793764  Valing loss:   16.836781724925235\n",
      "Pure loss: 17.809251219495348.....Total loss: 17.809251219495348\n",
      "Pure loss: 16.88487588716888.....Total loss: 16.88487588716888\n",
      "epoch 2964 learning rate:  0.010337381916329285   Training loss:   17.809251219495348  Valing loss:   16.88487588716888\n",
      "Pure loss: 17.937539124329675.....Total loss: 17.937539124329675\n",
      "Pure loss: 16.968497911273936.....Total loss: 16.968497911273936\n",
      "epoch 2965 learning rate:  0.01033726812816189   Training loss:   17.937539124329675  Valing loss:   16.968497911273936\n",
      "Pure loss: 17.79646920654341.....Total loss: 17.79646920654341\n",
      "Pure loss: 16.980969663382552.....Total loss: 16.980969663382552\n",
      "epoch 2966 learning rate:  0.010337154416722859   Training loss:   17.79646920654341  Valing loss:   16.980969663382552\n",
      "Pure loss: 17.811595286251706.....Total loss: 17.811595286251706\n",
      "Pure loss: 17.02500800654478.....Total loss: 17.02500800654478\n",
      "epoch 2967 learning rate:  0.010337040781934615   Training loss:   17.811595286251706  Valing loss:   17.02500800654478\n",
      "Pure loss: 17.792010675377732.....Total loss: 17.792010675377732\n",
      "Pure loss: 16.635078086979824.....Total loss: 16.635078086979824\n",
      "epoch 2968 learning rate:  0.010336927223719677   Training loss:   17.792010675377732  Valing loss:   16.635078086979824\n",
      "Pure loss: 17.82008447908551.....Total loss: 17.82008447908551\n",
      "Pure loss: 16.58125135907185.....Total loss: 16.58125135907185\n",
      "epoch 2969 learning rate:  0.010336813742000673   Training loss:   17.82008447908551  Valing loss:   16.58125135907185\n",
      "Pure loss: 17.82532000153712.....Total loss: 17.82532000153712\n",
      "Pure loss: 16.56437355493607.....Total loss: 16.56437355493607\n",
      "epoch 2970 learning rate:  0.010336700336700337   Training loss:   17.82532000153712  Valing loss:   16.56437355493607\n",
      "Pure loss: 18.03678065389546.....Total loss: 18.03678065389546\n",
      "Pure loss: 16.405163710427537.....Total loss: 16.405163710427537\n",
      "epoch 2971 learning rate:  0.010336587007741501   Training loss:   18.03678065389546  Valing loss:   16.405163710427537\n",
      "Pure loss: 17.984386738502156.....Total loss: 17.984386738502156\n",
      "Pure loss: 16.388072724297324.....Total loss: 16.388072724297324\n",
      "epoch 2972 learning rate:  0.010336473755047107   Training loss:   17.984386738502156  Valing loss:   16.388072724297324\n",
      "Pure loss: 17.739198693194883.....Total loss: 17.739198693194883\n",
      "Pure loss: 16.811329319498654.....Total loss: 16.811329319498654\n",
      "epoch 2973 learning rate:  0.010336360578540196   Training loss:   17.739198693194883  Valing loss:   16.811329319498654\n",
      "Pure loss: 17.721600439621348.....Total loss: 17.721600439621348\n",
      "Pure loss: 16.760315982495325.....Total loss: 16.760315982495325\n",
      "epoch 2974 learning rate:  0.010336247478143915   Training loss:   17.721600439621348  Valing loss:   16.760315982495325\n",
      "Pure loss: 17.811243383891675.....Total loss: 17.811243383891675\n",
      "Pure loss: 17.174467763220083.....Total loss: 17.174467763220083\n",
      "epoch 2975 learning rate:  0.010336134453781513   Training loss:   17.811243383891675  Valing loss:   17.174467763220083\n",
      "Pure loss: 17.842129367017908.....Total loss: 17.842129367017908\n",
      "Pure loss: 17.124887617638272.....Total loss: 17.124887617638272\n",
      "epoch 2976 learning rate:  0.010336021505376344   Training loss:   17.842129367017908  Valing loss:   17.124887617638272\n",
      "Pure loss: 18.001925514708347.....Total loss: 18.001925514708347\n",
      "Pure loss: 17.538347632137466.....Total loss: 17.538347632137466\n",
      "epoch 2977 learning rate:  0.010335908632851864   Training loss:   18.001925514708347  Valing loss:   17.538347632137466\n",
      "Pure loss: 18.0678082669476.....Total loss: 18.0678082669476\n",
      "Pure loss: 17.540139893314315.....Total loss: 17.540139893314315\n",
      "epoch 2978 learning rate:  0.010335795836131633   Training loss:   18.0678082669476  Valing loss:   17.540139893314315\n",
      "Pure loss: 18.102036285672742.....Total loss: 18.102036285672742\n",
      "Pure loss: 17.66483819565546.....Total loss: 17.66483819565546\n",
      "epoch 2979 learning rate:  0.01033568311513931   Training loss:   18.102036285672742  Valing loss:   17.66483819565546\n",
      "Pure loss: 18.053505867701574.....Total loss: 18.053505867701574\n",
      "Pure loss: 17.49668930341293.....Total loss: 17.49668930341293\n",
      "epoch 2980 learning rate:  0.010335570469798657   Training loss:   18.053505867701574  Valing loss:   17.49668930341293\n",
      "Pure loss: 18.035938283664898.....Total loss: 18.035938283664898\n",
      "Pure loss: 17.40467607810657.....Total loss: 17.40467607810657\n",
      "epoch 2981 learning rate:  0.010335457900033545   Training loss:   18.035938283664898  Valing loss:   17.40467607810657\n",
      "Pure loss: 17.897045197350742.....Total loss: 17.897045197350742\n",
      "Pure loss: 16.85386615085256.....Total loss: 16.85386615085256\n",
      "epoch 2982 learning rate:  0.010335345405767941   Training loss:   17.897045197350742  Valing loss:   16.85386615085256\n",
      "Pure loss: 17.92709328331819.....Total loss: 17.92709328331819\n",
      "Pure loss: 17.421490567813013.....Total loss: 17.421490567813013\n",
      "epoch 2983 learning rate:  0.010335232986925914   Training loss:   17.92709328331819  Valing loss:   17.421490567813013\n",
      "Pure loss: 17.867255857394603.....Total loss: 17.867255857394603\n",
      "Pure loss: 17.138160902877665.....Total loss: 17.138160902877665\n",
      "epoch 2984 learning rate:  0.010335120643431636   Training loss:   17.867255857394603  Valing loss:   17.138160902877665\n",
      "Pure loss: 17.885703484666138.....Total loss: 17.885703484666138\n",
      "Pure loss: 17.277264450336787.....Total loss: 17.277264450336787\n",
      "epoch 2985 learning rate:  0.01033500837520938   Training loss:   17.885703484666138  Valing loss:   17.277264450336787\n",
      "Pure loss: 17.775599341774328.....Total loss: 17.775599341774328\n",
      "Pure loss: 16.97300892492664.....Total loss: 16.97300892492664\n",
      "epoch 2986 learning rate:  0.010334896182183523   Training loss:   17.775599341774328  Valing loss:   16.97300892492664\n",
      "Pure loss: 17.839777921999524.....Total loss: 17.839777921999524\n",
      "Pure loss: 17.01894326181442.....Total loss: 17.01894326181442\n",
      "epoch 2987 learning rate:  0.01033478406427854   Training loss:   17.839777921999524  Valing loss:   17.01894326181442\n",
      "Pure loss: 17.866555861246958.....Total loss: 17.866555861246958\n",
      "Pure loss: 16.940516573350894.....Total loss: 16.940516573350894\n",
      "epoch 2988 learning rate:  0.01033467202141901   Training loss:   17.866555861246958  Valing loss:   16.940516573350894\n",
      "Pure loss: 17.925008627829683.....Total loss: 17.925008627829683\n",
      "Pure loss: 17.36261967681263.....Total loss: 17.36261967681263\n",
      "epoch 2989 learning rate:  0.01033456005352961   Training loss:   17.925008627829683  Valing loss:   17.36261967681263\n",
      "Pure loss: 17.917385099764434.....Total loss: 17.917385099764434\n",
      "Pure loss: 17.38211327209863.....Total loss: 17.38211327209863\n",
      "epoch 2990 learning rate:  0.010334448160535118   Training loss:   17.917385099764434  Valing loss:   17.38211327209863\n",
      "Pure loss: 17.8410295009445.....Total loss: 17.8410295009445\n",
      "Pure loss: 17.081942892308554.....Total loss: 17.081942892308554\n",
      "epoch 2991 learning rate:  0.010334336342360416   Training loss:   17.8410295009445  Valing loss:   17.081942892308554\n",
      "Pure loss: 17.84110992375397.....Total loss: 17.84110992375397\n",
      "Pure loss: 17.074737251206983.....Total loss: 17.074737251206983\n",
      "epoch 2992 learning rate:  0.010334224598930481   Training loss:   17.84110992375397  Valing loss:   17.074737251206983\n",
      "Pure loss: 17.807699342014217.....Total loss: 17.807699342014217\n",
      "Pure loss: 17.079039824533417.....Total loss: 17.079039824533417\n",
      "epoch 2993 learning rate:  0.010334112930170399   Training loss:   17.807699342014217  Valing loss:   17.079039824533417\n",
      "Pure loss: 17.807345310658164.....Total loss: 17.807345310658164\n",
      "Pure loss: 16.694846959533116.....Total loss: 16.694846959533116\n",
      "epoch 2994 learning rate:  0.010334001336005344   Training loss:   17.807345310658164  Valing loss:   16.694846959533116\n",
      "Pure loss: 17.546171723811295.....Total loss: 17.546171723811295\n",
      "Pure loss: 16.673049374433788.....Total loss: 16.673049374433788\n",
      "epoch 2995 learning rate:  0.010333889816360601   Training loss:   17.546171723811295  Valing loss:   16.673049374433788\n",
      "Pure loss: 17.543309974408515.....Total loss: 17.543309974408515\n",
      "Pure loss: 16.486895743952562.....Total loss: 16.486895743952562\n",
      "epoch 2996 learning rate:  0.01033377837116155   Training loss:   17.543309974408515  Valing loss:   16.486895743952562\n",
      "Pure loss: 17.54595068867384.....Total loss: 17.54595068867384\n",
      "Pure loss: 16.48218856851368.....Total loss: 16.48218856851368\n",
      "epoch 2997 learning rate:  0.010333667000333667   Training loss:   17.54595068867384  Valing loss:   16.48218856851368\n",
      "Pure loss: 17.543975952687223.....Total loss: 17.543975952687223\n",
      "Pure loss: 16.491854770444373.....Total loss: 16.491854770444373\n",
      "epoch 2998 learning rate:  0.010333555703802535   Training loss:   17.543975952687223  Valing loss:   16.491854770444373\n",
      "Pure loss: 17.631287654183765.....Total loss: 17.631287654183765\n",
      "Pure loss: 16.53330998191537.....Total loss: 16.53330998191537\n",
      "epoch 2999 learning rate:  0.010333444481493832   Training loss:   17.631287654183765  Valing loss:   16.53330998191537\n",
      "Pure loss: 17.720040095432026.....Total loss: 17.720040095432026\n",
      "Pure loss: 16.9818037700004.....Total loss: 16.9818037700004\n",
      "epoch 3000 learning rate:  0.010333333333333333   Training loss:   17.720040095432026  Valing loss:   16.9818037700004\n",
      "Pure loss: 18.374272744255798.....Total loss: 18.374272744255798\n",
      "Pure loss: 18.343214844103297.....Total loss: 18.343214844103297\n",
      "epoch 3001 learning rate:  0.010333222259246919   Training loss:   18.374272744255798  Valing loss:   18.343214844103297\n",
      "Pure loss: 18.292061355929206.....Total loss: 18.292061355929206\n",
      "Pure loss: 18.17576395537132.....Total loss: 18.17576395537132\n",
      "epoch 3002 learning rate:  0.01033311125916056   Training loss:   18.292061355929206  Valing loss:   18.17576395537132\n",
      "Pure loss: 18.089625460513343.....Total loss: 18.089625460513343\n",
      "Pure loss: 17.776076194651992.....Total loss: 17.776076194651992\n",
      "epoch 3003 learning rate:  0.010333000333000334   Training loss:   18.089625460513343  Valing loss:   17.776076194651992\n",
      "Pure loss: 18.158771279842057.....Total loss: 18.158771279842057\n",
      "Pure loss: 18.085593323697488.....Total loss: 18.085593323697488\n",
      "epoch 3004 learning rate:  0.01033288948069241   Training loss:   18.158771279842057  Valing loss:   18.085593323697488\n",
      "Pure loss: 17.717909168507077.....Total loss: 17.717909168507077\n",
      "Pure loss: 17.157698519682583.....Total loss: 17.157698519682583\n",
      "epoch 3005 learning rate:  0.010332778702163061   Training loss:   17.717909168507077  Valing loss:   17.157698519682583\n",
      "Pure loss: 17.853898810559684.....Total loss: 17.853898810559684\n",
      "Pure loss: 17.466818685403044.....Total loss: 17.466818685403044\n",
      "epoch 3006 learning rate:  0.010332667997338656   Training loss:   17.853898810559684  Valing loss:   17.466818685403044\n",
      "Pure loss: 17.956593722621136.....Total loss: 17.956593722621136\n",
      "Pure loss: 17.738763655595797.....Total loss: 17.738763655595797\n",
      "epoch 3007 learning rate:  0.01033255736614566   Training loss:   17.956593722621136  Valing loss:   17.738763655595797\n",
      "Pure loss: 18.896310129709413.....Total loss: 18.896310129709413\n",
      "Pure loss: 19.23155109230546.....Total loss: 19.23155109230546\n",
      "epoch 3008 learning rate:  0.010332446808510639   Training loss:   18.896310129709413  Valing loss:   19.23155109230546\n",
      "Pure loss: 19.282780056727375.....Total loss: 19.282780056727375\n",
      "Pure loss: 19.769461556775276.....Total loss: 19.769461556775276\n",
      "epoch 3009 learning rate:  0.010332336324360253   Training loss:   19.282780056727375  Valing loss:   19.769461556775276\n",
      "Pure loss: 18.24424057561977.....Total loss: 18.24424057561977\n",
      "Pure loss: 18.268575527642575.....Total loss: 18.268575527642575\n",
      "epoch 3010 learning rate:  0.010332225913621262   Training loss:   18.24424057561977  Valing loss:   18.268575527642575\n",
      "Pure loss: 17.977331550528106.....Total loss: 17.977331550528106\n",
      "Pure loss: 17.84022234877774.....Total loss: 17.84022234877774\n",
      "epoch 3011 learning rate:  0.010332115576220526   Training loss:   17.977331550528106  Valing loss:   17.84022234877774\n",
      "Pure loss: 18.44333117995738.....Total loss: 18.44333117995738\n",
      "Pure loss: 18.59659614103295.....Total loss: 18.59659614103295\n",
      "epoch 3012 learning rate:  0.010332005312084993   Training loss:   18.44333117995738  Valing loss:   18.59659614103295\n",
      "Pure loss: 18.338626886056776.....Total loss: 18.338626886056776\n",
      "Pure loss: 18.435753700490313.....Total loss: 18.435753700490313\n",
      "epoch 3013 learning rate:  0.010331895121141719   Training loss:   18.338626886056776  Valing loss:   18.435753700490313\n",
      "Pure loss: 18.217911385255665.....Total loss: 18.217911385255665\n",
      "Pure loss: 18.244618007505327.....Total loss: 18.244618007505327\n",
      "epoch 3014 learning rate:  0.01033178500331785   Training loss:   18.217911385255665  Valing loss:   18.244618007505327\n",
      "Pure loss: 18.259045979856136.....Total loss: 18.259045979856136\n",
      "Pure loss: 18.32491342841946.....Total loss: 18.32491342841946\n",
      "epoch 3015 learning rate:  0.01033167495854063   Training loss:   18.259045979856136  Valing loss:   18.32491342841946\n",
      "Pure loss: 18.78051322554018.....Total loss: 18.78051322554018\n",
      "Pure loss: 19.097181880196928.....Total loss: 19.097181880196928\n",
      "epoch 3016 learning rate:  0.010331564986737402   Training loss:   18.78051322554018  Valing loss:   19.097181880196928\n",
      "Pure loss: 18.937427775589157.....Total loss: 18.937427775589157\n",
      "Pure loss: 19.320031142986057.....Total loss: 19.320031142986057\n",
      "epoch 3017 learning rate:  0.010331455087835598   Training loss:   18.937427775589157  Valing loss:   19.320031142986057\n",
      "Pure loss: 18.719620676378685.....Total loss: 18.719620676378685\n",
      "Pure loss: 19.00498683002294.....Total loss: 19.00498683002294\n",
      "epoch 3018 learning rate:  0.010331345261762758   Training loss:   18.719620676378685  Valing loss:   19.00498683002294\n",
      "Pure loss: 18.726407189133315.....Total loss: 18.726407189133315\n",
      "Pure loss: 19.014339439655.....Total loss: 19.014339439655\n",
      "epoch 3019 learning rate:  0.010331235508446506   Training loss:   18.726407189133315  Valing loss:   19.014339439655\n",
      "Pure loss: 18.67873008416418.....Total loss: 18.67873008416418\n",
      "Pure loss: 18.942579441181007.....Total loss: 18.942579441181007\n",
      "epoch 3020 learning rate:  0.01033112582781457   Training loss:   18.67873008416418  Valing loss:   18.942579441181007\n",
      "Pure loss: 19.312981495566557.....Total loss: 19.312981495566557\n",
      "Pure loss: 19.85241995002708.....Total loss: 19.85241995002708\n",
      "epoch 3021 learning rate:  0.01033101621979477   Training loss:   19.312981495566557  Valing loss:   19.85241995002708\n",
      "Pure loss: 18.568699076411086.....Total loss: 18.568699076411086\n",
      "Pure loss: 18.78442426474131.....Total loss: 18.78442426474131\n",
      "epoch 3022 learning rate:  0.010330906684315024   Training loss:   18.568699076411086  Valing loss:   18.78442426474131\n",
      "Pure loss: 18.891446567979216.....Total loss: 18.891446567979216\n",
      "Pure loss: 19.268831010199712.....Total loss: 19.268831010199712\n",
      "epoch 3023 learning rate:  0.010330797221303341   Training loss:   18.891446567979216  Valing loss:   19.268831010199712\n",
      "Pure loss: 18.85631746435128.....Total loss: 18.85631746435128\n",
      "Pure loss: 19.219174857890437.....Total loss: 19.219174857890437\n",
      "epoch 3024 learning rate:  0.010330687830687831   Training loss:   18.85631746435128  Valing loss:   19.219174857890437\n",
      "Pure loss: 18.549683617839598.....Total loss: 18.549683617839598\n",
      "Pure loss: 18.79673409599769.....Total loss: 18.79673409599769\n",
      "epoch 3025 learning rate:  0.010330578512396695   Training loss:   18.549683617839598  Valing loss:   18.79673409599769\n",
      "Pure loss: 18.577220112524635.....Total loss: 18.577220112524635\n",
      "Pure loss: 18.838959092466084.....Total loss: 18.838959092466084\n",
      "epoch 3026 learning rate:  0.010330469266358229   Training loss:   18.577220112524635  Valing loss:   18.838959092466084\n",
      "Pure loss: 17.906160775729333.....Total loss: 17.906160775729333\n",
      "Pure loss: 17.764099329207028.....Total loss: 17.764099329207028\n",
      "epoch 3027 learning rate:  0.010330360092500827   Training loss:   17.906160775729333  Valing loss:   17.764099329207028\n",
      "Pure loss: 18.10527373032918.....Total loss: 18.10527373032918\n",
      "Pure loss: 18.119947307423057.....Total loss: 18.119947307423057\n",
      "epoch 3028 learning rate:  0.010330250990752972   Training loss:   18.10527373032918  Valing loss:   18.119947307423057\n",
      "Pure loss: 18.739217025748893.....Total loss: 18.739217025748893\n",
      "Pure loss: 19.072091478890847.....Total loss: 19.072091478890847\n",
      "epoch 3029 learning rate:  0.01033014196104325   Training loss:   18.739217025748893  Valing loss:   19.072091478890847\n",
      "Pure loss: 18.820370309478232.....Total loss: 18.820370309478232\n",
      "Pure loss: 19.187013499630762.....Total loss: 19.187013499630762\n",
      "epoch 3030 learning rate:  0.010330033003300331   Training loss:   18.820370309478232  Valing loss:   19.187013499630762\n",
      "Pure loss: 18.625293947108986.....Total loss: 18.625293947108986\n",
      "Pure loss: 18.88731399119541.....Total loss: 18.88731399119541\n",
      "epoch 3031 learning rate:  0.010329924117452986   Training loss:   18.625293947108986  Valing loss:   18.88731399119541\n",
      "Pure loss: 18.42344447706243.....Total loss: 18.42344447706243\n",
      "Pure loss: 18.577494464250844.....Total loss: 18.577494464250844\n",
      "epoch 3032 learning rate:  0.010329815303430079   Training loss:   18.42344447706243  Valing loss:   18.577494464250844\n",
      "Pure loss: 18.672159472461683.....Total loss: 18.672159472461683\n",
      "Pure loss: 18.979727257788472.....Total loss: 18.979727257788472\n",
      "epoch 3033 learning rate:  0.010329706561160567   Training loss:   18.672159472461683  Valing loss:   18.979727257788472\n",
      "Pure loss: 18.43945240182742.....Total loss: 18.43945240182742\n",
      "Pure loss: 18.617790584516328.....Total loss: 18.617790584516328\n",
      "epoch 3034 learning rate:  0.0103295978905735   Training loss:   18.43945240182742  Valing loss:   18.617790584516328\n",
      "Pure loss: 18.5402919613046.....Total loss: 18.5402919613046\n",
      "Pure loss: 18.772659031194742.....Total loss: 18.772659031194742\n",
      "epoch 3035 learning rate:  0.010329489291598022   Training loss:   18.5402919613046  Valing loss:   18.772659031194742\n",
      "Pure loss: 18.272057640048033.....Total loss: 18.272057640048033\n",
      "Pure loss: 18.360187944047624.....Total loss: 18.360187944047624\n",
      "epoch 3036 learning rate:  0.010329380764163372   Training loss:   18.272057640048033  Valing loss:   18.360187944047624\n",
      "Pure loss: 18.276417653813915.....Total loss: 18.276417653813915\n",
      "Pure loss: 18.36872864478307.....Total loss: 18.36872864478307\n",
      "epoch 3037 learning rate:  0.01032927230819888   Training loss:   18.276417653813915  Valing loss:   18.36872864478307\n",
      "Pure loss: 19.60601382812131.....Total loss: 19.60601382812131\n",
      "Pure loss: 20.27380474612249.....Total loss: 20.27380474612249\n",
      "epoch 3038 learning rate:  0.01032916392363397   Training loss:   19.60601382812131  Valing loss:   20.27380474612249\n",
      "Pure loss: 19.8461210735223.....Total loss: 19.8461210735223\n",
      "Pure loss: 20.594639858717542.....Total loss: 20.594639858717542\n",
      "epoch 3039 learning rate:  0.010329055610398158   Training loss:   19.8461210735223  Valing loss:   20.594639858717542\n",
      "Pure loss: 19.87380352941225.....Total loss: 19.87380352941225\n",
      "Pure loss: 20.642977286027882.....Total loss: 20.642977286027882\n",
      "epoch 3040 learning rate:  0.010328947368421052   Training loss:   19.87380352941225  Valing loss:   20.642977286027882\n",
      "Pure loss: 19.06541493758244.....Total loss: 19.06541493758244\n",
      "Pure loss: 19.518517064050982.....Total loss: 19.518517064050982\n",
      "epoch 3041 learning rate:  0.010328839197632358   Training loss:   19.06541493758244  Valing loss:   19.518517064050982\n",
      "Pure loss: 19.142578003624106.....Total loss: 19.142578003624106\n",
      "Pure loss: 19.66402576390102.....Total loss: 19.66402576390102\n",
      "epoch 3042 learning rate:  0.010328731097961867   Training loss:   19.142578003624106  Valing loss:   19.66402576390102\n",
      "Pure loss: 18.61693480593351.....Total loss: 18.61693480593351\n",
      "Pure loss: 18.915695699634213.....Total loss: 18.915695699634213\n",
      "epoch 3043 learning rate:  0.010328623069339469   Training loss:   18.61693480593351  Valing loss:   18.915695699634213\n",
      "Pure loss: 18.491439940819756.....Total loss: 18.491439940819756\n",
      "Pure loss: 18.572003943496426.....Total loss: 18.572003943496426\n",
      "epoch 3044 learning rate:  0.010328515111695137   Training loss:   18.491439940819756  Valing loss:   18.572003943496426\n",
      "Pure loss: 18.42018238308352.....Total loss: 18.42018238308352\n",
      "Pure loss: 18.45810933329355.....Total loss: 18.45810933329355\n",
      "epoch 3045 learning rate:  0.010328407224958949   Training loss:   18.42018238308352  Valing loss:   18.45810933329355\n",
      "Pure loss: 19.538994528010065.....Total loss: 19.538994528010065\n",
      "Pure loss: 20.202790423935323.....Total loss: 20.202790423935323\n",
      "epoch 3046 learning rate:  0.010328299409061064   Training loss:   19.538994528010065  Valing loss:   20.202790423935323\n",
      "Pure loss: 19.541879992522045.....Total loss: 19.541879992522045\n",
      "Pure loss: 20.206492746628236.....Total loss: 20.206492746628236\n",
      "epoch 3047 learning rate:  0.010328191663931736   Training loss:   19.541879992522045  Valing loss:   20.206492746628236\n",
      "Pure loss: 19.956012958088234.....Total loss: 19.956012958088234\n",
      "Pure loss: 20.752720043724036.....Total loss: 20.752720043724036\n",
      "epoch 3048 learning rate:  0.010328083989501313   Training loss:   19.956012958088234  Valing loss:   20.752720043724036\n",
      "Pure loss: 19.174020160310175.....Total loss: 19.174020160310175\n",
      "Pure loss: 19.707344865619945.....Total loss: 19.707344865619945\n",
      "epoch 3049 learning rate:  0.01032797638570023   Training loss:   19.174020160310175  Valing loss:   19.707344865619945\n",
      "Pure loss: 17.798771024747804.....Total loss: 17.798771024747804\n",
      "Pure loss: 17.646259359879995.....Total loss: 17.646259359879995\n",
      "epoch 3050 learning rate:  0.010327868852459017   Training loss:   17.798771024747804  Valing loss:   17.646259359879995\n",
      "Pure loss: 17.536951142602238.....Total loss: 17.536951142602238\n",
      "Pure loss: 17.21038692812091.....Total loss: 17.21038692812091\n",
      "epoch 3051 learning rate:  0.010327761389708293   Training loss:   17.536951142602238  Valing loss:   17.21038692812091\n",
      "Pure loss: 17.38967151640887.....Total loss: 17.38967151640887\n",
      "Pure loss: 16.92992551493202.....Total loss: 16.92992551493202\n",
      "epoch 3052 learning rate:  0.010327653997378769   Training loss:   17.38967151640887  Valing loss:   16.92992551493202\n",
      "Pure loss: 17.358936890519793.....Total loss: 17.358936890519793\n",
      "Pure loss: 16.867987454599504.....Total loss: 16.867987454599504\n",
      "epoch 3053 learning rate:  0.010327546675401245   Training loss:   17.358936890519793  Valing loss:   16.867987454599504\n",
      "Pure loss: 17.33525616217921.....Total loss: 17.33525616217921\n",
      "Pure loss: 16.818537928098678.....Total loss: 16.818537928098678\n",
      "epoch 3054 learning rate:  0.010327439423706614   Training loss:   17.33525616217921  Valing loss:   16.818537928098678\n",
      "Pure loss: 17.29519544036994.....Total loss: 17.29519544036994\n",
      "Pure loss: 16.73305341562759.....Total loss: 16.73305341562759\n",
      "epoch 3055 learning rate:  0.010327332242225859   Training loss:   17.29519544036994  Valing loss:   16.73305341562759\n",
      "Pure loss: 17.23638139889317.....Total loss: 17.23638139889317\n",
      "Pure loss: 16.595768105168602.....Total loss: 16.595768105168602\n",
      "epoch 3056 learning rate:  0.010327225130890053   Training loss:   17.23638139889317  Valing loss:   16.595768105168602\n",
      "Pure loss: 17.21897364982184.....Total loss: 17.21897364982184\n",
      "Pure loss: 16.54570597390759.....Total loss: 16.54570597390759\n",
      "epoch 3057 learning rate:  0.010327118089630357   Training loss:   17.21897364982184  Valing loss:   16.54570597390759\n",
      "Pure loss: 17.24557518012205.....Total loss: 17.24557518012205\n",
      "Pure loss: 16.613778906294336.....Total loss: 16.613778906294336\n",
      "epoch 3058 learning rate:  0.010327011118378025   Training loss:   17.24557518012205  Valing loss:   16.613778906294336\n",
      "Pure loss: 17.513707435942067.....Total loss: 17.513707435942067\n",
      "Pure loss: 17.176731060839302.....Total loss: 17.176731060839302\n",
      "epoch 3059 learning rate:  0.0103269042170644   Training loss:   17.513707435942067  Valing loss:   17.176731060839302\n",
      "Pure loss: 17.400760255978962.....Total loss: 17.400760255978962\n",
      "Pure loss: 16.965241763452813.....Total loss: 16.965241763452813\n",
      "epoch 3060 learning rate:  0.010326797385620916   Training loss:   17.400760255978962  Valing loss:   16.965241763452813\n",
      "Pure loss: 17.346377832411875.....Total loss: 17.346377832411875\n",
      "Pure loss: 16.840211834165032.....Total loss: 16.840211834165032\n",
      "epoch 3061 learning rate:  0.010326690623979092   Training loss:   17.346377832411875  Valing loss:   16.840211834165032\n",
      "Pure loss: 17.33664503594234.....Total loss: 17.33664503594234\n",
      "Pure loss: 16.820191757630848.....Total loss: 16.820191757630848\n",
      "epoch 3062 learning rate:  0.010326583932070543   Training loss:   17.33664503594234  Valing loss:   16.820191757630848\n",
      "Pure loss: 17.28161848364717.....Total loss: 17.28161848364717\n",
      "Pure loss: 16.672919155670026.....Total loss: 16.672919155670026\n",
      "epoch 3063 learning rate:  0.010326477309826967   Training loss:   17.28161848364717  Valing loss:   16.672919155670026\n",
      "Pure loss: 17.273415670304082.....Total loss: 17.273415670304082\n",
      "Pure loss: 16.542487184681104.....Total loss: 16.542487184681104\n",
      "epoch 3064 learning rate:  0.010326370757180157   Training loss:   17.273415670304082  Valing loss:   16.542487184681104\n",
      "Pure loss: 17.299549129113622.....Total loss: 17.299549129113622\n",
      "Pure loss: 16.525606786157496.....Total loss: 16.525606786157496\n",
      "epoch 3065 learning rate:  0.01032626427406199   Training loss:   17.299549129113622  Valing loss:   16.525606786157496\n",
      "Pure loss: 17.31709923914187.....Total loss: 17.31709923914187\n",
      "Pure loss: 16.532895496602425.....Total loss: 16.532895496602425\n",
      "epoch 3066 learning rate:  0.010326157860404436   Training loss:   17.31709923914187  Valing loss:   16.532895496602425\n",
      "Pure loss: 17.23017029084693.....Total loss: 17.23017029084693\n",
      "Pure loss: 16.302354997920986.....Total loss: 16.302354997920986\n",
      "epoch 3067 learning rate:  0.01032605151613955   Training loss:   17.23017029084693  Valing loss:   16.302354997920986\n",
      "Pure loss: 17.22574770616912.....Total loss: 17.22574770616912\n",
      "Pure loss: 16.30297466086284.....Total loss: 16.30297466086284\n",
      "epoch 3068 learning rate:  0.010325945241199478   Training loss:   17.22574770616912  Valing loss:   16.30297466086284\n",
      "Pure loss: 17.30651826420635.....Total loss: 17.30651826420635\n",
      "Pure loss: 16.15711546692794.....Total loss: 16.15711546692794\n",
      "epoch 3069 learning rate:  0.010325839035516454   Training loss:   17.30651826420635  Valing loss:   16.15711546692794\n",
      "Pure loss: 17.337620952806912.....Total loss: 17.337620952806912\n",
      "Pure loss: 16.16467924918941.....Total loss: 16.16467924918941\n",
      "epoch 3070 learning rate:  0.010325732899022801   Training loss:   17.337620952806912  Valing loss:   16.16467924918941\n",
      "Pure loss: 17.346955127471542.....Total loss: 17.346955127471542\n",
      "Pure loss: 16.148061276034397.....Total loss: 16.148061276034397\n",
      "epoch 3071 learning rate:  0.010325626831650928   Training loss:   17.346955127471542  Valing loss:   16.148061276034397\n",
      "Pure loss: 17.43340198995214.....Total loss: 17.43340198995214\n",
      "Pure loss: 16.033207441490486.....Total loss: 16.033207441490486\n",
      "epoch 3072 learning rate:  0.010325520833333334   Training loss:   17.43340198995214  Valing loss:   16.033207441490486\n",
      "Pure loss: 17.48896129621381.....Total loss: 17.48896129621381\n",
      "Pure loss: 15.982512640398536.....Total loss: 15.982512640398536\n",
      "epoch 3073 learning rate:  0.010325414904002603   Training loss:   17.48896129621381  Valing loss:   15.982512640398536\n",
      "Pure loss: 17.73395242566892.....Total loss: 17.73395242566892\n",
      "Pure loss: 16.179380212865805.....Total loss: 16.179380212865805\n",
      "epoch 3074 learning rate:  0.010325309043591411   Training loss:   17.73395242566892  Valing loss:   16.179380212865805\n",
      "Pure loss: 17.732048495438708.....Total loss: 17.732048495438708\n",
      "Pure loss: 16.180959412698172.....Total loss: 16.180959412698172\n",
      "epoch 3075 learning rate:  0.010325203252032521   Training loss:   17.732048495438708  Valing loss:   16.180959412698172\n",
      "Pure loss: 17.55410993253873.....Total loss: 17.55410993253873\n",
      "Pure loss: 16.17408121126058.....Total loss: 16.17408121126058\n",
      "epoch 3076 learning rate:  0.010325097529258778   Training loss:   17.55410993253873  Valing loss:   16.17408121126058\n",
      "Pure loss: 17.54559978822389.....Total loss: 17.54559978822389\n",
      "Pure loss: 16.20472167908399.....Total loss: 16.20472167908399\n",
      "epoch 3077 learning rate:  0.01032499187520312   Training loss:   17.54559978822389  Valing loss:   16.20472167908399\n",
      "Pure loss: 17.551200468505606.....Total loss: 17.551200468505606\n",
      "Pure loss: 16.204344764268658.....Total loss: 16.204344764268658\n",
      "epoch 3078 learning rate:  0.01032488628979857   Training loss:   17.551200468505606  Valing loss:   16.204344764268658\n",
      "Pure loss: 17.450697585066845.....Total loss: 17.450697585066845\n",
      "Pure loss: 16.239221479287377.....Total loss: 16.239221479287377\n",
      "epoch 3079 learning rate:  0.010324780772978239   Training loss:   17.450697585066845  Valing loss:   16.239221479287377\n",
      "Pure loss: 17.46091756415887.....Total loss: 17.46091756415887\n",
      "Pure loss: 16.22759997932126.....Total loss: 16.22759997932126\n",
      "epoch 3080 learning rate:  0.010324675324675325   Training loss:   17.46091756415887  Valing loss:   16.22759997932126\n",
      "Pure loss: 17.19687074264315.....Total loss: 17.19687074264315\n",
      "Pure loss: 16.085036677077785.....Total loss: 16.085036677077785\n",
      "epoch 3081 learning rate:  0.010324569944823109   Training loss:   17.19687074264315  Valing loss:   16.085036677077785\n",
      "Pure loss: 17.29457628164676.....Total loss: 17.29457628164676\n",
      "Pure loss: 16.111877911317915.....Total loss: 16.111877911317915\n",
      "epoch 3082 learning rate:  0.010324464633354964   Training loss:   17.29457628164676  Valing loss:   16.111877911317915\n",
      "Pure loss: 17.38689140635016.....Total loss: 17.38689140635016\n",
      "Pure loss: 16.05677897426243.....Total loss: 16.05677897426243\n",
      "epoch 3083 learning rate:  0.010324359390204346   Training loss:   17.38689140635016  Valing loss:   16.05677897426243\n",
      "Pure loss: 17.454773466514872.....Total loss: 17.454773466514872\n",
      "Pure loss: 16.099830064089588.....Total loss: 16.099830064089588\n",
      "epoch 3084 learning rate:  0.0103242542153048   Training loss:   17.454773466514872  Valing loss:   16.099830064089588\n",
      "Pure loss: 17.45702278991378.....Total loss: 17.45702278991378\n",
      "Pure loss: 16.089637637848206.....Total loss: 16.089637637848206\n",
      "epoch 3085 learning rate:  0.01032414910858995   Training loss:   17.45702278991378  Valing loss:   16.089637637848206\n",
      "Pure loss: 17.345895916675346.....Total loss: 17.345895916675346\n",
      "Pure loss: 16.3425192759076.....Total loss: 16.3425192759076\n",
      "epoch 3086 learning rate:  0.010324044069993519   Training loss:   17.345895916675346  Valing loss:   16.3425192759076\n",
      "Pure loss: 17.2604088011803.....Total loss: 17.2604088011803\n",
      "Pure loss: 16.32785340721338.....Total loss: 16.32785340721338\n",
      "epoch 3087 learning rate:  0.010323939099449304   Training loss:   17.2604088011803  Valing loss:   16.32785340721338\n",
      "Pure loss: 17.479518810315707.....Total loss: 17.479518810315707\n",
      "Pure loss: 16.206653913139846.....Total loss: 16.206653913139846\n",
      "epoch 3088 learning rate:  0.010323834196891191   Training loss:   17.479518810315707  Valing loss:   16.206653913139846\n",
      "Pure loss: 16.976152232429243.....Total loss: 16.976152232429243\n",
      "Pure loss: 15.949975808196415.....Total loss: 15.949975808196415\n",
      "epoch 3089 learning rate:  0.010323729362253157   Training loss:   16.976152232429243  Valing loss:   15.949975808196415\n",
      "Pure loss: 16.994803261143502.....Total loss: 16.994803261143502\n",
      "Pure loss: 16.13104887811095.....Total loss: 16.13104887811095\n",
      "epoch 3090 learning rate:  0.010323624595469255   Training loss:   16.994803261143502  Valing loss:   16.13104887811095\n",
      "Pure loss: 17.256453054892155.....Total loss: 17.256453054892155\n",
      "Pure loss: 16.771283396511162.....Total loss: 16.771283396511162\n",
      "epoch 3091 learning rate:  0.010323519896473633   Training loss:   17.256453054892155  Valing loss:   16.771283396511162\n",
      "Pure loss: 17.791064062345516.....Total loss: 17.791064062345516\n",
      "Pure loss: 17.75140398273596.....Total loss: 17.75140398273596\n",
      "epoch 3092 learning rate:  0.010323415265200519   Training loss:   17.791064062345516  Valing loss:   17.75140398273596\n",
      "Pure loss: 17.81186772354032.....Total loss: 17.81186772354032\n",
      "Pure loss: 17.781629377749958.....Total loss: 17.781629377749958\n",
      "epoch 3093 learning rate:  0.010323310701584223   Training loss:   17.81186772354032  Valing loss:   17.781629377749958\n",
      "Pure loss: 17.734725162519595.....Total loss: 17.734725162519595\n",
      "Pure loss: 17.657351141667004.....Total loss: 17.657351141667004\n",
      "epoch 3094 learning rate:  0.010323206205559148   Training loss:   17.734725162519595  Valing loss:   17.657351141667004\n",
      "Pure loss: 18.094949748620806.....Total loss: 18.094949748620806\n",
      "Pure loss: 18.163903141091428.....Total loss: 18.163903141091428\n",
      "epoch 3095 learning rate:  0.010323101777059774   Training loss:   18.094949748620806  Valing loss:   18.163903141091428\n",
      "Pure loss: 17.940374792488658.....Total loss: 17.940374792488658\n",
      "Pure loss: 17.965740362063443.....Total loss: 17.965740362063443\n",
      "epoch 3096 learning rate:  0.010322997416020671   Training loss:   17.940374792488658  Valing loss:   17.965740362063443\n",
      "Pure loss: 17.88277972169879.....Total loss: 17.88277972169879\n",
      "Pure loss: 17.882651578651274.....Total loss: 17.882651578651274\n",
      "epoch 3097 learning rate:  0.010322893122376494   Training loss:   17.88277972169879  Valing loss:   17.882651578651274\n",
      "Pure loss: 18.401002143643947.....Total loss: 18.401002143643947\n",
      "Pure loss: 18.638032428150062.....Total loss: 18.638032428150062\n",
      "epoch 3098 learning rate:  0.010322788896061976   Training loss:   18.401002143643947  Valing loss:   18.638032428150062\n",
      "Pure loss: 18.315243165510193.....Total loss: 18.315243165510193\n",
      "Pure loss: 18.51514114628168.....Total loss: 18.51514114628168\n",
      "epoch 3099 learning rate:  0.01032268473701194   Training loss:   18.315243165510193  Valing loss:   18.51514114628168\n",
      "Pure loss: 18.303209648057937.....Total loss: 18.303209648057937\n",
      "Pure loss: 18.49813980954472.....Total loss: 18.49813980954472\n",
      "epoch 3100 learning rate:  0.01032258064516129   Training loss:   18.303209648057937  Valing loss:   18.49813980954472\n",
      "Pure loss: 17.88271010861244.....Total loss: 17.88271010861244\n",
      "Pure loss: 17.871462277322884.....Total loss: 17.871462277322884\n",
      "epoch 3101 learning rate:  0.010322476620445017   Training loss:   17.88271010861244  Valing loss:   17.871462277322884\n",
      "Pure loss: 17.574358301047454.....Total loss: 17.574358301047454\n",
      "Pure loss: 17.368519786005763.....Total loss: 17.368519786005763\n",
      "epoch 3102 learning rate:  0.010322372662798195   Training loss:   17.574358301047454  Valing loss:   17.368519786005763\n",
      "Pure loss: 17.47431027651232.....Total loss: 17.47431027651232\n",
      "Pure loss: 17.221966190743238.....Total loss: 17.221966190743238\n",
      "epoch 3103 learning rate:  0.010322268772155979   Training loss:   17.47431027651232  Valing loss:   17.221966190743238\n",
      "Pure loss: 17.840208761501817.....Total loss: 17.840208761501817\n",
      "Pure loss: 17.832620732685953.....Total loss: 17.832620732685953\n",
      "epoch 3104 learning rate:  0.010322164948453609   Training loss:   17.840208761501817  Valing loss:   17.832620732685953\n",
      "Pure loss: 18.07599789732572.....Total loss: 18.07599789732572\n",
      "Pure loss: 18.19042071467445.....Total loss: 18.19042071467445\n",
      "epoch 3105 learning rate:  0.01032206119162641   Training loss:   18.07599789732572  Valing loss:   18.19042071467445\n",
      "Pure loss: 17.986497575494138.....Total loss: 17.986497575494138\n",
      "Pure loss: 18.056195062771597.....Total loss: 18.056195062771597\n",
      "epoch 3106 learning rate:  0.010321957501609788   Training loss:   17.986497575494138  Valing loss:   18.056195062771597\n",
      "Pure loss: 17.57919663951498.....Total loss: 17.57919663951498\n",
      "Pure loss: 17.37241980285414.....Total loss: 17.37241980285414\n",
      "epoch 3107 learning rate:  0.010321853878339234   Training loss:   17.57919663951498  Valing loss:   17.37241980285414\n",
      "Pure loss: 17.578306633330197.....Total loss: 17.578306633330197\n",
      "Pure loss: 17.205838759796094.....Total loss: 17.205838759796094\n",
      "epoch 3108 learning rate:  0.010321750321750322   Training loss:   17.578306633330197  Valing loss:   17.205838759796094\n",
      "Pure loss: 17.238360670009843.....Total loss: 17.238360670009843\n",
      "Pure loss: 16.461373891027414.....Total loss: 16.461373891027414\n",
      "epoch 3109 learning rate:  0.010321646831778707   Training loss:   17.238360670009843  Valing loss:   16.461373891027414\n",
      "Pure loss: 17.239796861419805.....Total loss: 17.239796861419805\n",
      "Pure loss: 16.543734479371565.....Total loss: 16.543734479371565\n",
      "epoch 3110 learning rate:  0.01032154340836013   Training loss:   17.239796861419805  Valing loss:   16.543734479371565\n",
      "Pure loss: 17.194560315782315.....Total loss: 17.194560315782315\n",
      "Pure loss: 16.73975305956588.....Total loss: 16.73975305956588\n",
      "epoch 3111 learning rate:  0.010321440051430408   Training loss:   17.194560315782315  Valing loss:   16.73975305956588\n",
      "Pure loss: 17.16279142884782.....Total loss: 17.16279142884782\n",
      "Pure loss: 16.654478705231405.....Total loss: 16.654478705231405\n",
      "epoch 3112 learning rate:  0.01032133676092545   Training loss:   17.16279142884782  Valing loss:   16.654478705231405\n",
      "Pure loss: 17.02456506638462.....Total loss: 17.02456506638462\n",
      "Pure loss: 16.29023113743997.....Total loss: 16.29023113743997\n",
      "epoch 3113 learning rate:  0.01032123353678124   Training loss:   17.02456506638462  Valing loss:   16.29023113743997\n",
      "Pure loss: 16.966796532078156.....Total loss: 16.966796532078156\n",
      "Pure loss: 16.103228893230863.....Total loss: 16.103228893230863\n",
      "epoch 3114 learning rate:  0.010321130378933847   Training loss:   16.966796532078156  Valing loss:   16.103228893230863\n",
      "Pure loss: 17.036472700858162.....Total loss: 17.036472700858162\n",
      "Pure loss: 16.07453605918416.....Total loss: 16.07453605918416\n",
      "epoch 3115 learning rate:  0.010321027287319422   Training loss:   17.036472700858162  Valing loss:   16.07453605918416\n",
      "Pure loss: 17.123548642796603.....Total loss: 17.123548642796603\n",
      "Pure loss: 15.916552538626274.....Total loss: 15.916552538626274\n",
      "epoch 3116 learning rate:  0.010320924261874198   Training loss:   17.123548642796603  Valing loss:   15.916552538626274\n",
      "Pure loss: 17.21316296671842.....Total loss: 17.21316296671842\n",
      "Pure loss: 15.90729402072299.....Total loss: 15.90729402072299\n",
      "epoch 3117 learning rate:  0.010320821302534488   Training loss:   17.21316296671842  Valing loss:   15.90729402072299\n",
      "Pure loss: 17.328949571879427.....Total loss: 17.328949571879427\n",
      "Pure loss: 15.852958633430633.....Total loss: 15.852958633430633\n",
      "epoch 3118 learning rate:  0.010320718409236691   Training loss:   17.328949571879427  Valing loss:   15.852958633430633\n",
      "Pure loss: 17.198072231597024.....Total loss: 17.198072231597024\n",
      "Pure loss: 15.846314656485994.....Total loss: 15.846314656485994\n",
      "epoch 3119 learning rate:  0.010320615581917282   Training loss:   17.198072231597024  Valing loss:   15.846314656485994\n",
      "Pure loss: 17.228001551019673.....Total loss: 17.228001551019673\n",
      "Pure loss: 15.837286503253853.....Total loss: 15.837286503253853\n",
      "epoch 3120 learning rate:  0.010320512820512821   Training loss:   17.228001551019673  Valing loss:   15.837286503253853\n",
      "Pure loss: 17.153014714567664.....Total loss: 17.153014714567664\n",
      "Pure loss: 15.817314303623066.....Total loss: 15.817314303623066\n",
      "epoch 3121 learning rate:  0.010320410124959949   Training loss:   17.153014714567664  Valing loss:   15.817314303623066\n",
      "Pure loss: 16.95099708738574.....Total loss: 16.95099708738574\n",
      "Pure loss: 15.761556157405185.....Total loss: 15.761556157405185\n",
      "epoch 3122 learning rate:  0.010320307495195387   Training loss:   16.95099708738574  Valing loss:   15.761556157405185\n",
      "Pure loss: 17.08753093761363.....Total loss: 17.08753093761363\n",
      "Pure loss: 15.790389042490865.....Total loss: 15.790389042490865\n",
      "epoch 3123 learning rate:  0.01032020493115594   Training loss:   17.08753093761363  Valing loss:   15.790389042490865\n",
      "Pure loss: 17.006335590512805.....Total loss: 17.006335590512805\n",
      "Pure loss: 15.89165282086093.....Total loss: 15.89165282086093\n",
      "epoch 3124 learning rate:  0.01032010243277849   Training loss:   17.006335590512805  Valing loss:   15.89165282086093\n",
      "Pure loss: 17.005617798169094.....Total loss: 17.005617798169094\n",
      "Pure loss: 15.891957225334627.....Total loss: 15.891957225334627\n",
      "epoch 3125 learning rate:  0.010320000000000001   Training loss:   17.005617798169094  Valing loss:   15.891957225334627\n",
      "Pure loss: 17.076479342800518.....Total loss: 17.076479342800518\n",
      "Pure loss: 15.946956504566698.....Total loss: 15.946956504566698\n",
      "epoch 3126 learning rate:  0.010319897632757518   Training loss:   17.076479342800518  Valing loss:   15.946956504566698\n",
      "Pure loss: 17.012853155580185.....Total loss: 17.012853155580185\n",
      "Pure loss: 16.12877456650443.....Total loss: 16.12877456650443\n",
      "epoch 3127 learning rate:  0.010319795330988167   Training loss:   17.012853155580185  Valing loss:   16.12877456650443\n",
      "Pure loss: 17.00682212489894.....Total loss: 17.00682212489894\n",
      "Pure loss: 16.12897752854186.....Total loss: 16.12897752854186\n",
      "epoch 3128 learning rate:  0.010319693094629157   Training loss:   17.00682212489894  Valing loss:   16.12897752854186\n",
      "Pure loss: 17.030175027837906.....Total loss: 17.030175027837906\n",
      "Pure loss: 16.28733981065641.....Total loss: 16.28733981065641\n",
      "epoch 3129 learning rate:  0.010319590923617769   Training loss:   17.030175027837906  Valing loss:   16.28733981065641\n",
      "Pure loss: 17.014190615106013.....Total loss: 17.014190615106013\n",
      "Pure loss: 16.221511928789035.....Total loss: 16.221511928789035\n",
      "epoch 3130 learning rate:  0.010319488817891374   Training loss:   17.014190615106013  Valing loss:   16.221511928789035\n",
      "Pure loss: 17.010359577239285.....Total loss: 17.010359577239285\n",
      "Pure loss: 16.203045779146024.....Total loss: 16.203045779146024\n",
      "epoch 3131 learning rate:  0.010319386777387416   Training loss:   17.010359577239285  Valing loss:   16.203045779146024\n",
      "Pure loss: 17.022882582644197.....Total loss: 17.022882582644197\n",
      "Pure loss: 16.321141643197592.....Total loss: 16.321141643197592\n",
      "epoch 3132 learning rate:  0.010319284802043423   Training loss:   17.022882582644197  Valing loss:   16.321141643197592\n",
      "Pure loss: 17.077052172672698.....Total loss: 17.077052172672698\n",
      "Pure loss: 16.495360991711042.....Total loss: 16.495360991711042\n",
      "epoch 3133 learning rate:  0.010319182891797   Training loss:   17.077052172672698  Valing loss:   16.495360991711042\n",
      "Pure loss: 17.06255592486766.....Total loss: 17.06255592486766\n",
      "Pure loss: 16.498969755285305.....Total loss: 16.498969755285305\n",
      "epoch 3134 learning rate:  0.010319081046585833   Training loss:   17.06255592486766  Valing loss:   16.498969755285305\n",
      "Pure loss: 16.973707638392288.....Total loss: 16.973707638392288\n",
      "Pure loss: 16.109993593440535.....Total loss: 16.109993593440535\n",
      "epoch 3135 learning rate:  0.010318979266347688   Training loss:   16.973707638392288  Valing loss:   16.109993593440535\n",
      "Pure loss: 16.961107279444597.....Total loss: 16.961107279444597\n",
      "Pure loss: 16.177578831220128.....Total loss: 16.177578831220128\n",
      "epoch 3136 learning rate:  0.010318877551020409   Training loss:   16.961107279444597  Valing loss:   16.177578831220128\n",
      "Pure loss: 16.93850377229902.....Total loss: 16.93850377229902\n",
      "Pure loss: 15.931089515362881.....Total loss: 15.931089515362881\n",
      "epoch 3137 learning rate:  0.01031877590054192   Training loss:   16.93850377229902  Valing loss:   15.931089515362881\n",
      "Pure loss: 16.888893495165288.....Total loss: 16.888893495165288\n",
      "Pure loss: 16.020460757074396.....Total loss: 16.020460757074396\n",
      "epoch 3138 learning rate:  0.010318674314850224   Training loss:   16.888893495165288  Valing loss:   16.020460757074396\n",
      "Pure loss: 16.87437861435644.....Total loss: 16.87437861435644\n",
      "Pure loss: 15.798082480441453.....Total loss: 15.798082480441453\n",
      "epoch 3139 learning rate:  0.010318572793883403   Training loss:   16.87437861435644  Valing loss:   15.798082480441453\n",
      "Pure loss: 16.862552792427607.....Total loss: 16.862552792427607\n",
      "Pure loss: 15.906548328837651.....Total loss: 15.906548328837651\n",
      "epoch 3140 learning rate:  0.010318471337579618   Training loss:   16.862552792427607  Valing loss:   15.906548328837651\n",
      "Pure loss: 16.872654459237022.....Total loss: 16.872654459237022\n",
      "Pure loss: 15.985373106802893.....Total loss: 15.985373106802893\n",
      "epoch 3141 learning rate:  0.01031836994587711   Training loss:   16.872654459237022  Valing loss:   15.985373106802893\n",
      "Pure loss: 16.87202941195606.....Total loss: 16.87202941195606\n",
      "Pure loss: 15.92095465658326.....Total loss: 15.92095465658326\n",
      "epoch 3142 learning rate:  0.010318268618714195   Training loss:   16.87202941195606  Valing loss:   15.92095465658326\n",
      "Pure loss: 16.873190572780302.....Total loss: 16.873190572780302\n",
      "Pure loss: 15.91512005087638.....Total loss: 15.91512005087638\n",
      "epoch 3143 learning rate:  0.010318167356029272   Training loss:   16.873190572780302  Valing loss:   15.91512005087638\n",
      "Pure loss: 16.871096111659728.....Total loss: 16.871096111659728\n",
      "Pure loss: 15.952273637755756.....Total loss: 15.952273637755756\n",
      "epoch 3144 learning rate:  0.010318066157760814   Training loss:   16.871096111659728  Valing loss:   15.952273637755756\n",
      "Pure loss: 16.872740486555795.....Total loss: 16.872740486555795\n",
      "Pure loss: 15.97052614143321.....Total loss: 15.97052614143321\n",
      "epoch 3145 learning rate:  0.010317965023847377   Training loss:   16.872740486555795  Valing loss:   15.97052614143321\n",
      "Pure loss: 16.911281900991973.....Total loss: 16.911281900991973\n",
      "Pure loss: 15.643474272026822.....Total loss: 15.643474272026822\n",
      "epoch 3146 learning rate:  0.010317863954227591   Training loss:   16.911281900991973  Valing loss:   15.643474272026822\n",
      "Pure loss: 16.84688520029994.....Total loss: 16.84688520029994\n",
      "Pure loss: 15.811875596586827.....Total loss: 15.811875596586827\n",
      "epoch 3147 learning rate:  0.010317762948840166   Training loss:   16.84688520029994  Valing loss:   15.811875596586827\n",
      "Pure loss: 16.86171818986886.....Total loss: 16.86171818986886\n",
      "Pure loss: 16.02834752757316.....Total loss: 16.02834752757316\n",
      "epoch 3148 learning rate:  0.010317662007623888   Training loss:   16.86171818986886  Valing loss:   16.02834752757316\n",
      "Pure loss: 16.839111895598247.....Total loss: 16.839111895598247\n",
      "Pure loss: 15.817423650292627.....Total loss: 15.817423650292627\n",
      "epoch 3149 learning rate:  0.010317561130517626   Training loss:   16.839111895598247  Valing loss:   15.817423650292627\n",
      "Pure loss: 16.807432650732828.....Total loss: 16.807432650732828\n",
      "Pure loss: 15.902925692976524.....Total loss: 15.902925692976524\n",
      "epoch 3150 learning rate:  0.010317460317460317   Training loss:   16.807432650732828  Valing loss:   15.902925692976524\n",
      "Pure loss: 16.748697192067464.....Total loss: 16.748697192067464\n",
      "Pure loss: 15.632012017643042.....Total loss: 15.632012017643042\n",
      "epoch 3151 learning rate:  0.010317359568390987   Training loss:   16.748697192067464  Valing loss:   15.632012017643042\n",
      "Pure loss: 16.767891638926606.....Total loss: 16.767891638926606\n",
      "Pure loss: 15.517876762271179.....Total loss: 15.517876762271179\n",
      "epoch 3152 learning rate:  0.01031725888324873   Training loss:   16.767891638926606  Valing loss:   15.517876762271179\n",
      "Pure loss: 16.77873615856708.....Total loss: 16.77873615856708\n",
      "Pure loss: 15.516745622583356.....Total loss: 15.516745622583356\n",
      "epoch 3153 learning rate:  0.010317158261972725   Training loss:   16.77873615856708  Valing loss:   15.516745622583356\n",
      "Pure loss: 16.81504029842201.....Total loss: 16.81504029842201\n",
      "Pure loss: 15.434609255478806.....Total loss: 15.434609255478806\n",
      "epoch 3154 learning rate:  0.01031705770450222   Training loss:   16.81504029842201  Valing loss:   15.434609255478806\n",
      "Pure loss: 16.891847684226995.....Total loss: 16.891847684226995\n",
      "Pure loss: 15.411735135971206.....Total loss: 15.411735135971206\n",
      "epoch 3155 learning rate:  0.010316957210776545   Training loss:   16.891847684226995  Valing loss:   15.411735135971206\n",
      "Pure loss: 16.884868615177755.....Total loss: 16.884868615177755\n",
      "Pure loss: 15.408961509430037.....Total loss: 15.408961509430037\n",
      "epoch 3156 learning rate:  0.010316856780735108   Training loss:   16.884868615177755  Valing loss:   15.408961509430037\n",
      "Pure loss: 17.118393525748516.....Total loss: 17.118393525748516\n",
      "Pure loss: 15.278001978085744.....Total loss: 15.278001978085744\n",
      "epoch 3157 learning rate:  0.01031675641431739   Training loss:   17.118393525748516  Valing loss:   15.278001978085744\n",
      "Pure loss: 17.19161979713111.....Total loss: 17.19161979713111\n",
      "Pure loss: 15.318113727712738.....Total loss: 15.318113727712738\n",
      "epoch 3158 learning rate:  0.010316656111462952   Training loss:   17.19161979713111  Valing loss:   15.318113727712738\n",
      "Pure loss: 17.384884120961473.....Total loss: 17.384884120961473\n",
      "Pure loss: 15.35652587375445.....Total loss: 15.35652587375445\n",
      "epoch 3159 learning rate:  0.010316555872111428   Training loss:   17.384884120961473  Valing loss:   15.35652587375445\n",
      "Pure loss: 17.460649291557644.....Total loss: 17.460649291557644\n",
      "Pure loss: 15.410801737781817.....Total loss: 15.410801737781817\n",
      "epoch 3160 learning rate:  0.010316455696202532   Training loss:   17.460649291557644  Valing loss:   15.410801737781817\n",
      "Pure loss: 17.379338698148835.....Total loss: 17.379338698148835\n",
      "Pure loss: 15.350902371670326.....Total loss: 15.350902371670326\n",
      "epoch 3161 learning rate:  0.010316355583676051   Training loss:   17.379338698148835  Valing loss:   15.350902371670326\n",
      "Pure loss: 17.25233686592118.....Total loss: 17.25233686592118\n",
      "Pure loss: 15.292323742090694.....Total loss: 15.292323742090694\n",
      "epoch 3162 learning rate:  0.010316255534471854   Training loss:   17.25233686592118  Valing loss:   15.292323742090694\n",
      "Pure loss: 17.4718602600672.....Total loss: 17.4718602600672\n",
      "Pure loss: 15.384430766247853.....Total loss: 15.384430766247853\n",
      "epoch 3163 learning rate:  0.010316155548529876   Training loss:   17.4718602600672  Valing loss:   15.384430766247853\n",
      "Pure loss: 17.203524930955457.....Total loss: 17.203524930955457\n",
      "Pure loss: 15.269824900515237.....Total loss: 15.269824900515237\n",
      "epoch 3164 learning rate:  0.01031605562579014   Training loss:   17.203524930955457  Valing loss:   15.269824900515237\n",
      "Pure loss: 16.748446624582055.....Total loss: 16.748446624582055\n",
      "Pure loss: 15.509840245208665.....Total loss: 15.509840245208665\n",
      "epoch 3165 learning rate:  0.010315955766192732   Training loss:   16.748446624582055  Valing loss:   15.509840245208665\n",
      "Pure loss: 16.73598632148599.....Total loss: 16.73598632148599\n",
      "Pure loss: 15.50426526643842.....Total loss: 15.50426526643842\n",
      "epoch 3166 learning rate:  0.010315855969677826   Training loss:   16.73598632148599  Valing loss:   15.50426526643842\n",
      "Pure loss: 16.73560070375637.....Total loss: 16.73560070375637\n",
      "Pure loss: 15.5047590657577.....Total loss: 15.5047590657577\n",
      "epoch 3167 learning rate:  0.010315756236185665   Training loss:   16.73560070375637  Valing loss:   15.5047590657577\n",
      "Pure loss: 16.848850203502195.....Total loss: 16.848850203502195\n",
      "Pure loss: 15.354497151501809.....Total loss: 15.354497151501809\n",
      "epoch 3168 learning rate:  0.010315656565656566   Training loss:   16.848850203502195  Valing loss:   15.354497151501809\n",
      "Pure loss: 16.816239801408706.....Total loss: 16.816239801408706\n",
      "Pure loss: 15.349231146417207.....Total loss: 15.349231146417207\n",
      "epoch 3169 learning rate:  0.010315556958030925   Training loss:   16.816239801408706  Valing loss:   15.349231146417207\n",
      "Pure loss: 16.844575292307027.....Total loss: 16.844575292307027\n",
      "Pure loss: 15.345796469631608.....Total loss: 15.345796469631608\n",
      "epoch 3170 learning rate:  0.010315457413249211   Training loss:   16.844575292307027  Valing loss:   15.345796469631608\n",
      "Pure loss: 16.807763033826532.....Total loss: 16.807763033826532\n",
      "Pure loss: 15.353286894677577.....Total loss: 15.353286894677577\n",
      "epoch 3171 learning rate:  0.010315357931251972   Training loss:   16.807763033826532  Valing loss:   15.353286894677577\n",
      "Pure loss: 16.744942193926878.....Total loss: 16.744942193926878\n",
      "Pure loss: 15.48144787300588.....Total loss: 15.48144787300588\n",
      "epoch 3172 learning rate:  0.010315258511979824   Training loss:   16.744942193926878  Valing loss:   15.48144787300588\n",
      "Pure loss: 16.7787712682709.....Total loss: 16.7787712682709\n",
      "Pure loss: 15.925352931189868.....Total loss: 15.925352931189868\n",
      "epoch 3173 learning rate:  0.010315159155373463   Training loss:   16.7787712682709  Valing loss:   15.925352931189868\n",
      "Pure loss: 16.776932420774834.....Total loss: 16.776932420774834\n",
      "Pure loss: 15.886988508444341.....Total loss: 15.886988508444341\n",
      "epoch 3174 learning rate:  0.010315059861373661   Training loss:   16.776932420774834  Valing loss:   15.886988508444341\n",
      "Pure loss: 16.790527336809546.....Total loss: 16.790527336809546\n",
      "Pure loss: 15.962139640691563.....Total loss: 15.962139640691563\n",
      "epoch 3175 learning rate:  0.01031496062992126   Training loss:   16.790527336809546  Valing loss:   15.962139640691563\n",
      "Pure loss: 16.858195992029103.....Total loss: 16.858195992029103\n",
      "Pure loss: 16.17022905441928.....Total loss: 16.17022905441928\n",
      "epoch 3176 learning rate:  0.010314861460957178   Training loss:   16.858195992029103  Valing loss:   16.17022905441928\n",
      "Pure loss: 16.856280357829405.....Total loss: 16.856280357829405\n",
      "Pure loss: 16.164893806854746.....Total loss: 16.164893806854746\n",
      "epoch 3177 learning rate:  0.010314762354422411   Training loss:   16.856280357829405  Valing loss:   16.164893806854746\n",
      "Pure loss: 16.822308522309072.....Total loss: 16.822308522309072\n",
      "Pure loss: 16.04771628049838.....Total loss: 16.04771628049838\n",
      "epoch 3178 learning rate:  0.010314663310258024   Training loss:   16.822308522309072  Valing loss:   16.04771628049838\n",
      "Pure loss: 16.752414680690045.....Total loss: 16.752414680690045\n",
      "Pure loss: 15.724231265144075.....Total loss: 15.724231265144075\n",
      "epoch 3179 learning rate:  0.010314564328405159   Training loss:   16.752414680690045  Valing loss:   15.724231265144075\n",
      "Pure loss: 16.761498123297454.....Total loss: 16.761498123297454\n",
      "Pure loss: 15.693496116884447.....Total loss: 15.693496116884447\n",
      "epoch 3180 learning rate:  0.010314465408805032   Training loss:   16.761498123297454  Valing loss:   15.693496116884447\n",
      "Pure loss: 16.81086136292702.....Total loss: 16.81086136292702\n",
      "Pure loss: 15.489427621355677.....Total loss: 15.489427621355677\n",
      "epoch 3181 learning rate:  0.010314366551398932   Training loss:   16.81086136292702  Valing loss:   15.489427621355677\n",
      "Pure loss: 16.79535165812923.....Total loss: 16.79535165812923\n",
      "Pure loss: 15.498401959107971.....Total loss: 15.498401959107971\n",
      "epoch 3182 learning rate:  0.010314267756128221   Training loss:   16.79535165812923  Valing loss:   15.498401959107971\n",
      "Pure loss: 16.768861856332514.....Total loss: 16.768861856332514\n",
      "Pure loss: 15.513540541717235.....Total loss: 15.513540541717235\n",
      "epoch 3183 learning rate:  0.010314169022934338   Training loss:   16.768861856332514  Valing loss:   15.513540541717235\n",
      "Pure loss: 16.73410874134169.....Total loss: 16.73410874134169\n",
      "Pure loss: 15.568095269052428.....Total loss: 15.568095269052428\n",
      "epoch 3184 learning rate:  0.010314070351758794   Training loss:   16.73410874134169  Valing loss:   15.568095269052428\n",
      "Pure loss: 16.785125715612047.....Total loss: 16.785125715612047\n",
      "Pure loss: 15.299559398896871.....Total loss: 15.299559398896871\n",
      "epoch 3185 learning rate:  0.010313971742543171   Training loss:   16.785125715612047  Valing loss:   15.299559398896871\n",
      "Pure loss: 17.043129762237278.....Total loss: 17.043129762237278\n",
      "Pure loss: 15.24068060019424.....Total loss: 15.24068060019424\n",
      "epoch 3186 learning rate:  0.010313873195229127   Training loss:   17.043129762237278  Valing loss:   15.24068060019424\n",
      "Pure loss: 17.311659085962884.....Total loss: 17.311659085962884\n",
      "Pure loss: 15.400555170396027.....Total loss: 15.400555170396027\n",
      "epoch 3187 learning rate:  0.010313774709758394   Training loss:   17.311659085962884  Valing loss:   15.400555170396027\n",
      "Pure loss: 17.300819535792233.....Total loss: 17.300819535792233\n",
      "Pure loss: 15.393169191506379.....Total loss: 15.393169191506379\n",
      "epoch 3188 learning rate:  0.010313676286072774   Training loss:   17.300819535792233  Valing loss:   15.393169191506379\n",
      "Pure loss: 17.115461335825152.....Total loss: 17.115461335825152\n",
      "Pure loss: 15.359671637280119.....Total loss: 15.359671637280119\n",
      "epoch 3189 learning rate:  0.010313577924114142   Training loss:   17.115461335825152  Valing loss:   15.359671637280119\n",
      "Pure loss: 17.016802736267778.....Total loss: 17.016802736267778\n",
      "Pure loss: 15.333554960219933.....Total loss: 15.333554960219933\n",
      "epoch 3190 learning rate:  0.01031347962382445   Training loss:   17.016802736267778  Valing loss:   15.333554960219933\n",
      "Pure loss: 16.847936873113735.....Total loss: 16.847936873113735\n",
      "Pure loss: 15.449666391115404.....Total loss: 15.449666391115404\n",
      "epoch 3191 learning rate:  0.010313381385145723   Training loss:   16.847936873113735  Valing loss:   15.449666391115404\n",
      "Pure loss: 17.002268555129515.....Total loss: 17.002268555129515\n",
      "Pure loss: 15.439651776940178.....Total loss: 15.439651776940178\n",
      "epoch 3192 learning rate:  0.01031328320802005   Training loss:   17.002268555129515  Valing loss:   15.439651776940178\n",
      "Pure loss: 17.08415912150637.....Total loss: 17.08415912150637\n",
      "Pure loss: 15.487671992882065.....Total loss: 15.487671992882065\n",
      "epoch 3193 learning rate:  0.010313185092389602   Training loss:   17.08415912150637  Valing loss:   15.487671992882065\n",
      "Pure loss: 16.985582316903443.....Total loss: 16.985582316903443\n",
      "Pure loss: 15.432832743563642.....Total loss: 15.432832743563642\n",
      "epoch 3194 learning rate:  0.010313087038196618   Training loss:   16.985582316903443  Valing loss:   15.432832743563642\n",
      "Pure loss: 17.021630237976755.....Total loss: 17.021630237976755\n",
      "Pure loss: 15.424149015455923.....Total loss: 15.424149015455923\n",
      "epoch 3195 learning rate:  0.010312989045383412   Training loss:   17.021630237976755  Valing loss:   15.424149015455923\n",
      "Pure loss: 17.010341873042552.....Total loss: 17.010341873042552\n",
      "Pure loss: 15.428637306431646.....Total loss: 15.428637306431646\n",
      "epoch 3196 learning rate:  0.010312891113892365   Training loss:   17.010341873042552  Valing loss:   15.428637306431646\n",
      "Pure loss: 17.03862075610857.....Total loss: 17.03862075610857\n",
      "Pure loss: 15.438465535906332.....Total loss: 15.438465535906332\n",
      "epoch 3197 learning rate:  0.010312793243665937   Training loss:   17.03862075610857  Valing loss:   15.438465535906332\n",
      "Pure loss: 17.00604065481648.....Total loss: 17.00604065481648\n",
      "Pure loss: 15.444610579242413.....Total loss: 15.444610579242413\n",
      "epoch 3198 learning rate:  0.010312695434646654   Training loss:   17.00604065481648  Valing loss:   15.444610579242413\n",
      "Pure loss: 16.889831458169407.....Total loss: 16.889831458169407\n",
      "Pure loss: 15.422739249023119.....Total loss: 15.422739249023119\n",
      "epoch 3199 learning rate:  0.010312597686777118   Training loss:   16.889831458169407  Valing loss:   15.422739249023119\n",
      "Pure loss: 17.24991363532864.....Total loss: 17.24991363532864\n",
      "Pure loss: 15.438730536579902.....Total loss: 15.438730536579902\n",
      "epoch 3200 learning rate:  0.0103125   Training loss:   17.24991363532864  Valing loss:   15.438730536579902\n",
      "Pure loss: 16.95253999500805.....Total loss: 16.95253999500805\n",
      "Pure loss: 15.315567075856164.....Total loss: 15.315567075856164\n",
      "epoch 3201 learning rate:  0.010312402374258045   Training loss:   16.95253999500805  Valing loss:   15.315567075856164\n",
      "Pure loss: 17.26865712902185.....Total loss: 17.26865712902185\n",
      "Pure loss: 15.278903818946167.....Total loss: 15.278903818946167\n",
      "epoch 3202 learning rate:  0.010312304809494067   Training loss:   17.26865712902185  Valing loss:   15.278903818946167\n",
      "Pure loss: 17.205617369254142.....Total loss: 17.205617369254142\n",
      "Pure loss: 15.265905616132837.....Total loss: 15.265905616132837\n",
      "epoch 3203 learning rate:  0.010312207305650953   Training loss:   17.205617369254142  Valing loss:   15.265905616132837\n",
      "Pure loss: 17.179962100840935.....Total loss: 17.179962100840935\n",
      "Pure loss: 15.244817785685106.....Total loss: 15.244817785685106\n",
      "epoch 3204 learning rate:  0.010312109862671661   Training loss:   17.179962100840935  Valing loss:   15.244817785685106\n",
      "Pure loss: 17.1231423297022.....Total loss: 17.1231423297022\n",
      "Pure loss: 15.218604954396362.....Total loss: 15.218604954396362\n",
      "epoch 3205 learning rate:  0.010312012480499221   Training loss:   17.1231423297022  Valing loss:   15.218604954396362\n",
      "Pure loss: 17.060508389067767.....Total loss: 17.060508389067767\n",
      "Pure loss: 15.214576962953366.....Total loss: 15.214576962953366\n",
      "epoch 3206 learning rate:  0.01031191515907673   Training loss:   17.060508389067767  Valing loss:   15.214576962953366\n",
      "Pure loss: 17.300205074235574.....Total loss: 17.300205074235574\n",
      "Pure loss: 15.180888109990601.....Total loss: 15.180888109990601\n",
      "epoch 3207 learning rate:  0.010311817898347366   Training loss:   17.300205074235574  Valing loss:   15.180888109990601\n",
      "Pure loss: 17.137345535736593.....Total loss: 17.137345535736593\n",
      "Pure loss: 15.162465913971367.....Total loss: 15.162465913971367\n",
      "epoch 3208 learning rate:  0.010311720698254364   Training loss:   17.137345535736593  Valing loss:   15.162465913971367\n",
      "Pure loss: 17.128817552892468.....Total loss: 17.128817552892468\n",
      "Pure loss: 15.161909235607533.....Total loss: 15.161909235607533\n",
      "epoch 3209 learning rate:  0.01031162355874104   Training loss:   17.128817552892468  Valing loss:   15.161909235607533\n",
      "Pure loss: 16.888276227650632.....Total loss: 16.888276227650632\n",
      "Pure loss: 15.19512323296998.....Total loss: 15.19512323296998\n",
      "epoch 3210 learning rate:  0.010311526479750779   Training loss:   16.888276227650632  Valing loss:   15.19512323296998\n",
      "Pure loss: 17.208325599814344.....Total loss: 17.208325599814344\n",
      "Pure loss: 15.162250910326346.....Total loss: 15.162250910326346\n",
      "epoch 3211 learning rate:  0.010311429461227033   Training loss:   17.208325599814344  Valing loss:   15.162250910326346\n",
      "Pure loss: 17.208954315673445.....Total loss: 17.208954315673445\n",
      "Pure loss: 15.162479688249725.....Total loss: 15.162479688249725\n",
      "epoch 3212 learning rate:  0.010311332503113325   Training loss:   17.208954315673445  Valing loss:   15.162479688249725\n",
      "Pure loss: 17.281903439293195.....Total loss: 17.281903439293195\n",
      "Pure loss: 15.88591122642246.....Total loss: 15.88591122642246\n",
      "epoch 3213 learning rate:  0.010311235605353252   Training loss:   17.281903439293195  Valing loss:   15.88591122642246\n",
      "Pure loss: 17.279706910075905.....Total loss: 17.279706910075905\n",
      "Pure loss: 15.861467324902257.....Total loss: 15.861467324902257\n",
      "epoch 3214 learning rate:  0.01031113876789048   Training loss:   17.279706910075905  Valing loss:   15.861467324902257\n",
      "Pure loss: 17.24835653159408.....Total loss: 17.24835653159408\n",
      "Pure loss: 15.815949636506724.....Total loss: 15.815949636506724\n",
      "epoch 3215 learning rate:  0.01031104199066874   Training loss:   17.24835653159408  Valing loss:   15.815949636506724\n",
      "Pure loss: 17.349292456240633.....Total loss: 17.349292456240633\n",
      "Pure loss: 16.055365488017916.....Total loss: 16.055365488017916\n",
      "epoch 3216 learning rate:  0.010310945273631841   Training loss:   17.349292456240633  Valing loss:   16.055365488017916\n",
      "Pure loss: 17.73469624019807.....Total loss: 17.73469624019807\n",
      "Pure loss: 16.738092829152567.....Total loss: 16.738092829152567\n",
      "epoch 3217 learning rate:  0.010310848616723656   Training loss:   17.73469624019807  Valing loss:   16.738092829152567\n",
      "Pure loss: 17.530075759365896.....Total loss: 17.530075759365896\n",
      "Pure loss: 16.433767663802854.....Total loss: 16.433767663802854\n",
      "epoch 3218 learning rate:  0.010310752019888129   Training loss:   17.530075759365896  Valing loss:   16.433767663802854\n",
      "Pure loss: 17.54001157930683.....Total loss: 17.54001157930683\n",
      "Pure loss: 16.463048420298858.....Total loss: 16.463048420298858\n",
      "epoch 3219 learning rate:  0.010310655483069276   Training loss:   17.54001157930683  Valing loss:   16.463048420298858\n",
      "Pure loss: 17.326389215384356.....Total loss: 17.326389215384356\n",
      "Pure loss: 16.105492315648434.....Total loss: 16.105492315648434\n",
      "epoch 3220 learning rate:  0.01031055900621118   Training loss:   17.326389215384356  Valing loss:   16.105492315648434\n",
      "Pure loss: 17.19640615899119.....Total loss: 17.19640615899119\n",
      "Pure loss: 15.693699937960902.....Total loss: 15.693699937960902\n",
      "epoch 3221 learning rate:  0.010310462589257994   Training loss:   17.19640615899119  Valing loss:   15.693699937960902\n",
      "Pure loss: 17.22242077310665.....Total loss: 17.22242077310665\n",
      "Pure loss: 15.796188653239227.....Total loss: 15.796188653239227\n",
      "epoch 3222 learning rate:  0.010310366232153942   Training loss:   17.22242077310665  Valing loss:   15.796188653239227\n",
      "Pure loss: 17.284834870350938.....Total loss: 17.284834870350938\n",
      "Pure loss: 16.05327677423823.....Total loss: 16.05327677423823\n",
      "epoch 3223 learning rate:  0.010310269934843314   Training loss:   17.284834870350938  Valing loss:   16.05327677423823\n",
      "Pure loss: 17.165971929336973.....Total loss: 17.165971929336973\n",
      "Pure loss: 15.849770399648184.....Total loss: 15.849770399648184\n",
      "epoch 3224 learning rate:  0.010310173697270471   Training loss:   17.165971929336973  Valing loss:   15.849770399648184\n",
      "Pure loss: 17.411289464428084.....Total loss: 17.411289464428084\n",
      "Pure loss: 16.325252346877082.....Total loss: 16.325252346877082\n",
      "epoch 3225 learning rate:  0.010310077519379846   Training loss:   17.411289464428084  Valing loss:   16.325252346877082\n",
      "Pure loss: 18.05805020384663.....Total loss: 18.05805020384663\n",
      "Pure loss: 17.63208877625448.....Total loss: 17.63208877625448\n",
      "epoch 3226 learning rate:  0.010309981401115933   Training loss:   18.05805020384663  Valing loss:   17.63208877625448\n",
      "Pure loss: 17.378191904053804.....Total loss: 17.378191904053804\n",
      "Pure loss: 16.532977523114614.....Total loss: 16.532977523114614\n",
      "epoch 3227 learning rate:  0.010309885342423303   Training loss:   17.378191904053804  Valing loss:   16.532977523114614\n",
      "Pure loss: 17.365279871253474.....Total loss: 17.365279871253474\n",
      "Pure loss: 16.517172153850343.....Total loss: 16.517172153850343\n",
      "epoch 3228 learning rate:  0.010309789343246592   Training loss:   17.365279871253474  Valing loss:   16.517172153850343\n",
      "Pure loss: 17.048586284380907.....Total loss: 17.048586284380907\n",
      "Pure loss: 15.84358551253299.....Total loss: 15.84358551253299\n",
      "epoch 3229 learning rate:  0.010309693403530506   Training loss:   17.048586284380907  Valing loss:   15.84358551253299\n",
      "Pure loss: 17.206218234537175.....Total loss: 17.206218234537175\n",
      "Pure loss: 16.094732694118946.....Total loss: 16.094732694118946\n",
      "epoch 3230 learning rate:  0.010309597523219815   Training loss:   17.206218234537175  Valing loss:   16.094732694118946\n",
      "Pure loss: 17.066634011220405.....Total loss: 17.066634011220405\n",
      "Pure loss: 15.869862519316015.....Total loss: 15.869862519316015\n",
      "epoch 3231 learning rate:  0.010309501702259362   Training loss:   17.066634011220405  Valing loss:   15.869862519316015\n",
      "Pure loss: 16.970081021755522.....Total loss: 16.970081021755522\n",
      "Pure loss: 15.41921319272607.....Total loss: 15.41921319272607\n",
      "epoch 3232 learning rate:  0.01030940594059406   Training loss:   16.970081021755522  Valing loss:   15.41921319272607\n",
      "Pure loss: 17.10978930228538.....Total loss: 17.10978930228538\n",
      "Pure loss: 15.666653714935267.....Total loss: 15.666653714935267\n",
      "epoch 3233 learning rate:  0.010309310238168884   Training loss:   17.10978930228538  Valing loss:   15.666653714935267\n",
      "Pure loss: 17.125531099667757.....Total loss: 17.125531099667757\n",
      "Pure loss: 15.744675915841379.....Total loss: 15.744675915841379\n",
      "epoch 3234 learning rate:  0.010309214594928881   Training loss:   17.125531099667757  Valing loss:   15.744675915841379\n",
      "Pure loss: 17.599898696000896.....Total loss: 17.599898696000896\n",
      "Pure loss: 16.213516190132985.....Total loss: 16.213516190132985\n",
      "epoch 3235 learning rate:  0.010309119010819165   Training loss:   17.599898696000896  Valing loss:   16.213516190132985\n",
      "Pure loss: 17.120581405361477.....Total loss: 17.120581405361477\n",
      "Pure loss: 15.656275245797769.....Total loss: 15.656275245797769\n",
      "epoch 3236 learning rate:  0.01030902348578492   Training loss:   17.120581405361477  Valing loss:   15.656275245797769\n",
      "Pure loss: 17.112291963392224.....Total loss: 17.112291963392224\n",
      "Pure loss: 15.641092401258854.....Total loss: 15.641092401258854\n",
      "epoch 3237 learning rate:  0.010308928019771393   Training loss:   17.112291963392224  Valing loss:   15.641092401258854\n",
      "Pure loss: 17.125540825367608.....Total loss: 17.125540825367608\n",
      "Pure loss: 15.657250812065978.....Total loss: 15.657250812065978\n",
      "epoch 3238 learning rate:  0.010308832612723903   Training loss:   17.125540825367608  Valing loss:   15.657250812065978\n",
      "Pure loss: 17.289476738915678.....Total loss: 17.289476738915678\n",
      "Pure loss: 16.302854384789566.....Total loss: 16.302854384789566\n",
      "epoch 3239 learning rate:  0.010308737264587836   Training loss:   17.289476738915678  Valing loss:   16.302854384789566\n",
      "Pure loss: 17.40398833267188.....Total loss: 17.40398833267188\n",
      "Pure loss: 16.509118756191874.....Total loss: 16.509118756191874\n",
      "epoch 3240 learning rate:  0.010308641975308641   Training loss:   17.40398833267188  Valing loss:   16.509118756191874\n",
      "Pure loss: 17.80677306783526.....Total loss: 17.80677306783526\n",
      "Pure loss: 17.24991015880562.....Total loss: 17.24991015880562\n",
      "epoch 3241 learning rate:  0.010308546744831842   Training loss:   17.80677306783526  Valing loss:   17.24991015880562\n",
      "Pure loss: 18.31255568896611.....Total loss: 18.31255568896611\n",
      "Pure loss: 18.065498029266408.....Total loss: 18.065498029266408\n",
      "epoch 3242 learning rate:  0.010308451573103023   Training loss:   18.31255568896611  Valing loss:   18.065498029266408\n",
      "Pure loss: 18.564284586525737.....Total loss: 18.564284586525737\n",
      "Pure loss: 18.39562110978455.....Total loss: 18.39562110978455\n",
      "epoch 3243 learning rate:  0.01030835646006784   Training loss:   18.564284586525737  Valing loss:   18.39562110978455\n",
      "Pure loss: 18.46620019319665.....Total loss: 18.46620019319665\n",
      "Pure loss: 18.233132213119873.....Total loss: 18.233132213119873\n",
      "epoch 3244 learning rate:  0.01030826140567201   Training loss:   18.46620019319665  Valing loss:   18.233132213119873\n",
      "Pure loss: 18.463170561694835.....Total loss: 18.463170561694835\n",
      "Pure loss: 18.229011557194408.....Total loss: 18.229011557194408\n",
      "epoch 3245 learning rate:  0.010308166409861326   Training loss:   18.463170561694835  Valing loss:   18.229011557194408\n",
      "Pure loss: 21.170133306941988.....Total loss: 21.170133306941988\n",
      "Pure loss: 21.642663968946554.....Total loss: 21.642663968946554\n",
      "epoch 3246 learning rate:  0.010308071472581639   Training loss:   21.170133306941988  Valing loss:   21.642663968946554\n",
      "Pure loss: 21.329215655235153.....Total loss: 21.329215655235153\n",
      "Pure loss: 21.888127254003695.....Total loss: 21.888127254003695\n",
      "epoch 3247 learning rate:  0.010307976593778873   Training loss:   21.329215655235153  Valing loss:   21.888127254003695\n",
      "Pure loss: 19.736523254867812.....Total loss: 19.736523254867812\n",
      "Pure loss: 19.772389084972453.....Total loss: 19.772389084972453\n",
      "epoch 3248 learning rate:  0.010307881773399015   Training loss:   19.736523254867812  Valing loss:   19.772389084972453\n",
      "Pure loss: 19.968118474514288.....Total loss: 19.968118474514288\n",
      "Pure loss: 20.10911994100965.....Total loss: 20.10911994100965\n",
      "epoch 3249 learning rate:  0.01030778701138812   Training loss:   19.968118474514288  Valing loss:   20.10911994100965\n",
      "Pure loss: 20.07107542841838.....Total loss: 20.07107542841838\n",
      "Pure loss: 20.253717775737858.....Total loss: 20.253717775737858\n",
      "epoch 3250 learning rate:  0.010307692307692308   Training loss:   20.07107542841838  Valing loss:   20.253717775737858\n",
      "Pure loss: 19.501501554785467.....Total loss: 19.501501554785467\n",
      "Pure loss: 19.49874879655494.....Total loss: 19.49874879655494\n",
      "epoch 3251 learning rate:  0.010307597662257768   Training loss:   19.501501554785467  Valing loss:   19.49874879655494\n",
      "Pure loss: 18.12300146897001.....Total loss: 18.12300146897001\n",
      "Pure loss: 17.739207961279245.....Total loss: 17.739207961279245\n",
      "epoch 3252 learning rate:  0.01030750307503075   Training loss:   18.12300146897001  Valing loss:   17.739207961279245\n",
      "Pure loss: 18.282471934099576.....Total loss: 18.282471934099576\n",
      "Pure loss: 18.05332978114131.....Total loss: 18.05332978114131\n",
      "epoch 3253 learning rate:  0.010307408545957578   Training loss:   18.282471934099576  Valing loss:   18.05332978114131\n",
      "Pure loss: 18.159752234598415.....Total loss: 18.159752234598415\n",
      "Pure loss: 17.874223462182933.....Total loss: 17.874223462182933\n",
      "epoch 3254 learning rate:  0.010307314074984635   Training loss:   18.159752234598415  Valing loss:   17.874223462182933\n",
      "Pure loss: 18.77780462057559.....Total loss: 18.77780462057559\n",
      "Pure loss: 18.7889266757564.....Total loss: 18.7889266757564\n",
      "epoch 3255 learning rate:  0.010307219662058373   Training loss:   18.77780462057559  Valing loss:   18.7889266757564\n",
      "Pure loss: 18.210228653614617.....Total loss: 18.210228653614617\n",
      "Pure loss: 18.005767677251335.....Total loss: 18.005767677251335\n",
      "epoch 3256 learning rate:  0.010307125307125308   Training loss:   18.210228653614617  Valing loss:   18.005767677251335\n",
      "Pure loss: 18.84596918483828.....Total loss: 18.84596918483828\n",
      "Pure loss: 18.92895028837198.....Total loss: 18.92895028837198\n",
      "epoch 3257 learning rate:  0.010307031010132023   Training loss:   18.84596918483828  Valing loss:   18.92895028837198\n",
      "Pure loss: 18.344115119873027.....Total loss: 18.344115119873027\n",
      "Pure loss: 18.188274966930837.....Total loss: 18.188274966930837\n",
      "epoch 3258 learning rate:  0.010306936771025168   Training loss:   18.344115119873027  Valing loss:   18.188274966930837\n",
      "Pure loss: 18.02143502768586.....Total loss: 18.02143502768586\n",
      "Pure loss: 17.736487905271883.....Total loss: 17.736487905271883\n",
      "epoch 3259 learning rate:  0.010306842589751458   Training loss:   18.02143502768586  Valing loss:   17.736487905271883\n",
      "Pure loss: 17.821647325775277.....Total loss: 17.821647325775277\n",
      "Pure loss: 17.41179355351771.....Total loss: 17.41179355351771\n",
      "epoch 3260 learning rate:  0.010306748466257669   Training loss:   17.821647325775277  Valing loss:   17.41179355351771\n",
      "Pure loss: 17.235156912843117.....Total loss: 17.235156912843117\n",
      "Pure loss: 16.3283103710699.....Total loss: 16.3283103710699\n",
      "epoch 3261 learning rate:  0.010306654400490648   Training loss:   17.235156912843117  Valing loss:   16.3283103710699\n",
      "Pure loss: 17.091146188640554.....Total loss: 17.091146188640554\n",
      "Pure loss: 16.015949475468144.....Total loss: 16.015949475468144\n",
      "epoch 3262 learning rate:  0.010306560392397303   Training loss:   17.091146188640554  Valing loss:   16.015949475468144\n",
      "Pure loss: 17.190165755264868.....Total loss: 17.190165755264868\n",
      "Pure loss: 16.191197985710428.....Total loss: 16.191197985710428\n",
      "epoch 3263 learning rate:  0.01030646644192461   Training loss:   17.190165755264868  Valing loss:   16.191197985710428\n",
      "Pure loss: 16.89093072931984.....Total loss: 16.89093072931984\n",
      "Pure loss: 15.741545234141963.....Total loss: 15.741545234141963\n",
      "epoch 3264 learning rate:  0.010306372549019609   Training loss:   16.89093072931984  Valing loss:   15.741545234141963\n",
      "Pure loss: 17.224742782120632.....Total loss: 17.224742782120632\n",
      "Pure loss: 16.347625781042353.....Total loss: 16.347625781042353\n",
      "epoch 3265 learning rate:  0.010306278713629402   Training loss:   17.224742782120632  Valing loss:   16.347625781042353\n",
      "Pure loss: 18.02162546279979.....Total loss: 18.02162546279979\n",
      "Pure loss: 17.663255916855153.....Total loss: 17.663255916855153\n",
      "epoch 3266 learning rate:  0.010306184935701164   Training loss:   18.02162546279979  Valing loss:   17.663255916855153\n",
      "Pure loss: 18.604521175766436.....Total loss: 18.604521175766436\n",
      "Pure loss: 18.599732964968467.....Total loss: 18.599732964968467\n",
      "epoch 3267 learning rate:  0.010306091215182124   Training loss:   18.604521175766436  Valing loss:   18.599732964968467\n",
      "Pure loss: 18.990103715347374.....Total loss: 18.990103715347374\n",
      "Pure loss: 19.17230817323958.....Total loss: 19.17230817323958\n",
      "epoch 3268 learning rate:  0.010305997552019584   Training loss:   18.990103715347374  Valing loss:   19.17230817323958\n",
      "Pure loss: 18.290457348515044.....Total loss: 18.290457348515044\n",
      "Pure loss: 18.28523266943039.....Total loss: 18.28523266943039\n",
      "epoch 3269 learning rate:  0.010305903946160907   Training loss:   18.290457348515044  Valing loss:   18.28523266943039\n",
      "Pure loss: 18.46027161150374.....Total loss: 18.46027161150374\n",
      "Pure loss: 18.548827054665043.....Total loss: 18.548827054665043\n",
      "epoch 3270 learning rate:  0.010305810397553516   Training loss:   18.46027161150374  Valing loss:   18.548827054665043\n",
      "Pure loss: 18.178340668448836.....Total loss: 18.178340668448836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 18.16935642423407.....Total loss: 18.16935642423407\n",
      "epoch 3271 learning rate:  0.01030571690614491   Training loss:   18.178340668448836  Valing loss:   18.16935642423407\n",
      "Pure loss: 17.64304412733503.....Total loss: 17.64304412733503\n",
      "Pure loss: 17.419898693336954.....Total loss: 17.419898693336954\n",
      "epoch 3272 learning rate:  0.01030562347188264   Training loss:   17.64304412733503  Valing loss:   17.419898693336954\n",
      "Pure loss: 17.116972292804927.....Total loss: 17.116972292804927\n",
      "Pure loss: 16.529681287924255.....Total loss: 16.529681287924255\n",
      "epoch 3273 learning rate:  0.01030553009471433   Training loss:   17.116972292804927  Valing loss:   16.529681287924255\n",
      "Pure loss: 17.004894894285865.....Total loss: 17.004894894285865\n",
      "Pure loss: 16.330003073609248.....Total loss: 16.330003073609248\n",
      "epoch 3274 learning rate:  0.01030543677458766   Training loss:   17.004894894285865  Valing loss:   16.330003073609248\n",
      "Pure loss: 16.919191430131054.....Total loss: 16.919191430131054\n",
      "Pure loss: 16.178530840605227.....Total loss: 16.178530840605227\n",
      "epoch 3275 learning rate:  0.010305343511450382   Training loss:   16.919191430131054  Valing loss:   16.178530840605227\n",
      "Pure loss: 16.6201447354851.....Total loss: 16.6201447354851\n",
      "Pure loss: 15.797331242512646.....Total loss: 15.797331242512646\n",
      "epoch 3276 learning rate:  0.010305250305250306   Training loss:   16.6201447354851  Valing loss:   15.797331242512646\n",
      "Pure loss: 16.471399504187094.....Total loss: 16.471399504187094\n",
      "Pure loss: 15.518600645552503.....Total loss: 15.518600645552503\n",
      "epoch 3277 learning rate:  0.010305157155935307   Training loss:   16.471399504187094  Valing loss:   15.518600645552503\n",
      "Pure loss: 16.55392539480791.....Total loss: 16.55392539480791\n",
      "Pure loss: 15.698133710633364.....Total loss: 15.698133710633364\n",
      "epoch 3278 learning rate:  0.010305064063453325   Training loss:   16.55392539480791  Valing loss:   15.698133710633364\n",
      "Pure loss: 16.361360561363508.....Total loss: 16.361360561363508\n",
      "Pure loss: 15.148842533700146.....Total loss: 15.148842533700146\n",
      "epoch 3279 learning rate:  0.010304971027752364   Training loss:   16.361360561363508  Valing loss:   15.148842533700146\n",
      "Pure loss: 16.39895249858716.....Total loss: 16.39895249858716\n",
      "Pure loss: 14.696357274288024.....Total loss: 14.696357274288024\n",
      "epoch 3280 learning rate:  0.010304878048780488   Training loss:   16.39895249858716  Valing loss:   14.696357274288024\n",
      "Pure loss: 16.396110951704944.....Total loss: 16.396110951704944\n",
      "Pure loss: 15.154992474282196.....Total loss: 15.154992474282196\n",
      "epoch 3281 learning rate:  0.010304785126485827   Training loss:   16.396110951704944  Valing loss:   15.154992474282196\n",
      "Pure loss: 16.431597898179863.....Total loss: 16.431597898179863\n",
      "Pure loss: 15.270539715311513.....Total loss: 15.270539715311513\n",
      "epoch 3282 learning rate:  0.010304692260816576   Training loss:   16.431597898179863  Valing loss:   15.270539715311513\n",
      "Pure loss: 16.31444195841797.....Total loss: 16.31444195841797\n",
      "Pure loss: 14.873849120520964.....Total loss: 14.873849120520964\n",
      "epoch 3283 learning rate:  0.010304599451720986   Training loss:   16.31444195841797  Valing loss:   14.873849120520964\n",
      "Pure loss: 16.268028685956743.....Total loss: 16.268028685956743\n",
      "Pure loss: 14.802694906000088.....Total loss: 14.802694906000088\n",
      "epoch 3284 learning rate:  0.010304506699147382   Training loss:   16.268028685956743  Valing loss:   14.802694906000088\n",
      "Pure loss: 16.396150399904897.....Total loss: 16.396150399904897\n",
      "Pure loss: 15.185268668407591.....Total loss: 15.185268668407591\n",
      "epoch 3285 learning rate:  0.010304414003044141   Training loss:   16.396150399904897  Valing loss:   15.185268668407591\n",
      "Pure loss: 16.399700811327452.....Total loss: 16.399700811327452\n",
      "Pure loss: 15.19539879985061.....Total loss: 15.19539879985061\n",
      "epoch 3286 learning rate:  0.010304321363359709   Training loss:   16.399700811327452  Valing loss:   15.19539879985061\n",
      "Pure loss: 16.273107899307043.....Total loss: 16.273107899307043\n",
      "Pure loss: 14.771965925707324.....Total loss: 14.771965925707324\n",
      "epoch 3287 learning rate:  0.010304228780042592   Training loss:   16.273107899307043  Valing loss:   14.771965925707324\n",
      "Pure loss: 16.360851315119003.....Total loss: 16.360851315119003\n",
      "Pure loss: 14.443429288442783.....Total loss: 14.443429288442783\n",
      "epoch 3288 learning rate:  0.010304136253041362   Training loss:   16.360851315119003  Valing loss:   14.443429288442783\n",
      "Pure loss: 16.454700493314512.....Total loss: 16.454700493314512\n",
      "Pure loss: 14.391571327299996.....Total loss: 14.391571327299996\n",
      "epoch 3289 learning rate:  0.010304043782304652   Training loss:   16.454700493314512  Valing loss:   14.391571327299996\n",
      "Pure loss: 16.35751117915803.....Total loss: 16.35751117915803\n",
      "Pure loss: 14.50039248476664.....Total loss: 14.50039248476664\n",
      "epoch 3290 learning rate:  0.010303951367781155   Training loss:   16.35751117915803  Valing loss:   14.50039248476664\n",
      "Pure loss: 16.478593366813723.....Total loss: 16.478593366813723\n",
      "Pure loss: 14.356431732375794.....Total loss: 14.356431732375794\n",
      "epoch 3291 learning rate:  0.01030385900941963   Training loss:   16.478593366813723  Valing loss:   14.356431732375794\n",
      "Pure loss: 16.457739303727145.....Total loss: 16.457739303727145\n",
      "Pure loss: 14.297742233815223.....Total loss: 14.297742233815223\n",
      "epoch 3292 learning rate:  0.010303766707168895   Training loss:   16.457739303727145  Valing loss:   14.297742233815223\n",
      "Pure loss: 16.434133800812738.....Total loss: 16.434133800812738\n",
      "Pure loss: 14.304913327237108.....Total loss: 14.304913327237108\n",
      "epoch 3293 learning rate:  0.010303674460977833   Training loss:   16.434133800812738  Valing loss:   14.304913327237108\n",
      "Pure loss: 16.458069405369326.....Total loss: 16.458069405369326\n",
      "Pure loss: 14.302313403916012.....Total loss: 14.302313403916012\n",
      "epoch 3294 learning rate:  0.010303582270795386   Training loss:   16.458069405369326  Valing loss:   14.302313403916012\n",
      "Pure loss: 16.35370621368542.....Total loss: 16.35370621368542\n",
      "Pure loss: 14.35028249655675.....Total loss: 14.35028249655675\n",
      "epoch 3295 learning rate:  0.010303490136570562   Training loss:   16.35370621368542  Valing loss:   14.35028249655675\n",
      "Pure loss: 16.391271051108244.....Total loss: 16.391271051108244\n",
      "Pure loss: 14.31348173750014.....Total loss: 14.31348173750014\n",
      "epoch 3296 learning rate:  0.010303398058252427   Training loss:   16.391271051108244  Valing loss:   14.31348173750014\n",
      "Pure loss: 16.396419177431337.....Total loss: 16.396419177431337\n",
      "Pure loss: 14.289169496563138.....Total loss: 14.289169496563138\n",
      "epoch 3297 learning rate:  0.010303306035790112   Training loss:   16.396419177431337  Valing loss:   14.289169496563138\n",
      "Pure loss: 16.700696255500407.....Total loss: 16.700696255500407\n",
      "Pure loss: 14.261849931357059.....Total loss: 14.261849931357059\n",
      "epoch 3298 learning rate:  0.010303214069132809   Training loss:   16.700696255500407  Valing loss:   14.261849931357059\n",
      "Pure loss: 16.601181738283938.....Total loss: 16.601181738283938\n",
      "Pure loss: 14.274773557349627.....Total loss: 14.274773557349627\n",
      "epoch 3299 learning rate:  0.010303122158229767   Training loss:   16.601181738283938  Valing loss:   14.274773557349627\n",
      "Pure loss: 16.58947596829108.....Total loss: 16.58947596829108\n",
      "Pure loss: 14.301607300858457.....Total loss: 14.301607300858457\n",
      "epoch 3300 learning rate:  0.010303030303030303   Training loss:   16.58947596829108  Valing loss:   14.301607300858457\n",
      "Pure loss: 16.63544482189011.....Total loss: 16.63544482189011\n",
      "Pure loss: 14.398336135413956.....Total loss: 14.398336135413956\n",
      "epoch 3301 learning rate:  0.010302938503483792   Training loss:   16.63544482189011  Valing loss:   14.398336135413956\n",
      "Pure loss: 16.647716696874873.....Total loss: 16.647716696874873\n",
      "Pure loss: 14.39011625302673.....Total loss: 14.39011625302673\n",
      "epoch 3302 learning rate:  0.010302846759539673   Training loss:   16.647716696874873  Valing loss:   14.39011625302673\n",
      "Pure loss: 16.658194663399968.....Total loss: 16.658194663399968\n",
      "Pure loss: 14.411550765864302.....Total loss: 14.411550765864302\n",
      "epoch 3303 learning rate:  0.010302755071147442   Training loss:   16.658194663399968  Valing loss:   14.411550765864302\n",
      "Pure loss: 16.657193512778218.....Total loss: 16.657193512778218\n",
      "Pure loss: 14.372478845873262.....Total loss: 14.372478845873262\n",
      "epoch 3304 learning rate:  0.010302663438256658   Training loss:   16.657193512778218  Valing loss:   14.372478845873262\n",
      "Pure loss: 16.65307411640821.....Total loss: 16.65307411640821\n",
      "Pure loss: 14.366652812967311.....Total loss: 14.366652812967311\n",
      "epoch 3305 learning rate:  0.010302571860816944   Training loss:   16.65307411640821  Valing loss:   14.366652812967311\n",
      "Pure loss: 16.66516767589397.....Total loss: 16.66516767589397\n",
      "Pure loss: 14.253102679948025.....Total loss: 14.253102679948025\n",
      "epoch 3306 learning rate:  0.010302480338777979   Training loss:   16.66516767589397  Valing loss:   14.253102679948025\n",
      "Pure loss: 16.56607405288312.....Total loss: 16.56607405288312\n",
      "Pure loss: 14.267433941109045.....Total loss: 14.267433941109045\n",
      "epoch 3307 learning rate:  0.010302388872089507   Training loss:   16.56607405288312  Valing loss:   14.267433941109045\n",
      "Pure loss: 16.44423362160725.....Total loss: 16.44423362160725\n",
      "Pure loss: 14.313934289959729.....Total loss: 14.313934289959729\n",
      "epoch 3308 learning rate:  0.01030229746070133   Training loss:   16.44423362160725  Valing loss:   14.313934289959729\n",
      "Pure loss: 16.35549935822501.....Total loss: 16.35549935822501\n",
      "Pure loss: 14.71815459842835.....Total loss: 14.71815459842835\n",
      "epoch 3309 learning rate:  0.010302206104563312   Training loss:   16.35549935822501  Valing loss:   14.71815459842835\n",
      "Pure loss: 16.373490113045225.....Total loss: 16.373490113045225\n",
      "Pure loss: 14.758916439222155.....Total loss: 14.758916439222155\n",
      "epoch 3310 learning rate:  0.010302114803625377   Training loss:   16.373490113045225  Valing loss:   14.758916439222155\n",
      "Pure loss: 16.290582239606742.....Total loss: 16.290582239606742\n",
      "Pure loss: 14.598655992143026.....Total loss: 14.598655992143026\n",
      "epoch 3311 learning rate:  0.010302023557837511   Training loss:   16.290582239606742  Valing loss:   14.598655992143026\n",
      "Pure loss: 16.269173098390194.....Total loss: 16.269173098390194\n",
      "Pure loss: 14.631850546272988.....Total loss: 14.631850546272988\n",
      "epoch 3312 learning rate:  0.010301932367149758   Training loss:   16.269173098390194  Valing loss:   14.631850546272988\n",
      "Pure loss: 16.451715658334603.....Total loss: 16.451715658334603\n",
      "Pure loss: 14.355330089184074.....Total loss: 14.355330089184074\n",
      "epoch 3313 learning rate:  0.010301841231512225   Training loss:   16.451715658334603  Valing loss:   14.355330089184074\n",
      "Pure loss: 16.46077644267198.....Total loss: 16.46077644267198\n",
      "Pure loss: 14.34215811408683.....Total loss: 14.34215811408683\n",
      "epoch 3314 learning rate:  0.010301750150875076   Training loss:   16.46077644267198  Valing loss:   14.34215811408683\n",
      "Pure loss: 16.34290196313805.....Total loss: 16.34290196313805\n",
      "Pure loss: 14.40839162214017.....Total loss: 14.40839162214017\n",
      "epoch 3315 learning rate:  0.010301659125188538   Training loss:   16.34290196313805  Valing loss:   14.40839162214017\n",
      "Pure loss: 16.31662752765781.....Total loss: 16.31662752765781\n",
      "Pure loss: 14.307852417101403.....Total loss: 14.307852417101403\n",
      "epoch 3316 learning rate:  0.010301568154402896   Training loss:   16.31662752765781  Valing loss:   14.307852417101403\n",
      "Pure loss: 16.33623293216178.....Total loss: 16.33623293216178\n",
      "Pure loss: 14.356586657345925.....Total loss: 14.356586657345925\n",
      "epoch 3317 learning rate:  0.010301477238468497   Training loss:   16.33623293216178  Valing loss:   14.356586657345925\n",
      "Pure loss: 16.357471173027843.....Total loss: 16.357471173027843\n",
      "Pure loss: 14.332956670469367.....Total loss: 14.332956670469367\n",
      "epoch 3318 learning rate:  0.010301386377335745   Training loss:   16.357471173027843  Valing loss:   14.332956670469367\n",
      "Pure loss: 16.365139780840938.....Total loss: 16.365139780840938\n",
      "Pure loss: 14.310672174162688.....Total loss: 14.310672174162688\n",
      "epoch 3319 learning rate:  0.010301295570955106   Training loss:   16.365139780840938  Valing loss:   14.310672174162688\n",
      "Pure loss: 16.3301450259699.....Total loss: 16.3301450259699\n",
      "Pure loss: 14.40029272374519.....Total loss: 14.40029272374519\n",
      "epoch 3320 learning rate:  0.01030120481927711   Training loss:   16.3301450259699  Valing loss:   14.40029272374519\n",
      "Pure loss: 16.29074202622174.....Total loss: 16.29074202622174\n",
      "Pure loss: 14.328068728823641.....Total loss: 14.328068728823641\n",
      "epoch 3321 learning rate:  0.010301114122252333   Training loss:   16.29074202622174  Valing loss:   14.328068728823641\n",
      "Pure loss: 16.229854355627.....Total loss: 16.229854355627\n",
      "Pure loss: 14.4723250511214.....Total loss: 14.4723250511214\n",
      "epoch 3322 learning rate:  0.010301023479831427   Training loss:   16.229854355627  Valing loss:   14.4723250511214\n",
      "Pure loss: 16.271368317462507.....Total loss: 16.271368317462507\n",
      "Pure loss: 14.253918726191436.....Total loss: 14.253918726191436\n",
      "epoch 3323 learning rate:  0.010300932891965093   Training loss:   16.271368317462507  Valing loss:   14.253918726191436\n",
      "Pure loss: 16.459818149176872.....Total loss: 16.459818149176872\n",
      "Pure loss: 14.182755377561731.....Total loss: 14.182755377561731\n",
      "epoch 3324 learning rate:  0.010300842358604092   Training loss:   16.459818149176872  Valing loss:   14.182755377561731\n",
      "Pure loss: 16.567010986625885.....Total loss: 16.567010986625885\n",
      "Pure loss: 14.164061947706692.....Total loss: 14.164061947706692\n",
      "epoch 3325 learning rate:  0.010300751879699248   Training loss:   16.567010986625885  Valing loss:   14.164061947706692\n",
      "Pure loss: 16.59233416082288.....Total loss: 16.59233416082288\n",
      "Pure loss: 14.164853681192458.....Total loss: 14.164853681192458\n",
      "epoch 3326 learning rate:  0.010300661455201443   Training loss:   16.59233416082288  Valing loss:   14.164853681192458\n",
      "Pure loss: 16.7014231929852.....Total loss: 16.7014231929852\n",
      "Pure loss: 14.185618710207397.....Total loss: 14.185618710207397\n",
      "epoch 3327 learning rate:  0.010300571085061617   Training loss:   16.7014231929852  Valing loss:   14.185618710207397\n",
      "Pure loss: 16.668897550630984.....Total loss: 16.668897550630984\n",
      "Pure loss: 14.177345524892537.....Total loss: 14.177345524892537\n",
      "epoch 3328 learning rate:  0.01030048076923077   Training loss:   16.668897550630984  Valing loss:   14.177345524892537\n",
      "Pure loss: 16.524390436770634.....Total loss: 16.524390436770634\n",
      "Pure loss: 14.177663575605466.....Total loss: 14.177663575605466\n",
      "epoch 3329 learning rate:  0.010300390507659959   Training loss:   16.524390436770634  Valing loss:   14.177663575605466\n",
      "Pure loss: 16.431139613221276.....Total loss: 16.431139613221276\n",
      "Pure loss: 14.212141611066992.....Total loss: 14.212141611066992\n",
      "epoch 3330 learning rate:  0.0103003003003003   Training loss:   16.431139613221276  Valing loss:   14.212141611066992\n",
      "Pure loss: 16.504988626537926.....Total loss: 16.504988626537926\n",
      "Pure loss: 14.163190853152875.....Total loss: 14.163190853152875\n",
      "epoch 3331 learning rate:  0.010300210147102973   Training loss:   16.504988626537926  Valing loss:   14.163190853152875\n",
      "Pure loss: 16.514716521013263.....Total loss: 16.514716521013263\n",
      "Pure loss: 14.163799522777921.....Total loss: 14.163799522777921\n",
      "epoch 3332 learning rate:  0.010300120048019208   Training loss:   16.514716521013263  Valing loss:   14.163799522777921\n",
      "Pure loss: 16.41250565374452.....Total loss: 16.41250565374452\n",
      "Pure loss: 14.168203769851615.....Total loss: 14.168203769851615\n",
      "epoch 3333 learning rate:  0.0103000300030003   Training loss:   16.41250565374452  Valing loss:   14.168203769851615\n",
      "Pure loss: 16.383141597736707.....Total loss: 16.383141597736707\n",
      "Pure loss: 14.177644042701274.....Total loss: 14.177644042701274\n",
      "epoch 3334 learning rate:  0.010299940011997601   Training loss:   16.383141597736707  Valing loss:   14.177644042701274\n",
      "Pure loss: 16.496610061554733.....Total loss: 16.496610061554733\n",
      "Pure loss: 14.167230571386519.....Total loss: 14.167230571386519\n",
      "epoch 3335 learning rate:  0.010299850074962518   Training loss:   16.496610061554733  Valing loss:   14.167230571386519\n",
      "Pure loss: 16.304432126503023.....Total loss: 16.304432126503023\n",
      "Pure loss: 14.207445635603804.....Total loss: 14.207445635603804\n",
      "epoch 3336 learning rate:  0.010299760191846523   Training loss:   16.304432126503023  Valing loss:   14.207445635603804\n",
      "Pure loss: 16.405499891435998.....Total loss: 16.405499891435998\n",
      "Pure loss: 14.176130361537295.....Total loss: 14.176130361537295\n",
      "epoch 3337 learning rate:  0.010299670362601139   Training loss:   16.405499891435998  Valing loss:   14.176130361537295\n",
      "Pure loss: 16.2380415916968.....Total loss: 16.2380415916968\n",
      "Pure loss: 14.250214672583784.....Total loss: 14.250214672583784\n",
      "epoch 3338 learning rate:  0.010299580587177951   Training loss:   16.2380415916968  Valing loss:   14.250214672583784\n",
      "Pure loss: 16.16919499936324.....Total loss: 16.16919499936324\n",
      "Pure loss: 14.328268809149382.....Total loss: 14.328268809149382\n",
      "epoch 3339 learning rate:  0.010299490865528602   Training loss:   16.16919499936324  Valing loss:   14.328268809149382\n",
      "Pure loss: 16.17807398070929.....Total loss: 16.17807398070929\n",
      "Pure loss: 14.360651858916842.....Total loss: 14.360651858916842\n",
      "epoch 3340 learning rate:  0.01029940119760479   Training loss:   16.17807398070929  Valing loss:   14.360651858916842\n",
      "Pure loss: 16.172131797841075.....Total loss: 16.172131797841075\n",
      "Pure loss: 14.348413957169234.....Total loss: 14.348413957169234\n",
      "epoch 3341 learning rate:  0.010299311583358277   Training loss:   16.172131797841075  Valing loss:   14.348413957169234\n",
      "Pure loss: 16.169025004753546.....Total loss: 16.169025004753546\n",
      "Pure loss: 14.376207127107588.....Total loss: 14.376207127107588\n",
      "epoch 3342 learning rate:  0.010299222022740874   Training loss:   16.169025004753546  Valing loss:   14.376207127107588\n",
      "Pure loss: 16.15876027324294.....Total loss: 16.15876027324294\n",
      "Pure loss: 14.313369556205078.....Total loss: 14.313369556205078\n",
      "epoch 3343 learning rate:  0.010299132515704457   Training loss:   16.15876027324294  Valing loss:   14.313369556205078\n",
      "Pure loss: 16.1956046343969.....Total loss: 16.1956046343969\n",
      "Pure loss: 14.431997802454175.....Total loss: 14.431997802454175\n",
      "epoch 3344 learning rate:  0.010299043062200957   Training loss:   16.1956046343969  Valing loss:   14.431997802454175\n",
      "Pure loss: 16.165171033825334.....Total loss: 16.165171033825334\n",
      "Pure loss: 14.351627086714522.....Total loss: 14.351627086714522\n",
      "epoch 3345 learning rate:  0.010298953662182362   Training loss:   16.165171033825334  Valing loss:   14.351627086714522\n",
      "Pure loss: 16.165886709979727.....Total loss: 16.165886709979727\n",
      "Pure loss: 14.334529495893257.....Total loss: 14.334529495893257\n",
      "epoch 3346 learning rate:  0.010298864315600717   Training loss:   16.165886709979727  Valing loss:   14.334529495893257\n",
      "Pure loss: 16.157747919886525.....Total loss: 16.157747919886525\n",
      "Pure loss: 14.187905149298224.....Total loss: 14.187905149298224\n",
      "epoch 3347 learning rate:  0.010298775022408127   Training loss:   16.157747919886525  Valing loss:   14.187905149298224\n",
      "Pure loss: 16.161558968250258.....Total loss: 16.161558968250258\n",
      "Pure loss: 14.20085173993986.....Total loss: 14.20085173993986\n",
      "epoch 3348 learning rate:  0.01029868578255675   Training loss:   16.161558968250258  Valing loss:   14.20085173993986\n",
      "Pure loss: 16.345686710719296.....Total loss: 16.345686710719296\n",
      "Pure loss: 15.419716350436286.....Total loss: 15.419716350436286\n",
      "epoch 3349 learning rate:  0.010298596595998806   Training loss:   16.345686710719296  Valing loss:   15.419716350436286\n",
      "Pure loss: 16.339516877159177.....Total loss: 16.339516877159177\n",
      "Pure loss: 15.406679087272586.....Total loss: 15.406679087272586\n",
      "epoch 3350 learning rate:  0.010298507462686568   Training loss:   16.339516877159177  Valing loss:   15.406679087272586\n",
      "Pure loss: 16.21942248602155.....Total loss: 16.21942248602155\n",
      "Pure loss: 15.1709417713802.....Total loss: 15.1709417713802\n",
      "epoch 3351 learning rate:  0.010298418382572366   Training loss:   16.21942248602155  Valing loss:   15.1709417713802\n",
      "Pure loss: 16.42156095850147.....Total loss: 16.42156095850147\n",
      "Pure loss: 15.508476036850446.....Total loss: 15.508476036850446\n",
      "epoch 3352 learning rate:  0.010298329355608591   Training loss:   16.42156095850147  Valing loss:   15.508476036850446\n",
      "Pure loss: 16.27838389017126.....Total loss: 16.27838389017126\n",
      "Pure loss: 15.200037047702516.....Total loss: 15.200037047702516\n",
      "epoch 3353 learning rate:  0.01029824038174769   Training loss:   16.27838389017126  Valing loss:   15.200037047702516\n",
      "Pure loss: 16.339670844702056.....Total loss: 16.339670844702056\n",
      "Pure loss: 15.27056799105159.....Total loss: 15.27056799105159\n",
      "epoch 3354 learning rate:  0.01029815146094216   Training loss:   16.339670844702056  Valing loss:   15.27056799105159\n",
      "Pure loss: 16.390431875255945.....Total loss: 16.390431875255945\n",
      "Pure loss: 15.407886282767222.....Total loss: 15.407886282767222\n",
      "epoch 3355 learning rate:  0.010298062593144561   Training loss:   16.390431875255945  Valing loss:   15.407886282767222\n",
      "Pure loss: 16.852954944404015.....Total loss: 16.852954944404015\n",
      "Pure loss: 16.310476537471875.....Total loss: 16.310476537471875\n",
      "epoch 3356 learning rate:  0.010297973778307509   Training loss:   16.852954944404015  Valing loss:   16.310476537471875\n",
      "Pure loss: 17.018935147715318.....Total loss: 17.018935147715318\n",
      "Pure loss: 16.581736106958076.....Total loss: 16.581736106958076\n",
      "epoch 3357 learning rate:  0.010297885016383675   Training loss:   17.018935147715318  Valing loss:   16.581736106958076\n",
      "Pure loss: 16.735543974087996.....Total loss: 16.735543974087996\n",
      "Pure loss: 16.127631828564798.....Total loss: 16.127631828564798\n",
      "epoch 3358 learning rate:  0.01029779630732579   Training loss:   16.735543974087996  Valing loss:   16.127631828564798\n",
      "Pure loss: 16.75153853848929.....Total loss: 16.75153853848929\n",
      "Pure loss: 16.155439575651883.....Total loss: 16.155439575651883\n",
      "epoch 3359 learning rate:  0.010297707651086634   Training loss:   16.75153853848929  Valing loss:   16.155439575651883\n",
      "Pure loss: 16.424685876339563.....Total loss: 16.424685876339563\n",
      "Pure loss: 15.64561847644314.....Total loss: 15.64561847644314\n",
      "epoch 3360 learning rate:  0.010297619047619049   Training loss:   16.424685876339563  Valing loss:   15.64561847644314\n",
      "Pure loss: 17.373094890725824.....Total loss: 17.373094890725824\n",
      "Pure loss: 16.902870516911324.....Total loss: 16.902870516911324\n",
      "epoch 3361 learning rate:  0.01029753049687593   Training loss:   17.373094890725824  Valing loss:   16.902870516911324\n",
      "Pure loss: 16.830599042965815.....Total loss: 16.830599042965815\n",
      "Pure loss: 16.0873986642392.....Total loss: 16.0873986642392\n",
      "epoch 3362 learning rate:  0.010297441998810232   Training loss:   16.830599042965815  Valing loss:   16.0873986642392\n",
      "Pure loss: 16.913866848813186.....Total loss: 16.913866848813186\n",
      "Pure loss: 16.197376170754737.....Total loss: 16.197376170754737\n",
      "epoch 3363 learning rate:  0.010297353553374963   Training loss:   16.913866848813186  Valing loss:   16.197376170754737\n",
      "Pure loss: 16.93324007223559.....Total loss: 16.93324007223559\n",
      "Pure loss: 16.238135032423028.....Total loss: 16.238135032423028\n",
      "epoch 3364 learning rate:  0.010297265160523187   Training loss:   16.93324007223559  Valing loss:   16.238135032423028\n",
      "Pure loss: 17.2606769684078.....Total loss: 17.2606769684078\n",
      "Pure loss: 16.772797041081468.....Total loss: 16.772797041081468\n",
      "epoch 3365 learning rate:  0.010297176820208025   Training loss:   17.2606769684078  Valing loss:   16.772797041081468\n",
      "Pure loss: 17.124346209332906.....Total loss: 17.124346209332906\n",
      "Pure loss: 16.508984604740274.....Total loss: 16.508984604740274\n",
      "epoch 3366 learning rate:  0.01029708853238265   Training loss:   17.124346209332906  Valing loss:   16.508984604740274\n",
      "Pure loss: 16.6197398825215.....Total loss: 16.6197398825215\n",
      "Pure loss: 15.824348387184068.....Total loss: 15.824348387184068\n",
      "epoch 3367 learning rate:  0.010297000297000296   Training loss:   16.6197398825215  Valing loss:   15.824348387184068\n",
      "Pure loss: 16.191038971459008.....Total loss: 16.191038971459008\n",
      "Pure loss: 14.885640193508747.....Total loss: 14.885640193508747\n",
      "epoch 3368 learning rate:  0.010296912114014251   Training loss:   16.191038971459008  Valing loss:   14.885640193508747\n",
      "Pure loss: 16.180570958444754.....Total loss: 16.180570958444754\n",
      "Pure loss: 14.861726650454466.....Total loss: 14.861726650454466\n",
      "epoch 3369 learning rate:  0.010296823983377857   Training loss:   16.180570958444754  Valing loss:   14.861726650454466\n",
      "Pure loss: 15.989884371886532.....Total loss: 15.989884371886532\n",
      "Pure loss: 14.39487342336691.....Total loss: 14.39487342336691\n",
      "epoch 3370 learning rate:  0.010296735905044511   Training loss:   15.989884371886532  Valing loss:   14.39487342336691\n",
      "Pure loss: 15.998884425628281.....Total loss: 15.998884425628281\n",
      "Pure loss: 14.42509692619556.....Total loss: 14.42509692619556\n",
      "epoch 3371 learning rate:  0.010296647878967665   Training loss:   15.998884425628281  Valing loss:   14.42509692619556\n",
      "Pure loss: 15.995862909509562.....Total loss: 15.995862909509562\n",
      "Pure loss: 14.37620900436569.....Total loss: 14.37620900436569\n",
      "epoch 3372 learning rate:  0.010296559905100831   Training loss:   15.995862909509562  Valing loss:   14.37620900436569\n",
      "Pure loss: 16.023565231477455.....Total loss: 16.023565231477455\n",
      "Pure loss: 14.270139597839426.....Total loss: 14.270139597839426\n",
      "epoch 3373 learning rate:  0.01029647198339757   Training loss:   16.023565231477455  Valing loss:   14.270139597839426\n",
      "Pure loss: 16.025693291965954.....Total loss: 16.025693291965954\n",
      "Pure loss: 14.337203173352686.....Total loss: 14.337203173352686\n",
      "epoch 3374 learning rate:  0.0102963841138115   Training loss:   16.025693291965954  Valing loss:   14.337203173352686\n",
      "Pure loss: 16.08438736228746.....Total loss: 16.08438736228746\n",
      "Pure loss: 14.223490777114689.....Total loss: 14.223490777114689\n",
      "epoch 3375 learning rate:  0.010296296296296296   Training loss:   16.08438736228746  Valing loss:   14.223490777114689\n",
      "Pure loss: 16.07911470973154.....Total loss: 16.07911470973154\n",
      "Pure loss: 14.245643882100286.....Total loss: 14.245643882100286\n",
      "epoch 3376 learning rate:  0.010296208530805687   Training loss:   16.07911470973154  Valing loss:   14.245643882100286\n",
      "Pure loss: 16.037362275283364.....Total loss: 16.037362275283364\n",
      "Pure loss: 14.466558703577961.....Total loss: 14.466558703577961\n",
      "epoch 3377 learning rate:  0.010296120817293456   Training loss:   16.037362275283364  Valing loss:   14.466558703577961\n",
      "Pure loss: 16.047056850037613.....Total loss: 16.047056850037613\n",
      "Pure loss: 14.602611528869907.....Total loss: 14.602611528869907\n",
      "epoch 3378 learning rate:  0.01029603315571344   Training loss:   16.047056850037613  Valing loss:   14.602611528869907\n",
      "Pure loss: 16.067790571494875.....Total loss: 16.067790571494875\n",
      "Pure loss: 14.74925613973391.....Total loss: 14.74925613973391\n",
      "epoch 3379 learning rate:  0.010295945546019532   Training loss:   16.067790571494875  Valing loss:   14.74925613973391\n",
      "Pure loss: 16.129301144152958.....Total loss: 16.129301144152958\n",
      "Pure loss: 14.88506047362699.....Total loss: 14.88506047362699\n",
      "epoch 3380 learning rate:  0.010295857988165681   Training loss:   16.129301144152958  Valing loss:   14.88506047362699\n",
      "Pure loss: 16.09517490564645.....Total loss: 16.09517490564645\n",
      "Pure loss: 14.815491592967213.....Total loss: 14.815491592967213\n",
      "epoch 3381 learning rate:  0.010295770482105886   Training loss:   16.09517490564645  Valing loss:   14.815491592967213\n",
      "Pure loss: 16.00108552495833.....Total loss: 16.00108552495833\n",
      "Pure loss: 14.593884501337794.....Total loss: 14.593884501337794\n",
      "epoch 3382 learning rate:  0.010295683027794204   Training loss:   16.00108552495833  Valing loss:   14.593884501337794\n",
      "Pure loss: 16.03573473192217.....Total loss: 16.03573473192217\n",
      "Pure loss: 14.744967940064761.....Total loss: 14.744967940064761\n",
      "epoch 3383 learning rate:  0.010295595625184747   Training loss:   16.03573473192217  Valing loss:   14.744967940064761\n",
      "Pure loss: 15.97675502998632.....Total loss: 15.97675502998632\n",
      "Pure loss: 14.465934363611037.....Total loss: 14.465934363611037\n",
      "epoch 3384 learning rate:  0.010295508274231679   Training loss:   15.97675502998632  Valing loss:   14.465934363611037\n",
      "Pure loss: 15.996014239475242.....Total loss: 15.996014239475242\n",
      "Pure loss: 14.326048487724826.....Total loss: 14.326048487724826\n",
      "epoch 3385 learning rate:  0.010295420974889218   Training loss:   15.996014239475242  Valing loss:   14.326048487724826\n",
      "Pure loss: 16.26926462771361.....Total loss: 16.26926462771361\n",
      "Pure loss: 14.075480014087214.....Total loss: 14.075480014087214\n",
      "epoch 3386 learning rate:  0.010295333727111637   Training loss:   16.26926462771361  Valing loss:   14.075480014087214\n",
      "Pure loss: 16.19272549405708.....Total loss: 16.19272549405708\n",
      "Pure loss: 14.095588494044241.....Total loss: 14.095588494044241\n",
      "epoch 3387 learning rate:  0.010295246530853263   Training loss:   16.19272549405708  Valing loss:   14.095588494044241\n",
      "Pure loss: 16.59106523969694.....Total loss: 16.59106523969694\n",
      "Pure loss: 15.257505454765454.....Total loss: 15.257505454765454\n",
      "epoch 3388 learning rate:  0.010295159386068477   Training loss:   16.59106523969694  Valing loss:   15.257505454765454\n",
      "Pure loss: 16.554763497668773.....Total loss: 16.554763497668773\n",
      "Pure loss: 15.173356121976354.....Total loss: 15.173356121976354\n",
      "epoch 3389 learning rate:  0.010295072292711715   Training loss:   16.554763497668773  Valing loss:   15.173356121976354\n",
      "Pure loss: 16.529649792985968.....Total loss: 16.529649792985968\n",
      "Pure loss: 15.139206000431725.....Total loss: 15.139206000431725\n",
      "epoch 3390 learning rate:  0.010294985250737463   Training loss:   16.529649792985968  Valing loss:   15.139206000431725\n",
      "Pure loss: 16.476901837121023.....Total loss: 16.476901837121023\n",
      "Pure loss: 14.90710031147748.....Total loss: 14.90710031147748\n",
      "epoch 3391 learning rate:  0.010294898260100266   Training loss:   16.476901837121023  Valing loss:   14.90710031147748\n",
      "Pure loss: 16.601727421250498.....Total loss: 16.601727421250498\n",
      "Pure loss: 14.649595086882167.....Total loss: 14.649595086882167\n",
      "epoch 3392 learning rate:  0.010294811320754717   Training loss:   16.601727421250498  Valing loss:   14.649595086882167\n",
      "Pure loss: 16.577640756222948.....Total loss: 16.577640756222948\n",
      "Pure loss: 14.60358546300771.....Total loss: 14.60358546300771\n",
      "epoch 3393 learning rate:  0.010294724432655467   Training loss:   16.577640756222948  Valing loss:   14.60358546300771\n",
      "Pure loss: 16.543149819377007.....Total loss: 16.543149819377007\n",
      "Pure loss: 14.679974973095772.....Total loss: 14.679974973095772\n",
      "epoch 3394 learning rate:  0.010294637595757218   Training loss:   16.543149819377007  Valing loss:   14.679974973095772\n",
      "Pure loss: 16.540669180215804.....Total loss: 16.540669180215804\n",
      "Pure loss: 14.768184302471571.....Total loss: 14.768184302471571\n",
      "epoch 3395 learning rate:  0.010294550810014728   Training loss:   16.540669180215804  Valing loss:   14.768184302471571\n",
      "Pure loss: 16.470948511194965.....Total loss: 16.470948511194965\n",
      "Pure loss: 14.678610560550094.....Total loss: 14.678610560550094\n",
      "epoch 3396 learning rate:  0.010294464075382804   Training loss:   16.470948511194965  Valing loss:   14.678610560550094\n",
      "Pure loss: 16.46994241913022.....Total loss: 16.46994241913022\n",
      "Pure loss: 14.67678500186281.....Total loss: 14.67678500186281\n",
      "epoch 3397 learning rate:  0.010294377391816309   Training loss:   16.46994241913022  Valing loss:   14.67678500186281\n",
      "Pure loss: 16.47218964564137.....Total loss: 16.47218964564137\n",
      "Pure loss: 14.74193691279472.....Total loss: 14.74193691279472\n",
      "epoch 3398 learning rate:  0.010294290759270158   Training loss:   16.47218964564137  Valing loss:   14.74193691279472\n",
      "Pure loss: 16.560093343864487.....Total loss: 16.560093343864487\n",
      "Pure loss: 14.908038031530564.....Total loss: 14.908038031530564\n",
      "epoch 3399 learning rate:  0.010294204177699324   Training loss:   16.560093343864487  Valing loss:   14.908038031530564\n",
      "Pure loss: 16.86569762669904.....Total loss: 16.86569762669904\n",
      "Pure loss: 15.67110923982521.....Total loss: 15.67110923982521\n",
      "epoch 3400 learning rate:  0.010294117647058823   Training loss:   16.86569762669904  Valing loss:   15.67110923982521\n",
      "Pure loss: 16.68031943959576.....Total loss: 16.68031943959576\n",
      "Pure loss: 15.431852584983364.....Total loss: 15.431852584983364\n",
      "epoch 3401 learning rate:  0.010294031167303734   Training loss:   16.68031943959576  Valing loss:   15.431852584983364\n",
      "Pure loss: 16.706276324283746.....Total loss: 16.706276324283746\n",
      "Pure loss: 15.551956222193681.....Total loss: 15.551956222193681\n",
      "epoch 3402 learning rate:  0.010293944738389182   Training loss:   16.706276324283746  Valing loss:   15.551956222193681\n",
      "Pure loss: 17.912967962160437.....Total loss: 17.912967962160437\n",
      "Pure loss: 17.754152484973325.....Total loss: 17.754152484973325\n",
      "epoch 3403 learning rate:  0.01029385836027035   Training loss:   17.912967962160437  Valing loss:   17.754152484973325\n",
      "Pure loss: 17.526208674707267.....Total loss: 17.526208674707267\n",
      "Pure loss: 17.302544527910282.....Total loss: 17.302544527910282\n",
      "epoch 3404 learning rate:  0.010293772032902469   Training loss:   17.526208674707267  Valing loss:   17.302544527910282\n",
      "Pure loss: 16.917560510604616.....Total loss: 16.917560510604616\n",
      "Pure loss: 16.43399170971586.....Total loss: 16.43399170971586\n",
      "epoch 3405 learning rate:  0.010293685756240822   Training loss:   16.917560510604616  Valing loss:   16.43399170971586\n",
      "Pure loss: 16.77772380056192.....Total loss: 16.77772380056192\n",
      "Pure loss: 16.23130137224946.....Total loss: 16.23130137224946\n",
      "epoch 3406 learning rate:  0.010293599530240751   Training loss:   16.77772380056192  Valing loss:   16.23130137224946\n",
      "Pure loss: 16.40448031078721.....Total loss: 16.40448031078721\n",
      "Pure loss: 15.616383222638799.....Total loss: 15.616383222638799\n",
      "epoch 3407 learning rate:  0.010293513354857646   Training loss:   16.40448031078721  Valing loss:   15.616383222638799\n",
      "Pure loss: 16.488478119152926.....Total loss: 16.488478119152926\n",
      "Pure loss: 15.7637526186101.....Total loss: 15.7637526186101\n",
      "epoch 3408 learning rate:  0.01029342723004695   Training loss:   16.488478119152926  Valing loss:   15.7637526186101\n",
      "Pure loss: 16.473666574822396.....Total loss: 16.473666574822396\n",
      "Pure loss: 15.73797856030969.....Total loss: 15.73797856030969\n",
      "epoch 3409 learning rate:  0.010293341155764154   Training loss:   16.473666574822396  Valing loss:   15.73797856030969\n",
      "Pure loss: 16.7949091952619.....Total loss: 16.7949091952619\n",
      "Pure loss: 16.22535755966642.....Total loss: 16.22535755966642\n",
      "epoch 3410 learning rate:  0.01029325513196481   Training loss:   16.7949091952619  Valing loss:   16.22535755966642\n",
      "Pure loss: 16.38239140664174.....Total loss: 16.38239140664174\n",
      "Pure loss: 15.551029770449736.....Total loss: 15.551029770449736\n",
      "epoch 3411 learning rate:  0.010293169158604515   Training loss:   16.38239140664174  Valing loss:   15.551029770449736\n",
      "Pure loss: 16.24604285449876.....Total loss: 16.24604285449876\n",
      "Pure loss: 15.273730733248216.....Total loss: 15.273730733248216\n",
      "epoch 3412 learning rate:  0.010293083235638921   Training loss:   16.24604285449876  Valing loss:   15.273730733248216\n",
      "Pure loss: 16.188128265005595.....Total loss: 16.188128265005595\n",
      "Pure loss: 15.154502015204974.....Total loss: 15.154502015204974\n",
      "epoch 3413 learning rate:  0.010292997363023733   Training loss:   16.188128265005595  Valing loss:   15.154502015204974\n",
      "Pure loss: 16.140316500155883.....Total loss: 16.140316500155883\n",
      "Pure loss: 15.049964027032345.....Total loss: 15.049964027032345\n",
      "epoch 3414 learning rate:  0.010292911540714704   Training loss:   16.140316500155883  Valing loss:   15.049964027032345\n",
      "Pure loss: 16.12735464933649.....Total loss: 16.12735464933649\n",
      "Pure loss: 15.015560951893667.....Total loss: 15.015560951893667\n",
      "epoch 3415 learning rate:  0.010292825768667644   Training loss:   16.12735464933649  Valing loss:   15.015560951893667\n",
      "Pure loss: 17.1915439384472.....Total loss: 17.1915439384472\n",
      "Pure loss: 17.027332519964077.....Total loss: 17.027332519964077\n",
      "epoch 3416 learning rate:  0.010292740046838408   Training loss:   17.1915439384472  Valing loss:   17.027332519964077\n",
      "Pure loss: 17.781173456729.....Total loss: 17.781173456729\n",
      "Pure loss: 17.91515477676735.....Total loss: 17.91515477676735\n",
      "epoch 3417 learning rate:  0.010292654375182909   Training loss:   17.781173456729  Valing loss:   17.91515477676735\n",
      "Pure loss: 17.775055203591016.....Total loss: 17.775055203591016\n",
      "Pure loss: 17.90722490114036.....Total loss: 17.90722490114036\n",
      "epoch 3418 learning rate:  0.01029256875365711   Training loss:   17.775055203591016  Valing loss:   17.90722490114036\n",
      "Pure loss: 18.20831435233666.....Total loss: 18.20831435233666\n",
      "Pure loss: 18.511003360605706.....Total loss: 18.511003360605706\n",
      "epoch 3419 learning rate:  0.010292483182217023   Training loss:   18.20831435233666  Valing loss:   18.511003360605706\n",
      "Pure loss: 18.052860825665732.....Total loss: 18.052860825665732\n",
      "Pure loss: 18.33686652248205.....Total loss: 18.33686652248205\n",
      "epoch 3420 learning rate:  0.010292397660818714   Training loss:   18.052860825665732  Valing loss:   18.33686652248205\n",
      "Pure loss: 17.962009481562546.....Total loss: 17.962009481562546\n",
      "Pure loss: 18.20815004147452.....Total loss: 18.20815004147452\n",
      "epoch 3421 learning rate:  0.010292312189418298   Training loss:   17.962009481562546  Valing loss:   18.20815004147452\n",
      "Pure loss: 17.25235603061122.....Total loss: 17.25235603061122\n",
      "Pure loss: 17.238100289603764.....Total loss: 17.238100289603764\n",
      "epoch 3422 learning rate:  0.010292226767971946   Training loss:   17.25235603061122  Valing loss:   17.238100289603764\n",
      "Pure loss: 17.36908945851955.....Total loss: 17.36908945851955\n",
      "Pure loss: 17.37115053238703.....Total loss: 17.37115053238703\n",
      "epoch 3423 learning rate:  0.010292141396435875   Training loss:   17.36908945851955  Valing loss:   17.37115053238703\n",
      "Pure loss: 16.92393625707183.....Total loss: 16.92393625707183\n",
      "Pure loss: 16.778744316828554.....Total loss: 16.778744316828554\n",
      "epoch 3424 learning rate:  0.010292056074766355   Training loss:   16.92393625707183  Valing loss:   16.778744316828554\n",
      "Pure loss: 16.773567979819177.....Total loss: 16.773567979819177\n",
      "Pure loss: 16.565362329704925.....Total loss: 16.565362329704925\n",
      "epoch 3425 learning rate:  0.010291970802919707   Training loss:   16.773567979819177  Valing loss:   16.565362329704925\n",
      "Pure loss: 16.343993677640263.....Total loss: 16.343993677640263\n",
      "Pure loss: 15.837533896297058.....Total loss: 15.837533896297058\n",
      "epoch 3426 learning rate:  0.010291885580852306   Training loss:   16.343993677640263  Valing loss:   15.837533896297058\n",
      "Pure loss: 16.539545362611975.....Total loss: 16.539545362611975\n",
      "Pure loss: 16.166722622000673.....Total loss: 16.166722622000673\n",
      "epoch 3427 learning rate:  0.010291800408520572   Training loss:   16.539545362611975  Valing loss:   16.166722622000673\n",
      "Pure loss: 16.575456274020308.....Total loss: 16.575456274020308\n",
      "Pure loss: 16.222529692277753.....Total loss: 16.222529692277753\n",
      "epoch 3428 learning rate:  0.01029171528588098   Training loss:   16.575456274020308  Valing loss:   16.222529692277753\n",
      "Pure loss: 16.40140388411471.....Total loss: 16.40140388411471\n",
      "Pure loss: 15.952932956534672.....Total loss: 15.952932956534672\n",
      "epoch 3429 learning rate:  0.010291630212890056   Training loss:   16.40140388411471  Valing loss:   15.952932956534672\n",
      "Pure loss: 16.663500196983467.....Total loss: 16.663500196983467\n",
      "Pure loss: 16.385971397984843.....Total loss: 16.385971397984843\n",
      "epoch 3430 learning rate:  0.010291545189504373   Training loss:   16.663500196983467  Valing loss:   16.385971397984843\n",
      "Pure loss: 16.643891639099174.....Total loss: 16.643891639099174\n",
      "Pure loss: 16.353260838865634.....Total loss: 16.353260838865634\n",
      "epoch 3431 learning rate:  0.010291460215680559   Training loss:   16.643891639099174  Valing loss:   16.353260838865634\n",
      "Pure loss: 16.481289904019608.....Total loss: 16.481289904019608\n",
      "Pure loss: 16.113474937319467.....Total loss: 16.113474937319467\n",
      "epoch 3432 learning rate:  0.010291375291375292   Training loss:   16.481289904019608  Valing loss:   16.113474937319467\n",
      "Pure loss: 16.214293035023505.....Total loss: 16.214293035023505\n",
      "Pure loss: 15.717556596005812.....Total loss: 15.717556596005812\n",
      "epoch 3433 learning rate:  0.010291290416545296   Training loss:   16.214293035023505  Valing loss:   15.717556596005812\n",
      "Pure loss: 16.191929384658362.....Total loss: 16.191929384658362\n",
      "Pure loss: 15.677698316082658.....Total loss: 15.677698316082658\n",
      "epoch 3434 learning rate:  0.010291205591147351   Training loss:   16.191929384658362  Valing loss:   15.677698316082658\n",
      "Pure loss: 16.225209101769654.....Total loss: 16.225209101769654\n",
      "Pure loss: 15.735861065177055.....Total loss: 15.735861065177055\n",
      "epoch 3435 learning rate:  0.010291120815138282   Training loss:   16.225209101769654  Valing loss:   15.735861065177055\n",
      "Pure loss: 16.15347229588324.....Total loss: 16.15347229588324\n",
      "Pure loss: 15.61007055760796.....Total loss: 15.61007055760796\n",
      "epoch 3436 learning rate:  0.01029103608847497   Training loss:   16.15347229588324  Valing loss:   15.61007055760796\n",
      "Pure loss: 16.083100745052313.....Total loss: 16.083100745052313\n",
      "Pure loss: 15.471618699424974.....Total loss: 15.471618699424974\n",
      "epoch 3437 learning rate:  0.010290951411114344   Training loss:   16.083100745052313  Valing loss:   15.471618699424974\n",
      "Pure loss: 16.11512774291816.....Total loss: 16.11512774291816\n",
      "Pure loss: 15.520309748059194.....Total loss: 15.520309748059194\n",
      "epoch 3438 learning rate:  0.01029086678301338   Training loss:   16.11512774291816  Valing loss:   15.520309748059194\n",
      "Pure loss: 15.98417787229987.....Total loss: 15.98417787229987\n",
      "Pure loss: 15.286135121409593.....Total loss: 15.286135121409593\n",
      "epoch 3439 learning rate:  0.010290782204129108   Training loss:   15.98417787229987  Valing loss:   15.286135121409593\n",
      "Pure loss: 15.922239183746816.....Total loss: 15.922239183746816\n",
      "Pure loss: 15.117363090910576.....Total loss: 15.117363090910576\n",
      "epoch 3440 learning rate:  0.010290697674418606   Training loss:   15.922239183746816  Valing loss:   15.117363090910576\n",
      "Pure loss: 15.966002516063629.....Total loss: 15.966002516063629\n",
      "Pure loss: 15.221605063667822.....Total loss: 15.221605063667822\n",
      "epoch 3441 learning rate:  0.010290613193839001   Training loss:   15.966002516063629  Valing loss:   15.221605063667822\n",
      "Pure loss: 15.866875190792543.....Total loss: 15.866875190792543\n",
      "Pure loss: 14.942085856506518.....Total loss: 14.942085856506518\n",
      "epoch 3442 learning rate:  0.010290528762347472   Training loss:   15.866875190792543  Valing loss:   14.942085856506518\n",
      "Pure loss: 15.889890209456304.....Total loss: 15.889890209456304\n",
      "Pure loss: 15.104009136818744.....Total loss: 15.104009136818744\n",
      "epoch 3443 learning rate:  0.010290444379901249   Training loss:   15.889890209456304  Valing loss:   15.104009136818744\n",
      "Pure loss: 15.90905709689576.....Total loss: 15.90905709689576\n",
      "Pure loss: 15.142909403984142.....Total loss: 15.142909403984142\n",
      "epoch 3444 learning rate:  0.010290360046457607   Training loss:   15.90905709689576  Valing loss:   15.142909403984142\n",
      "Pure loss: 15.818121756859751.....Total loss: 15.818121756859751\n",
      "Pure loss: 14.920769009742505.....Total loss: 14.920769009742505\n",
      "epoch 3445 learning rate:  0.010290275761973876   Training loss:   15.818121756859751  Valing loss:   14.920769009742505\n",
      "Pure loss: 15.679706721392138.....Total loss: 15.679706721392138\n",
      "Pure loss: 14.49413192352105.....Total loss: 14.49413192352105\n",
      "epoch 3446 learning rate:  0.010290191526407429   Training loss:   15.679706721392138  Valing loss:   14.49413192352105\n",
      "Pure loss: 15.735968600539843.....Total loss: 15.735968600539843\n",
      "Pure loss: 14.210031885953267.....Total loss: 14.210031885953267\n",
      "epoch 3447 learning rate:  0.010290107339715695   Training loss:   15.735968600539843  Valing loss:   14.210031885953267\n",
      "Pure loss: 15.716278557450442.....Total loss: 15.716278557450442\n",
      "Pure loss: 14.224829466453892.....Total loss: 14.224829466453892\n",
      "epoch 3448 learning rate:  0.010290023201856149   Training loss:   15.716278557450442  Valing loss:   14.224829466453892\n",
      "Pure loss: 15.715064783960731.....Total loss: 15.715064783960731\n",
      "Pure loss: 14.228321693144256.....Total loss: 14.228321693144256\n",
      "epoch 3449 learning rate:  0.010289939112786315   Training loss:   15.715064783960731  Valing loss:   14.228321693144256\n",
      "Pure loss: 15.700276555278128.....Total loss: 15.700276555278128\n",
      "Pure loss: 14.280227216571978.....Total loss: 14.280227216571978\n",
      "epoch 3450 learning rate:  0.010289855072463768   Training loss:   15.700276555278128  Valing loss:   14.280227216571978\n",
      "Pure loss: 15.700975805170984.....Total loss: 15.700975805170984\n",
      "Pure loss: 14.279789409590498.....Total loss: 14.279789409590498\n",
      "epoch 3451 learning rate:  0.010289771080846132   Training loss:   15.700975805170984  Valing loss:   14.279789409590498\n",
      "Pure loss: 15.681688219638184.....Total loss: 15.681688219638184\n",
      "Pure loss: 14.29827287336869.....Total loss: 14.29827287336869\n",
      "epoch 3452 learning rate:  0.010289687137891078   Training loss:   15.681688219638184  Valing loss:   14.29827287336869\n",
      "Pure loss: 15.669949434751246.....Total loss: 15.669949434751246\n",
      "Pure loss: 14.44210028822314.....Total loss: 14.44210028822314\n",
      "epoch 3453 learning rate:  0.010289603243556327   Training loss:   15.669949434751246  Valing loss:   14.44210028822314\n",
      "Pure loss: 15.669976924403722.....Total loss: 15.669976924403722\n",
      "Pure loss: 14.442901202062943.....Total loss: 14.442901202062943\n",
      "epoch 3454 learning rate:  0.010289519397799653   Training loss:   15.669976924403722  Valing loss:   14.442901202062943\n",
      "Pure loss: 15.670241575600702.....Total loss: 15.670241575600702\n",
      "Pure loss: 14.448367564546677.....Total loss: 14.448367564546677\n",
      "epoch 3455 learning rate:  0.010289435600578871   Training loss:   15.670241575600702  Valing loss:   14.448367564546677\n",
      "Pure loss: 15.694739738729572.....Total loss: 15.694739738729572\n",
      "Pure loss: 14.409461948721335.....Total loss: 14.409461948721335\n",
      "epoch 3456 learning rate:  0.010289351851851852   Training loss:   15.694739738729572  Valing loss:   14.409461948721335\n",
      "Pure loss: 15.687360761391034.....Total loss: 15.687360761391034\n",
      "Pure loss: 14.405405293307497.....Total loss: 14.405405293307497\n",
      "epoch 3457 learning rate:  0.010289268151576512   Training loss:   15.687360761391034  Valing loss:   14.405405293307497\n",
      "Pure loss: 15.701225581184229.....Total loss: 15.701225581184229\n",
      "Pure loss: 14.397628803242066.....Total loss: 14.397628803242066\n",
      "epoch 3458 learning rate:  0.010289184499710816   Training loss:   15.701225581184229  Valing loss:   14.397628803242066\n",
      "Pure loss: 15.70304466562304.....Total loss: 15.70304466562304\n",
      "Pure loss: 14.386704763245596.....Total loss: 14.386704763245596\n",
      "epoch 3459 learning rate:  0.010289100896212778   Training loss:   15.70304466562304  Valing loss:   14.386704763245596\n",
      "Pure loss: 15.746532876417584.....Total loss: 15.746532876417584\n",
      "Pure loss: 14.319316750104079.....Total loss: 14.319316750104079\n",
      "epoch 3460 learning rate:  0.010289017341040462   Training loss:   15.746532876417584  Valing loss:   14.319316750104079\n",
      "Pure loss: 15.914568726311675.....Total loss: 15.914568726311675\n",
      "Pure loss: 14.157892682797053.....Total loss: 14.157892682797053\n",
      "epoch 3461 learning rate:  0.01028893383415198   Training loss:   15.914568726311675  Valing loss:   14.157892682797053\n",
      "Pure loss: 15.938714974142426.....Total loss: 15.938714974142426\n",
      "Pure loss: 14.153403885150155.....Total loss: 14.153403885150155\n",
      "epoch 3462 learning rate:  0.010288850375505488   Training loss:   15.938714974142426  Valing loss:   14.153403885150155\n",
      "Pure loss: 15.842837597827423.....Total loss: 15.842837597827423\n",
      "Pure loss: 14.158742593966993.....Total loss: 14.158742593966993\n",
      "epoch 3463 learning rate:  0.010288766965059197   Training loss:   15.842837597827423  Valing loss:   14.158742593966993\n",
      "Pure loss: 15.9850518043574.....Total loss: 15.9850518043574\n",
      "Pure loss: 14.13792232121138.....Total loss: 14.13792232121138\n",
      "epoch 3464 learning rate:  0.010288683602771362   Training loss:   15.9850518043574  Valing loss:   14.13792232121138\n",
      "Pure loss: 16.038775058082923.....Total loss: 16.038775058082923\n",
      "Pure loss: 14.15226753936208.....Total loss: 14.15226753936208\n",
      "epoch 3465 learning rate:  0.010288600288600288   Training loss:   16.038775058082923  Valing loss:   14.15226753936208\n",
      "Pure loss: 16.181188146132598.....Total loss: 16.181188146132598\n",
      "Pure loss: 14.135115098434525.....Total loss: 14.135115098434525\n",
      "epoch 3466 learning rate:  0.010288517022504329   Training loss:   16.181188146132598  Valing loss:   14.135115098434525\n",
      "Pure loss: 16.601009953777083.....Total loss: 16.601009953777083\n",
      "Pure loss: 14.255096750046386.....Total loss: 14.255096750046386\n",
      "epoch 3467 learning rate:  0.01028843380444188   Training loss:   16.601009953777083  Valing loss:   14.255096750046386\n",
      "Pure loss: 16.544786054921197.....Total loss: 16.544786054921197\n",
      "Pure loss: 14.196665338231686.....Total loss: 14.196665338231686\n",
      "epoch 3468 learning rate:  0.010288350634371396   Training loss:   16.544786054921197  Valing loss:   14.196665338231686\n",
      "Pure loss: 16.572981727078606.....Total loss: 16.572981727078606\n",
      "Pure loss: 14.206967027437836.....Total loss: 14.206967027437836\n",
      "epoch 3469 learning rate:  0.01028826751225137   Training loss:   16.572981727078606  Valing loss:   14.206967027437836\n",
      "Pure loss: 17.18103113304543.....Total loss: 17.18103113304543\n",
      "Pure loss: 14.631760913076098.....Total loss: 14.631760913076098\n",
      "epoch 3470 learning rate:  0.010288184438040346   Training loss:   17.18103113304543  Valing loss:   14.631760913076098\n",
      "Pure loss: 16.71249577538896.....Total loss: 16.71249577538896\n",
      "Pure loss: 14.421543410331033.....Total loss: 14.421543410331033\n",
      "epoch 3471 learning rate:  0.010288101411696917   Training loss:   16.71249577538896  Valing loss:   14.421543410331033\n",
      "Pure loss: 16.87409814320336.....Total loss: 16.87409814320336\n",
      "Pure loss: 14.523579399669812.....Total loss: 14.523579399669812\n",
      "epoch 3472 learning rate:  0.010288018433179724   Training loss:   16.87409814320336  Valing loss:   14.523579399669812\n",
      "Pure loss: 16.818832937398238.....Total loss: 16.818832937398238\n",
      "Pure loss: 14.48270897503918.....Total loss: 14.48270897503918\n",
      "epoch 3473 learning rate:  0.010287935502447451   Training loss:   16.818832937398238  Valing loss:   14.48270897503918\n",
      "Pure loss: 16.71041575118564.....Total loss: 16.71041575118564\n",
      "Pure loss: 14.411079001090576.....Total loss: 14.411079001090576\n",
      "epoch 3474 learning rate:  0.010287852619458837   Training loss:   16.71041575118564  Valing loss:   14.411079001090576\n",
      "Pure loss: 16.89850589932429.....Total loss: 16.89850589932429\n",
      "Pure loss: 14.469133571276988.....Total loss: 14.469133571276988\n",
      "epoch 3475 learning rate:  0.010287769784172661   Training loss:   16.89850589932429  Valing loss:   14.469133571276988\n",
      "Pure loss: 16.897056790075055.....Total loss: 16.897056790075055\n",
      "Pure loss: 14.468031195614754.....Total loss: 14.468031195614754\n",
      "epoch 3476 learning rate:  0.010287686996547757   Training loss:   16.897056790075055  Valing loss:   14.468031195614754\n",
      "Pure loss: 16.829376649348113.....Total loss: 16.829376649348113\n",
      "Pure loss: 14.416329489102242.....Total loss: 14.416329489102242\n",
      "epoch 3477 learning rate:  0.010287604256542997   Training loss:   16.829376649348113  Valing loss:   14.416329489102242\n",
      "Pure loss: 17.619188943017804.....Total loss: 17.619188943017804\n",
      "Pure loss: 14.828985384407503.....Total loss: 14.828985384407503\n",
      "epoch 3478 learning rate:  0.01028752156411731   Training loss:   17.619188943017804  Valing loss:   14.828985384407503\n",
      "Pure loss: 17.884075386161104.....Total loss: 17.884075386161104\n",
      "Pure loss: 15.036962707733542.....Total loss: 15.036962707733542\n",
      "epoch 3479 learning rate:  0.010287438919229664   Training loss:   17.884075386161104  Valing loss:   15.036962707733542\n",
      "Pure loss: 17.409611105858026.....Total loss: 17.409611105858026\n",
      "Pure loss: 14.73744109186255.....Total loss: 14.73744109186255\n",
      "epoch 3480 learning rate:  0.01028735632183908   Training loss:   17.409611105858026  Valing loss:   14.73744109186255\n",
      "Pure loss: 17.791791849849993.....Total loss: 17.791791849849993\n",
      "Pure loss: 14.940209714286507.....Total loss: 14.940209714286507\n",
      "epoch 3481 learning rate:  0.010287273771904625   Training loss:   17.791791849849993  Valing loss:   14.940209714286507\n",
      "Pure loss: 17.693130022386487.....Total loss: 17.693130022386487\n",
      "Pure loss: 14.859880445239067.....Total loss: 14.859880445239067\n",
      "epoch 3482 learning rate:  0.010287191269385411   Training loss:   17.693130022386487  Valing loss:   14.859880445239067\n",
      "Pure loss: 17.51559578063486.....Total loss: 17.51559578063486\n",
      "Pure loss: 14.734680900598091.....Total loss: 14.734680900598091\n",
      "epoch 3483 learning rate:  0.010287108814240598   Training loss:   17.51559578063486  Valing loss:   14.734680900598091\n",
      "Pure loss: 17.158747759929323.....Total loss: 17.158747759929323\n",
      "Pure loss: 14.514882857672111.....Total loss: 14.514882857672111\n",
      "epoch 3484 learning rate:  0.010287026406429391   Training loss:   17.158747759929323  Valing loss:   14.514882857672111\n",
      "Pure loss: 16.94095347470571.....Total loss: 16.94095347470571\n",
      "Pure loss: 14.332890683849296.....Total loss: 14.332890683849296\n",
      "epoch 3485 learning rate:  0.010286944045911047   Training loss:   16.94095347470571  Valing loss:   14.332890683849296\n",
      "Pure loss: 16.836168395468732.....Total loss: 16.836168395468732\n",
      "Pure loss: 14.241566864477498.....Total loss: 14.241566864477498\n",
      "epoch 3486 learning rate:  0.010286861732644866   Training loss:   16.836168395468732  Valing loss:   14.241566864477498\n",
      "Pure loss: 16.694107846834328.....Total loss: 16.694107846834328\n",
      "Pure loss: 14.161333328372047.....Total loss: 14.161333328372047\n",
      "epoch 3487 learning rate:  0.010286779466590193   Training loss:   16.694107846834328  Valing loss:   14.161333328372047\n",
      "Pure loss: 16.673002633462826.....Total loss: 16.673002633462826\n",
      "Pure loss: 14.147140370877617.....Total loss: 14.147140370877617\n",
      "epoch 3488 learning rate:  0.010286697247706422   Training loss:   16.673002633462826  Valing loss:   14.147140370877617\n",
      "Pure loss: 16.29746526336974.....Total loss: 16.29746526336974\n",
      "Pure loss: 14.008192848689527.....Total loss: 14.008192848689527\n",
      "epoch 3489 learning rate:  0.010286615075952995   Training loss:   16.29746526336974  Valing loss:   14.008192848689527\n",
      "Pure loss: 16.40152174957909.....Total loss: 16.40152174957909\n",
      "Pure loss: 14.024635732244976.....Total loss: 14.024635732244976\n",
      "epoch 3490 learning rate:  0.010286532951289399   Training loss:   16.40152174957909  Valing loss:   14.024635732244976\n",
      "Pure loss: 16.59371839281211.....Total loss: 16.59371839281211\n",
      "Pure loss: 14.0581465674605.....Total loss: 14.0581465674605\n",
      "epoch 3491 learning rate:  0.010286450873675164   Training loss:   16.59371839281211  Valing loss:   14.0581465674605\n",
      "Pure loss: 16.885627682914254.....Total loss: 16.885627682914254\n",
      "Pure loss: 14.15758927760428.....Total loss: 14.15758927760428\n",
      "epoch 3492 learning rate:  0.010286368843069875   Training loss:   16.885627682914254  Valing loss:   14.15758927760428\n",
      "Pure loss: 16.664744861939372.....Total loss: 16.664744861939372\n",
      "Pure loss: 14.082386623381636.....Total loss: 14.082386623381636\n",
      "epoch 3493 learning rate:  0.010286286859433152   Training loss:   16.664744861939372  Valing loss:   14.082386623381636\n",
      "Pure loss: 16.54269936807001.....Total loss: 16.54269936807001\n",
      "Pure loss: 14.055493147308457.....Total loss: 14.055493147308457\n",
      "epoch 3494 learning rate:  0.010286204922724671   Training loss:   16.54269936807001  Valing loss:   14.055493147308457\n",
      "Pure loss: 16.348916900600482.....Total loss: 16.348916900600482\n",
      "Pure loss: 14.069491047973589.....Total loss: 14.069491047973589\n",
      "epoch 3495 learning rate:  0.01028612303290415   Training loss:   16.348916900600482  Valing loss:   14.069491047973589\n",
      "Pure loss: 16.046788163745415.....Total loss: 16.046788163745415\n",
      "Pure loss: 14.217338526342342.....Total loss: 14.217338526342342\n",
      "epoch 3496 learning rate:  0.010286041189931351   Training loss:   16.046788163745415  Valing loss:   14.217338526342342\n",
      "Pure loss: 16.051268945689742.....Total loss: 16.051268945689742\n",
      "Pure loss: 14.163200691225542.....Total loss: 14.163200691225542\n",
      "epoch 3497 learning rate:  0.010285959393766085   Training loss:   16.051268945689742  Valing loss:   14.163200691225542\n",
      "Pure loss: 15.939945601223247.....Total loss: 15.939945601223247\n",
      "Pure loss: 14.475110164710618.....Total loss: 14.475110164710618\n",
      "epoch 3498 learning rate:  0.01028587764436821   Training loss:   15.939945601223247  Valing loss:   14.475110164710618\n",
      "Pure loss: 16.04697779141725.....Total loss: 16.04697779141725\n",
      "Pure loss: 15.09146567294874.....Total loss: 15.09146567294874\n",
      "epoch 3499 learning rate:  0.010285795941697628   Training loss:   16.04697779141725  Valing loss:   15.09146567294874\n",
      "Pure loss: 16.009839785399127.....Total loss: 16.009839785399127\n",
      "Pure loss: 15.021486478245182.....Total loss: 15.021486478245182\n",
      "epoch 3500 learning rate:  0.010285714285714285   Training loss:   16.009839785399127  Valing loss:   15.021486478245182\n",
      "Pure loss: 15.801026290675297.....Total loss: 15.801026290675297\n",
      "Pure loss: 14.734710585263318.....Total loss: 14.734710585263318\n",
      "epoch 3501 learning rate:  0.010285632676378179   Training loss:   15.801026290675297  Valing loss:   14.734710585263318\n",
      "Pure loss: 15.753536836845191.....Total loss: 15.753536836845191\n",
      "Pure loss: 14.614072611287918.....Total loss: 14.614072611287918\n",
      "epoch 3502 learning rate:  0.010285551113649344   Training loss:   15.753536836845191  Valing loss:   14.614072611287918\n",
      "Pure loss: 15.711328664598588.....Total loss: 15.711328664598588\n",
      "Pure loss: 14.378592882013955.....Total loss: 14.378592882013955\n",
      "epoch 3503 learning rate:  0.010285469597487868   Training loss:   15.711328664598588  Valing loss:   14.378592882013955\n",
      "Pure loss: 15.787596595502725.....Total loss: 15.787596595502725\n",
      "Pure loss: 14.51449759066378.....Total loss: 14.51449759066378\n",
      "epoch 3504 learning rate:  0.010285388127853881   Training loss:   15.787596595502725  Valing loss:   14.51449759066378\n",
      "Pure loss: 15.743292117778784.....Total loss: 15.743292117778784\n",
      "Pure loss: 14.202958475052906.....Total loss: 14.202958475052906\n",
      "epoch 3505 learning rate:  0.01028530670470756   Training loss:   15.743292117778784  Valing loss:   14.202958475052906\n",
      "Pure loss: 15.82665719442804.....Total loss: 15.82665719442804\n",
      "Pure loss: 14.362274652974252.....Total loss: 14.362274652974252\n",
      "epoch 3506 learning rate:  0.010285225328009127   Training loss:   15.82665719442804  Valing loss:   14.362274652974252\n",
      "Pure loss: 15.826887193634834.....Total loss: 15.826887193634834\n",
      "Pure loss: 14.14741040665823.....Total loss: 14.14741040665823\n",
      "epoch 3507 learning rate:  0.010285143997718849   Training loss:   15.826887193634834  Valing loss:   14.14741040665823\n",
      "Pure loss: 15.893443910301631.....Total loss: 15.893443910301631\n",
      "Pure loss: 14.40004189154319.....Total loss: 14.40004189154319\n",
      "epoch 3508 learning rate:  0.010285062713797036   Training loss:   15.893443910301631  Valing loss:   14.40004189154319\n",
      "Pure loss: 15.897859674982913.....Total loss: 15.897859674982913\n",
      "Pure loss: 14.421899906648834.....Total loss: 14.421899906648834\n",
      "epoch 3509 learning rate:  0.010284981476204047   Training loss:   15.897859674982913  Valing loss:   14.421899906648834\n",
      "Pure loss: 15.992316421198742.....Total loss: 15.992316421198742\n",
      "Pure loss: 14.886302266739962.....Total loss: 14.886302266739962\n",
      "epoch 3510 learning rate:  0.010284900284900285   Training loss:   15.992316421198742  Valing loss:   14.886302266739962\n",
      "Pure loss: 15.867513601873902.....Total loss: 15.867513601873902\n",
      "Pure loss: 14.382173787219678.....Total loss: 14.382173787219678\n",
      "epoch 3511 learning rate:  0.010284819139846198   Training loss:   15.867513601873902  Valing loss:   14.382173787219678\n",
      "Pure loss: 15.86685661886316.....Total loss: 15.86685661886316\n",
      "Pure loss: 14.27288826654749.....Total loss: 14.27288826654749\n",
      "epoch 3512 learning rate:  0.010284738041002278   Training loss:   15.86685661886316  Valing loss:   14.27288826654749\n",
      "Pure loss: 15.86589138928754.....Total loss: 15.86589138928754\n",
      "Pure loss: 14.388237647602287.....Total loss: 14.388237647602287\n",
      "epoch 3513 learning rate:  0.010284656988329064   Training loss:   15.86589138928754  Valing loss:   14.388237647602287\n",
      "Pure loss: 15.759560240854361.....Total loss: 15.759560240854361\n",
      "Pure loss: 14.120256762120565.....Total loss: 14.120256762120565\n",
      "epoch 3514 learning rate:  0.010284575981787137   Training loss:   15.759560240854361  Valing loss:   14.120256762120565\n",
      "Pure loss: 15.905317995440914.....Total loss: 15.905317995440914\n",
      "Pure loss: 13.972180701877615.....Total loss: 13.972180701877615\n",
      "epoch 3515 learning rate:  0.010284495021337128   Training loss:   15.905317995440914  Valing loss:   13.972180701877615\n",
      "Pure loss: 15.986039278548004.....Total loss: 15.986039278548004\n",
      "Pure loss: 13.932730734292367.....Total loss: 13.932730734292367\n",
      "epoch 3516 learning rate:  0.010284414106939704   Training loss:   15.986039278548004  Valing loss:   13.932730734292367\n",
      "Pure loss: 16.111692865457403.....Total loss: 16.111692865457403\n",
      "Pure loss: 13.916545923419838.....Total loss: 13.916545923419838\n",
      "epoch 3517 learning rate:  0.010284333238555588   Training loss:   16.111692865457403  Valing loss:   13.916545923419838\n",
      "Pure loss: 16.138332644862487.....Total loss: 16.138332644862487\n",
      "Pure loss: 13.899306860291249.....Total loss: 13.899306860291249\n",
      "epoch 3518 learning rate:  0.010284252416145537   Training loss:   16.138332644862487  Valing loss:   13.899306860291249\n",
      "Pure loss: 16.30944972167951.....Total loss: 16.30944972167951\n",
      "Pure loss: 13.923546439301028.....Total loss: 13.923546439301028\n",
      "epoch 3519 learning rate:  0.010284171639670362   Training loss:   16.30944972167951  Valing loss:   13.923546439301028\n",
      "Pure loss: 16.290989733709043.....Total loss: 16.290989733709043\n",
      "Pure loss: 13.930458401518486.....Total loss: 13.930458401518486\n",
      "epoch 3520 learning rate:  0.01028409090909091   Training loss:   16.290989733709043  Valing loss:   13.930458401518486\n",
      "Pure loss: 15.920321074671978.....Total loss: 15.920321074671978\n",
      "Pure loss: 14.082168631224361.....Total loss: 14.082168631224361\n",
      "epoch 3521 learning rate:  0.010284010224368078   Training loss:   15.920321074671978  Valing loss:   14.082168631224361\n",
      "Pure loss: 15.98633872871015.....Total loss: 15.98633872871015\n",
      "Pure loss: 13.966221156075147.....Total loss: 13.966221156075147\n",
      "epoch 3522 learning rate:  0.010283929585462806   Training loss:   15.98633872871015  Valing loss:   13.966221156075147\n",
      "Pure loss: 15.960791926717144.....Total loss: 15.960791926717144\n",
      "Pure loss: 14.01867421552811.....Total loss: 14.01867421552811\n",
      "epoch 3523 learning rate:  0.010283848992336078   Training loss:   15.960791926717144  Valing loss:   14.01867421552811\n",
      "Pure loss: 15.952291755449252.....Total loss: 15.952291755449252\n",
      "Pure loss: 14.03364957842449.....Total loss: 14.03364957842449\n",
      "epoch 3524 learning rate:  0.010283768444948922   Training loss:   15.952291755449252  Valing loss:   14.03364957842449\n",
      "Pure loss: 15.970573297141762.....Total loss: 15.970573297141762\n",
      "Pure loss: 14.015676154244865.....Total loss: 14.015676154244865\n",
      "epoch 3525 learning rate:  0.010283687943262412   Training loss:   15.970573297141762  Valing loss:   14.015676154244865\n",
      "Pure loss: 15.894733574316875.....Total loss: 15.894733574316875\n",
      "Pure loss: 14.112662672510885.....Total loss: 14.112662672510885\n",
      "epoch 3526 learning rate:  0.010283607487237664   Training loss:   15.894733574316875  Valing loss:   14.112662672510885\n",
      "Pure loss: 15.866171389574694.....Total loss: 15.866171389574694\n",
      "Pure loss: 14.350489635164736.....Total loss: 14.350489635164736\n",
      "epoch 3527 learning rate:  0.010283527076835839   Training loss:   15.866171389574694  Valing loss:   14.350489635164736\n",
      "Pure loss: 15.865539547582276.....Total loss: 15.865539547582276\n",
      "Pure loss: 14.287853481185248.....Total loss: 14.287853481185248\n",
      "epoch 3528 learning rate:  0.01028344671201814   Training loss:   15.865539547582276  Valing loss:   14.287853481185248\n",
      "Pure loss: 15.937182972508019.....Total loss: 15.937182972508019\n",
      "Pure loss: 14.404658470519788.....Total loss: 14.404658470519788\n",
      "epoch 3529 learning rate:  0.01028336639274582   Training loss:   15.937182972508019  Valing loss:   14.404658470519788\n",
      "Pure loss: 15.891865328379675.....Total loss: 15.891865328379675\n",
      "Pure loss: 14.152931555750072.....Total loss: 14.152931555750072\n",
      "epoch 3530 learning rate:  0.01028328611898017   Training loss:   15.891865328379675  Valing loss:   14.152931555750072\n",
      "Pure loss: 16.18195701908831.....Total loss: 16.18195701908831\n",
      "Pure loss: 14.58098991993622.....Total loss: 14.58098991993622\n",
      "epoch 3531 learning rate:  0.010283205890682526   Training loss:   16.18195701908831  Valing loss:   14.58098991993622\n",
      "Pure loss: 16.286303160830364.....Total loss: 16.286303160830364\n",
      "Pure loss: 15.276121382273917.....Total loss: 15.276121382273917\n",
      "epoch 3532 learning rate:  0.01028312570781427   Training loss:   16.286303160830364  Valing loss:   15.276121382273917\n",
      "Pure loss: 16.590059680647382.....Total loss: 16.590059680647382\n",
      "Pure loss: 15.91118963190819.....Total loss: 15.91118963190819\n",
      "epoch 3533 learning rate:  0.010283045570336824   Training loss:   16.590059680647382  Valing loss:   15.91118963190819\n",
      "Pure loss: 16.40634716499139.....Total loss: 16.40634716499139\n",
      "Pure loss: 15.589045072253748.....Total loss: 15.589045072253748\n",
      "epoch 3534 learning rate:  0.010282965478211658   Training loss:   16.40634716499139  Valing loss:   15.589045072253748\n",
      "Pure loss: 16.41563396882614.....Total loss: 16.41563396882614\n",
      "Pure loss: 15.605674727803251.....Total loss: 15.605674727803251\n",
      "epoch 3535 learning rate:  0.010282885431400284   Training loss:   16.41563396882614  Valing loss:   15.605674727803251\n",
      "Pure loss: 16.125392820223276.....Total loss: 16.125392820223276\n",
      "Pure loss: 15.257185066491129.....Total loss: 15.257185066491129\n",
      "epoch 3536 learning rate:  0.010282805429864254   Training loss:   16.125392820223276  Valing loss:   15.257185066491129\n",
      "Pure loss: 16.204064320935025.....Total loss: 16.204064320935025\n",
      "Pure loss: 15.44753385395227.....Total loss: 15.44753385395227\n",
      "epoch 3537 learning rate:  0.01028272547356517   Training loss:   16.204064320935025  Valing loss:   15.44753385395227\n",
      "Pure loss: 15.998163915590235.....Total loss: 15.998163915590235\n",
      "Pure loss: 15.065591783256421.....Total loss: 15.065591783256421\n",
      "epoch 3538 learning rate:  0.01028264556246467   Training loss:   15.998163915590235  Valing loss:   15.065591783256421\n",
      "Pure loss: 16.044654867096405.....Total loss: 16.044654867096405\n",
      "Pure loss: 15.222240566036326.....Total loss: 15.222240566036326\n",
      "epoch 3539 learning rate:  0.010282565696524443   Training loss:   16.044654867096405  Valing loss:   15.222240566036326\n",
      "Pure loss: 15.931942846669855.....Total loss: 15.931942846669855\n",
      "Pure loss: 14.974295458078265.....Total loss: 14.974295458078265\n",
      "epoch 3540 learning rate:  0.010282485875706215   Training loss:   15.931942846669855  Valing loss:   14.974295458078265\n",
      "Pure loss: 16.098909911304307.....Total loss: 16.098909911304307\n",
      "Pure loss: 15.341205127508093.....Total loss: 15.341205127508093\n",
      "epoch 3541 learning rate:  0.01028240609997176   Training loss:   16.098909911304307  Valing loss:   15.341205127508093\n",
      "Pure loss: 16.339547684768494.....Total loss: 16.339547684768494\n",
      "Pure loss: 15.66880242367927.....Total loss: 15.66880242367927\n",
      "epoch 3542 learning rate:  0.01028232636928289   Training loss:   16.339547684768494  Valing loss:   15.66880242367927\n",
      "Pure loss: 15.929301338360872.....Total loss: 15.929301338360872\n",
      "Pure loss: 15.084417258768257.....Total loss: 15.084417258768257\n",
      "epoch 3543 learning rate:  0.010282246683601467   Training loss:   15.929301338360872  Valing loss:   15.084417258768257\n",
      "Pure loss: 15.848902096305714.....Total loss: 15.848902096305714\n",
      "Pure loss: 14.899400978743472.....Total loss: 14.899400978743472\n",
      "epoch 3544 learning rate:  0.01028216704288939   Training loss:   15.848902096305714  Valing loss:   14.899400978743472\n",
      "Pure loss: 15.756348362858995.....Total loss: 15.756348362858995\n",
      "Pure loss: 14.665663072619028.....Total loss: 14.665663072619028\n",
      "epoch 3545 learning rate:  0.010282087447108604   Training loss:   15.756348362858995  Valing loss:   14.665663072619028\n",
      "Pure loss: 15.769641481561498.....Total loss: 15.769641481561498\n",
      "Pure loss: 14.690144662747889.....Total loss: 14.690144662747889\n",
      "epoch 3546 learning rate:  0.010282007896221095   Training loss:   15.769641481561498  Valing loss:   14.690144662747889\n",
      "Pure loss: 15.773808605978964.....Total loss: 15.773808605978964\n",
      "Pure loss: 14.696127252716462.....Total loss: 14.696127252716462\n",
      "epoch 3547 learning rate:  0.010281928390188893   Training loss:   15.773808605978964  Valing loss:   14.696127252716462\n",
      "Pure loss: 15.805896160480644.....Total loss: 15.805896160480644\n",
      "Pure loss: 14.752584436031627.....Total loss: 14.752584436031627\n",
      "epoch 3548 learning rate:  0.010281848928974071   Training loss:   15.805896160480644  Valing loss:   14.752584436031627\n",
      "Pure loss: 15.828400378046684.....Total loss: 15.828400378046684\n",
      "Pure loss: 14.813618349466038.....Total loss: 14.813618349466038\n",
      "epoch 3549 learning rate:  0.010281769512538744   Training loss:   15.828400378046684  Valing loss:   14.813618349466038\n",
      "Pure loss: 15.98961048960011.....Total loss: 15.98961048960011\n",
      "Pure loss: 15.179372041485452.....Total loss: 15.179372041485452\n",
      "epoch 3550 learning rate:  0.010281690140845071   Training loss:   15.98961048960011  Valing loss:   15.179372041485452\n",
      "Pure loss: 16.10478603921304.....Total loss: 16.10478603921304\n",
      "Pure loss: 15.413195740006898.....Total loss: 15.413195740006898\n",
      "epoch 3551 learning rate:  0.010281610813855252   Training loss:   16.10478603921304  Valing loss:   15.413195740006898\n",
      "Pure loss: 15.937831600320685.....Total loss: 15.937831600320685\n",
      "Pure loss: 15.087916913686888.....Total loss: 15.087916913686888\n",
      "epoch 3552 learning rate:  0.010281531531531532   Training loss:   15.937831600320685  Valing loss:   15.087916913686888\n",
      "Pure loss: 15.818485516276086.....Total loss: 15.818485516276086\n",
      "Pure loss: 14.95363504491053.....Total loss: 14.95363504491053\n",
      "epoch 3553 learning rate:  0.010281452293836195   Training loss:   15.818485516276086  Valing loss:   14.95363504491053\n",
      "Pure loss: 15.677234497388254.....Total loss: 15.677234497388254\n",
      "Pure loss: 14.603711151252883.....Total loss: 14.603711151252883\n",
      "epoch 3554 learning rate:  0.01028137310073157   Training loss:   15.677234497388254  Valing loss:   14.603711151252883\n",
      "Pure loss: 15.6544128994723.....Total loss: 15.6544128994723\n",
      "Pure loss: 14.567364600517232.....Total loss: 14.567364600517232\n",
      "epoch 3555 learning rate:  0.010281293952180028   Training loss:   15.6544128994723  Valing loss:   14.567364600517232\n",
      "Pure loss: 15.730664734403883.....Total loss: 15.730664734403883\n",
      "Pure loss: 14.730844185369078.....Total loss: 14.730844185369078\n",
      "epoch 3556 learning rate:  0.010281214848143982   Training loss:   15.730664734403883  Valing loss:   14.730844185369078\n",
      "Pure loss: 15.737548719635003.....Total loss: 15.737548719635003\n",
      "Pure loss: 14.746531754929132.....Total loss: 14.746531754929132\n",
      "epoch 3557 learning rate:  0.010281135788585888   Training loss:   15.737548719635003  Valing loss:   14.746531754929132\n",
      "Pure loss: 15.577660346717233.....Total loss: 15.577660346717233\n",
      "Pure loss: 14.514198891340348.....Total loss: 14.514198891340348\n",
      "epoch 3558 learning rate:  0.010281056773468241   Training loss:   15.577660346717233  Valing loss:   14.514198891340348\n",
      "Pure loss: 15.543355790800113.....Total loss: 15.543355790800113\n",
      "Pure loss: 14.42866744936112.....Total loss: 14.42866744936112\n",
      "epoch 3559 learning rate:  0.010280977802753583   Training loss:   15.543355790800113  Valing loss:   14.42866744936112\n",
      "Pure loss: 15.542643066884368.....Total loss: 15.542643066884368\n",
      "Pure loss: 14.426898299976997.....Total loss: 14.426898299976997\n",
      "epoch 3560 learning rate:  0.010280898876404494   Training loss:   15.542643066884368  Valing loss:   14.426898299976997\n",
      "Pure loss: 15.479422064531327.....Total loss: 15.479422064531327\n",
      "Pure loss: 14.3264374412415.....Total loss: 14.3264374412415\n",
      "epoch 3561 learning rate:  0.0102808199943836   Training loss:   15.479422064531327  Valing loss:   14.3264374412415\n",
      "Pure loss: 15.481656384397612.....Total loss: 15.481656384397612\n",
      "Pure loss: 14.212956238725372.....Total loss: 14.212956238725372\n",
      "epoch 3562 learning rate:  0.010280741156653566   Training loss:   15.481656384397612  Valing loss:   14.212956238725372\n",
      "Pure loss: 15.479022607798353.....Total loss: 15.479022607798353\n",
      "Pure loss: 14.32608123379812.....Total loss: 14.32608123379812\n",
      "epoch 3563 learning rate:  0.010280662363177099   Training loss:   15.479022607798353  Valing loss:   14.32608123379812\n",
      "Pure loss: 15.473577309356719.....Total loss: 15.473577309356719\n",
      "Pure loss: 14.26755511474602.....Total loss: 14.26755511474602\n",
      "epoch 3564 learning rate:  0.010280583613916948   Training loss:   15.473577309356719  Valing loss:   14.26755511474602\n",
      "Pure loss: 15.475039615772141.....Total loss: 15.475039615772141\n",
      "Pure loss: 14.146503244243146.....Total loss: 14.146503244243146\n",
      "epoch 3565 learning rate:  0.010280504908835905   Training loss:   15.475039615772141  Valing loss:   14.146503244243146\n",
      "Pure loss: 15.465231553417928.....Total loss: 15.465231553417928\n",
      "Pure loss: 14.184685689849898.....Total loss: 14.184685689849898\n",
      "epoch 3566 learning rate:  0.010280426247896803   Training loss:   15.465231553417928  Valing loss:   14.184685689849898\n",
      "Pure loss: 15.464856298873398.....Total loss: 15.464856298873398\n",
      "Pure loss: 14.185864539040008.....Total loss: 14.185864539040008\n",
      "epoch 3567 learning rate:  0.010280347631062517   Training loss:   15.464856298873398  Valing loss:   14.185864539040008\n",
      "Pure loss: 15.476564172399907.....Total loss: 15.476564172399907\n",
      "Pure loss: 14.165591758658401.....Total loss: 14.165591758658401\n",
      "epoch 3568 learning rate:  0.010280269058295965   Training loss:   15.476564172399907  Valing loss:   14.165591758658401\n",
      "Pure loss: 15.527517565643368.....Total loss: 15.527517565643368\n",
      "Pure loss: 14.05359712644167.....Total loss: 14.05359712644167\n",
      "epoch 3569 learning rate:  0.0102801905295601   Training loss:   15.527517565643368  Valing loss:   14.05359712644167\n",
      "Pure loss: 15.524325001666252.....Total loss: 15.524325001666252\n",
      "Pure loss: 14.05545765760035.....Total loss: 14.05545765760035\n",
      "epoch 3570 learning rate:  0.010280112044817928   Training loss:   15.524325001666252  Valing loss:   14.05545765760035\n",
      "Pure loss: 15.522899145429518.....Total loss: 15.522899145429518\n",
      "Pure loss: 14.05923210484293.....Total loss: 14.05923210484293\n",
      "epoch 3571 learning rate:  0.010280033604032485   Training loss:   15.522899145429518  Valing loss:   14.05923210484293\n",
      "Pure loss: 15.640602039798608.....Total loss: 15.640602039798608\n",
      "Pure loss: 14.078317224898443.....Total loss: 14.078317224898443\n",
      "epoch 3572 learning rate:  0.010279955207166853   Training loss:   15.640602039798608  Valing loss:   14.078317224898443\n",
      "Pure loss: 15.577455619033694.....Total loss: 15.577455619033694\n",
      "Pure loss: 14.051113355481696.....Total loss: 14.051113355481696\n",
      "epoch 3573 learning rate:  0.01027987685418416   Training loss:   15.577455619033694  Valing loss:   14.051113355481696\n",
      "Pure loss: 15.693393459832642.....Total loss: 15.693393459832642\n",
      "Pure loss: 13.999571333584218.....Total loss: 13.999571333584218\n",
      "epoch 3574 learning rate:  0.010279798545047566   Training loss:   15.693393459832642  Valing loss:   13.999571333584218\n",
      "Pure loss: 15.547184312441042.....Total loss: 15.547184312441042\n",
      "Pure loss: 14.153175718207025.....Total loss: 14.153175718207025\n",
      "epoch 3575 learning rate:  0.01027972027972028   Training loss:   15.547184312441042  Valing loss:   14.153175718207025\n",
      "Pure loss: 15.59477666272803.....Total loss: 15.59477666272803\n",
      "Pure loss: 14.09362066376954.....Total loss: 14.09362066376954\n",
      "epoch 3576 learning rate:  0.010279642058165549   Training loss:   15.59477666272803  Valing loss:   14.09362066376954\n",
      "Pure loss: 15.734704869913301.....Total loss: 15.734704869913301\n",
      "Pure loss: 14.029143952825457.....Total loss: 14.029143952825457\n",
      "epoch 3577 learning rate:  0.010279563880346659   Training loss:   15.734704869913301  Valing loss:   14.029143952825457\n",
      "Pure loss: 15.744232387198958.....Total loss: 15.744232387198958\n",
      "Pure loss: 14.032103182686848.....Total loss: 14.032103182686848\n",
      "epoch 3578 learning rate:  0.010279485746226943   Training loss:   15.744232387198958  Valing loss:   14.032103182686848\n",
      "Pure loss: 15.770128259668319.....Total loss: 15.770128259668319\n",
      "Pure loss: 14.029687889598993.....Total loss: 14.029687889598993\n",
      "epoch 3579 learning rate:  0.01027940765576977   Training loss:   15.770128259668319  Valing loss:   14.029687889598993\n",
      "Pure loss: 15.715118073713738.....Total loss: 15.715118073713738\n",
      "Pure loss: 14.074570176359558.....Total loss: 14.074570176359558\n",
      "epoch 3580 learning rate:  0.010279329608938547   Training loss:   15.715118073713738  Valing loss:   14.074570176359558\n",
      "Pure loss: 15.906241739116343.....Total loss: 15.906241739116343\n",
      "Pure loss: 14.07899888458393.....Total loss: 14.07899888458393\n",
      "epoch 3581 learning rate:  0.010279251605696734   Training loss:   15.906241739116343  Valing loss:   14.07899888458393\n",
      "Pure loss: 15.577920483608635.....Total loss: 15.577920483608635\n",
      "Pure loss: 14.02564441051701.....Total loss: 14.02564441051701\n",
      "epoch 3582 learning rate:  0.010279173646007818   Training loss:   15.577920483608635  Valing loss:   14.02564441051701\n",
      "Pure loss: 15.578922548239108.....Total loss: 15.578922548239108\n",
      "Pure loss: 14.025959972061129.....Total loss: 14.025959972061129\n",
      "epoch 3583 learning rate:  0.010279095729835333   Training loss:   15.578922548239108  Valing loss:   14.025959972061129\n",
      "Pure loss: 15.51312519885148.....Total loss: 15.51312519885148\n",
      "Pure loss: 14.077515001037844.....Total loss: 14.077515001037844\n",
      "epoch 3584 learning rate:  0.010279017857142858   Training loss:   15.51312519885148  Valing loss:   14.077515001037844\n",
      "Pure loss: 15.626714835872864.....Total loss: 15.626714835872864\n",
      "Pure loss: 14.143015461395047.....Total loss: 14.143015461395047\n",
      "epoch 3585 learning rate:  0.010278940027894003   Training loss:   15.626714835872864  Valing loss:   14.143015461395047\n",
      "Pure loss: 15.730264124641078.....Total loss: 15.730264124641078\n",
      "Pure loss: 14.09865649957297.....Total loss: 14.09865649957297\n",
      "epoch 3586 learning rate:  0.010278862242052426   Training loss:   15.730264124641078  Valing loss:   14.09865649957297\n",
      "Pure loss: 15.783992571111506.....Total loss: 15.783992571111506\n",
      "Pure loss: 14.101715200739951.....Total loss: 14.101715200739951\n",
      "epoch 3587 learning rate:  0.010278784499581823   Training loss:   15.783992571111506  Valing loss:   14.101715200739951\n",
      "Pure loss: 15.763405311208444.....Total loss: 15.763405311208444\n",
      "Pure loss: 14.109006297578226.....Total loss: 14.109006297578226\n",
      "epoch 3588 learning rate:  0.01027870680044593   Training loss:   15.763405311208444  Valing loss:   14.109006297578226\n",
      "Pure loss: 15.66687184095081.....Total loss: 15.66687184095081\n",
      "Pure loss: 14.065899539586761.....Total loss: 14.065899539586761\n",
      "epoch 3589 learning rate:  0.010278629144608526   Training loss:   15.66687184095081  Valing loss:   14.065899539586761\n",
      "Pure loss: 15.82868627683268.....Total loss: 15.82868627683268\n",
      "Pure loss: 13.926987391051345.....Total loss: 13.926987391051345\n",
      "epoch 3590 learning rate:  0.010278551532033426   Training loss:   15.82868627683268  Valing loss:   13.926987391051345\n",
      "Pure loss: 15.536985613360805.....Total loss: 15.536985613360805\n",
      "Pure loss: 14.006202328064745.....Total loss: 14.006202328064745\n",
      "epoch 3591 learning rate:  0.01027847396268449   Training loss:   15.536985613360805  Valing loss:   14.006202328064745\n",
      "Pure loss: 15.538092492952979.....Total loss: 15.538092492952979\n",
      "Pure loss: 14.00653635216328.....Total loss: 14.00653635216328\n",
      "epoch 3592 learning rate:  0.010278396436525613   Training loss:   15.538092492952979  Valing loss:   14.00653635216328\n",
      "Pure loss: 15.645400291576665.....Total loss: 15.645400291576665\n",
      "Pure loss: 14.057805494516257.....Total loss: 14.057805494516257\n",
      "epoch 3593 learning rate:  0.010278318953520734   Training loss:   15.645400291576665  Valing loss:   14.057805494516257\n",
      "Pure loss: 16.045818998942927.....Total loss: 16.045818998942927\n",
      "Pure loss: 14.054174922579524.....Total loss: 14.054174922579524\n",
      "epoch 3594 learning rate:  0.010278241513633834   Training loss:   16.045818998942927  Valing loss:   14.054174922579524\n",
      "Pure loss: 16.098101604517666.....Total loss: 16.098101604517666\n",
      "Pure loss: 14.067052128233163.....Total loss: 14.067052128233163\n",
      "epoch 3595 learning rate:  0.01027816411682893   Training loss:   16.098101604517666  Valing loss:   14.067052128233163\n",
      "Pure loss: 15.917887617900101.....Total loss: 15.917887617900101\n",
      "Pure loss: 13.995459338655511.....Total loss: 13.995459338655511\n",
      "epoch 3596 learning rate:  0.010278086763070078   Training loss:   15.917887617900101  Valing loss:   13.995459338655511\n",
      "Pure loss: 16.358046860355504.....Total loss: 16.358046860355504\n",
      "Pure loss: 14.105150068931216.....Total loss: 14.105150068931216\n",
      "epoch 3597 learning rate:  0.01027800945232138   Training loss:   16.358046860355504  Valing loss:   14.105150068931216\n",
      "Pure loss: 16.20407643526234.....Total loss: 16.20407643526234\n",
      "Pure loss: 14.058287746849004.....Total loss: 14.058287746849004\n",
      "epoch 3598 learning rate:  0.010277932184546971   Training loss:   16.20407643526234  Valing loss:   14.058287746849004\n",
      "Pure loss: 16.16211289095317.....Total loss: 16.16211289095317\n",
      "Pure loss: 14.027417849080592.....Total loss: 14.027417849080592\n",
      "epoch 3599 learning rate:  0.01027785495971103   Training loss:   16.16211289095317  Valing loss:   14.027417849080592\n",
      "Pure loss: 16.3044610822382.....Total loss: 16.3044610822382\n",
      "Pure loss: 14.077146008239467.....Total loss: 14.077146008239467\n",
      "epoch 3600 learning rate:  0.010277777777777778   Training loss:   16.3044610822382  Valing loss:   14.077146008239467\n",
      "Pure loss: 16.19486412374728.....Total loss: 16.19486412374728\n",
      "Pure loss: 14.007954694027775.....Total loss: 14.007954694027775\n",
      "epoch 3601 learning rate:  0.01027770063871147   Training loss:   16.19486412374728  Valing loss:   14.007954694027775\n",
      "Pure loss: 16.45111603469529.....Total loss: 16.45111603469529\n",
      "Pure loss: 14.14943811785953.....Total loss: 14.14943811785953\n",
      "epoch 3602 learning rate:  0.010277623542476402   Training loss:   16.45111603469529  Valing loss:   14.14943811785953\n",
      "Pure loss: 16.907736141329124.....Total loss: 16.907736141329124\n",
      "Pure loss: 14.31602528078514.....Total loss: 14.31602528078514\n",
      "epoch 3603 learning rate:  0.010277546489036914   Training loss:   16.907736141329124  Valing loss:   14.31602528078514\n",
      "Pure loss: 16.634648585141466.....Total loss: 16.634648585141466\n",
      "Pure loss: 14.195364631436528.....Total loss: 14.195364631436528\n",
      "epoch 3604 learning rate:  0.01027746947835738   Training loss:   16.634648585141466  Valing loss:   14.195364631436528\n",
      "Pure loss: 16.946417268220213.....Total loss: 16.946417268220213\n",
      "Pure loss: 14.369640531039675.....Total loss: 14.369640531039675\n",
      "epoch 3605 learning rate:  0.010277392510402219   Training loss:   16.946417268220213  Valing loss:   14.369640531039675\n",
      "Pure loss: 16.450243699008762.....Total loss: 16.450243699008762\n",
      "Pure loss: 14.099546562001752.....Total loss: 14.099546562001752\n",
      "epoch 3606 learning rate:  0.010277315585135885   Training loss:   16.450243699008762  Valing loss:   14.099546562001752\n",
      "Pure loss: 16.244556255350464.....Total loss: 16.244556255350464\n",
      "Pure loss: 13.952431500134258.....Total loss: 13.952431500134258\n",
      "epoch 3607 learning rate:  0.010277238702522873   Training loss:   16.244556255350464  Valing loss:   13.952431500134258\n",
      "Pure loss: 16.173228782718013.....Total loss: 16.173228782718013\n",
      "Pure loss: 13.919137842805702.....Total loss: 13.919137842805702\n",
      "epoch 3608 learning rate:  0.010277161862527716   Training loss:   16.173228782718013  Valing loss:   13.919137842805702\n",
      "Pure loss: 16.734031465456503.....Total loss: 16.734031465456503\n",
      "Pure loss: 14.08016256245665.....Total loss: 14.08016256245665\n",
      "epoch 3609 learning rate:  0.01027708506511499   Training loss:   16.734031465456503  Valing loss:   14.08016256245665\n",
      "Pure loss: 16.720753604671117.....Total loss: 16.720753604671117\n",
      "Pure loss: 14.073654116425038.....Total loss: 14.073654116425038\n",
      "epoch 3610 learning rate:  0.010277008310249308   Training loss:   16.720753604671117  Valing loss:   14.073654116425038\n",
      "Pure loss: 16.607971723137037.....Total loss: 16.607971723137037\n",
      "Pure loss: 14.008991557844029.....Total loss: 14.008991557844029\n",
      "epoch 3611 learning rate:  0.01027693159789532   Training loss:   16.607971723137037  Valing loss:   14.008991557844029\n",
      "Pure loss: 16.842040437006514.....Total loss: 16.842040437006514\n",
      "Pure loss: 14.16097214805689.....Total loss: 14.16097214805689\n",
      "epoch 3612 learning rate:  0.010276854928017718   Training loss:   16.842040437006514  Valing loss:   14.16097214805689\n",
      "Pure loss: 16.119889249918813.....Total loss: 16.119889249918813\n",
      "Pure loss: 13.656537111698754.....Total loss: 13.656537111698754\n",
      "epoch 3613 learning rate:  0.010276778300581234   Training loss:   16.119889249918813  Valing loss:   13.656537111698754\n",
      "Pure loss: 16.165210337054013.....Total loss: 16.165210337054013\n",
      "Pure loss: 13.701393823773994.....Total loss: 13.701393823773994\n",
      "epoch 3614 learning rate:  0.010276701715550637   Training loss:   16.165210337054013  Valing loss:   13.701393823773994\n",
      "Pure loss: 15.757415014555134.....Total loss: 15.757415014555134\n",
      "Pure loss: 13.619537905248173.....Total loss: 13.619537905248173\n",
      "epoch 3615 learning rate:  0.010276625172890734   Training loss:   15.757415014555134  Valing loss:   13.619537905248173\n",
      "Pure loss: 16.036299701031282.....Total loss: 16.036299701031282\n",
      "Pure loss: 13.695269715479983.....Total loss: 13.695269715479983\n",
      "epoch 3616 learning rate:  0.010276548672566372   Training loss:   16.036299701031282  Valing loss:   13.695269715479983\n",
      "Pure loss: 16.059888896300173.....Total loss: 16.059888896300173\n",
      "Pure loss: 13.69992534294074.....Total loss: 13.69992534294074\n",
      "epoch 3617 learning rate:  0.01027647221454244   Training loss:   16.059888896300173  Valing loss:   13.69992534294074\n",
      "Pure loss: 15.938639811693756.....Total loss: 15.938639811693756\n",
      "Pure loss: 13.673533193587543.....Total loss: 13.673533193587543\n",
      "epoch 3618 learning rate:  0.01027639579878386   Training loss:   15.938639811693756  Valing loss:   13.673533193587543\n",
      "Pure loss: 16.111654560052717.....Total loss: 16.111654560052717\n",
      "Pure loss: 13.74409171224794.....Total loss: 13.74409171224794\n",
      "epoch 3619 learning rate:  0.010276319425255595   Training loss:   16.111654560052717  Valing loss:   13.74409171224794\n",
      "Pure loss: 16.162558770235034.....Total loss: 16.162558770235034\n",
      "Pure loss: 13.7728905065177.....Total loss: 13.7728905065177\n",
      "epoch 3620 learning rate:  0.010276243093922652   Training loss:   16.162558770235034  Valing loss:   13.7728905065177\n",
      "Pure loss: 15.780034408396169.....Total loss: 15.780034408396169\n",
      "Pure loss: 13.577881194987802.....Total loss: 13.577881194987802\n",
      "epoch 3621 learning rate:  0.010276166804750068   Training loss:   15.780034408396169  Valing loss:   13.577881194987802\n",
      "Pure loss: 15.734351264104621.....Total loss: 15.734351264104621\n",
      "Pure loss: 13.567591389819649.....Total loss: 13.567591389819649\n",
      "epoch 3622 learning rate:  0.010276090557702927   Training loss:   15.734351264104621  Valing loss:   13.567591389819649\n",
      "Pure loss: 15.647353978557495.....Total loss: 15.647353978557495\n",
      "Pure loss: 13.560155422107988.....Total loss: 13.560155422107988\n",
      "epoch 3623 learning rate:  0.010276014352746343   Training loss:   15.647353978557495  Valing loss:   13.560155422107988\n",
      "Pure loss: 15.451654521324805.....Total loss: 15.451654521324805\n",
      "Pure loss: 13.626055935788049.....Total loss: 13.626055935788049\n",
      "epoch 3624 learning rate:  0.010275938189845474   Training loss:   15.451654521324805  Valing loss:   13.626055935788049\n",
      "Pure loss: 15.538157214037614.....Total loss: 15.538157214037614\n",
      "Pure loss: 13.575675509326585.....Total loss: 13.575675509326585\n",
      "epoch 3625 learning rate:  0.010275862068965518   Training loss:   15.538157214037614  Valing loss:   13.575675509326585\n",
      "Pure loss: 15.528279474636511.....Total loss: 15.528279474636511\n",
      "Pure loss: 13.576697866437755.....Total loss: 13.576697866437755\n",
      "epoch 3626 learning rate:  0.010275785990071704   Training loss:   15.528279474636511  Valing loss:   13.576697866437755\n",
      "Pure loss: 15.642903667967825.....Total loss: 15.642903667967825\n",
      "Pure loss: 13.57886573648384.....Total loss: 13.57886573648384\n",
      "epoch 3627 learning rate:  0.010275709953129309   Training loss:   15.642903667967825  Valing loss:   13.57886573648384\n",
      "Pure loss: 15.618442397774155.....Total loss: 15.618442397774155\n",
      "Pure loss: 13.575693188674624.....Total loss: 13.575693188674624\n",
      "epoch 3628 learning rate:  0.010275633958103639   Training loss:   15.618442397774155  Valing loss:   13.575693188674624\n",
      "Pure loss: 15.747246597660965.....Total loss: 15.747246597660965\n",
      "Pure loss: 13.59640734570961.....Total loss: 13.59640734570961\n",
      "epoch 3629 learning rate:  0.010275558004960045   Training loss:   15.747246597660965  Valing loss:   13.59640734570961\n",
      "Pure loss: 15.724910558603124.....Total loss: 15.724910558603124\n",
      "Pure loss: 13.591210069029227.....Total loss: 13.591210069029227\n",
      "epoch 3630 learning rate:  0.010275482093663912   Training loss:   15.724910558603124  Valing loss:   13.591210069029227\n",
      "Pure loss: 15.872912556345302.....Total loss: 15.872912556345302\n",
      "Pure loss: 13.674332243718075.....Total loss: 13.674332243718075\n",
      "epoch 3631 learning rate:  0.010275406224180666   Training loss:   15.872912556345302  Valing loss:   13.674332243718075\n",
      "Pure loss: 16.33376283595645.....Total loss: 16.33376283595645\n",
      "Pure loss: 13.850602948043617.....Total loss: 13.850602948043617\n",
      "epoch 3632 learning rate:  0.010275330396475772   Training loss:   16.33376283595645  Valing loss:   13.850602948043617\n",
      "Pure loss: 16.58549223550219.....Total loss: 16.58549223550219\n",
      "Pure loss: 14.02966478056891.....Total loss: 14.02966478056891\n",
      "epoch 3633 learning rate:  0.010275254610514727   Training loss:   16.58549223550219  Valing loss:   14.02966478056891\n",
      "Pure loss: 16.41783875142143.....Total loss: 16.41783875142143\n",
      "Pure loss: 13.85637141456853.....Total loss: 13.85637141456853\n",
      "epoch 3634 learning rate:  0.010275178866263071   Training loss:   16.41783875142143  Valing loss:   13.85637141456853\n",
      "Pure loss: 16.29975629089307.....Total loss: 16.29975629089307\n",
      "Pure loss: 13.75152695725789.....Total loss: 13.75152695725789\n",
      "epoch 3635 learning rate:  0.010275103163686383   Training loss:   16.29975629089307  Valing loss:   13.75152695725789\n",
      "Pure loss: 16.022893814949654.....Total loss: 16.022893814949654\n",
      "Pure loss: 13.625934171022864.....Total loss: 13.625934171022864\n",
      "epoch 3636 learning rate:  0.010275027502750275   Training loss:   16.022893814949654  Valing loss:   13.625934171022864\n",
      "Pure loss: 15.659752224228063.....Total loss: 15.659752224228063\n",
      "Pure loss: 13.536010949706117.....Total loss: 13.536010949706117\n",
      "epoch 3637 learning rate:  0.010274951883420402   Training loss:   15.659752224228063  Valing loss:   13.536010949706117\n",
      "Pure loss: 15.42479949232222.....Total loss: 15.42479949232222\n",
      "Pure loss: 13.585880526027838.....Total loss: 13.585880526027838\n",
      "epoch 3638 learning rate:  0.010274876305662452   Training loss:   15.42479949232222  Valing loss:   13.585880526027838\n",
      "Pure loss: 15.515139395168845.....Total loss: 15.515139395168845\n",
      "Pure loss: 13.575092987865933.....Total loss: 13.575092987865933\n",
      "epoch 3639 learning rate:  0.010274800769442155   Training loss:   15.515139395168845  Valing loss:   13.575092987865933\n",
      "Pure loss: 15.458887799726606.....Total loss: 15.458887799726606\n",
      "Pure loss: 13.599179618006442.....Total loss: 13.599179618006442\n",
      "epoch 3640 learning rate:  0.010274725274725276   Training loss:   15.458887799726606  Valing loss:   13.599179618006442\n",
      "Pure loss: 15.352159136332913.....Total loss: 15.352159136332913\n",
      "Pure loss: 13.852224654504568.....Total loss: 13.852224654504568\n",
      "epoch 3641 learning rate:  0.010274649821477615   Training loss:   15.352159136332913  Valing loss:   13.852224654504568\n",
      "Pure loss: 15.356094909955607.....Total loss: 15.356094909955607\n",
      "Pure loss: 13.952704133515837.....Total loss: 13.952704133515837\n",
      "epoch 3642 learning rate:  0.010274574409665019   Training loss:   15.356094909955607  Valing loss:   13.952704133515837\n",
      "Pure loss: 15.336466072592701.....Total loss: 15.336466072592701\n",
      "Pure loss: 13.81759298758715.....Total loss: 13.81759298758715\n",
      "epoch 3643 learning rate:  0.010274499039253362   Training loss:   15.336466072592701  Valing loss:   13.81759298758715\n",
      "Pure loss: 15.346594382337402.....Total loss: 15.346594382337402\n",
      "Pure loss: 13.796851760403618.....Total loss: 13.796851760403618\n",
      "epoch 3644 learning rate:  0.010274423710208562   Training loss:   15.346594382337402  Valing loss:   13.796851760403618\n",
      "Pure loss: 15.366077860029744.....Total loss: 15.366077860029744\n",
      "Pure loss: 13.702837508416216.....Total loss: 13.702837508416216\n",
      "epoch 3645 learning rate:  0.010274348422496572   Training loss:   15.366077860029744  Valing loss:   13.702837508416216\n",
      "Pure loss: 15.402022334050868.....Total loss: 15.402022334050868\n",
      "Pure loss: 13.624216495362713.....Total loss: 13.624216495362713\n",
      "epoch 3646 learning rate:  0.01027427317608338   Training loss:   15.402022334050868  Valing loss:   13.624216495362713\n",
      "Pure loss: 15.473890942490524.....Total loss: 15.473890942490524\n",
      "Pure loss: 13.584156450705148.....Total loss: 13.584156450705148\n",
      "epoch 3647 learning rate:  0.010274197970935015   Training loss:   15.473890942490524  Valing loss:   13.584156450705148\n",
      "Pure loss: 15.557924884301666.....Total loss: 15.557924884301666\n",
      "Pure loss: 13.563567100293863.....Total loss: 13.563567100293863\n",
      "epoch 3648 learning rate:  0.010274122807017543   Training loss:   15.557924884301666  Valing loss:   13.563567100293863\n",
      "Pure loss: 15.955942404985993.....Total loss: 15.955942404985993\n",
      "Pure loss: 13.602819053222975.....Total loss: 13.602819053222975\n",
      "epoch 3649 learning rate:  0.010274047684297068   Training loss:   15.955942404985993  Valing loss:   13.602819053222975\n",
      "Pure loss: 15.979847702478718.....Total loss: 15.979847702478718\n",
      "Pure loss: 13.617715559237023.....Total loss: 13.617715559237023\n",
      "epoch 3650 learning rate:  0.010273972602739725   Training loss:   15.979847702478718  Valing loss:   13.617715559237023\n",
      "Pure loss: 15.836447616160061.....Total loss: 15.836447616160061\n",
      "Pure loss: 13.591455371348312.....Total loss: 13.591455371348312\n",
      "epoch 3651 learning rate:  0.010273897562311696   Training loss:   15.836447616160061  Valing loss:   13.591455371348312\n",
      "Pure loss: 15.93505369371714.....Total loss: 15.93505369371714\n",
      "Pure loss: 13.653452706819158.....Total loss: 13.653452706819158\n",
      "epoch 3652 learning rate:  0.01027382256297919   Training loss:   15.93505369371714  Valing loss:   13.653452706819158\n",
      "Pure loss: 15.942286720545638.....Total loss: 15.942286720545638\n",
      "Pure loss: 13.657744897846248.....Total loss: 13.657744897846248\n",
      "epoch 3653 learning rate:  0.010273747604708458   Training loss:   15.942286720545638  Valing loss:   13.657744897846248\n",
      "Pure loss: 16.52469127483257.....Total loss: 16.52469127483257\n",
      "Pure loss: 13.901361178299867.....Total loss: 13.901361178299867\n",
      "epoch 3654 learning rate:  0.010273672687465792   Training loss:   16.52469127483257  Valing loss:   13.901361178299867\n",
      "Pure loss: 16.64462115310932.....Total loss: 16.64462115310932\n",
      "Pure loss: 13.977927273327747.....Total loss: 13.977927273327747\n",
      "epoch 3655 learning rate:  0.01027359781121751   Training loss:   16.64462115310932  Valing loss:   13.977927273327747\n",
      "Pure loss: 16.911277485989302.....Total loss: 16.911277485989302\n",
      "Pure loss: 14.134327959961052.....Total loss: 14.134327959961052\n",
      "epoch 3656 learning rate:  0.010273522975929978   Training loss:   16.911277485989302  Valing loss:   14.134327959961052\n",
      "Pure loss: 16.82414831940128.....Total loss: 16.82414831940128\n",
      "Pure loss: 14.084127649118722.....Total loss: 14.084127649118722\n",
      "epoch 3657 learning rate:  0.010273448181569592   Training loss:   16.82414831940128  Valing loss:   14.084127649118722\n",
      "Pure loss: 16.26881180635229.....Total loss: 16.26881180635229\n",
      "Pure loss: 13.841114868608368.....Total loss: 13.841114868608368\n",
      "epoch 3658 learning rate:  0.010273373428102788   Training loss:   16.26881180635229  Valing loss:   13.841114868608368\n",
      "Pure loss: 16.138710274081085.....Total loss: 16.138710274081085\n",
      "Pure loss: 13.77123991510921.....Total loss: 13.77123991510921\n",
      "epoch 3659 learning rate:  0.010273298715496038   Training loss:   16.138710274081085  Valing loss:   13.77123991510921\n",
      "Pure loss: 16.11367028935461.....Total loss: 16.11367028935461\n",
      "Pure loss: 13.768198719535494.....Total loss: 13.768198719535494\n",
      "epoch 3660 learning rate:  0.010273224043715848   Training loss:   16.11367028935461  Valing loss:   13.768198719535494\n",
      "Pure loss: 15.615363788379328.....Total loss: 15.615363788379328\n",
      "Pure loss: 13.69632548736998.....Total loss: 13.69632548736998\n",
      "epoch 3661 learning rate:  0.010273149412728763   Training loss:   15.615363788379328  Valing loss:   13.69632548736998\n",
      "Pure loss: 15.515879131757659.....Total loss: 15.515879131757659\n",
      "Pure loss: 13.63785689345535.....Total loss: 13.63785689345535\n",
      "epoch 3662 learning rate:  0.010273074822501366   Training loss:   15.515879131757659  Valing loss:   13.63785689345535\n",
      "Pure loss: 15.496087053534719.....Total loss: 15.496087053534719\n",
      "Pure loss: 13.62637661951392.....Total loss: 13.62637661951392\n",
      "epoch 3663 learning rate:  0.010273000273000273   Training loss:   15.496087053534719  Valing loss:   13.62637661951392\n",
      "Pure loss: 15.569963512759289.....Total loss: 15.569963512759289\n",
      "Pure loss: 13.61497557652116.....Total loss: 13.61497557652116\n",
      "epoch 3664 learning rate:  0.01027292576419214   Training loss:   15.569963512759289  Valing loss:   13.61497557652116\n",
      "Pure loss: 15.40392576471457.....Total loss: 15.40392576471457\n",
      "Pure loss: 13.756938666595062.....Total loss: 13.756938666595062\n",
      "epoch 3665 learning rate:  0.010272851296043656   Training loss:   15.40392576471457  Valing loss:   13.756938666595062\n",
      "Pure loss: 15.390472012576813.....Total loss: 15.390472012576813\n",
      "Pure loss: 13.783057909420773.....Total loss: 13.783057909420773\n",
      "epoch 3666 learning rate:  0.010272776868521549   Training loss:   15.390472012576813  Valing loss:   13.783057909420773\n",
      "Pure loss: 15.37937164408122.....Total loss: 15.37937164408122\n",
      "Pure loss: 13.813206518798784.....Total loss: 13.813206518798784\n",
      "epoch 3667 learning rate:  0.010272702481592583   Training loss:   15.37937164408122  Valing loss:   13.813206518798784\n",
      "Pure loss: 15.3684624598168.....Total loss: 15.3684624598168\n",
      "Pure loss: 13.8186284655782.....Total loss: 13.8186284655782\n",
      "epoch 3668 learning rate:  0.010272628135223555   Training loss:   15.3684624598168  Valing loss:   13.8186284655782\n",
      "Pure loss: 15.514733347948086.....Total loss: 15.514733347948086\n",
      "Pure loss: 13.762701692920297.....Total loss: 13.762701692920297\n",
      "epoch 3669 learning rate:  0.010272553829381303   Training loss:   15.514733347948086  Valing loss:   13.762701692920297\n",
      "Pure loss: 15.446576983086121.....Total loss: 15.446576983086121\n",
      "Pure loss: 13.794843060867022.....Total loss: 13.794843060867022\n",
      "epoch 3670 learning rate:  0.010272479564032698   Training loss:   15.446576983086121  Valing loss:   13.794843060867022\n",
      "Pure loss: 15.61905309537767.....Total loss: 15.61905309537767\n",
      "Pure loss: 13.784319321594799.....Total loss: 13.784319321594799\n",
      "epoch 3671 learning rate:  0.010272405339144648   Training loss:   15.61905309537767  Valing loss:   13.784319321594799\n",
      "Pure loss: 15.52492827507308.....Total loss: 15.52492827507308\n",
      "Pure loss: 13.779781767413159.....Total loss: 13.779781767413159\n",
      "epoch 3672 learning rate:  0.010272331154684096   Training loss:   15.52492827507308  Valing loss:   13.779781767413159\n",
      "Pure loss: 15.504916732755145.....Total loss: 15.504916732755145\n",
      "Pure loss: 13.778077028520446.....Total loss: 13.778077028520446\n",
      "epoch 3673 learning rate:  0.010272257010618024   Training loss:   15.504916732755145  Valing loss:   13.778077028520446\n",
      "Pure loss: 15.60931501835163.....Total loss: 15.60931501835163\n",
      "Pure loss: 13.748106427189013.....Total loss: 13.748106427189013\n",
      "epoch 3674 learning rate:  0.010272182906913447   Training loss:   15.60931501835163  Valing loss:   13.748106427189013\n",
      "Pure loss: 15.691695202587782.....Total loss: 15.691695202587782\n",
      "Pure loss: 13.747092308170537.....Total loss: 13.747092308170537\n",
      "epoch 3675 learning rate:  0.010272108843537416   Training loss:   15.691695202587782  Valing loss:   13.747092308170537\n",
      "Pure loss: 15.429618032457354.....Total loss: 15.429618032457354\n",
      "Pure loss: 13.840005649073497.....Total loss: 13.840005649073497\n",
      "epoch 3676 learning rate:  0.01027203482045702   Training loss:   15.429618032457354  Valing loss:   13.840005649073497\n",
      "Pure loss: 15.351884396160152.....Total loss: 15.351884396160152\n",
      "Pure loss: 13.813328185588619.....Total loss: 13.813328185588619\n",
      "epoch 3677 learning rate:  0.01027196083763938   Training loss:   15.351884396160152  Valing loss:   13.813328185588619\n",
      "Pure loss: 15.350142234237147.....Total loss: 15.350142234237147\n",
      "Pure loss: 13.812795317098596.....Total loss: 13.812795317098596\n",
      "epoch 3678 learning rate:  0.010271886895051659   Training loss:   15.350142234237147  Valing loss:   13.812795317098596\n",
      "Pure loss: 15.330464700062128.....Total loss: 15.330464700062128\n",
      "Pure loss: 14.049509831878863.....Total loss: 14.049509831878863\n",
      "epoch 3679 learning rate:  0.010271812992661049   Training loss:   15.330464700062128  Valing loss:   14.049509831878863\n",
      "Pure loss: 15.351779956832107.....Total loss: 15.351779956832107\n",
      "Pure loss: 13.744818330066254.....Total loss: 13.744818330066254\n",
      "epoch 3680 learning rate:  0.010271739130434783   Training loss:   15.351779956832107  Valing loss:   13.744818330066254\n",
      "Pure loss: 15.485705014448602.....Total loss: 15.485705014448602\n",
      "Pure loss: 13.78952583333473.....Total loss: 13.78952583333473\n",
      "epoch 3681 learning rate:  0.010271665308340125   Training loss:   15.485705014448602  Valing loss:   13.78952583333473\n",
      "Pure loss: 15.512519691567594.....Total loss: 15.512519691567594\n",
      "Pure loss: 13.769686127395103.....Total loss: 13.769686127395103\n",
      "epoch 3682 learning rate:  0.010271591526344378   Training loss:   15.512519691567594  Valing loss:   13.769686127395103\n",
      "Pure loss: 15.604733694483905.....Total loss: 15.604733694483905\n",
      "Pure loss: 13.774093395561653.....Total loss: 13.774093395561653\n",
      "epoch 3683 learning rate:  0.01027151778441488   Training loss:   15.604733694483905  Valing loss:   13.774093395561653\n",
      "Pure loss: 15.58925995982474.....Total loss: 15.58925995982474\n",
      "Pure loss: 13.772228138103673.....Total loss: 13.772228138103673\n",
      "epoch 3684 learning rate:  0.010271444082519001   Training loss:   15.58925995982474  Valing loss:   13.772228138103673\n",
      "Pure loss: 15.576550194600891.....Total loss: 15.576550194600891\n",
      "Pure loss: 13.765158143953778.....Total loss: 13.765158143953778\n",
      "epoch 3685 learning rate:  0.010271370420624152   Training loss:   15.576550194600891  Valing loss:   13.765158143953778\n",
      "Pure loss: 15.83393015867158.....Total loss: 15.83393015867158\n",
      "Pure loss: 13.810667411668796.....Total loss: 13.810667411668796\n",
      "epoch 3686 learning rate:  0.010271296798697776   Training loss:   15.83393015867158  Valing loss:   13.810667411668796\n",
      "Pure loss: 15.911789526487985.....Total loss: 15.911789526487985\n",
      "Pure loss: 13.852083534806738.....Total loss: 13.852083534806738\n",
      "epoch 3687 learning rate:  0.010271223216707351   Training loss:   15.911789526487985  Valing loss:   13.852083534806738\n",
      "Pure loss: 15.934485801418665.....Total loss: 15.934485801418665\n",
      "Pure loss: 13.861997356202705.....Total loss: 13.861997356202705\n",
      "epoch 3688 learning rate:  0.010271149674620391   Training loss:   15.934485801418665  Valing loss:   13.861997356202705\n",
      "Pure loss: 15.776459773809735.....Total loss: 15.776459773809735\n",
      "Pure loss: 13.699793877202525.....Total loss: 13.699793877202525\n",
      "epoch 3689 learning rate:  0.010271076172404445   Training loss:   15.776459773809735  Valing loss:   13.699793877202525\n",
      "Pure loss: 15.751929811216417.....Total loss: 15.751929811216417\n",
      "Pure loss: 13.692490655146694.....Total loss: 13.692490655146694\n",
      "epoch 3690 learning rate:  0.010271002710027101   Training loss:   15.751929811216417  Valing loss:   13.692490655146694\n",
      "Pure loss: 15.573697572399963.....Total loss: 15.573697572399963\n",
      "Pure loss: 13.642821210004797.....Total loss: 13.642821210004797\n",
      "epoch 3691 learning rate:  0.010270929287455974   Training loss:   15.573697572399963  Valing loss:   13.642821210004797\n",
      "Pure loss: 15.98781974241892.....Total loss: 15.98781974241892\n",
      "Pure loss: 13.736103263333248.....Total loss: 13.736103263333248\n",
      "epoch 3692 learning rate:  0.010270855904658722   Training loss:   15.98781974241892  Valing loss:   13.736103263333248\n",
      "Pure loss: 16.054381038303585.....Total loss: 16.054381038303585\n",
      "Pure loss: 13.75294790904397.....Total loss: 13.75294790904397\n",
      "epoch 3693 learning rate:  0.010270782561603032   Training loss:   16.054381038303585  Valing loss:   13.75294790904397\n",
      "Pure loss: 15.534748158231244.....Total loss: 15.534748158231244\n",
      "Pure loss: 13.692500389679218.....Total loss: 13.692500389679218\n",
      "epoch 3694 learning rate:  0.010270709258256632   Training loss:   15.534748158231244  Valing loss:   13.692500389679218\n",
      "Pure loss: 15.708204953234743.....Total loss: 15.708204953234743\n",
      "Pure loss: 13.823427761583936.....Total loss: 13.823427761583936\n",
      "epoch 3695 learning rate:  0.01027063599458728   Training loss:   15.708204953234743  Valing loss:   13.823427761583936\n",
      "Pure loss: 15.640074367058409.....Total loss: 15.640074367058409\n",
      "Pure loss: 13.776187798644047.....Total loss: 13.776187798644047\n",
      "epoch 3696 learning rate:  0.01027056277056277   Training loss:   15.640074367058409  Valing loss:   13.776187798644047\n",
      "Pure loss: 15.656329679919317.....Total loss: 15.656329679919317\n",
      "Pure loss: 13.790807694710963.....Total loss: 13.790807694710963\n",
      "epoch 3697 learning rate:  0.010270489586150934   Training loss:   15.656329679919317  Valing loss:   13.790807694710963\n",
      "Pure loss: 15.279628454951816.....Total loss: 15.279628454951816\n",
      "Pure loss: 14.155342874344065.....Total loss: 14.155342874344065\n",
      "epoch 3698 learning rate:  0.010270416441319633   Training loss:   15.279628454951816  Valing loss:   14.155342874344065\n",
      "Pure loss: 15.304825790823434.....Total loss: 15.304825790823434\n",
      "Pure loss: 14.229431153206882.....Total loss: 14.229431153206882\n",
      "epoch 3699 learning rate:  0.010270343336036767   Training loss:   15.304825790823434  Valing loss:   14.229431153206882\n",
      "Pure loss: 15.19039250257355.....Total loss: 15.19039250257355\n",
      "Pure loss: 13.726578887030449.....Total loss: 13.726578887030449\n",
      "epoch 3700 learning rate:  0.010270270270270271   Training loss:   15.19039250257355  Valing loss:   13.726578887030449\n",
      "Pure loss: 15.188735684336239.....Total loss: 15.188735684336239\n",
      "Pure loss: 13.682417797565577.....Total loss: 13.682417797565577\n",
      "epoch 3701 learning rate:  0.010270197243988111   Training loss:   15.188735684336239  Valing loss:   13.682417797565577\n",
      "Pure loss: 15.208284980124004.....Total loss: 15.208284980124004\n",
      "Pure loss: 14.07839201715677.....Total loss: 14.07839201715677\n",
      "epoch 3702 learning rate:  0.010270124257158293   Training loss:   15.208284980124004  Valing loss:   14.07839201715677\n",
      "Pure loss: 15.155744810278877.....Total loss: 15.155744810278877\n",
      "Pure loss: 13.784477255640276.....Total loss: 13.784477255640276\n",
      "epoch 3703 learning rate:  0.010270051309748852   Training loss:   15.155744810278877  Valing loss:   13.784477255640276\n",
      "Pure loss: 15.189125501841286.....Total loss: 15.189125501841286\n",
      "Pure loss: 13.602280529856484.....Total loss: 13.602280529856484\n",
      "epoch 3704 learning rate:  0.010269978401727862   Training loss:   15.189125501841286  Valing loss:   13.602280529856484\n",
      "Pure loss: 15.195312468622133.....Total loss: 15.195312468622133\n",
      "Pure loss: 13.74085957625459.....Total loss: 13.74085957625459\n",
      "epoch 3705 learning rate:  0.010269905533063428   Training loss:   15.195312468622133  Valing loss:   13.74085957625459\n",
      "Pure loss: 15.200775936103303.....Total loss: 15.200775936103303\n",
      "Pure loss: 14.023105086814077.....Total loss: 14.023105086814077\n",
      "epoch 3706 learning rate:  0.010269832703723692   Training loss:   15.200775936103303  Valing loss:   14.023105086814077\n",
      "Pure loss: 15.302291891434411.....Total loss: 15.302291891434411\n",
      "Pure loss: 14.33348241931755.....Total loss: 14.33348241931755\n",
      "epoch 3707 learning rate:  0.010269759913676828   Training loss:   15.302291891434411  Valing loss:   14.33348241931755\n",
      "Pure loss: 15.471910277606508.....Total loss: 15.471910277606508\n",
      "Pure loss: 14.641228128280481.....Total loss: 14.641228128280481\n",
      "epoch 3708 learning rate:  0.010269687162891047   Training loss:   15.471910277606508  Valing loss:   14.641228128280481\n",
      "Pure loss: 16.425168652913328.....Total loss: 16.425168652913328\n",
      "Pure loss: 15.925483000861531.....Total loss: 15.925483000861531\n",
      "epoch 3709 learning rate:  0.010269614451334591   Training loss:   16.425168652913328  Valing loss:   15.925483000861531\n",
      "Pure loss: 16.218012537609724.....Total loss: 16.218012537609724\n",
      "Pure loss: 15.597381595803677.....Total loss: 15.597381595803677\n",
      "epoch 3710 learning rate:  0.010269541778975742   Training loss:   16.218012537609724  Valing loss:   15.597381595803677\n",
      "Pure loss: 15.898214265515556.....Total loss: 15.898214265515556\n",
      "Pure loss: 15.026377193212157.....Total loss: 15.026377193212157\n",
      "epoch 3711 learning rate:  0.010269469145782808   Training loss:   15.898214265515556  Valing loss:   15.026377193212157\n",
      "Pure loss: 15.814876098554931.....Total loss: 15.814876098554931\n",
      "Pure loss: 14.613904805241145.....Total loss: 14.613904805241145\n",
      "epoch 3712 learning rate:  0.010269396551724139   Training loss:   15.814876098554931  Valing loss:   14.613904805241145\n",
      "Pure loss: 15.763189084238956.....Total loss: 15.763189084238956\n",
      "Pure loss: 14.395487216193787.....Total loss: 14.395487216193787\n",
      "epoch 3713 learning rate:  0.010269323996768113   Training loss:   15.763189084238956  Valing loss:   14.395487216193787\n",
      "Pure loss: 15.566028516000156.....Total loss: 15.566028516000156\n",
      "Pure loss: 14.067447996529005.....Total loss: 14.067447996529005\n",
      "epoch 3714 learning rate:  0.010269251480883146   Training loss:   15.566028516000156  Valing loss:   14.067447996529005\n",
      "Pure loss: 15.568390291480648.....Total loss: 15.568390291480648\n",
      "Pure loss: 14.101236088372564.....Total loss: 14.101236088372564\n",
      "epoch 3715 learning rate:  0.010269179004037685   Training loss:   15.568390291480648  Valing loss:   14.101236088372564\n",
      "Pure loss: 15.567785247952287.....Total loss: 15.567785247952287\n",
      "Pure loss: 14.094021658945621.....Total loss: 14.094021658945621\n",
      "epoch 3716 learning rate:  0.010269106566200215   Training loss:   15.567785247952287  Valing loss:   14.094021658945621\n",
      "Pure loss: 15.544876309657871.....Total loss: 15.544876309657871\n",
      "Pure loss: 13.805157573726664.....Total loss: 13.805157573726664\n",
      "epoch 3717 learning rate:  0.010269034167339252   Training loss:   15.544876309657871  Valing loss:   13.805157573726664\n",
      "Pure loss: 15.435007417468197.....Total loss: 15.435007417468197\n",
      "Pure loss: 13.963477209110355.....Total loss: 13.963477209110355\n",
      "epoch 3718 learning rate:  0.010268961807423345   Training loss:   15.435007417468197  Valing loss:   13.963477209110355\n",
      "Pure loss: 15.45198154540446.....Total loss: 15.45198154540446\n",
      "Pure loss: 13.987245196444661.....Total loss: 13.987245196444661\n",
      "epoch 3719 learning rate:  0.01026888948642108   Training loss:   15.45198154540446  Valing loss:   13.987245196444661\n",
      "Pure loss: 15.37187625086047.....Total loss: 15.37187625086047\n",
      "Pure loss: 13.894765997330557.....Total loss: 13.894765997330557\n",
      "epoch 3720 learning rate:  0.010268817204301076   Training loss:   15.37187625086047  Valing loss:   13.894765997330557\n",
      "Pure loss: 15.371957611524483.....Total loss: 15.371957611524483\n",
      "Pure loss: 13.887064017605859.....Total loss: 13.887064017605859\n",
      "epoch 3721 learning rate:  0.010268744961031981   Training loss:   15.371957611524483  Valing loss:   13.887064017605859\n",
      "Pure loss: 15.397546926217785.....Total loss: 15.397546926217785\n",
      "Pure loss: 13.727650798284998.....Total loss: 13.727650798284998\n",
      "epoch 3722 learning rate:  0.010268672756582482   Training loss:   15.397546926217785  Valing loss:   13.727650798284998\n",
      "Pure loss: 15.37567198665307.....Total loss: 15.37567198665307\n",
      "Pure loss: 13.595835469343992.....Total loss: 13.595835469343992\n",
      "epoch 3723 learning rate:  0.0102686005909213   Training loss:   15.37567198665307  Valing loss:   13.595835469343992\n",
      "Pure loss: 15.385822675549889.....Total loss: 15.385822675549889\n",
      "Pure loss: 13.572955305982044.....Total loss: 13.572955305982044\n",
      "epoch 3724 learning rate:  0.010268528464017187   Training loss:   15.385822675549889  Valing loss:   13.572955305982044\n",
      "Pure loss: 15.404692194661326.....Total loss: 15.404692194661326\n",
      "Pure loss: 13.563628594674759.....Total loss: 13.563628594674759\n",
      "epoch 3725 learning rate:  0.010268456375838926   Training loss:   15.404692194661326  Valing loss:   13.563628594674759\n",
      "Pure loss: 15.589172065540422.....Total loss: 15.589172065540422\n",
      "Pure loss: 13.524208639423394.....Total loss: 13.524208639423394\n",
      "epoch 3726 learning rate:  0.01026838432635534   Training loss:   15.589172065540422  Valing loss:   13.524208639423394\n",
      "Pure loss: 15.617559765803193.....Total loss: 15.617559765803193\n",
      "Pure loss: 13.528030236928911.....Total loss: 13.528030236928911\n",
      "epoch 3727 learning rate:  0.010268312315535284   Training loss:   15.617559765803193  Valing loss:   13.528030236928911\n",
      "Pure loss: 15.388287909331146.....Total loss: 15.388287909331146\n",
      "Pure loss: 13.55946797173832.....Total loss: 13.55946797173832\n",
      "epoch 3728 learning rate:  0.01026824034334764   Training loss:   15.388287909331146  Valing loss:   13.55946797173832\n",
      "Pure loss: 15.266722285636437.....Total loss: 15.266722285636437\n",
      "Pure loss: 13.644272857738063.....Total loss: 13.644272857738063\n",
      "epoch 3729 learning rate:  0.01026816840976133   Training loss:   15.266722285636437  Valing loss:   13.644272857738063\n",
      "Pure loss: 15.282820885162664.....Total loss: 15.282820885162664\n",
      "Pure loss: 13.629722846199854.....Total loss: 13.629722846199854\n",
      "epoch 3730 learning rate:  0.010268096514745308   Training loss:   15.282820885162664  Valing loss:   13.629722846199854\n",
      "Pure loss: 15.24477187065075.....Total loss: 15.24477187065075\n",
      "Pure loss: 13.686976616607474.....Total loss: 13.686976616607474\n",
      "epoch 3731 learning rate:  0.010268024658268562   Training loss:   15.24477187065075  Valing loss:   13.686976616607474\n",
      "Pure loss: 15.2963319700738.....Total loss: 15.2963319700738\n",
      "Pure loss: 13.632080552604476.....Total loss: 13.632080552604476\n",
      "epoch 3732 learning rate:  0.010267952840300108   Training loss:   15.2963319700738  Valing loss:   13.632080552604476\n",
      "Pure loss: 15.176269226738892.....Total loss: 15.176269226738892\n",
      "Pure loss: 13.979190670905217.....Total loss: 13.979190670905217\n",
      "epoch 3733 learning rate:  0.010267881060809001   Training loss:   15.176269226738892  Valing loss:   13.979190670905217\n",
      "Pure loss: 15.192110070100703.....Total loss: 15.192110070100703\n",
      "Pure loss: 14.02209992748854.....Total loss: 14.02209992748854\n",
      "epoch 3734 learning rate:  0.010267809319764329   Training loss:   15.192110070100703  Valing loss:   14.02209992748854\n",
      "Pure loss: 15.202296857775401.....Total loss: 15.202296857775401\n",
      "Pure loss: 14.07541385075146.....Total loss: 14.07541385075146\n",
      "epoch 3735 learning rate:  0.010267737617135208   Training loss:   15.202296857775401  Valing loss:   14.07541385075146\n",
      "Pure loss: 15.179307273606758.....Total loss: 15.179307273606758\n",
      "Pure loss: 13.921626229080491.....Total loss: 13.921626229080491\n",
      "epoch 3736 learning rate:  0.010267665952890793   Training loss:   15.179307273606758  Valing loss:   13.921626229080491\n",
      "Pure loss: 15.1855295734475.....Total loss: 15.1855295734475\n",
      "Pure loss: 13.737946350992967.....Total loss: 13.737946350992967\n",
      "epoch 3737 learning rate:  0.010267594327000269   Training loss:   15.1855295734475  Valing loss:   13.737946350992967\n",
      "Pure loss: 15.193250805018845.....Total loss: 15.193250805018845\n",
      "Pure loss: 13.826402325015033.....Total loss: 13.826402325015033\n",
      "epoch 3738 learning rate:  0.010267522739432851   Training loss:   15.193250805018845  Valing loss:   13.826402325015033\n",
      "Pure loss: 15.193227768596437.....Total loss: 15.193227768596437\n",
      "Pure loss: 13.810108558938376.....Total loss: 13.810108558938376\n",
      "epoch 3739 learning rate:  0.010267451190157796   Training loss:   15.193227768596437  Valing loss:   13.810108558938376\n",
      "Pure loss: 15.1891853589281.....Total loss: 15.1891853589281\n",
      "Pure loss: 13.800209996599214.....Total loss: 13.800209996599214\n",
      "epoch 3740 learning rate:  0.010267379679144385   Training loss:   15.1891853589281  Valing loss:   13.800209996599214\n",
      "Pure loss: 15.178920059590421.....Total loss: 15.178920059590421\n",
      "Pure loss: 14.213752026037115.....Total loss: 14.213752026037115\n",
      "epoch 3741 learning rate:  0.010267308206361936   Training loss:   15.178920059590421  Valing loss:   14.213752026037115\n",
      "Pure loss: 15.137130796304861.....Total loss: 15.137130796304861\n",
      "Pure loss: 13.777766553720431.....Total loss: 13.777766553720431\n",
      "epoch 3742 learning rate:  0.010267236771779797   Training loss:   15.137130796304861  Valing loss:   13.777766553720431\n",
      "Pure loss: 15.148416773964511.....Total loss: 15.148416773964511\n",
      "Pure loss: 13.756147200582673.....Total loss: 13.756147200582673\n",
      "epoch 3743 learning rate:  0.010267165375367353   Training loss:   15.148416773964511  Valing loss:   13.756147200582673\n",
      "Pure loss: 15.22386104884663.....Total loss: 15.22386104884663\n",
      "Pure loss: 13.67306256726982.....Total loss: 13.67306256726982\n",
      "epoch 3744 learning rate:  0.010267094017094018   Training loss:   15.22386104884663  Valing loss:   13.67306256726982\n",
      "Pure loss: 15.11059334602698.....Total loss: 15.11059334602698\n",
      "Pure loss: 13.84059233678638.....Total loss: 13.84059233678638\n",
      "epoch 3745 learning rate:  0.010267022696929239   Training loss:   15.11059334602698  Valing loss:   13.84059233678638\n",
      "Pure loss: 15.097724262003057.....Total loss: 15.097724262003057\n",
      "Pure loss: 13.886980279607991.....Total loss: 13.886980279607991\n",
      "epoch 3746 learning rate:  0.010266951414842499   Training loss:   15.097724262003057  Valing loss:   13.886980279607991\n",
      "Pure loss: 15.125990957274169.....Total loss: 15.125990957274169\n",
      "Pure loss: 13.969179435112625.....Total loss: 13.969179435112625\n",
      "epoch 3747 learning rate:  0.01026688017080331   Training loss:   15.125990957274169  Valing loss:   13.969179435112625\n",
      "Pure loss: 15.152357135001571.....Total loss: 15.152357135001571\n",
      "Pure loss: 14.013370813909088.....Total loss: 14.013370813909088\n",
      "epoch 3748 learning rate:  0.010266808964781217   Training loss:   15.152357135001571  Valing loss:   14.013370813909088\n",
      "Pure loss: 15.174299642332738.....Total loss: 15.174299642332738\n",
      "Pure loss: 14.067182449475235.....Total loss: 14.067182449475235\n",
      "epoch 3749 learning rate:  0.0102667377967458   Training loss:   15.174299642332738  Valing loss:   14.067182449475235\n",
      "Pure loss: 15.178679418147723.....Total loss: 15.178679418147723\n",
      "Pure loss: 14.016242479782404.....Total loss: 14.016242479782404\n",
      "epoch 3750 learning rate:  0.010266666666666667   Training loss:   15.178679418147723  Valing loss:   14.016242479782404\n",
      "Pure loss: 15.204508723669901.....Total loss: 15.204508723669901\n",
      "Pure loss: 13.775885232805381.....Total loss: 13.775885232805381\n",
      "epoch 3751 learning rate:  0.010266595574513463   Training loss:   15.204508723669901  Valing loss:   13.775885232805381\n",
      "Pure loss: 15.187743446068104.....Total loss: 15.187743446068104\n",
      "Pure loss: 13.838083912478897.....Total loss: 13.838083912478897\n",
      "epoch 3752 learning rate:  0.010266524520255864   Training loss:   15.187743446068104  Valing loss:   13.838083912478897\n",
      "Pure loss: 15.175467253121484.....Total loss: 15.175467253121484\n",
      "Pure loss: 13.742875204230028.....Total loss: 13.742875204230028\n",
      "epoch 3753 learning rate:  0.010266453503863576   Training loss:   15.175467253121484  Valing loss:   13.742875204230028\n",
      "Pure loss: 15.225918230563853.....Total loss: 15.225918230563853\n",
      "Pure loss: 13.6976860682154.....Total loss: 13.6976860682154\n",
      "epoch 3754 learning rate:  0.01026638252530634   Training loss:   15.225918230563853  Valing loss:   13.6976860682154\n",
      "Pure loss: 15.158127956941657.....Total loss: 15.158127956941657\n",
      "Pure loss: 13.77067911328484.....Total loss: 13.77067911328484\n",
      "epoch 3755 learning rate:  0.010266311584553928   Training loss:   15.158127956941657  Valing loss:   13.77067911328484\n",
      "Pure loss: 15.228061906411135.....Total loss: 15.228061906411135\n",
      "Pure loss: 13.678923423458901.....Total loss: 13.678923423458901\n",
      "epoch 3756 learning rate:  0.010266240681576146   Training loss:   15.228061906411135  Valing loss:   13.678923423458901\n",
      "Pure loss: 15.41640240553535.....Total loss: 15.41640240553535\n",
      "Pure loss: 13.63269946001375.....Total loss: 13.63269946001375\n",
      "epoch 3757 learning rate:  0.010266169816342827   Training loss:   15.41640240553535  Valing loss:   13.63269946001375\n",
      "Pure loss: 15.401847271535017.....Total loss: 15.401847271535017\n",
      "Pure loss: 13.62555608541124.....Total loss: 13.62555608541124\n",
      "epoch 3758 learning rate:  0.010266098988823844   Training loss:   15.401847271535017  Valing loss:   13.62555608541124\n",
      "Pure loss: 15.516732601863234.....Total loss: 15.516732601863234\n",
      "Pure loss: 13.641131436827937.....Total loss: 13.641131436827937\n",
      "epoch 3759 learning rate:  0.010266028198989093   Training loss:   15.516732601863234  Valing loss:   13.641131436827937\n",
      "Pure loss: 15.483775111532786.....Total loss: 15.483775111532786\n",
      "Pure loss: 13.635291587978358.....Total loss: 13.635291587978358\n",
      "epoch 3760 learning rate:  0.010265957446808511   Training loss:   15.483775111532786  Valing loss:   13.635291587978358\n",
      "Pure loss: 15.265717094406416.....Total loss: 15.265717094406416\n",
      "Pure loss: 13.643720464772937.....Total loss: 13.643720464772937\n",
      "epoch 3761 learning rate:  0.010265886732252061   Training loss:   15.265717094406416  Valing loss:   13.643720464772937\n",
      "Pure loss: 15.277580267484066.....Total loss: 15.277580267484066\n",
      "Pure loss: 13.640688567068649.....Total loss: 13.640688567068649\n",
      "epoch 3762 learning rate:  0.01026581605528974   Training loss:   15.277580267484066  Valing loss:   13.640688567068649\n",
      "Pure loss: 15.410850095188382.....Total loss: 15.410850095188382\n",
      "Pure loss: 13.618059553016037.....Total loss: 13.618059553016037\n",
      "epoch 3763 learning rate:  0.010265745415891576   Training loss:   15.410850095188382  Valing loss:   13.618059553016037\n",
      "Pure loss: 15.383343762263355.....Total loss: 15.383343762263355\n",
      "Pure loss: 13.592643873261645.....Total loss: 13.592643873261645\n",
      "epoch 3764 learning rate:  0.01026567481402763   Training loss:   15.383343762263355  Valing loss:   13.592643873261645\n",
      "Pure loss: 15.751460444919779.....Total loss: 15.751460444919779\n",
      "Pure loss: 13.620630538222077.....Total loss: 13.620630538222077\n",
      "epoch 3765 learning rate:  0.010265604249667996   Training loss:   15.751460444919779  Valing loss:   13.620630538222077\n",
      "Pure loss: 15.599443856190522.....Total loss: 15.599443856190522\n",
      "Pure loss: 13.566836561103328.....Total loss: 13.566836561103328\n",
      "epoch 3766 learning rate:  0.010265533722782794   Training loss:   15.599443856190522  Valing loss:   13.566836561103328\n",
      "Pure loss: 15.221230368873577.....Total loss: 15.221230368873577\n",
      "Pure loss: 13.62758490766182.....Total loss: 13.62758490766182\n",
      "epoch 3767 learning rate:  0.010265463233342183   Training loss:   15.221230368873577  Valing loss:   13.62758490766182\n",
      "Pure loss: 15.166528328155742.....Total loss: 15.166528328155742\n",
      "Pure loss: 13.655915513173122.....Total loss: 13.655915513173122\n",
      "epoch 3768 learning rate:  0.010265392781316348   Training loss:   15.166528328155742  Valing loss:   13.655915513173122\n",
      "Pure loss: 15.324240353499254.....Total loss: 15.324240353499254\n",
      "Pure loss: 13.583367772235047.....Total loss: 13.583367772235047\n",
      "epoch 3769 learning rate:  0.010265322366675511   Training loss:   15.324240353499254  Valing loss:   13.583367772235047\n",
      "Pure loss: 15.34109179387873.....Total loss: 15.34109179387873\n",
      "Pure loss: 13.590321192633402.....Total loss: 13.590321192633402\n",
      "epoch 3770 learning rate:  0.01026525198938992   Training loss:   15.34109179387873  Valing loss:   13.590321192633402\n",
      "Pure loss: 15.273972750895481.....Total loss: 15.273972750895481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 13.60678541555545.....Total loss: 13.60678541555545\n",
      "epoch 3771 learning rate:  0.010265181649429859   Training loss:   15.273972750895481  Valing loss:   13.60678541555545\n",
      "Pure loss: 15.282083395382365.....Total loss: 15.282083395382365\n",
      "Pure loss: 13.60330630431227.....Total loss: 13.60330630431227\n",
      "epoch 3772 learning rate:  0.010265111346765643   Training loss:   15.282083395382365  Valing loss:   13.60330630431227\n",
      "Pure loss: 15.404676268852775.....Total loss: 15.404676268852775\n",
      "Pure loss: 13.59096333080505.....Total loss: 13.59096333080505\n",
      "epoch 3773 learning rate:  0.010265041081367611   Training loss:   15.404676268852775  Valing loss:   13.59096333080505\n",
      "Pure loss: 15.766658961441056.....Total loss: 15.766658961441056\n",
      "Pure loss: 13.67702290005598.....Total loss: 13.67702290005598\n",
      "epoch 3774 learning rate:  0.010264970853206148   Training loss:   15.766658961441056  Valing loss:   13.67702290005598\n",
      "Pure loss: 15.751461162683588.....Total loss: 15.751461162683588\n",
      "Pure loss: 13.669613715119878.....Total loss: 13.669613715119878\n",
      "epoch 3775 learning rate:  0.010264900662251657   Training loss:   15.751461162683588  Valing loss:   13.669613715119878\n",
      "Pure loss: 15.712906669215887.....Total loss: 15.712906669215887\n",
      "Pure loss: 13.631597005655646.....Total loss: 13.631597005655646\n",
      "epoch 3776 learning rate:  0.010264830508474576   Training loss:   15.712906669215887  Valing loss:   13.631597005655646\n",
      "Pure loss: 15.542140660049519.....Total loss: 15.542140660049519\n",
      "Pure loss: 13.587060054835163.....Total loss: 13.587060054835163\n",
      "epoch 3777 learning rate:  0.01026476039184538   Training loss:   15.542140660049519  Valing loss:   13.587060054835163\n",
      "Pure loss: 15.57750514828908.....Total loss: 15.57750514828908\n",
      "Pure loss: 13.599574811770205.....Total loss: 13.599574811770205\n",
      "epoch 3778 learning rate:  0.01026469031233457   Training loss:   15.57750514828908  Valing loss:   13.599574811770205\n",
      "Pure loss: 15.732978277407218.....Total loss: 15.732978277407218\n",
      "Pure loss: 13.638168898168951.....Total loss: 13.638168898168951\n",
      "epoch 3779 learning rate:  0.010264620269912676   Training loss:   15.732978277407218  Valing loss:   13.638168898168951\n",
      "Pure loss: 15.355892676548226.....Total loss: 15.355892676548226\n",
      "Pure loss: 13.56545512737002.....Total loss: 13.56545512737002\n",
      "epoch 3780 learning rate:  0.010264550264550264   Training loss:   15.355892676548226  Valing loss:   13.56545512737002\n",
      "Pure loss: 15.487653025303239.....Total loss: 15.487653025303239\n",
      "Pure loss: 13.552065145509113.....Total loss: 13.552065145509113\n",
      "epoch 3781 learning rate:  0.010264480296217932   Training loss:   15.487653025303239  Valing loss:   13.552065145509113\n",
      "Pure loss: 15.122685498720417.....Total loss: 15.122685498720417\n",
      "Pure loss: 13.656445745425126.....Total loss: 13.656445745425126\n",
      "epoch 3782 learning rate:  0.010264410364886303   Training loss:   15.122685498720417  Valing loss:   13.656445745425126\n",
      "Pure loss: 15.146635425164584.....Total loss: 15.146635425164584\n",
      "Pure loss: 13.632983755944217.....Total loss: 13.632983755944217\n",
      "epoch 3783 learning rate:  0.010264340470526039   Training loss:   15.146635425164584  Valing loss:   13.632983755944217\n",
      "Pure loss: 15.132741582360088.....Total loss: 15.132741582360088\n",
      "Pure loss: 13.647785123985667.....Total loss: 13.647785123985667\n",
      "epoch 3784 learning rate:  0.010264270613107822   Training loss:   15.132741582360088  Valing loss:   13.647785123985667\n",
      "Pure loss: 15.080044041929817.....Total loss: 15.080044041929817\n",
      "Pure loss: 13.730700316464864.....Total loss: 13.730700316464864\n",
      "epoch 3785 learning rate:  0.010264200792602377   Training loss:   15.080044041929817  Valing loss:   13.730700316464864\n",
      "Pure loss: 15.246009045664406.....Total loss: 15.246009045664406\n",
      "Pure loss: 14.466220247088602.....Total loss: 14.466220247088602\n",
      "epoch 3786 learning rate:  0.010264131008980455   Training loss:   15.246009045664406  Valing loss:   14.466220247088602\n",
      "Pure loss: 15.153891129776694.....Total loss: 15.153891129776694\n",
      "Pure loss: 14.27364280690841.....Total loss: 14.27364280690841\n",
      "epoch 3787 learning rate:  0.010264061262212833   Training loss:   15.153891129776694  Valing loss:   14.27364280690841\n",
      "Pure loss: 15.181833845832749.....Total loss: 15.181833845832749\n",
      "Pure loss: 14.34364997048525.....Total loss: 14.34364997048525\n",
      "epoch 3788 learning rate:  0.010263991552270327   Training loss:   15.181833845832749  Valing loss:   14.34364997048525\n",
      "Pure loss: 15.106210546196117.....Total loss: 15.106210546196117\n",
      "Pure loss: 14.10328256330151.....Total loss: 14.10328256330151\n",
      "epoch 3789 learning rate:  0.01026392187912378   Training loss:   15.106210546196117  Valing loss:   14.10328256330151\n",
      "Pure loss: 15.123562496730418.....Total loss: 15.123562496730418\n",
      "Pure loss: 14.197887202485951.....Total loss: 14.197887202485951\n",
      "epoch 3790 learning rate:  0.010263852242744063   Training loss:   15.123562496730418  Valing loss:   14.197887202485951\n",
      "Pure loss: 15.097628307371048.....Total loss: 15.097628307371048\n",
      "Pure loss: 14.134788426801322.....Total loss: 14.134788426801322\n",
      "epoch 3791 learning rate:  0.010263782643102085   Training loss:   15.097628307371048  Valing loss:   14.134788426801322\n",
      "Pure loss: 15.047451883317875.....Total loss: 15.047451883317875\n",
      "Pure loss: 13.914025304107849.....Total loss: 13.914025304107849\n",
      "epoch 3792 learning rate:  0.010263713080168777   Training loss:   15.047451883317875  Valing loss:   13.914025304107849\n",
      "Pure loss: 15.007624583702986.....Total loss: 15.007624583702986\n",
      "Pure loss: 13.732047549521647.....Total loss: 13.732047549521647\n",
      "epoch 3793 learning rate:  0.010263643553915108   Training loss:   15.007624583702986  Valing loss:   13.732047549521647\n",
      "Pure loss: 15.070174140791568.....Total loss: 15.070174140791568\n",
      "Pure loss: 13.598201289742585.....Total loss: 13.598201289742585\n",
      "epoch 3794 learning rate:  0.010263574064312072   Training loss:   15.070174140791568  Valing loss:   13.598201289742585\n",
      "Pure loss: 15.13047507110606.....Total loss: 15.13047507110606\n",
      "Pure loss: 13.565318432669319.....Total loss: 13.565318432669319\n",
      "epoch 3795 learning rate:  0.0102635046113307   Training loss:   15.13047507110606  Valing loss:   13.565318432669319\n",
      "Pure loss: 15.026042939064594.....Total loss: 15.026042939064594\n",
      "Pure loss: 14.080545323852826.....Total loss: 14.080545323852826\n",
      "epoch 3796 learning rate:  0.010263435194942045   Training loss:   15.026042939064594  Valing loss:   14.080545323852826\n",
      "Pure loss: 15.019933968124874.....Total loss: 15.019933968124874\n",
      "Pure loss: 14.057863971904982.....Total loss: 14.057863971904982\n",
      "epoch 3797 learning rate:  0.010263365815117198   Training loss:   15.019933968124874  Valing loss:   14.057863971904982\n",
      "Pure loss: 15.062104080954244.....Total loss: 15.062104080954244\n",
      "Pure loss: 14.16900543698073.....Total loss: 14.16900543698073\n",
      "epoch 3798 learning rate:  0.010263296471827279   Training loss:   15.062104080954244  Valing loss:   14.16900543698073\n",
      "Pure loss: 15.015176299721073.....Total loss: 15.015176299721073\n",
      "Pure loss: 13.936412707035213.....Total loss: 13.936412707035213\n",
      "epoch 3799 learning rate:  0.010263227165043434   Training loss:   15.015176299721073  Valing loss:   13.936412707035213\n",
      "Pure loss: 14.99240743779302.....Total loss: 14.99240743779302\n",
      "Pure loss: 13.849407608509134.....Total loss: 13.849407608509134\n",
      "epoch 3800 learning rate:  0.010263157894736842   Training loss:   14.99240743779302  Valing loss:   13.849407608509134\n",
      "Pure loss: 15.04247125707833.....Total loss: 15.04247125707833\n",
      "Pure loss: 13.646033285258918.....Total loss: 13.646033285258918\n",
      "epoch 3801 learning rate:  0.010263088660878716   Training loss:   15.04247125707833  Valing loss:   13.646033285258918\n",
      "Pure loss: 15.06915053055174.....Total loss: 15.06915053055174\n",
      "Pure loss: 13.62440807536239.....Total loss: 13.62440807536239\n",
      "epoch 3802 learning rate:  0.010263019463440295   Training loss:   15.06915053055174  Valing loss:   13.62440807536239\n",
      "Pure loss: 15.07527935848382.....Total loss: 15.07527935848382\n",
      "Pure loss: 13.616106709709863.....Total loss: 13.616106709709863\n",
      "epoch 3803 learning rate:  0.010262950302392848   Training loss:   15.07527935848382  Valing loss:   13.616106709709863\n",
      "Pure loss: 15.249235910777001.....Total loss: 15.249235910777001\n",
      "Pure loss: 13.558573097583176.....Total loss: 13.558573097583176\n",
      "epoch 3804 learning rate:  0.010262881177707676   Training loss:   15.249235910777001  Valing loss:   13.558573097583176\n",
      "Pure loss: 15.267991106267576.....Total loss: 15.267991106267576\n",
      "Pure loss: 13.556606256942228.....Total loss: 13.556606256942228\n",
      "epoch 3805 learning rate:  0.01026281208935611   Training loss:   15.267991106267576  Valing loss:   13.556606256942228\n",
      "Pure loss: 15.625615384499579.....Total loss: 15.625615384499579\n",
      "Pure loss: 13.561357261040547.....Total loss: 13.561357261040547\n",
      "epoch 3806 learning rate:  0.010262743037309511   Training loss:   15.625615384499579  Valing loss:   13.561357261040547\n",
      "Pure loss: 15.345185908009475.....Total loss: 15.345185908009475\n",
      "Pure loss: 13.480578966189317.....Total loss: 13.480578966189317\n",
      "epoch 3807 learning rate:  0.01026267402153927   Training loss:   15.345185908009475  Valing loss:   13.480578966189317\n",
      "Pure loss: 15.34827455719622.....Total loss: 15.34827455719622\n",
      "Pure loss: 13.480739253173569.....Total loss: 13.480739253173569\n",
      "epoch 3808 learning rate:  0.010262605042016808   Training loss:   15.34827455719622  Valing loss:   13.480739253173569\n",
      "Pure loss: 15.480665548907753.....Total loss: 15.480665548907753\n",
      "Pure loss: 13.516180147800425.....Total loss: 13.516180147800425\n",
      "epoch 3809 learning rate:  0.010262536098713573   Training loss:   15.480665548907753  Valing loss:   13.516180147800425\n",
      "Pure loss: 15.781018145654526.....Total loss: 15.781018145654526\n",
      "Pure loss: 13.619571244037928.....Total loss: 13.619571244037928\n",
      "epoch 3810 learning rate:  0.01026246719160105   Training loss:   15.781018145654526  Valing loss:   13.619571244037928\n",
      "Pure loss: 15.552496158118773.....Total loss: 15.552496158118773\n",
      "Pure loss: 13.536987977541415.....Total loss: 13.536987977541415\n",
      "epoch 3811 learning rate:  0.010262398320650748   Training loss:   15.552496158118773  Valing loss:   13.536987977541415\n",
      "Pure loss: 15.308632346670741.....Total loss: 15.308632346670741\n",
      "Pure loss: 13.536982392709126.....Total loss: 13.536982392709126\n",
      "epoch 3812 learning rate:  0.010262329485834209   Training loss:   15.308632346670741  Valing loss:   13.536982392709126\n",
      "Pure loss: 15.395407077095708.....Total loss: 15.395407077095708\n",
      "Pure loss: 13.51803609018569.....Total loss: 13.51803609018569\n",
      "epoch 3813 learning rate:  0.010262260687123   Training loss:   15.395407077095708  Valing loss:   13.51803609018569\n",
      "Pure loss: 15.317899195679468.....Total loss: 15.317899195679468\n",
      "Pure loss: 13.529994758184978.....Total loss: 13.529994758184978\n",
      "epoch 3814 learning rate:  0.010262191924488726   Training loss:   15.317899195679468  Valing loss:   13.529994758184978\n",
      "Pure loss: 15.296947961360331.....Total loss: 15.296947961360331\n",
      "Pure loss: 13.548752546980872.....Total loss: 13.548752546980872\n",
      "epoch 3815 learning rate:  0.010262123197903015   Training loss:   15.296947961360331  Valing loss:   13.548752546980872\n",
      "Pure loss: 15.402705100517059.....Total loss: 15.402705100517059\n",
      "Pure loss: 13.524362106500963.....Total loss: 13.524362106500963\n",
      "epoch 3816 learning rate:  0.010262054507337526   Training loss:   15.402705100517059  Valing loss:   13.524362106500963\n",
      "Pure loss: 15.696688433633732.....Total loss: 15.696688433633732\n",
      "Pure loss: 13.545925190759894.....Total loss: 13.545925190759894\n",
      "epoch 3817 learning rate:  0.010261985852763951   Training loss:   15.696688433633732  Valing loss:   13.545925190759894\n",
      "Pure loss: 15.73491211163781.....Total loss: 15.73491211163781\n",
      "Pure loss: 13.554730789647023.....Total loss: 13.554730789647023\n",
      "epoch 3818 learning rate:  0.010261917234154008   Training loss:   15.73491211163781  Valing loss:   13.554730789647023\n",
      "Pure loss: 15.859546233900309.....Total loss: 15.859546233900309\n",
      "Pure loss: 13.600142013104515.....Total loss: 13.600142013104515\n",
      "epoch 3819 learning rate:  0.010261848651479446   Training loss:   15.859546233900309  Valing loss:   13.600142013104515\n",
      "Pure loss: 15.844443580198778.....Total loss: 15.844443580198778\n",
      "Pure loss: 13.59245885324432.....Total loss: 13.59245885324432\n",
      "epoch 3820 learning rate:  0.010261780104712043   Training loss:   15.844443580198778  Valing loss:   13.59245885324432\n",
      "Pure loss: 16.200550080688867.....Total loss: 16.200550080688867\n",
      "Pure loss: 13.733301375413197.....Total loss: 13.733301375413197\n",
      "epoch 3821 learning rate:  0.010261711593823606   Training loss:   16.200550080688867  Valing loss:   13.733301375413197\n",
      "Pure loss: 16.197447191801405.....Total loss: 16.197447191801405\n",
      "Pure loss: 13.731975643851053.....Total loss: 13.731975643851053\n",
      "epoch 3822 learning rate:  0.010261643118785977   Training loss:   16.197447191801405  Valing loss:   13.731975643851053\n",
      "Pure loss: 16.203711718641376.....Total loss: 16.203711718641376\n",
      "Pure loss: 13.73468925148491.....Total loss: 13.73468925148491\n",
      "epoch 3823 learning rate:  0.010261574679571018   Training loss:   16.203711718641376  Valing loss:   13.73468925148491\n",
      "Pure loss: 16.449815303110885.....Total loss: 16.449815303110885\n",
      "Pure loss: 13.873287650548706.....Total loss: 13.873287650548706\n",
      "epoch 3824 learning rate:  0.010261506276150629   Training loss:   16.449815303110885  Valing loss:   13.873287650548706\n",
      "Pure loss: 16.78088287375507.....Total loss: 16.78088287375507\n",
      "Pure loss: 14.061484301997824.....Total loss: 14.061484301997824\n",
      "epoch 3825 learning rate:  0.010261437908496733   Training loss:   16.78088287375507  Valing loss:   14.061484301997824\n",
      "Pure loss: 16.694475171619217.....Total loss: 16.694475171619217\n",
      "Pure loss: 13.982784379722373.....Total loss: 13.982784379722373\n",
      "epoch 3826 learning rate:  0.010261369576581286   Training loss:   16.694475171619217  Valing loss:   13.982784379722373\n",
      "Pure loss: 16.86926973453052.....Total loss: 16.86926973453052\n",
      "Pure loss: 14.105458800565234.....Total loss: 14.105458800565234\n",
      "epoch 3827 learning rate:  0.010261301280376275   Training loss:   16.86926973453052  Valing loss:   14.105458800565234\n",
      "Pure loss: 16.94394406088348.....Total loss: 16.94394406088348\n",
      "Pure loss: 14.15995656236467.....Total loss: 14.15995656236467\n",
      "epoch 3828 learning rate:  0.01026123301985371   Training loss:   16.94394406088348  Valing loss:   14.15995656236467\n",
      "Pure loss: 16.88929027810325.....Total loss: 16.88929027810325\n",
      "Pure loss: 14.128862312262482.....Total loss: 14.128862312262482\n",
      "epoch 3829 learning rate:  0.010261164794985636   Training loss:   16.88929027810325  Valing loss:   14.128862312262482\n",
      "Pure loss: 16.66222978518239.....Total loss: 16.66222978518239\n",
      "Pure loss: 13.998194922232736.....Total loss: 13.998194922232736\n",
      "epoch 3830 learning rate:  0.010261096605744126   Training loss:   16.66222978518239  Valing loss:   13.998194922232736\n",
      "Pure loss: 16.478880073699187.....Total loss: 16.478880073699187\n",
      "Pure loss: 13.885392586499467.....Total loss: 13.885392586499467\n",
      "epoch 3831 learning rate:  0.010261028452101278   Training loss:   16.478880073699187  Valing loss:   13.885392586499467\n",
      "Pure loss: 16.404119191088316.....Total loss: 16.404119191088316\n",
      "Pure loss: 13.848230541143204.....Total loss: 13.848230541143204\n",
      "epoch 3832 learning rate:  0.010260960334029227   Training loss:   16.404119191088316  Valing loss:   13.848230541143204\n",
      "Pure loss: 17.10984116501304.....Total loss: 17.10984116501304\n",
      "Pure loss: 14.372481194082537.....Total loss: 14.372481194082537\n",
      "epoch 3833 learning rate:  0.01026089225150013   Training loss:   17.10984116501304  Valing loss:   14.372481194082537\n",
      "Pure loss: 17.056893788174264.....Total loss: 17.056893788174264\n",
      "Pure loss: 14.343568727045044.....Total loss: 14.343568727045044\n",
      "epoch 3834 learning rate:  0.010260824204486176   Training loss:   17.056893788174264  Valing loss:   14.343568727045044\n",
      "Pure loss: 16.526031542294113.....Total loss: 16.526031542294113\n",
      "Pure loss: 14.005233418087881.....Total loss: 14.005233418087881\n",
      "epoch 3835 learning rate:  0.010260756192959583   Training loss:   16.526031542294113  Valing loss:   14.005233418087881\n",
      "Pure loss: 16.268091372073584.....Total loss: 16.268091372073584\n",
      "Pure loss: 13.862375598355428.....Total loss: 13.862375598355428\n",
      "epoch 3836 learning rate:  0.010260688216892596   Training loss:   16.268091372073584  Valing loss:   13.862375598355428\n",
      "Pure loss: 15.967696054702776.....Total loss: 15.967696054702776\n",
      "Pure loss: 13.749890877708404.....Total loss: 13.749890877708404\n",
      "epoch 3837 learning rate:  0.010260620276257492   Training loss:   15.967696054702776  Valing loss:   13.749890877708404\n",
      "Pure loss: 15.46922745266638.....Total loss: 15.46922745266638\n",
      "Pure loss: 13.56004135365305.....Total loss: 13.56004135365305\n",
      "epoch 3838 learning rate:  0.010260552371026577   Training loss:   15.46922745266638  Valing loss:   13.56004135365305\n",
      "Pure loss: 15.525367785932863.....Total loss: 15.525367785932863\n",
      "Pure loss: 13.591292473967066.....Total loss: 13.591292473967066\n",
      "epoch 3839 learning rate:  0.01026048450117218   Training loss:   15.525367785932863  Valing loss:   13.591292473967066\n",
      "Pure loss: 15.20491856796316.....Total loss: 15.20491856796316\n",
      "Pure loss: 13.860513193451391.....Total loss: 13.860513193451391\n",
      "epoch 3840 learning rate:  0.010260416666666668   Training loss:   15.20491856796316  Valing loss:   13.860513193451391\n",
      "Pure loss: 15.214492457298835.....Total loss: 15.214492457298835\n",
      "Pure loss: 14.020612654345078.....Total loss: 14.020612654345078\n",
      "epoch 3841 learning rate:  0.010260348867482427   Training loss:   15.214492457298835  Valing loss:   14.020612654345078\n",
      "Pure loss: 15.463261285195635.....Total loss: 15.463261285195635\n",
      "Pure loss: 14.495469798999611.....Total loss: 14.495469798999611\n",
      "epoch 3842 learning rate:  0.01026028110359188   Training loss:   15.463261285195635  Valing loss:   14.495469798999611\n",
      "Pure loss: 15.761358346118874.....Total loss: 15.761358346118874\n",
      "Pure loss: 15.186768958467427.....Total loss: 15.186768958467427\n",
      "epoch 3843 learning rate:  0.010260213374967474   Training loss:   15.761358346118874  Valing loss:   15.186768958467427\n",
      "Pure loss: 15.330612436147867.....Total loss: 15.330612436147867\n",
      "Pure loss: 14.590692100792337.....Total loss: 14.590692100792337\n",
      "epoch 3844 learning rate:  0.010260145681581685   Training loss:   15.330612436147867  Valing loss:   14.590692100792337\n",
      "Pure loss: 15.083169305716954.....Total loss: 15.083169305716954\n",
      "Pure loss: 14.1756740621601.....Total loss: 14.1756740621601\n",
      "epoch 3845 learning rate:  0.010260078023407022   Training loss:   15.083169305716954  Valing loss:   14.1756740621601\n",
      "Pure loss: 15.990930515169168.....Total loss: 15.990930515169168\n",
      "Pure loss: 15.939683398927928.....Total loss: 15.939683398927928\n",
      "epoch 3846 learning rate:  0.010260010400416016   Training loss:   15.990930515169168  Valing loss:   15.939683398927928\n",
      "Pure loss: 15.72113375235013.....Total loss: 15.72113375235013\n",
      "Pure loss: 15.564744221764155.....Total loss: 15.564744221764155\n",
      "epoch 3847 learning rate:  0.010259942812581233   Training loss:   15.72113375235013  Valing loss:   15.564744221764155\n",
      "Pure loss: 15.247280064476708.....Total loss: 15.247280064476708\n",
      "Pure loss: 14.799063746359021.....Total loss: 14.799063746359021\n",
      "epoch 3848 learning rate:  0.01025987525987526   Training loss:   15.247280064476708  Valing loss:   14.799063746359021\n",
      "Pure loss: 15.131349118610611.....Total loss: 15.131349118610611\n",
      "Pure loss: 14.590924378196261.....Total loss: 14.590924378196261\n",
      "epoch 3849 learning rate:  0.01025980774227072   Training loss:   15.131349118610611  Valing loss:   14.590924378196261\n",
      "Pure loss: 15.09550311266989.....Total loss: 15.09550311266989\n",
      "Pure loss: 14.509585152376296.....Total loss: 14.509585152376296\n",
      "epoch 3850 learning rate:  0.01025974025974026   Training loss:   15.09550311266989  Valing loss:   14.509585152376296\n",
      "Pure loss: 14.958373992852096.....Total loss: 14.958373992852096\n",
      "Pure loss: 14.19134831632499.....Total loss: 14.19134831632499\n",
      "epoch 3851 learning rate:  0.010259672812256556   Training loss:   14.958373992852096  Valing loss:   14.19134831632499\n",
      "Pure loss: 14.963506730454156.....Total loss: 14.963506730454156\n",
      "Pure loss: 14.20368026079769.....Total loss: 14.20368026079769\n",
      "epoch 3852 learning rate:  0.010259605399792315   Training loss:   14.963506730454156  Valing loss:   14.20368026079769\n",
      "Pure loss: 14.981627324027759.....Total loss: 14.981627324027759\n",
      "Pure loss: 14.25214315384177.....Total loss: 14.25214315384177\n",
      "epoch 3853 learning rate:  0.01025953802232027   Training loss:   14.981627324027759  Valing loss:   14.25214315384177\n",
      "Pure loss: 14.994149444258325.....Total loss: 14.994149444258325\n",
      "Pure loss: 14.285651071691635.....Total loss: 14.285651071691635\n",
      "epoch 3854 learning rate:  0.010259470679813181   Training loss:   14.994149444258325  Valing loss:   14.285651071691635\n",
      "Pure loss: 14.975751503676252.....Total loss: 14.975751503676252\n",
      "Pure loss: 14.237848405453706.....Total loss: 14.237848405453706\n",
      "epoch 3855 learning rate:  0.01025940337224384   Training loss:   14.975751503676252  Valing loss:   14.237848405453706\n",
      "Pure loss: 14.949605767108245.....Total loss: 14.949605767108245\n",
      "Pure loss: 14.13527475272125.....Total loss: 14.13527475272125\n",
      "epoch 3856 learning rate:  0.010259336099585063   Training loss:   14.949605767108245  Valing loss:   14.13527475272125\n",
      "Pure loss: 14.924290400240688.....Total loss: 14.924290400240688\n",
      "Pure loss: 14.029344220598439.....Total loss: 14.029344220598439\n",
      "epoch 3857 learning rate:  0.010259268861809697   Training loss:   14.924290400240688  Valing loss:   14.029344220598439\n",
      "Pure loss: 14.933248841837614.....Total loss: 14.933248841837614\n",
      "Pure loss: 14.071636897709373.....Total loss: 14.071636897709373\n",
      "epoch 3858 learning rate:  0.010259201658890616   Training loss:   14.933248841837614  Valing loss:   14.071636897709373\n",
      "Pure loss: 14.939739458670614.....Total loss: 14.939739458670614\n",
      "Pure loss: 14.118790616426496.....Total loss: 14.118790616426496\n",
      "epoch 3859 learning rate:  0.010259134490800726   Training loss:   14.939739458670614  Valing loss:   14.118790616426496\n",
      "Pure loss: 14.888573719663063.....Total loss: 14.888573719663063\n",
      "Pure loss: 13.78467840601996.....Total loss: 13.78467840601996\n",
      "epoch 3860 learning rate:  0.010259067357512953   Training loss:   14.888573719663063  Valing loss:   13.78467840601996\n",
      "Pure loss: 14.903384738952656.....Total loss: 14.903384738952656\n",
      "Pure loss: 13.729815161434178.....Total loss: 13.729815161434178\n",
      "epoch 3861 learning rate:  0.01025900025900026   Training loss:   14.903384738952656  Valing loss:   13.729815161434178\n",
      "Pure loss: 14.864226308397908.....Total loss: 14.864226308397908\n",
      "Pure loss: 13.771944767510982.....Total loss: 13.771944767510982\n",
      "epoch 3862 learning rate:  0.01025893319523563   Training loss:   14.864226308397908  Valing loss:   13.771944767510982\n",
      "Pure loss: 14.877618519544466.....Total loss: 14.877618519544466\n",
      "Pure loss: 13.879799583057869.....Total loss: 13.879799583057869\n",
      "epoch 3863 learning rate:  0.01025886616619208   Training loss:   14.877618519544466  Valing loss:   13.879799583057869\n",
      "Pure loss: 14.887635076078094.....Total loss: 14.887635076078094\n",
      "Pure loss: 13.903229273860044.....Total loss: 13.903229273860044\n",
      "epoch 3864 learning rate:  0.01025879917184265   Training loss:   14.887635076078094  Valing loss:   13.903229273860044\n",
      "Pure loss: 14.922179633119795.....Total loss: 14.922179633119795\n",
      "Pure loss: 14.021459373394448.....Total loss: 14.021459373394448\n",
      "epoch 3865 learning rate:  0.010258732212160414   Training loss:   14.922179633119795  Valing loss:   14.021459373394448\n",
      "Pure loss: 15.032618398235241.....Total loss: 15.032618398235241\n",
      "Pure loss: 14.263463956453256.....Total loss: 14.263463956453256\n",
      "epoch 3866 learning rate:  0.010258665287118468   Training loss:   15.032618398235241  Valing loss:   14.263463956453256\n",
      "Pure loss: 14.899147978656249.....Total loss: 14.899147978656249\n",
      "Pure loss: 13.943401367069772.....Total loss: 13.943401367069772\n",
      "epoch 3867 learning rate:  0.01025859839668994   Training loss:   14.899147978656249  Valing loss:   13.943401367069772\n",
      "Pure loss: 14.936199200578685.....Total loss: 14.936199200578685\n",
      "Pure loss: 14.073815769396171.....Total loss: 14.073815769396171\n",
      "epoch 3868 learning rate:  0.010258531540847984   Training loss:   14.936199200578685  Valing loss:   14.073815769396171\n",
      "Pure loss: 14.840401578933594.....Total loss: 14.840401578933594\n",
      "Pure loss: 13.647154246220364.....Total loss: 13.647154246220364\n",
      "epoch 3869 learning rate:  0.01025846471956578   Training loss:   14.840401578933594  Valing loss:   13.647154246220364\n",
      "Pure loss: 14.856363561227775.....Total loss: 14.856363561227775\n",
      "Pure loss: 13.585198257370044.....Total loss: 13.585198257370044\n",
      "epoch 3870 learning rate:  0.010258397932816538   Training loss:   14.856363561227775  Valing loss:   13.585198257370044\n",
      "Pure loss: 14.838375381267161.....Total loss: 14.838375381267161\n",
      "Pure loss: 13.654219154676575.....Total loss: 13.654219154676575\n",
      "epoch 3871 learning rate:  0.010258331180573496   Training loss:   14.838375381267161  Valing loss:   13.654219154676575\n",
      "Pure loss: 14.840611935834524.....Total loss: 14.840611935834524\n",
      "Pure loss: 13.752889404182467.....Total loss: 13.752889404182467\n",
      "epoch 3872 learning rate:  0.010258264462809918   Training loss:   14.840611935834524  Valing loss:   13.752889404182467\n",
      "Pure loss: 14.85086345799504.....Total loss: 14.85086345799504\n",
      "Pure loss: 13.815111670638428.....Total loss: 13.815111670638428\n",
      "epoch 3873 learning rate:  0.010258197779499097   Training loss:   14.85086345799504  Valing loss:   13.815111670638428\n",
      "Pure loss: 14.8681770672056.....Total loss: 14.8681770672056\n",
      "Pure loss: 13.915564702821706.....Total loss: 13.915564702821706\n",
      "epoch 3874 learning rate:  0.010258131130614353   Training loss:   14.8681770672056  Valing loss:   13.915564702821706\n",
      "Pure loss: 14.864339534625737.....Total loss: 14.864339534625737\n",
      "Pure loss: 13.897896636957348.....Total loss: 13.897896636957348\n",
      "epoch 3875 learning rate:  0.010258064516129033   Training loss:   14.864339534625737  Valing loss:   13.897896636957348\n",
      "Pure loss: 14.92446810175455.....Total loss: 14.92446810175455\n",
      "Pure loss: 14.12863312646643.....Total loss: 14.12863312646643\n",
      "epoch 3876 learning rate:  0.010257997936016512   Training loss:   14.92446810175455  Valing loss:   14.12863312646643\n",
      "Pure loss: 14.872035615986213.....Total loss: 14.872035615986213\n",
      "Pure loss: 13.981233337677695.....Total loss: 13.981233337677695\n",
      "epoch 3877 learning rate:  0.010257931390250193   Training loss:   14.872035615986213  Valing loss:   13.981233337677695\n",
      "Pure loss: 15.011184280341181.....Total loss: 15.011184280341181\n",
      "Pure loss: 14.381201138856012.....Total loss: 14.381201138856012\n",
      "epoch 3878 learning rate:  0.010257864878803508   Training loss:   15.011184280341181  Valing loss:   14.381201138856012\n",
      "Pure loss: 14.965861375302923.....Total loss: 14.965861375302923\n",
      "Pure loss: 14.283986243068759.....Total loss: 14.283986243068759\n",
      "epoch 3879 learning rate:  0.01025779840164991   Training loss:   14.965861375302923  Valing loss:   14.283986243068759\n",
      "Pure loss: 14.891263641381931.....Total loss: 14.891263641381931\n",
      "Pure loss: 14.0203902132164.....Total loss: 14.0203902132164\n",
      "epoch 3880 learning rate:  0.010257731958762888   Training loss:   14.891263641381931  Valing loss:   14.0203902132164\n",
      "Pure loss: 14.967967915085167.....Total loss: 14.967967915085167\n",
      "Pure loss: 14.002799218229066.....Total loss: 14.002799218229066\n",
      "epoch 3881 learning rate:  0.01025766555011595   Training loss:   14.967967915085167  Valing loss:   14.002799218229066\n",
      "Pure loss: 14.962558599142273.....Total loss: 14.962558599142273\n",
      "Pure loss: 13.789298829891392.....Total loss: 13.789298829891392\n",
      "epoch 3882 learning rate:  0.010257599175682638   Training loss:   14.962558599142273  Valing loss:   13.789298829891392\n",
      "Pure loss: 14.944781482434509.....Total loss: 14.944781482434509\n",
      "Pure loss: 13.80974034019189.....Total loss: 13.80974034019189\n",
      "epoch 3883 learning rate:  0.010257532835436518   Training loss:   14.944781482434509  Valing loss:   13.80974034019189\n",
      "Pure loss: 14.950874966030815.....Total loss: 14.950874966030815\n",
      "Pure loss: 14.114496616700286.....Total loss: 14.114496616700286\n",
      "epoch 3884 learning rate:  0.010257466529351184   Training loss:   14.950874966030815  Valing loss:   14.114496616700286\n",
      "Pure loss: 14.939857564252844.....Total loss: 14.939857564252844\n",
      "Pure loss: 14.075905861217592.....Total loss: 14.075905861217592\n",
      "epoch 3885 learning rate:  0.010257400257400257   Training loss:   14.939857564252844  Valing loss:   14.075905861217592\n",
      "Pure loss: 14.912544726284732.....Total loss: 14.912544726284732\n",
      "Pure loss: 14.06229897398106.....Total loss: 14.06229897398106\n",
      "epoch 3886 learning rate:  0.010257334019557385   Training loss:   14.912544726284732  Valing loss:   14.06229897398106\n",
      "Pure loss: 14.923814532303478.....Total loss: 14.923814532303478\n",
      "Pure loss: 14.110680624307225.....Total loss: 14.110680624307225\n",
      "epoch 3887 learning rate:  0.010257267815796244   Training loss:   14.923814532303478  Valing loss:   14.110680624307225\n",
      "Pure loss: 14.89098742648986.....Total loss: 14.89098742648986\n",
      "Pure loss: 13.924360913293912.....Total loss: 13.924360913293912\n",
      "epoch 3888 learning rate:  0.010257201646090536   Training loss:   14.89098742648986  Valing loss:   13.924360913293912\n",
      "Pure loss: 14.891057509868489.....Total loss: 14.891057509868489\n",
      "Pure loss: 13.895551855314368.....Total loss: 13.895551855314368\n",
      "epoch 3889 learning rate:  0.010257135510413988   Training loss:   14.891057509868489  Valing loss:   13.895551855314368\n",
      "Pure loss: 14.912414737380123.....Total loss: 14.912414737380123\n",
      "Pure loss: 14.072492009602733.....Total loss: 14.072492009602733\n",
      "epoch 3890 learning rate:  0.01025706940874036   Training loss:   14.912414737380123  Valing loss:   14.072492009602733\n",
      "Pure loss: 14.988053462610788.....Total loss: 14.988053462610788\n",
      "Pure loss: 13.80767106241572.....Total loss: 13.80767106241572\n",
      "epoch 3891 learning rate:  0.010257003341043434   Training loss:   14.988053462610788  Valing loss:   13.80767106241572\n",
      "Pure loss: 15.125620271016105.....Total loss: 15.125620271016105\n",
      "Pure loss: 13.743582095033936.....Total loss: 13.743582095033936\n",
      "epoch 3892 learning rate:  0.01025693730729702   Training loss:   15.125620271016105  Valing loss:   13.743582095033936\n",
      "Pure loss: 15.102080782941233.....Total loss: 15.102080782941233\n",
      "Pure loss: 13.732946633212642.....Total loss: 13.732946633212642\n",
      "epoch 3893 learning rate:  0.010256871307474956   Training loss:   15.102080782941233  Valing loss:   13.732946633212642\n",
      "Pure loss: 15.295374051323325.....Total loss: 15.295374051323325\n",
      "Pure loss: 13.766796901798998.....Total loss: 13.766796901798998\n",
      "epoch 3894 learning rate:  0.010256805341551104   Training loss:   15.295374051323325  Valing loss:   13.766796901798998\n",
      "Pure loss: 15.4872504806487.....Total loss: 15.4872504806487\n",
      "Pure loss: 13.817178851744103.....Total loss: 13.817178851744103\n",
      "epoch 3895 learning rate:  0.010256739409499359   Training loss:   15.4872504806487  Valing loss:   13.817178851744103\n",
      "Pure loss: 15.25955184982282.....Total loss: 15.25955184982282\n",
      "Pure loss: 13.852821073764828.....Total loss: 13.852821073764828\n",
      "epoch 3896 learning rate:  0.010256673511293635   Training loss:   15.25955184982282  Valing loss:   13.852821073764828\n",
      "Pure loss: 15.228247063808785.....Total loss: 15.228247063808785\n",
      "Pure loss: 13.893389511851316.....Total loss: 13.893389511851316\n",
      "epoch 3897 learning rate:  0.010256607646907879   Training loss:   15.228247063808785  Valing loss:   13.893389511851316\n",
      "Pure loss: 14.883429395751149.....Total loss: 14.883429395751149\n",
      "Pure loss: 13.658973304942917.....Total loss: 13.658973304942917\n",
      "epoch 3898 learning rate:  0.010256541816316059   Training loss:   14.883429395751149  Valing loss:   13.658973304942917\n",
      "Pure loss: 14.881779509366723.....Total loss: 14.881779509366723\n",
      "Pure loss: 13.659772404573342.....Total loss: 13.659772404573342\n",
      "epoch 3899 learning rate:  0.010256476019492177   Training loss:   14.881779509366723  Valing loss:   13.659772404573342\n",
      "Pure loss: 14.862948511281592.....Total loss: 14.862948511281592\n",
      "Pure loss: 14.011031933608358.....Total loss: 14.011031933608358\n",
      "epoch 3900 learning rate:  0.010256410256410256   Training loss:   14.862948511281592  Valing loss:   14.011031933608358\n",
      "Pure loss: 14.863494600206948.....Total loss: 14.863494600206948\n",
      "Pure loss: 13.868371284568967.....Total loss: 13.868371284568967\n",
      "epoch 3901 learning rate:  0.010256344527044348   Training loss:   14.863494600206948  Valing loss:   13.868371284568967\n",
      "Pure loss: 15.064454229175738.....Total loss: 15.064454229175738\n",
      "Pure loss: 13.740317866489594.....Total loss: 13.740317866489594\n",
      "epoch 3902 learning rate:  0.010256278831368528   Training loss:   15.064454229175738  Valing loss:   13.740317866489594\n",
      "Pure loss: 15.347984076233166.....Total loss: 15.347984076233166\n",
      "Pure loss: 13.719703517190439.....Total loss: 13.719703517190439\n",
      "epoch 3903 learning rate:  0.010256213169356905   Training loss:   15.347984076233166  Valing loss:   13.719703517190439\n",
      "Pure loss: 15.265207139078147.....Total loss: 15.265207139078147\n",
      "Pure loss: 13.671735858648562.....Total loss: 13.671735858648562\n",
      "epoch 3904 learning rate:  0.010256147540983607   Training loss:   15.265207139078147  Valing loss:   13.671735858648562\n",
      "Pure loss: 15.2662567401565.....Total loss: 15.2662567401565\n",
      "Pure loss: 13.671683973854902.....Total loss: 13.671683973854902\n",
      "epoch 3905 learning rate:  0.010256081946222791   Training loss:   15.2662567401565  Valing loss:   13.671683973854902\n",
      "Pure loss: 15.252433562180288.....Total loss: 15.252433562180288\n",
      "Pure loss: 13.66863254236825.....Total loss: 13.66863254236825\n",
      "epoch 3906 learning rate:  0.010256016385048644   Training loss:   15.252433562180288  Valing loss:   13.66863254236825\n",
      "Pure loss: 15.139068451756225.....Total loss: 15.139068451756225\n",
      "Pure loss: 13.60948590457013.....Total loss: 13.60948590457013\n",
      "epoch 3907 learning rate:  0.010255950857435372   Training loss:   15.139068451756225  Valing loss:   13.60948590457013\n",
      "Pure loss: 15.076285586636546.....Total loss: 15.076285586636546\n",
      "Pure loss: 13.588278877141175.....Total loss: 13.588278877141175\n",
      "epoch 3908 learning rate:  0.010255885363357217   Training loss:   15.076285586636546  Valing loss:   13.588278877141175\n",
      "Pure loss: 15.016702493997947.....Total loss: 15.016702493997947\n",
      "Pure loss: 13.551031633271892.....Total loss: 13.551031633271892\n",
      "epoch 3909 learning rate:  0.010255819902788437   Training loss:   15.016702493997947  Valing loss:   13.551031633271892\n",
      "Pure loss: 14.9975355323868.....Total loss: 14.9975355323868\n",
      "Pure loss: 13.553306268156168.....Total loss: 13.553306268156168\n",
      "epoch 3910 learning rate:  0.010255754475703325   Training loss:   14.9975355323868  Valing loss:   13.553306268156168\n",
      "Pure loss: 14.941699445319063.....Total loss: 14.941699445319063\n",
      "Pure loss: 13.508595057256944.....Total loss: 13.508595057256944\n",
      "epoch 3911 learning rate:  0.010255689082076195   Training loss:   14.941699445319063  Valing loss:   13.508595057256944\n",
      "Pure loss: 14.84850376774543.....Total loss: 14.84850376774543\n",
      "Pure loss: 13.539125776729932.....Total loss: 13.539125776729932\n",
      "epoch 3912 learning rate:  0.010255623721881392   Training loss:   14.84850376774543  Valing loss:   13.539125776729932\n",
      "Pure loss: 14.862865389374228.....Total loss: 14.862865389374228\n",
      "Pure loss: 13.517071513296738.....Total loss: 13.517071513296738\n",
      "epoch 3913 learning rate:  0.010255558395093279   Training loss:   14.862865389374228  Valing loss:   13.517071513296738\n",
      "Pure loss: 14.877119341838613.....Total loss: 14.877119341838613\n",
      "Pure loss: 13.514174747736234.....Total loss: 13.514174747736234\n",
      "epoch 3914 learning rate:  0.010255493101686254   Training loss:   14.877119341838613  Valing loss:   13.514174747736234\n",
      "Pure loss: 14.92765274465853.....Total loss: 14.92765274465853\n",
      "Pure loss: 13.50380588395518.....Total loss: 13.50380588395518\n",
      "epoch 3915 learning rate:  0.010255427841634739   Training loss:   14.92765274465853  Valing loss:   13.50380588395518\n",
      "Pure loss: 14.79342809158218.....Total loss: 14.79342809158218\n",
      "Pure loss: 13.548299646200562.....Total loss: 13.548299646200562\n",
      "epoch 3916 learning rate:  0.010255362614913176   Training loss:   14.79342809158218  Valing loss:   13.548299646200562\n",
      "Pure loss: 14.79296696840106.....Total loss: 14.79296696840106\n",
      "Pure loss: 13.660896474322637.....Total loss: 13.660896474322637\n",
      "epoch 3917 learning rate:  0.010255297421496043   Training loss:   14.79296696840106  Valing loss:   13.660896474322637\n",
      "Pure loss: 14.922967560314714.....Total loss: 14.922967560314714\n",
      "Pure loss: 14.167808220311064.....Total loss: 14.167808220311064\n",
      "epoch 3918 learning rate:  0.010255232261357836   Training loss:   14.922967560314714  Valing loss:   14.167808220311064\n",
      "Pure loss: 14.89258850151828.....Total loss: 14.89258850151828\n",
      "Pure loss: 14.081481531002755.....Total loss: 14.081481531002755\n",
      "epoch 3919 learning rate:  0.01025516713447308   Training loss:   14.89258850151828  Valing loss:   14.081481531002755\n",
      "Pure loss: 14.857606217550815.....Total loss: 14.857606217550815\n",
      "Pure loss: 14.010068261026118.....Total loss: 14.010068261026118\n",
      "epoch 3920 learning rate:  0.010255102040816326   Training loss:   14.857606217550815  Valing loss:   14.010068261026118\n",
      "Pure loss: 14.97707342999258.....Total loss: 14.97707342999258\n",
      "Pure loss: 14.29463439389593.....Total loss: 14.29463439389593\n",
      "epoch 3921 learning rate:  0.010255036980362153   Training loss:   14.97707342999258  Valing loss:   14.29463439389593\n",
      "Pure loss: 15.101776677121482.....Total loss: 15.101776677121482\n",
      "Pure loss: 14.55459514992597.....Total loss: 14.55459514992597\n",
      "epoch 3922 learning rate:  0.010254971953085161   Training loss:   15.101776677121482  Valing loss:   14.55459514992597\n",
      "Pure loss: 15.056599637336948.....Total loss: 15.056599637336948\n",
      "Pure loss: 14.476378255293945.....Total loss: 14.476378255293945\n",
      "epoch 3923 learning rate:  0.01025490695895998   Training loss:   15.056599637336948  Valing loss:   14.476378255293945\n",
      "Pure loss: 15.091846052188151.....Total loss: 15.091846052188151\n",
      "Pure loss: 14.542989274008383.....Total loss: 14.542989274008383\n",
      "epoch 3924 learning rate:  0.010254841997961265   Training loss:   15.091846052188151  Valing loss:   14.542989274008383\n",
      "Pure loss: 15.036356863073427.....Total loss: 15.036356863073427\n",
      "Pure loss: 14.426456141883666.....Total loss: 14.426456141883666\n",
      "epoch 3925 learning rate:  0.010254777070063694   Training loss:   15.036356863073427  Valing loss:   14.426456141883666\n",
      "Pure loss: 14.988104099497312.....Total loss: 14.988104099497312\n",
      "Pure loss: 14.347496431871708.....Total loss: 14.347496431871708\n",
      "epoch 3926 learning rate:  0.010254712175241976   Training loss:   14.988104099497312  Valing loss:   14.347496431871708\n",
      "Pure loss: 15.053699066335101.....Total loss: 15.053699066335101\n",
      "Pure loss: 14.455849615709774.....Total loss: 14.455849615709774\n",
      "epoch 3927 learning rate:  0.010254647313470843   Training loss:   15.053699066335101  Valing loss:   14.455849615709774\n",
      "Pure loss: 15.011314159511095.....Total loss: 15.011314159511095\n",
      "Pure loss: 14.371510113300495.....Total loss: 14.371510113300495\n",
      "epoch 3928 learning rate:  0.010254582484725052   Training loss:   15.011314159511095  Valing loss:   14.371510113300495\n",
      "Pure loss: 14.909009828601949.....Total loss: 14.909009828601949\n",
      "Pure loss: 14.10042627000653.....Total loss: 14.10042627000653\n",
      "epoch 3929 learning rate:  0.010254517688979385   Training loss:   14.909009828601949  Valing loss:   14.10042627000653\n",
      "Pure loss: 14.805842772990149.....Total loss: 14.805842772990149\n",
      "Pure loss: 13.901808636814087.....Total loss: 13.901808636814087\n",
      "epoch 3930 learning rate:  0.010254452926208652   Training loss:   14.805842772990149  Valing loss:   13.901808636814087\n",
      "Pure loss: 14.761146884998066.....Total loss: 14.761146884998066\n",
      "Pure loss: 13.734463389929513.....Total loss: 13.734463389929513\n",
      "epoch 3931 learning rate:  0.010254388196387687   Training loss:   14.761146884998066  Valing loss:   13.734463389929513\n",
      "Pure loss: 14.768644618501577.....Total loss: 14.768644618501577\n",
      "Pure loss: 13.761246847497345.....Total loss: 13.761246847497345\n",
      "epoch 3932 learning rate:  0.010254323499491354   Training loss:   14.768644618501577  Valing loss:   13.761246847497345\n",
      "Pure loss: 14.752149282251219.....Total loss: 14.752149282251219\n",
      "Pure loss: 13.62703602166576.....Total loss: 13.62703602166576\n",
      "epoch 3933 learning rate:  0.010254258835494533   Training loss:   14.752149282251219  Valing loss:   13.62703602166576\n",
      "Pure loss: 14.771084835758588.....Total loss: 14.771084835758588\n",
      "Pure loss: 13.512002985910081.....Total loss: 13.512002985910081\n",
      "epoch 3934 learning rate:  0.01025419420437214   Training loss:   14.771084835758588  Valing loss:   13.512002985910081\n",
      "Pure loss: 14.943100845992142.....Total loss: 14.943100845992142\n",
      "Pure loss: 14.2176998431206.....Total loss: 14.2176998431206\n",
      "epoch 3935 learning rate:  0.01025412960609911   Training loss:   14.943100845992142  Valing loss:   14.2176998431206\n",
      "Pure loss: 14.808989627772755.....Total loss: 14.808989627772755\n",
      "Pure loss: 13.82207228669786.....Total loss: 13.82207228669786\n",
      "epoch 3936 learning rate:  0.010254065040650406   Training loss:   14.808989627772755  Valing loss:   13.82207228669786\n",
      "Pure loss: 14.804181896413496.....Total loss: 14.804181896413496\n",
      "Pure loss: 13.808450584457933.....Total loss: 13.808450584457933\n",
      "epoch 3937 learning rate:  0.010254000508001015   Training loss:   14.804181896413496  Valing loss:   13.808450584457933\n",
      "Pure loss: 14.797181504363508.....Total loss: 14.797181504363508\n",
      "Pure loss: 13.781346483543842.....Total loss: 13.781346483543842\n",
      "epoch 3938 learning rate:  0.010253936008125953   Training loss:   14.797181504363508  Valing loss:   13.781346483543842\n",
      "Pure loss: 14.852892414814987.....Total loss: 14.852892414814987\n",
      "Pure loss: 13.949806251648702.....Total loss: 13.949806251648702\n",
      "epoch 3939 learning rate:  0.010253871541000254   Training loss:   14.852892414814987  Valing loss:   13.949806251648702\n",
      "Pure loss: 14.798854971102239.....Total loss: 14.798854971102239\n",
      "Pure loss: 13.731285137335927.....Total loss: 13.731285137335927\n",
      "epoch 3940 learning rate:  0.010253807106598985   Training loss:   14.798854971102239  Valing loss:   13.731285137335927\n",
      "Pure loss: 14.73238285270676.....Total loss: 14.73238285270676\n",
      "Pure loss: 13.486539253923597.....Total loss: 13.486539253923597\n",
      "epoch 3941 learning rate:  0.010253742704897234   Training loss:   14.73238285270676  Valing loss:   13.486539253923597\n",
      "Pure loss: 14.729072791897316.....Total loss: 14.729072791897316\n",
      "Pure loss: 13.583175821172718.....Total loss: 13.583175821172718\n",
      "epoch 3942 learning rate:  0.010253678335870117   Training loss:   14.729072791897316  Valing loss:   13.583175821172718\n",
      "Pure loss: 14.734681444575402.....Total loss: 14.734681444575402\n",
      "Pure loss: 13.627668550084085.....Total loss: 13.627668550084085\n",
      "epoch 3943 learning rate:  0.010253613999492771   Training loss:   14.734681444575402  Valing loss:   13.627668550084085\n",
      "Pure loss: 14.83938013988518.....Total loss: 14.83938013988518\n",
      "Pure loss: 14.03644093210088.....Total loss: 14.03644093210088\n",
      "epoch 3944 learning rate:  0.010253549695740365   Training loss:   14.83938013988518  Valing loss:   14.03644093210088\n",
      "Pure loss: 14.728112450292286.....Total loss: 14.728112450292286\n",
      "Pure loss: 13.691043014291333.....Total loss: 13.691043014291333\n",
      "epoch 3945 learning rate:  0.010253485424588087   Training loss:   14.728112450292286  Valing loss:   13.691043014291333\n",
      "Pure loss: 14.718728823954804.....Total loss: 14.718728823954804\n",
      "Pure loss: 13.565154491662241.....Total loss: 13.565154491662241\n",
      "epoch 3946 learning rate:  0.010253421186011151   Training loss:   14.718728823954804  Valing loss:   13.565154491662241\n",
      "Pure loss: 14.718274722162764.....Total loss: 14.718274722162764\n",
      "Pure loss: 13.57974129229959.....Total loss: 13.57974129229959\n",
      "epoch 3947 learning rate:  0.010253356979984798   Training loss:   14.718274722162764  Valing loss:   13.57974129229959\n",
      "Pure loss: 14.911546575342792.....Total loss: 14.911546575342792\n",
      "Pure loss: 14.067819191429628.....Total loss: 14.067819191429628\n",
      "epoch 3948 learning rate:  0.010253292806484297   Training loss:   14.911546575342792  Valing loss:   14.067819191429628\n",
      "Pure loss: 14.74089059888446.....Total loss: 14.74089059888446\n",
      "Pure loss: 13.765871670658493.....Total loss: 13.765871670658493\n",
      "epoch 3949 learning rate:  0.010253228665484932   Training loss:   14.74089059888446  Valing loss:   13.765871670658493\n",
      "Pure loss: 14.75453280312599.....Total loss: 14.75453280312599\n",
      "Pure loss: 13.7999470065079.....Total loss: 13.7999470065079\n",
      "epoch 3950 learning rate:  0.010253164556962025   Training loss:   14.75453280312599  Valing loss:   13.7999470065079\n",
      "Pure loss: 14.816028003433626.....Total loss: 14.816028003433626\n",
      "Pure loss: 14.019189448466191.....Total loss: 14.019189448466191\n",
      "epoch 3951 learning rate:  0.010253100480890913   Training loss:   14.816028003433626  Valing loss:   14.019189448466191\n",
      "Pure loss: 14.906648434874437.....Total loss: 14.906648434874437\n",
      "Pure loss: 14.240088061070757.....Total loss: 14.240088061070757\n",
      "epoch 3952 learning rate:  0.010253036437246964   Training loss:   14.906648434874437  Valing loss:   14.240088061070757\n",
      "Pure loss: 15.198185005009401.....Total loss: 15.198185005009401\n",
      "Pure loss: 14.72196235033917.....Total loss: 14.72196235033917\n",
      "epoch 3953 learning rate:  0.010252972426005565   Training loss:   15.198185005009401  Valing loss:   14.72196235033917\n",
      "Pure loss: 15.173644479993527.....Total loss: 15.173644479993527\n",
      "Pure loss: 14.680405154255547.....Total loss: 14.680405154255547\n",
      "epoch 3954 learning rate:  0.010252908447142134   Training loss:   15.173644479993527  Valing loss:   14.680405154255547\n",
      "Pure loss: 15.307751642340527.....Total loss: 15.307751642340527\n",
      "Pure loss: 14.908404029088246.....Total loss: 14.908404029088246\n",
      "epoch 3955 learning rate:  0.010252844500632111   Training loss:   15.307751642340527  Valing loss:   14.908404029088246\n",
      "Pure loss: 15.658323820844151.....Total loss: 15.658323820844151\n",
      "Pure loss: 15.401982257501578.....Total loss: 15.401982257501578\n",
      "epoch 3956 learning rate:  0.01025278058645096   Training loss:   15.658323820844151  Valing loss:   15.401982257501578\n",
      "Pure loss: 15.412695289876332.....Total loss: 15.412695289876332\n",
      "Pure loss: 15.016784196416225.....Total loss: 15.016784196416225\n",
      "epoch 3957 learning rate:  0.010252716704574172   Training loss:   15.412695289876332  Valing loss:   15.016784196416225\n",
      "Pure loss: 15.336492353879914.....Total loss: 15.336492353879914\n",
      "Pure loss: 14.924389378004328.....Total loss: 14.924389378004328\n",
      "epoch 3958 learning rate:  0.010252652854977262   Training loss:   15.336492353879914  Valing loss:   14.924389378004328\n",
      "Pure loss: 14.93372770067908.....Total loss: 14.93372770067908\n",
      "Pure loss: 14.197943442383034.....Total loss: 14.197943442383034\n",
      "epoch 3959 learning rate:  0.010252589037635767   Training loss:   14.93372770067908  Valing loss:   14.197943442383034\n",
      "Pure loss: 14.819583808579702.....Total loss: 14.819583808579702\n",
      "Pure loss: 14.024271849437717.....Total loss: 14.024271849437717\n",
      "epoch 3960 learning rate:  0.010252525252525253   Training loss:   14.819583808579702  Valing loss:   14.024271849437717\n",
      "Pure loss: 14.984315571639243.....Total loss: 14.984315571639243\n",
      "Pure loss: 14.391782290150827.....Total loss: 14.391782290150827\n",
      "epoch 3961 learning rate:  0.010252461499621309   Training loss:   14.984315571639243  Valing loss:   14.391782290150827\n",
      "Pure loss: 14.934331816994577.....Total loss: 14.934331816994577\n",
      "Pure loss: 14.29185635084611.....Total loss: 14.29185635084611\n",
      "epoch 3962 learning rate:  0.010252397778899545   Training loss:   14.934331816994577  Valing loss:   14.29185635084611\n",
      "Pure loss: 14.87982501432775.....Total loss: 14.87982501432775\n",
      "Pure loss: 14.167729523352309.....Total loss: 14.167729523352309\n",
      "epoch 3963 learning rate:  0.010252334090335605   Training loss:   14.87982501432775  Valing loss:   14.167729523352309\n",
      "Pure loss: 15.245689825173505.....Total loss: 15.245689825173505\n",
      "Pure loss: 14.814579820674437.....Total loss: 14.814579820674437\n",
      "epoch 3964 learning rate:  0.010252270433905147   Training loss:   15.245689825173505  Valing loss:   14.814579820674437\n",
      "Pure loss: 15.117182261100576.....Total loss: 15.117182261100576\n",
      "Pure loss: 14.585830396007749.....Total loss: 14.585830396007749\n",
      "epoch 3965 learning rate:  0.010252206809583858   Training loss:   15.117182261100576  Valing loss:   14.585830396007749\n",
      "Pure loss: 15.031266494735638.....Total loss: 15.031266494735638\n",
      "Pure loss: 14.432226891041227.....Total loss: 14.432226891041227\n",
      "epoch 3966 learning rate:  0.010252143217347454   Training loss:   15.031266494735638  Valing loss:   14.432226891041227\n",
      "Pure loss: 14.782823697206812.....Total loss: 14.782823697206812\n",
      "Pure loss: 13.900987142396351.....Total loss: 13.900987142396351\n",
      "epoch 3967 learning rate:  0.010252079657171667   Training loss:   14.782823697206812  Valing loss:   13.900987142396351\n",
      "Pure loss: 14.865657345585209.....Total loss: 14.865657345585209\n",
      "Pure loss: 14.099376030687461.....Total loss: 14.099376030687461\n",
      "epoch 3968 learning rate:  0.010252016129032258   Training loss:   14.865657345585209  Valing loss:   14.099376030687461\n",
      "Pure loss: 15.03766223342845.....Total loss: 15.03766223342845\n",
      "Pure loss: 14.436470516373445.....Total loss: 14.436470516373445\n",
      "epoch 3969 learning rate:  0.010251952632905015   Training loss:   15.03766223342845  Valing loss:   14.436470516373445\n",
      "Pure loss: 15.071213561064972.....Total loss: 15.071213561064972\n",
      "Pure loss: 14.484192571865124.....Total loss: 14.484192571865124\n",
      "epoch 3970 learning rate:  0.010251889168765743   Training loss:   15.071213561064972  Valing loss:   14.484192571865124\n",
      "Pure loss: 14.845122083353782.....Total loss: 14.845122083353782\n",
      "Pure loss: 14.062366461056943.....Total loss: 14.062366461056943\n",
      "epoch 3971 learning rate:  0.01025182573659028   Training loss:   14.845122083353782  Valing loss:   14.062366461056943\n",
      "Pure loss: 14.775229426670837.....Total loss: 14.775229426670837\n",
      "Pure loss: 13.864554485080692.....Total loss: 13.864554485080692\n",
      "epoch 3972 learning rate:  0.010251762336354482   Training loss:   14.775229426670837  Valing loss:   13.864554485080692\n",
      "Pure loss: 15.9643165975642.....Total loss: 15.9643165975642\n",
      "Pure loss: 15.680013024718846.....Total loss: 15.680013024718846\n",
      "epoch 3973 learning rate:  0.010251698968034231   Training loss:   15.9643165975642  Valing loss:   15.680013024718846\n",
      "Pure loss: 16.296054458769227.....Total loss: 16.296054458769227\n",
      "Pure loss: 16.21752919693781.....Total loss: 16.21752919693781\n",
      "epoch 3974 learning rate:  0.010251635631605435   Training loss:   16.296054458769227  Valing loss:   16.21752919693781\n",
      "Pure loss: 16.094891192084898.....Total loss: 16.094891192084898\n",
      "Pure loss: 15.876060054450424.....Total loss: 15.876060054450424\n",
      "epoch 3975 learning rate:  0.010251572327044026   Training loss:   16.094891192084898  Valing loss:   15.876060054450424\n",
      "Pure loss: 15.807736586001626.....Total loss: 15.807736586001626\n",
      "Pure loss: 15.462069212299681.....Total loss: 15.462069212299681\n",
      "epoch 3976 learning rate:  0.010251509054325956   Training loss:   15.807736586001626  Valing loss:   15.462069212299681\n",
      "Pure loss: 16.08400626856483.....Total loss: 16.08400626856483\n",
      "Pure loss: 15.845054624831144.....Total loss: 15.845054624831144\n",
      "epoch 3977 learning rate:  0.010251445813427207   Training loss:   16.08400626856483  Valing loss:   15.845054624831144\n",
      "Pure loss: 16.80835198551983.....Total loss: 16.80835198551983\n",
      "Pure loss: 16.99764556572125.....Total loss: 16.99764556572125\n",
      "epoch 3978 learning rate:  0.010251382604323782   Training loss:   16.80835198551983  Valing loss:   16.99764556572125\n",
      "Pure loss: 15.994767572639113.....Total loss: 15.994767572639113\n",
      "Pure loss: 15.861784192867873.....Total loss: 15.861784192867873\n",
      "epoch 3979 learning rate:  0.010251319426991706   Training loss:   15.994767572639113  Valing loss:   15.861784192867873\n",
      "Pure loss: 15.738482192420983.....Total loss: 15.738482192420983\n",
      "Pure loss: 15.49611105670247.....Total loss: 15.49611105670247\n",
      "epoch 3980 learning rate:  0.010251256281407035   Training loss:   15.738482192420983  Valing loss:   15.49611105670247\n",
      "Pure loss: 15.057959374734093.....Total loss: 15.057959374734093\n",
      "Pure loss: 14.525613695893849.....Total loss: 14.525613695893849\n",
      "epoch 3981 learning rate:  0.010251193167545842   Training loss:   15.057959374734093  Valing loss:   14.525613695893849\n",
      "Pure loss: 14.87915962228657.....Total loss: 14.87915962228657\n",
      "Pure loss: 14.2389847724224.....Total loss: 14.2389847724224\n",
      "epoch 3982 learning rate:  0.01025113008538423   Training loss:   14.87915962228657  Valing loss:   14.2389847724224\n",
      "Pure loss: 14.785614594613973.....Total loss: 14.785614594613973\n",
      "Pure loss: 14.039575034839439.....Total loss: 14.039575034839439\n",
      "epoch 3983 learning rate:  0.010251067034898318   Training loss:   14.785614594613973  Valing loss:   14.039575034839439\n",
      "Pure loss: 14.732352613448295.....Total loss: 14.732352613448295\n",
      "Pure loss: 13.892762896005403.....Total loss: 13.892762896005403\n",
      "epoch 3984 learning rate:  0.010251004016064257   Training loss:   14.732352613448295  Valing loss:   13.892762896005403\n",
      "Pure loss: 14.966161880326473.....Total loss: 14.966161880326473\n",
      "Pure loss: 14.407138212872423.....Total loss: 14.407138212872423\n",
      "epoch 3985 learning rate:  0.01025094102885822   Training loss:   14.966161880326473  Valing loss:   14.407138212872423\n",
      "Pure loss: 14.843261358215619.....Total loss: 14.843261358215619\n",
      "Pure loss: 14.168497132515919.....Total loss: 14.168497132515919\n",
      "epoch 3986 learning rate:  0.010250878073256397   Training loss:   14.843261358215619  Valing loss:   14.168497132515919\n",
      "Pure loss: 14.774281285043823.....Total loss: 14.774281285043823\n",
      "Pure loss: 14.002494701997655.....Total loss: 14.002494701997655\n",
      "epoch 3987 learning rate:  0.010250815149235013   Training loss:   14.774281285043823  Valing loss:   14.002494701997655\n",
      "Pure loss: 14.774063162721381.....Total loss: 14.774063162721381\n",
      "Pure loss: 13.957800517141036.....Total loss: 13.957800517141036\n",
      "epoch 3988 learning rate:  0.010250752256770312   Training loss:   14.774063162721381  Valing loss:   13.957800517141036\n",
      "Pure loss: 14.737474493610044.....Total loss: 14.737474493610044\n",
      "Pure loss: 13.847063863053846.....Total loss: 13.847063863053846\n",
      "epoch 3989 learning rate:  0.010250689395838557   Training loss:   14.737474493610044  Valing loss:   13.847063863053846\n",
      "Pure loss: 14.751799908284855.....Total loss: 14.751799908284855\n",
      "Pure loss: 13.898323913125406.....Total loss: 13.898323913125406\n",
      "epoch 3990 learning rate:  0.01025062656641604   Training loss:   14.751799908284855  Valing loss:   13.898323913125406\n",
      "Pure loss: 14.964686181172201.....Total loss: 14.964686181172201\n",
      "Pure loss: 14.455325996599646.....Total loss: 14.455325996599646\n",
      "epoch 3991 learning rate:  0.010250563768479078   Training loss:   14.964686181172201  Valing loss:   14.455325996599646\n",
      "Pure loss: 15.154449661341378.....Total loss: 15.154449661341378\n",
      "Pure loss: 14.83225871285658.....Total loss: 14.83225871285658\n",
      "epoch 3992 learning rate:  0.010250501002004008   Training loss:   15.154449661341378  Valing loss:   14.83225871285658\n",
      "Pure loss: 15.100236554539313.....Total loss: 15.100236554539313\n",
      "Pure loss: 14.72043240678017.....Total loss: 14.72043240678017\n",
      "epoch 3993 learning rate:  0.010250438266967192   Training loss:   15.100236554539313  Valing loss:   14.72043240678017\n",
      "Pure loss: 15.189727064096079.....Total loss: 15.189727064096079\n",
      "Pure loss: 14.889685830800202.....Total loss: 14.889685830800202\n",
      "epoch 3994 learning rate:  0.010250375563345017   Training loss:   15.189727064096079  Valing loss:   14.889685830800202\n",
      "Pure loss: 15.238337510747053.....Total loss: 15.238337510747053\n",
      "Pure loss: 14.97136287026553.....Total loss: 14.97136287026553\n",
      "epoch 3995 learning rate:  0.010250312891113892   Training loss:   15.238337510747053  Valing loss:   14.97136287026553\n",
      "Pure loss: 15.056495064705347.....Total loss: 15.056495064705347\n",
      "Pure loss: 14.644366856060355.....Total loss: 14.644366856060355\n",
      "epoch 3996 learning rate:  0.01025025025025025   Training loss:   15.056495064705347  Valing loss:   14.644366856060355\n",
      "Pure loss: 14.961602702485695.....Total loss: 14.961602702485695\n",
      "Pure loss: 14.453774083300143.....Total loss: 14.453774083300143\n",
      "epoch 3997 learning rate:  0.010250187640730548   Training loss:   14.961602702485695  Valing loss:   14.453774083300143\n",
      "Pure loss: 15.238238830365956.....Total loss: 15.238238830365956\n",
      "Pure loss: 14.999513381963752.....Total loss: 14.999513381963752\n",
      "epoch 3998 learning rate:  0.010250125062531265   Training loss:   15.238238830365956  Valing loss:   14.999513381963752\n",
      "Pure loss: 15.339265813672812.....Total loss: 15.339265813672812\n",
      "Pure loss: 15.173979387125504.....Total loss: 15.173979387125504\n",
      "epoch 3999 learning rate:  0.010250062515628907   Training loss:   15.339265813672812  Valing loss:   15.173979387125504\n",
      "Pure loss: 15.511545283438112.....Total loss: 15.511545283438112\n",
      "Pure loss: 15.412495186343754.....Total loss: 15.412495186343754\n",
      "epoch 4000 learning rate:  0.01025   Training loss:   15.511545283438112  Valing loss:   15.412495186343754\n",
      "Pure loss: 15.721029755915529.....Total loss: 15.721029755915529\n",
      "Pure loss: 15.709598821028285.....Total loss: 15.709598821028285\n",
      "epoch 4001 learning rate:  0.010249937515621095   Training loss:   15.721029755915529  Valing loss:   15.709598821028285\n",
      "Pure loss: 15.75455114073154.....Total loss: 15.75455114073154\n",
      "Pure loss: 15.751596893619961.....Total loss: 15.751596893619961\n",
      "epoch 4002 learning rate:  0.010249875062468767   Training loss:   15.75455114073154  Valing loss:   15.751596893619961\n",
      "Pure loss: 15.509757320278032.....Total loss: 15.509757320278032\n",
      "Pure loss: 15.386553236040466.....Total loss: 15.386553236040466\n",
      "epoch 4003 learning rate:  0.01024981264051961   Training loss:   15.509757320278032  Valing loss:   15.386553236040466\n",
      "Pure loss: 14.902499574928129.....Total loss: 14.902499574928129\n",
      "Pure loss: 14.355773056587905.....Total loss: 14.355773056587905\n",
      "epoch 4004 learning rate:  0.01024975024975025   Training loss:   14.902499574928129  Valing loss:   14.355773056587905\n",
      "Pure loss: 14.71481572305087.....Total loss: 14.71481572305087\n",
      "Pure loss: 13.940566492133593.....Total loss: 13.940566492133593\n",
      "epoch 4005 learning rate:  0.010249687890137329   Training loss:   14.71481572305087  Valing loss:   13.940566492133593\n",
      "Pure loss: 14.851224197905783.....Total loss: 14.851224197905783\n",
      "Pure loss: 14.245613770444477.....Total loss: 14.245613770444477\n",
      "epoch 4006 learning rate:  0.010249625561657514   Training loss:   14.851224197905783  Valing loss:   14.245613770444477\n",
      "Pure loss: 14.701758385874175.....Total loss: 14.701758385874175\n",
      "Pure loss: 13.922162116932698.....Total loss: 13.922162116932698\n",
      "epoch 4007 learning rate:  0.010249563264287498   Training loss:   14.701758385874175  Valing loss:   13.922162116932698\n",
      "Pure loss: 14.731435621711524.....Total loss: 14.731435621711524\n",
      "Pure loss: 13.99603188684712.....Total loss: 13.99603188684712\n",
      "epoch 4008 learning rate:  0.010249500998003992   Training loss:   14.731435621711524  Valing loss:   13.99603188684712\n",
      "Pure loss: 14.894886907403986.....Total loss: 14.894886907403986\n",
      "Pure loss: 14.341691954694259.....Total loss: 14.341691954694259\n",
      "epoch 4009 learning rate:  0.010249438762783737   Training loss:   14.894886907403986  Valing loss:   14.341691954694259\n",
      "Pure loss: 14.83078118975693.....Total loss: 14.83078118975693\n",
      "Pure loss: 14.219519450057948.....Total loss: 14.219519450057948\n",
      "epoch 4010 learning rate:  0.01024937655860349   Training loss:   14.83078118975693  Valing loss:   14.219519450057948\n",
      "Pure loss: 14.824875202378008.....Total loss: 14.824875202378008\n",
      "Pure loss: 14.207046079732692.....Total loss: 14.207046079732692\n",
      "epoch 4011 learning rate:  0.01024931438544004   Training loss:   14.824875202378008  Valing loss:   14.207046079732692\n",
      "Pure loss: 14.984005248853668.....Total loss: 14.984005248853668\n",
      "Pure loss: 14.328039671556166.....Total loss: 14.328039671556166\n",
      "epoch 4012 learning rate:  0.010249252243270189   Training loss:   14.984005248853668  Valing loss:   14.328039671556166\n",
      "Pure loss: 14.85822022060285.....Total loss: 14.85822022060285\n",
      "Pure loss: 14.0888498393952.....Total loss: 14.0888498393952\n",
      "epoch 4013 learning rate:  0.010249190132070771   Training loss:   14.85822022060285  Valing loss:   14.0888498393952\n",
      "Pure loss: 14.89275898017922.....Total loss: 14.89275898017922\n",
      "Pure loss: 14.143783099940222.....Total loss: 14.143783099940222\n",
      "epoch 4014 learning rate:  0.010249128051818634   Training loss:   14.89275898017922  Valing loss:   14.143783099940222\n",
      "Pure loss: 15.10057061586951.....Total loss: 15.10057061586951\n",
      "Pure loss: 14.478184921796425.....Total loss: 14.478184921796425\n",
      "epoch 4015 learning rate:  0.01024906600249066   Training loss:   15.10057061586951  Valing loss:   14.478184921796425\n",
      "Pure loss: 15.50872245790311.....Total loss: 15.50872245790311\n",
      "Pure loss: 14.87497226457549.....Total loss: 14.87497226457549\n",
      "epoch 4016 learning rate:  0.010249003984063745   Training loss:   15.50872245790311  Valing loss:   14.87497226457549\n",
      "Pure loss: 15.373403717792382.....Total loss: 15.373403717792382\n",
      "Pure loss: 14.63480942405115.....Total loss: 14.63480942405115\n",
      "epoch 4017 learning rate:  0.010248941996514813   Training loss:   15.373403717792382  Valing loss:   14.63480942405115\n",
      "Pure loss: 15.237322983291898.....Total loss: 15.237322983291898\n",
      "Pure loss: 14.44472881053829.....Total loss: 14.44472881053829\n",
      "epoch 4018 learning rate:  0.010248880039820807   Training loss:   15.237322983291898  Valing loss:   14.44472881053829\n",
      "Pure loss: 15.092568612429586.....Total loss: 15.092568612429586\n",
      "Pure loss: 14.193173388863345.....Total loss: 14.193173388863345\n",
      "epoch 4019 learning rate:  0.010248818113958696   Training loss:   15.092568612429586  Valing loss:   14.193173388863345\n",
      "Pure loss: 14.953292966412587.....Total loss: 14.953292966412587\n",
      "Pure loss: 13.848000779248055.....Total loss: 13.848000779248055\n",
      "epoch 4020 learning rate:  0.010248756218905473   Training loss:   14.953292966412587  Valing loss:   13.848000779248055\n",
      "Pure loss: 14.79674601705688.....Total loss: 14.79674601705688\n",
      "Pure loss: 13.395741072192813.....Total loss: 13.395741072192813\n",
      "epoch 4021 learning rate:  0.01024869435463815   Training loss:   14.79674601705688  Valing loss:   13.395741072192813\n",
      "Pure loss: 14.8184487871835.....Total loss: 14.8184487871835\n",
      "Pure loss: 13.105252249383309.....Total loss: 13.105252249383309\n",
      "epoch 4022 learning rate:  0.010248632521133765   Training loss:   14.8184487871835  Valing loss:   13.105252249383309\n",
      "Pure loss: 14.876113069216002.....Total loss: 14.876113069216002\n",
      "Pure loss: 13.008997119998996.....Total loss: 13.008997119998996\n",
      "epoch 4023 learning rate:  0.010248570718369376   Training loss:   14.876113069216002  Valing loss:   13.008997119998996\n",
      "Pure loss: 14.863104768317774.....Total loss: 14.863104768317774\n",
      "Pure loss: 13.016705072106834.....Total loss: 13.016705072106834\n",
      "epoch 4024 learning rate:  0.010248508946322068   Training loss:   14.863104768317774  Valing loss:   13.016705072106834\n",
      "Pure loss: 14.782561476557218.....Total loss: 14.782561476557218\n",
      "Pure loss: 13.184866037506342.....Total loss: 13.184866037506342\n",
      "epoch 4025 learning rate:  0.010248447204968944   Training loss:   14.782561476557218  Valing loss:   13.184866037506342\n",
      "Pure loss: 14.880475249672475.....Total loss: 14.880475249672475\n",
      "Pure loss: 13.026915000145008.....Total loss: 13.026915000145008\n",
      "epoch 4026 learning rate:  0.010248385494287134   Training loss:   14.880475249672475  Valing loss:   13.026915000145008\n",
      "Pure loss: 14.734255182280528.....Total loss: 14.734255182280528\n",
      "Pure loss: 13.159896352243896.....Total loss: 13.159896352243896\n",
      "epoch 4027 learning rate:  0.010248323814253788   Training loss:   14.734255182280528  Valing loss:   13.159896352243896\n",
      "Pure loss: 14.743465096551633.....Total loss: 14.743465096551633\n",
      "Pure loss: 13.094081250421063.....Total loss: 13.094081250421063\n",
      "epoch 4028 learning rate:  0.010248262164846077   Training loss:   14.743465096551633  Valing loss:   13.094081250421063\n",
      "Pure loss: 14.759301186631411.....Total loss: 14.759301186631411\n",
      "Pure loss: 13.54312083643218.....Total loss: 13.54312083643218\n",
      "epoch 4029 learning rate:  0.0102482005460412   Training loss:   14.759301186631411  Valing loss:   13.54312083643218\n",
      "Pure loss: 15.004041573263414.....Total loss: 15.004041573263414\n",
      "Pure loss: 14.26984454159077.....Total loss: 14.26984454159077\n",
      "epoch 4030 learning rate:  0.010248138957816377   Training loss:   15.004041573263414  Valing loss:   14.26984454159077\n",
      "Pure loss: 14.796769763054465.....Total loss: 14.796769763054465\n",
      "Pure loss: 13.855240434462335.....Total loss: 13.855240434462335\n",
      "epoch 4031 learning rate:  0.010248077400148846   Training loss:   14.796769763054465  Valing loss:   13.855240434462335\n",
      "Pure loss: 14.823049448829162.....Total loss: 14.823049448829162\n",
      "Pure loss: 13.91193538989415.....Total loss: 13.91193538989415\n",
      "epoch 4032 learning rate:  0.010248015873015874   Training loss:   14.823049448829162  Valing loss:   13.91193538989415\n",
      "Pure loss: 15.08662269106455.....Total loss: 15.08662269106455\n",
      "Pure loss: 14.479289560208654.....Total loss: 14.479289560208654\n",
      "epoch 4033 learning rate:  0.010247954376394744   Training loss:   15.08662269106455  Valing loss:   14.479289560208654\n",
      "Pure loss: 15.036850117230456.....Total loss: 15.036850117230456\n",
      "Pure loss: 14.390887758296541.....Total loss: 14.390887758296541\n",
      "epoch 4034 learning rate:  0.010247892910262766   Training loss:   15.036850117230456  Valing loss:   14.390887758296541\n",
      "Pure loss: 15.27818054920738.....Total loss: 15.27818054920738\n",
      "Pure loss: 14.847819592898205.....Total loss: 14.847819592898205\n",
      "epoch 4035 learning rate:  0.010247831474597274   Training loss:   15.27818054920738  Valing loss:   14.847819592898205\n",
      "Pure loss: 14.950403468683275.....Total loss: 14.950403468683275\n",
      "Pure loss: 14.295858665507275.....Total loss: 14.295858665507275\n",
      "epoch 4036 learning rate:  0.01024777006937562   Training loss:   14.950403468683275  Valing loss:   14.295858665507275\n",
      "Pure loss: 14.900370804524709.....Total loss: 14.900370804524709\n",
      "Pure loss: 14.212157393685649.....Total loss: 14.212157393685649\n",
      "epoch 4037 learning rate:  0.01024770869457518   Training loss:   14.900370804524709  Valing loss:   14.212157393685649\n",
      "Pure loss: 14.902055632477198.....Total loss: 14.902055632477198\n",
      "Pure loss: 14.215599969061104.....Total loss: 14.215599969061104\n",
      "epoch 4038 learning rate:  0.010247647350173354   Training loss:   14.902055632477198  Valing loss:   14.215599969061104\n",
      "Pure loss: 14.871982038504738.....Total loss: 14.871982038504738\n",
      "Pure loss: 14.162010671497155.....Total loss: 14.162010671497155\n",
      "epoch 4039 learning rate:  0.010247586036147562   Training loss:   14.871982038504738  Valing loss:   14.162010671497155\n",
      "Pure loss: 14.693435580398768.....Total loss: 14.693435580398768\n",
      "Pure loss: 13.777522807610012.....Total loss: 13.777522807610012\n",
      "epoch 4040 learning rate:  0.010247524752475248   Training loss:   14.693435580398768  Valing loss:   13.777522807610012\n",
      "Pure loss: 14.556460659278976.....Total loss: 14.556460659278976\n",
      "Pure loss: 13.248855530281277.....Total loss: 13.248855530281277\n",
      "epoch 4041 learning rate:  0.010247463499133878   Training loss:   14.556460659278976  Valing loss:   13.248855530281277\n",
      "Pure loss: 14.588773036375052.....Total loss: 14.588773036375052\n",
      "Pure loss: 13.476043354724336.....Total loss: 13.476043354724336\n",
      "epoch 4042 learning rate:  0.01024740227610094   Training loss:   14.588773036375052  Valing loss:   13.476043354724336\n",
      "Pure loss: 14.57803305082279.....Total loss: 14.57803305082279\n",
      "Pure loss: 13.446851265777951.....Total loss: 13.446851265777951\n",
      "epoch 4043 learning rate:  0.010247341083353945   Training loss:   14.57803305082279  Valing loss:   13.446851265777951\n",
      "Pure loss: 14.642003079444377.....Total loss: 14.642003079444377\n",
      "Pure loss: 13.669685145982509.....Total loss: 13.669685145982509\n",
      "epoch 4044 learning rate:  0.010247279920870425   Training loss:   14.642003079444377  Valing loss:   13.669685145982509\n",
      "Pure loss: 14.560661391061007.....Total loss: 14.560661391061007\n",
      "Pure loss: 13.42192217295703.....Total loss: 13.42192217295703\n",
      "epoch 4045 learning rate:  0.010247218788627936   Training loss:   14.560661391061007  Valing loss:   13.42192217295703\n",
      "Pure loss: 14.543517691879206.....Total loss: 14.543517691879206\n",
      "Pure loss: 13.197196548405648.....Total loss: 13.197196548405648\n",
      "epoch 4046 learning rate:  0.010247157686604053   Training loss:   14.543517691879206  Valing loss:   13.197196548405648\n",
      "Pure loss: 14.547382986930382.....Total loss: 14.547382986930382\n",
      "Pure loss: 13.211372338584672.....Total loss: 13.211372338584672\n",
      "epoch 4047 learning rate:  0.010247096614776378   Training loss:   14.547382986930382  Valing loss:   13.211372338584672\n",
      "Pure loss: 14.547595575489275.....Total loss: 14.547595575489275\n",
      "Pure loss: 13.272333954156707.....Total loss: 13.272333954156707\n",
      "epoch 4048 learning rate:  0.01024703557312253   Training loss:   14.547595575489275  Valing loss:   13.272333954156707\n",
      "Pure loss: 14.55960650364882.....Total loss: 14.55960650364882\n",
      "Pure loss: 13.388243609745329.....Total loss: 13.388243609745329\n",
      "epoch 4049 learning rate:  0.010246974561620154   Training loss:   14.55960650364882  Valing loss:   13.388243609745329\n",
      "Pure loss: 14.742846460083626.....Total loss: 14.742846460083626\n",
      "Pure loss: 13.858980663559556.....Total loss: 13.858980663559556\n",
      "epoch 4050 learning rate:  0.010246913580246915   Training loss:   14.742846460083626  Valing loss:   13.858980663559556\n",
      "Pure loss: 14.721016776524914.....Total loss: 14.721016776524914\n",
      "Pure loss: 13.817202353850119.....Total loss: 13.817202353850119\n",
      "epoch 4051 learning rate:  0.010246852628980499   Training loss:   14.721016776524914  Valing loss:   13.817202353850119\n",
      "Pure loss: 14.632876935654092.....Total loss: 14.632876935654092\n",
      "Pure loss: 13.629407037730648.....Total loss: 13.629407037730648\n",
      "epoch 4052 learning rate:  0.010246791707798618   Training loss:   14.632876935654092  Valing loss:   13.629407037730648\n",
      "Pure loss: 14.640570721543376.....Total loss: 14.640570721543376\n",
      "Pure loss: 13.643954245358454.....Total loss: 13.643954245358454\n",
      "epoch 4053 learning rate:  0.010246730816679003   Training loss:   14.640570721543376  Valing loss:   13.643954245358454\n",
      "Pure loss: 14.61125095932559.....Total loss: 14.61125095932559\n",
      "Pure loss: 13.581875401036127.....Total loss: 13.581875401036127\n",
      "epoch 4054 learning rate:  0.010246669955599409   Training loss:   14.61125095932559  Valing loss:   13.581875401036127\n",
      "Pure loss: 14.574902670497636.....Total loss: 14.574902670497636\n",
      "Pure loss: 13.490877273549138.....Total loss: 13.490877273549138\n",
      "epoch 4055 learning rate:  0.010246609124537608   Training loss:   14.574902670497636  Valing loss:   13.490877273549138\n",
      "Pure loss: 14.569218033166278.....Total loss: 14.569218033166278\n",
      "Pure loss: 13.47735371561828.....Total loss: 13.47735371561828\n",
      "epoch 4056 learning rate:  0.0102465483234714   Training loss:   14.569218033166278  Valing loss:   13.47735371561828\n",
      "Pure loss: 14.593511842305166.....Total loss: 14.593511842305166\n",
      "Pure loss: 13.586473592430705.....Total loss: 13.586473592430705\n",
      "epoch 4057 learning rate:  0.010246487552378605   Training loss:   14.593511842305166  Valing loss:   13.586473592430705\n",
      "Pure loss: 14.618362550487824.....Total loss: 14.618362550487824\n",
      "Pure loss: 13.648305395902183.....Total loss: 13.648305395902183\n",
      "epoch 4058 learning rate:  0.010246426811237063   Training loss:   14.618362550487824  Valing loss:   13.648305395902183\n",
      "Pure loss: 14.575481221720567.....Total loss: 14.575481221720567\n",
      "Pure loss: 13.550994885204647.....Total loss: 13.550994885204647\n",
      "epoch 4059 learning rate:  0.010246366100024638   Training loss:   14.575481221720567  Valing loss:   13.550994885204647\n",
      "Pure loss: 14.527921801608246.....Total loss: 14.527921801608246\n",
      "Pure loss: 13.182799782883468.....Total loss: 13.182799782883468\n",
      "epoch 4060 learning rate:  0.010246305418719213   Training loss:   14.527921801608246  Valing loss:   13.182799782883468\n",
      "Pure loss: 14.525439854273843.....Total loss: 14.525439854273843\n",
      "Pure loss: 13.201587177178869.....Total loss: 13.201587177178869\n",
      "epoch 4061 learning rate:  0.010246244767298695   Training loss:   14.525439854273843  Valing loss:   13.201587177178869\n",
      "Pure loss: 14.524551548404045.....Total loss: 14.524551548404045\n",
      "Pure loss: 13.194076021301525.....Total loss: 13.194076021301525\n",
      "epoch 4062 learning rate:  0.010246184145741015   Training loss:   14.524551548404045  Valing loss:   13.194076021301525\n",
      "Pure loss: 14.63334146312448.....Total loss: 14.63334146312448\n",
      "Pure loss: 13.580006989198107.....Total loss: 13.580006989198107\n",
      "epoch 4063 learning rate:  0.010246123554024121   Training loss:   14.63334146312448  Valing loss:   13.580006989198107\n",
      "Pure loss: 14.614120691980988.....Total loss: 14.614120691980988\n",
      "Pure loss: 13.526769625736192.....Total loss: 13.526769625736192\n",
      "epoch 4064 learning rate:  0.010246062992125984   Training loss:   14.614120691980988  Valing loss:   13.526769625736192\n",
      "Pure loss: 14.553483696077345.....Total loss: 14.553483696077345\n",
      "Pure loss: 13.29239555030876.....Total loss: 13.29239555030876\n",
      "epoch 4065 learning rate:  0.0102460024600246   Training loss:   14.553483696077345  Valing loss:   13.29239555030876\n",
      "Pure loss: 14.555931914613684.....Total loss: 14.555931914613684\n",
      "Pure loss: 13.041307593286453.....Total loss: 13.041307593286453\n",
      "epoch 4066 learning rate:  0.010245941957697983   Training loss:   14.555931914613684  Valing loss:   13.041307593286453\n",
      "Pure loss: 14.753366694153705.....Total loss: 14.753366694153705\n",
      "Pure loss: 12.90806771229734.....Total loss: 12.90806771229734\n",
      "epoch 4067 learning rate:  0.01024588148512417   Training loss:   14.753366694153705  Valing loss:   12.90806771229734\n",
      "Pure loss: 14.818634086012976.....Total loss: 14.818634086012976\n",
      "Pure loss: 12.909882362071537.....Total loss: 12.909882362071537\n",
      "epoch 4068 learning rate:  0.010245821042281219   Training loss:   14.818634086012976  Valing loss:   12.909882362071537\n",
      "Pure loss: 14.99244223974601.....Total loss: 14.99244223974601\n",
      "Pure loss: 13.003354875737598.....Total loss: 13.003354875737598\n",
      "epoch 4069 learning rate:  0.010245760629147211   Training loss:   14.99244223974601  Valing loss:   13.003354875737598\n",
      "Pure loss: 14.880379851827207.....Total loss: 14.880379851827207\n",
      "Pure loss: 13.017095344686526.....Total loss: 13.017095344686526\n",
      "epoch 4070 learning rate:  0.010245700245700247   Training loss:   14.880379851827207  Valing loss:   13.017095344686526\n",
      "Pure loss: 14.669957250316424.....Total loss: 14.669957250316424\n",
      "Pure loss: 12.931959374839284.....Total loss: 12.931959374839284\n",
      "epoch 4071 learning rate:  0.010245639891918448   Training loss:   14.669957250316424  Valing loss:   12.931959374839284\n",
      "Pure loss: 14.91926167834429.....Total loss: 14.91926167834429\n",
      "Pure loss: 13.043363852470353.....Total loss: 13.043363852470353\n",
      "epoch 4072 learning rate:  0.010245579567779961   Training loss:   14.91926167834429  Valing loss:   13.043363852470353\n",
      "Pure loss: 14.909712987043289.....Total loss: 14.909712987043289\n",
      "Pure loss: 13.041353462852475.....Total loss: 13.041353462852475\n",
      "epoch 4073 learning rate:  0.010245519273262952   Training loss:   14.909712987043289  Valing loss:   13.041353462852475\n",
      "Pure loss: 14.996962154911666.....Total loss: 14.996962154911666\n",
      "Pure loss: 13.098874402910516.....Total loss: 13.098874402910516\n",
      "epoch 4074 learning rate:  0.010245459008345607   Training loss:   14.996962154911666  Valing loss:   13.098874402910516\n",
      "Pure loss: 15.423133172023752.....Total loss: 15.423133172023752\n",
      "Pure loss: 13.40609279843204.....Total loss: 13.40609279843204\n",
      "epoch 4075 learning rate:  0.010245398773006136   Training loss:   15.423133172023752  Valing loss:   13.40609279843204\n",
      "Pure loss: 15.481912352368111.....Total loss: 15.481912352368111\n",
      "Pure loss: 13.428627475203283.....Total loss: 13.428627475203283\n",
      "epoch 4076 learning rate:  0.010245338567222768   Training loss:   15.481912352368111  Valing loss:   13.428627475203283\n",
      "Pure loss: 15.80382244688894.....Total loss: 15.80382244688894\n",
      "Pure loss: 13.543126809206692.....Total loss: 13.543126809206692\n",
      "epoch 4077 learning rate:  0.010245278390973755   Training loss:   15.80382244688894  Valing loss:   13.543126809206692\n",
      "Pure loss: 15.985062291069703.....Total loss: 15.985062291069703\n",
      "Pure loss: 13.652778123936546.....Total loss: 13.652778123936546\n",
      "epoch 4078 learning rate:  0.010245218244237372   Training loss:   15.985062291069703  Valing loss:   13.652778123936546\n",
      "Pure loss: 15.418997101269794.....Total loss: 15.418997101269794\n",
      "Pure loss: 13.342725922358353.....Total loss: 13.342725922358353\n",
      "epoch 4079 learning rate:  0.01024515812699191   Training loss:   15.418997101269794  Valing loss:   13.342725922358353\n",
      "Pure loss: 15.144370659484506.....Total loss: 15.144370659484506\n",
      "Pure loss: 13.352315904903845.....Total loss: 13.352315904903845\n",
      "epoch 4080 learning rate:  0.010245098039215687   Training loss:   15.144370659484506  Valing loss:   13.352315904903845\n",
      "Pure loss: 15.437590131813117.....Total loss: 15.437590131813117\n",
      "Pure loss: 13.467831556507393.....Total loss: 13.467831556507393\n",
      "epoch 4081 learning rate:  0.010245037980887039   Training loss:   15.437590131813117  Valing loss:   13.467831556507393\n",
      "Pure loss: 15.332049151536927.....Total loss: 15.332049151536927\n",
      "Pure loss: 13.410394379180834.....Total loss: 13.410394379180834\n",
      "epoch 4082 learning rate:  0.010244977951984322   Training loss:   15.332049151536927  Valing loss:   13.410394379180834\n",
      "Pure loss: 15.32545807350138.....Total loss: 15.32545807350138\n",
      "Pure loss: 13.408174166012625.....Total loss: 13.408174166012625\n",
      "epoch 4083 learning rate:  0.010244917952485917   Training loss:   15.32545807350138  Valing loss:   13.408174166012625\n",
      "Pure loss: 15.100714061710214.....Total loss: 15.100714061710214\n",
      "Pure loss: 13.353835193415845.....Total loss: 13.353835193415845\n",
      "epoch 4084 learning rate:  0.010244857982370225   Training loss:   15.100714061710214  Valing loss:   13.353835193415845\n",
      "Pure loss: 14.793541503675941.....Total loss: 14.793541503675941\n",
      "Pure loss: 13.35035363771411.....Total loss: 13.35035363771411\n",
      "epoch 4085 learning rate:  0.010244798041615667   Training loss:   14.793541503675941  Valing loss:   13.35035363771411\n",
      "Pure loss: 14.831829469441292.....Total loss: 14.831829469441292\n",
      "Pure loss: 13.342126766770745.....Total loss: 13.342126766770745\n",
      "epoch 4086 learning rate:  0.010244738130200685   Training loss:   14.831829469441292  Valing loss:   13.342126766770745\n",
      "Pure loss: 14.918743829595817.....Total loss: 14.918743829595817\n",
      "Pure loss: 13.277971665216947.....Total loss: 13.277971665216947\n",
      "epoch 4087 learning rate:  0.010244678248103744   Training loss:   14.918743829595817  Valing loss:   13.277971665216947\n",
      "Pure loss: 14.83491291527039.....Total loss: 14.83491291527039\n",
      "Pure loss: 13.190387632329326.....Total loss: 13.190387632329326\n",
      "epoch 4088 learning rate:  0.010244618395303327   Training loss:   14.83491291527039  Valing loss:   13.190387632329326\n",
      "Pure loss: 14.767521236000528.....Total loss: 14.767521236000528\n",
      "Pure loss: 13.175144214745892.....Total loss: 13.175144214745892\n",
      "epoch 4089 learning rate:  0.010244558571777942   Training loss:   14.767521236000528  Valing loss:   13.175144214745892\n",
      "Pure loss: 14.701538968220387.....Total loss: 14.701538968220387\n",
      "Pure loss: 13.195499707050569.....Total loss: 13.195499707050569\n",
      "epoch 4090 learning rate:  0.010244498777506113   Training loss:   14.701538968220387  Valing loss:   13.195499707050569\n",
      "Pure loss: 14.72642924042506.....Total loss: 14.72642924042506\n",
      "Pure loss: 13.178761648671331.....Total loss: 13.178761648671331\n",
      "epoch 4091 learning rate:  0.01024443901246639   Training loss:   14.72642924042506  Valing loss:   13.178761648671331\n",
      "Pure loss: 14.895648056316476.....Total loss: 14.895648056316476\n",
      "Pure loss: 13.189322185185862.....Total loss: 13.189322185185862\n",
      "epoch 4092 learning rate:  0.010244379276637341   Training loss:   14.895648056316476  Valing loss:   13.189322185185862\n",
      "Pure loss: 14.758891596173143.....Total loss: 14.758891596173143\n",
      "Pure loss: 13.181821359576023.....Total loss: 13.181821359576023\n",
      "epoch 4093 learning rate:  0.010244319569997558   Training loss:   14.758891596173143  Valing loss:   13.181821359576023\n",
      "Pure loss: 14.778024151374295.....Total loss: 14.778024151374295\n",
      "Pure loss: 13.17659285871431.....Total loss: 13.17659285871431\n",
      "epoch 4094 learning rate:  0.010244259892525647   Training loss:   14.778024151374295  Valing loss:   13.17659285871431\n",
      "Pure loss: 14.796696731231878.....Total loss: 14.796696731231878\n",
      "Pure loss: 13.172699718566541.....Total loss: 13.172699718566541\n",
      "epoch 4095 learning rate:  0.010244200244200245   Training loss:   14.796696731231878  Valing loss:   13.172699718566541\n",
      "Pure loss: 14.609634471070478.....Total loss: 14.609634471070478\n",
      "Pure loss: 13.053986187613063.....Total loss: 13.053986187613063\n",
      "epoch 4096 learning rate:  0.010244140625   Training loss:   14.609634471070478  Valing loss:   13.053986187613063\n",
      "Pure loss: 14.635986160177708.....Total loss: 14.635986160177708\n",
      "Pure loss: 13.03913303494905.....Total loss: 13.03913303494905\n",
      "epoch 4097 learning rate:  0.010244081034903589   Training loss:   14.635986160177708  Valing loss:   13.03913303494905\n",
      "Pure loss: 14.669031356054843.....Total loss: 14.669031356054843\n",
      "Pure loss: 13.05173966552923.....Total loss: 13.05173966552923\n",
      "epoch 4098 learning rate:  0.010244021473889703   Training loss:   14.669031356054843  Valing loss:   13.05173966552923\n",
      "Pure loss: 14.567927249040203.....Total loss: 14.567927249040203\n",
      "Pure loss: 13.042338655312925.....Total loss: 13.042338655312925\n",
      "epoch 4099 learning rate:  0.010243961941937057   Training loss:   14.567927249040203  Valing loss:   13.042338655312925\n",
      "Pure loss: 14.710538579554855.....Total loss: 14.710538579554855\n",
      "Pure loss: 13.096608195474237.....Total loss: 13.096608195474237\n",
      "epoch 4100 learning rate:  0.01024390243902439   Training loss:   14.710538579554855  Valing loss:   13.096608195474237\n",
      "Pure loss: 14.728320543681932.....Total loss: 14.728320543681932\n",
      "Pure loss: 13.083718384117425.....Total loss: 13.083718384117425\n",
      "epoch 4101 learning rate:  0.010243842965130456   Training loss:   14.728320543681932  Valing loss:   13.083718384117425\n",
      "Pure loss: 14.982846316144055.....Total loss: 14.982846316144055\n",
      "Pure loss: 13.081612078780424.....Total loss: 13.081612078780424\n",
      "epoch 4102 learning rate:  0.010243783520234032   Training loss:   14.982846316144055  Valing loss:   13.081612078780424\n",
      "Pure loss: 14.535300602537161.....Total loss: 14.535300602537161\n",
      "Pure loss: 13.140815192675133.....Total loss: 13.140815192675133\n",
      "epoch 4103 learning rate:  0.010243724104313916   Training loss:   14.535300602537161  Valing loss:   13.140815192675133\n",
      "Pure loss: 14.541847041684433.....Total loss: 14.541847041684433\n",
      "Pure loss: 13.102513365880318.....Total loss: 13.102513365880318\n",
      "epoch 4104 learning rate:  0.010243664717348928   Training loss:   14.541847041684433  Valing loss:   13.102513365880318\n",
      "Pure loss: 14.5434337858257.....Total loss: 14.5434337858257\n",
      "Pure loss: 13.096368281688573.....Total loss: 13.096368281688573\n",
      "epoch 4105 learning rate:  0.010243605359317905   Training loss:   14.5434337858257  Valing loss:   13.096368281688573\n",
      "Pure loss: 14.540158581381709.....Total loss: 14.540158581381709\n",
      "Pure loss: 13.285772523125365.....Total loss: 13.285772523125365\n",
      "epoch 4106 learning rate:  0.010243546030199708   Training loss:   14.540158581381709  Valing loss:   13.285772523125365\n",
      "Pure loss: 14.594016775738515.....Total loss: 14.594016775738515\n",
      "Pure loss: 13.25935582567728.....Total loss: 13.25935582567728\n",
      "epoch 4107 learning rate:  0.010243486729973217   Training loss:   14.594016775738515  Valing loss:   13.25935582567728\n",
      "Pure loss: 14.605826379261696.....Total loss: 14.605826379261696\n",
      "Pure loss: 13.162129997900802.....Total loss: 13.162129997900802\n",
      "epoch 4108 learning rate:  0.010243427458617333   Training loss:   14.605826379261696  Valing loss:   13.162129997900802\n",
      "Pure loss: 14.585858778443285.....Total loss: 14.585858778443285\n",
      "Pure loss: 13.212619771109809.....Total loss: 13.212619771109809\n",
      "epoch 4109 learning rate:  0.010243368216110976   Training loss:   14.585858778443285  Valing loss:   13.212619771109809\n",
      "Pure loss: 14.632292256137065.....Total loss: 14.632292256137065\n",
      "Pure loss: 13.554376823064022.....Total loss: 13.554376823064022\n",
      "epoch 4110 learning rate:  0.010243309002433091   Training loss:   14.632292256137065  Valing loss:   13.554376823064022\n",
      "Pure loss: 14.604433614647519.....Total loss: 14.604433614647519\n",
      "Pure loss: 13.419092185430502.....Total loss: 13.419092185430502\n",
      "epoch 4111 learning rate:  0.010243249817562637   Training loss:   14.604433614647519  Valing loss:   13.419092185430502\n",
      "Pure loss: 14.644752873632042.....Total loss: 14.644752873632042\n",
      "Pure loss: 13.158941497497416.....Total loss: 13.158941497497416\n",
      "epoch 4112 learning rate:  0.0102431906614786   Training loss:   14.644752873632042  Valing loss:   13.158941497497416\n",
      "Pure loss: 14.826120831238727.....Total loss: 14.826120831238727\n",
      "Pure loss: 13.06622191836031.....Total loss: 13.06622191836031\n",
      "epoch 4113 learning rate:  0.010243131534159981   Training loss:   14.826120831238727  Valing loss:   13.06622191836031\n",
      "Pure loss: 14.934319694909062.....Total loss: 14.934319694909062\n",
      "Pure loss: 13.06185482391748.....Total loss: 13.06185482391748\n",
      "epoch 4114 learning rate:  0.010243072435585804   Training loss:   14.934319694909062  Valing loss:   13.06185482391748\n",
      "Pure loss: 14.978809187189707.....Total loss: 14.978809187189707\n",
      "Pure loss: 13.068656175738873.....Total loss: 13.068656175738873\n",
      "epoch 4115 learning rate:  0.010243013365735116   Training loss:   14.978809187189707  Valing loss:   13.068656175738873\n",
      "Pure loss: 15.013668335745255.....Total loss: 15.013668335745255\n",
      "Pure loss: 13.068794048769238.....Total loss: 13.068794048769238\n",
      "epoch 4116 learning rate:  0.010242954324586977   Training loss:   15.013668335745255  Valing loss:   13.068794048769238\n",
      "Pure loss: 14.801728780522033.....Total loss: 14.801728780522033\n",
      "Pure loss: 13.117937715769578.....Total loss: 13.117937715769578\n",
      "epoch 4117 learning rate:  0.010242895312120477   Training loss:   14.801728780522033  Valing loss:   13.117937715769578\n",
      "Pure loss: 14.829842943406156.....Total loss: 14.829842943406156\n",
      "Pure loss: 13.130664609246358.....Total loss: 13.130664609246358\n",
      "epoch 4118 learning rate:  0.010242836328314716   Training loss:   14.829842943406156  Valing loss:   13.130664609246358\n",
      "Pure loss: 14.734375356144058.....Total loss: 14.734375356144058\n",
      "Pure loss: 13.135845212118737.....Total loss: 13.135845212118737\n",
      "epoch 4119 learning rate:  0.010242777373148822   Training loss:   14.734375356144058  Valing loss:   13.135845212118737\n",
      "Pure loss: 14.815727439688464.....Total loss: 14.815727439688464\n",
      "Pure loss: 13.133788246956504.....Total loss: 13.133788246956504\n",
      "epoch 4120 learning rate:  0.010242718446601943   Training loss:   14.815727439688464  Valing loss:   13.133788246956504\n",
      "Pure loss: 14.92696249471411.....Total loss: 14.92696249471411\n",
      "Pure loss: 13.108055574415058.....Total loss: 13.108055574415058\n",
      "epoch 4121 learning rate:  0.01024265954865324   Training loss:   14.92696249471411  Valing loss:   13.108055574415058\n",
      "Pure loss: 15.03315313055133.....Total loss: 15.03315313055133\n",
      "Pure loss: 13.116909239822586.....Total loss: 13.116909239822586\n",
      "epoch 4122 learning rate:  0.010242600679281903   Training loss:   15.03315313055133  Valing loss:   13.116909239822586\n",
      "Pure loss: 15.049732792147834.....Total loss: 15.049732792147834\n",
      "Pure loss: 13.119613585990425.....Total loss: 13.119613585990425\n",
      "epoch 4123 learning rate:  0.010242541838467135   Training loss:   15.049732792147834  Valing loss:   13.119613585990425\n",
      "Pure loss: 14.89119941629119.....Total loss: 14.89119941629119\n",
      "Pure loss: 13.011053387968449.....Total loss: 13.011053387968449\n",
      "epoch 4124 learning rate:  0.010242483026188166   Training loss:   14.89119941629119  Valing loss:   13.011053387968449\n",
      "Pure loss: 14.927404320637077.....Total loss: 14.927404320637077\n",
      "Pure loss: 13.022507456449814.....Total loss: 13.022507456449814\n",
      "epoch 4125 learning rate:  0.010242424242424242   Training loss:   14.927404320637077  Valing loss:   13.022507456449814\n",
      "Pure loss: 15.062278712764313.....Total loss: 15.062278712764313\n",
      "Pure loss: 13.085402237062201.....Total loss: 13.085402237062201\n",
      "epoch 4126 learning rate:  0.010242365487154629   Training loss:   15.062278712764313  Valing loss:   13.085402237062201\n",
      "Pure loss: 14.911188221630464.....Total loss: 14.911188221630464\n",
      "Pure loss: 13.082677797055407.....Total loss: 13.082677797055407\n",
      "epoch 4127 learning rate:  0.010242306760358614   Training loss:   14.911188221630464  Valing loss:   13.082677797055407\n",
      "Pure loss: 14.879806018714527.....Total loss: 14.879806018714527\n",
      "Pure loss: 13.081109225011593.....Total loss: 13.081109225011593\n",
      "epoch 4128 learning rate:  0.010242248062015505   Training loss:   14.879806018714527  Valing loss:   13.081109225011593\n",
      "Pure loss: 14.964533776877492.....Total loss: 14.964533776877492\n",
      "Pure loss: 13.139516245347806.....Total loss: 13.139516245347806\n",
      "epoch 4129 learning rate:  0.010242189392104625   Training loss:   14.964533776877492  Valing loss:   13.139516245347806\n",
      "Pure loss: 14.868665496136762.....Total loss: 14.868665496136762\n",
      "Pure loss: 13.150905152625167.....Total loss: 13.150905152625167\n",
      "epoch 4130 learning rate:  0.010242130750605326   Training loss:   14.868665496136762  Valing loss:   13.150905152625167\n",
      "Pure loss: 14.935063357421821.....Total loss: 14.935063357421821\n",
      "Pure loss: 13.11351474567063.....Total loss: 13.11351474567063\n",
      "epoch 4131 learning rate:  0.010242072137496974   Training loss:   14.935063357421821  Valing loss:   13.11351474567063\n",
      "Pure loss: 14.918892083586114.....Total loss: 14.918892083586114\n",
      "Pure loss: 13.10243388705416.....Total loss: 13.10243388705416\n",
      "epoch 4132 learning rate:  0.010242013552758954   Training loss:   14.918892083586114  Valing loss:   13.10243388705416\n",
      "Pure loss: 14.857642533689607.....Total loss: 14.857642533689607\n",
      "Pure loss: 13.10247835144466.....Total loss: 13.10247835144466\n",
      "epoch 4133 learning rate:  0.010241954996370674   Training loss:   14.857642533689607  Valing loss:   13.10247835144466\n",
      "Pure loss: 14.684474628377625.....Total loss: 14.684474628377625\n",
      "Pure loss: 13.322515972994871.....Total loss: 13.322515972994871\n",
      "epoch 4134 learning rate:  0.010241896468311563   Training loss:   14.684474628377625  Valing loss:   13.322515972994871\n",
      "Pure loss: 14.682821113929034.....Total loss: 14.682821113929034\n",
      "Pure loss: 13.375435419275194.....Total loss: 13.375435419275194\n",
      "epoch 4135 learning rate:  0.010241837968561065   Training loss:   14.682821113929034  Valing loss:   13.375435419275194\n",
      "Pure loss: 14.805235771311768.....Total loss: 14.805235771311768\n",
      "Pure loss: 13.43569089387664.....Total loss: 13.43569089387664\n",
      "epoch 4136 learning rate:  0.010241779497098646   Training loss:   14.805235771311768  Valing loss:   13.43569089387664\n",
      "Pure loss: 14.7439869741046.....Total loss: 14.7439869741046\n",
      "Pure loss: 13.479925256976587.....Total loss: 13.479925256976587\n",
      "epoch 4137 learning rate:  0.010241721053903795   Training loss:   14.7439869741046  Valing loss:   13.479925256976587\n",
      "Pure loss: 14.705725491407694.....Total loss: 14.705725491407694\n",
      "Pure loss: 13.606145000656221.....Total loss: 13.606145000656221\n",
      "epoch 4138 learning rate:  0.010241662638956017   Training loss:   14.705725491407694  Valing loss:   13.606145000656221\n",
      "Pure loss: 14.705514304949396.....Total loss: 14.705514304949396\n",
      "Pure loss: 13.606654283686987.....Total loss: 13.606654283686987\n",
      "epoch 4139 learning rate:  0.01024160425223484   Training loss:   14.705514304949396  Valing loss:   13.606654283686987\n",
      "Pure loss: 14.71503804169879.....Total loss: 14.71503804169879\n",
      "Pure loss: 13.593204466185806.....Total loss: 13.593204466185806\n",
      "epoch 4140 learning rate:  0.010241545893719808   Training loss:   14.71503804169879  Valing loss:   13.593204466185806\n",
      "Pure loss: 14.72941773006261.....Total loss: 14.72941773006261\n",
      "Pure loss: 13.656265556446373.....Total loss: 13.656265556446373\n",
      "epoch 4141 learning rate:  0.010241487563390486   Training loss:   14.72941773006261  Valing loss:   13.656265556446373\n",
      "Pure loss: 14.677987794611004.....Total loss: 14.677987794611004\n",
      "Pure loss: 13.622623101538265.....Total loss: 13.622623101538265\n",
      "epoch 4142 learning rate:  0.01024142926122646   Training loss:   14.677987794611004  Valing loss:   13.622623101538265\n",
      "Pure loss: 14.677600695971204.....Total loss: 14.677600695971204\n",
      "Pure loss: 13.59513354118205.....Total loss: 13.59513354118205\n",
      "epoch 4143 learning rate:  0.010241370987207338   Training loss:   14.677600695971204  Valing loss:   13.59513354118205\n",
      "Pure loss: 14.63098964048729.....Total loss: 14.63098964048729\n",
      "Pure loss: 13.587480563911242.....Total loss: 13.587480563911242\n",
      "epoch 4144 learning rate:  0.010241312741312741   Training loss:   14.63098964048729  Valing loss:   13.587480563911242\n",
      "Pure loss: 14.621782623974903.....Total loss: 14.621782623974903\n",
      "Pure loss: 13.678834135649717.....Total loss: 13.678834135649717\n",
      "epoch 4145 learning rate:  0.010241254523522316   Training loss:   14.621782623974903  Valing loss:   13.678834135649717\n",
      "Pure loss: 14.62108221473301.....Total loss: 14.62108221473301\n",
      "Pure loss: 13.684351636801225.....Total loss: 13.684351636801225\n",
      "epoch 4146 learning rate:  0.010241196333815726   Training loss:   14.62108221473301  Valing loss:   13.684351636801225\n",
      "Pure loss: 14.654543192335497.....Total loss: 14.654543192335497\n",
      "Pure loss: 13.77240276609098.....Total loss: 13.77240276609098\n",
      "epoch 4147 learning rate:  0.010241138172172655   Training loss:   14.654543192335497  Valing loss:   13.77240276609098\n",
      "Pure loss: 15.18926969798762.....Total loss: 15.18926969798762\n",
      "Pure loss: 14.773024104757301.....Total loss: 14.773024104757301\n",
      "epoch 4148 learning rate:  0.010241080038572806   Training loss:   15.18926969798762  Valing loss:   14.773024104757301\n",
      "Pure loss: 15.133185469813204.....Total loss: 15.133185469813204\n",
      "Pure loss: 14.669237732382292.....Total loss: 14.669237732382292\n",
      "epoch 4149 learning rate:  0.010241021932995902   Training loss:   15.133185469813204  Valing loss:   14.669237732382292\n",
      "Pure loss: 15.675738411192711.....Total loss: 15.675738411192711\n",
      "Pure loss: 15.510275891395109.....Total loss: 15.510275891395109\n",
      "epoch 4150 learning rate:  0.010240963855421687   Training loss:   15.675738411192711  Valing loss:   15.510275891395109\n",
      "Pure loss: 14.885329970593787.....Total loss: 14.885329970593787\n",
      "Pure loss: 14.320523246660105.....Total loss: 14.320523246660105\n",
      "epoch 4151 learning rate:  0.01024090580582992   Training loss:   14.885329970593787  Valing loss:   14.320523246660105\n",
      "Pure loss: 14.917332632469899.....Total loss: 14.917332632469899\n",
      "Pure loss: 14.378123362018394.....Total loss: 14.378123362018394\n",
      "epoch 4152 learning rate:  0.010240847784200386   Training loss:   14.917332632469899  Valing loss:   14.378123362018394\n",
      "Pure loss: 14.860333425811499.....Total loss: 14.860333425811499\n",
      "Pure loss: 14.276611855980867.....Total loss: 14.276611855980867\n",
      "epoch 4153 learning rate:  0.010240789790512883   Training loss:   14.860333425811499  Valing loss:   14.276611855980867\n",
      "Pure loss: 14.735797650868397.....Total loss: 14.735797650868397\n",
      "Pure loss: 14.051877673004139.....Total loss: 14.051877673004139\n",
      "epoch 4154 learning rate:  0.010240731824747232   Training loss:   14.735797650868397  Valing loss:   14.051877673004139\n",
      "Pure loss: 14.710823008535359.....Total loss: 14.710823008535359\n",
      "Pure loss: 14.002544398424877.....Total loss: 14.002544398424877\n",
      "epoch 4155 learning rate:  0.010240673886883274   Training loss:   14.710823008535359  Valing loss:   14.002544398424877\n",
      "Pure loss: 14.561143860505922.....Total loss: 14.561143860505922\n",
      "Pure loss: 13.684130689218204.....Total loss: 13.684130689218204\n",
      "epoch 4156 learning rate:  0.010240615976900867   Training loss:   14.561143860505922  Valing loss:   13.684130689218204\n",
      "Pure loss: 14.553935984400713.....Total loss: 14.553935984400713\n",
      "Pure loss: 13.669228444728128.....Total loss: 13.669228444728128\n",
      "epoch 4157 learning rate:  0.010240558094779889   Training loss:   14.553935984400713  Valing loss:   13.669228444728128\n",
      "Pure loss: 14.543583880737861.....Total loss: 14.543583880737861\n",
      "Pure loss: 13.620849205149089.....Total loss: 13.620849205149089\n",
      "epoch 4158 learning rate:  0.01024050024050024   Training loss:   14.543583880737861  Valing loss:   13.620849205149089\n",
      "Pure loss: 14.566444823250272.....Total loss: 14.566444823250272\n",
      "Pure loss: 13.674172459928892.....Total loss: 13.674172459928892\n",
      "epoch 4159 learning rate:  0.010240442414041838   Training loss:   14.566444823250272  Valing loss:   13.674172459928892\n",
      "Pure loss: 14.62079470284555.....Total loss: 14.62079470284555\n",
      "Pure loss: 13.802053446697904.....Total loss: 13.802053446697904\n",
      "epoch 4160 learning rate:  0.010240384615384616   Training loss:   14.62079470284555  Valing loss:   13.802053446697904\n",
      "Pure loss: 14.966519951829707.....Total loss: 14.966519951829707\n",
      "Pure loss: 14.495882126786034.....Total loss: 14.495882126786034\n",
      "epoch 4161 learning rate:  0.010240326844508532   Training loss:   14.966519951829707  Valing loss:   14.495882126786034\n",
      "Pure loss: 14.943048208089122.....Total loss: 14.943048208089122\n",
      "Pure loss: 14.455633352948444.....Total loss: 14.455633352948444\n",
      "epoch 4162 learning rate:  0.010240269101393562   Training loss:   14.943048208089122  Valing loss:   14.455633352948444\n",
      "Pure loss: 14.635016845854786.....Total loss: 14.635016845854786\n",
      "Pure loss: 13.870964846794887.....Total loss: 13.870964846794887\n",
      "epoch 4163 learning rate:  0.010240211386019698   Training loss:   14.635016845854786  Valing loss:   13.870964846794887\n",
      "Pure loss: 14.665862966992263.....Total loss: 14.665862966992263\n",
      "Pure loss: 13.938702503886423.....Total loss: 13.938702503886423\n",
      "epoch 4164 learning rate:  0.010240153698366955   Training loss:   14.665862966992263  Valing loss:   13.938702503886423\n",
      "Pure loss: 14.657418740749529.....Total loss: 14.657418740749529\n",
      "Pure loss: 13.908598181609252.....Total loss: 13.908598181609252\n",
      "epoch 4165 learning rate:  0.010240096038415367   Training loss:   14.657418740749529  Valing loss:   13.908598181609252\n",
      "Pure loss: 14.630262616206831.....Total loss: 14.630262616206831\n",
      "Pure loss: 13.837839221434548.....Total loss: 13.837839221434548\n",
      "epoch 4166 learning rate:  0.010240038406144984   Training loss:   14.630262616206831  Valing loss:   13.837839221434548\n",
      "Pure loss: 14.49636553044776.....Total loss: 14.49636553044776\n",
      "Pure loss: 13.407613389112706.....Total loss: 13.407613389112706\n",
      "epoch 4167 learning rate:  0.010239980801535877   Training loss:   14.49636553044776  Valing loss:   13.407613389112706\n",
      "Pure loss: 14.501533853570429.....Total loss: 14.501533853570429\n",
      "Pure loss: 13.388638880576991.....Total loss: 13.388638880576991\n",
      "epoch 4168 learning rate:  0.010239923224568138   Training loss:   14.501533853570429  Valing loss:   13.388638880576991\n",
      "Pure loss: 14.502178219853391.....Total loss: 14.502178219853391\n",
      "Pure loss: 13.40579236143718.....Total loss: 13.40579236143718\n",
      "epoch 4169 learning rate:  0.010239865675221876   Training loss:   14.502178219853391  Valing loss:   13.40579236143718\n",
      "Pure loss: 14.45468949276738.....Total loss: 14.45468949276738\n",
      "Pure loss: 13.431525625896704.....Total loss: 13.431525625896704\n",
      "epoch 4170 learning rate:  0.010239808153477218   Training loss:   14.45468949276738  Valing loss:   13.431525625896704\n",
      "Pure loss: 14.466839909541156.....Total loss: 14.466839909541156\n",
      "Pure loss: 13.57061627302152.....Total loss: 13.57061627302152\n",
      "epoch 4171 learning rate:  0.010239750659314314   Training loss:   14.466839909541156  Valing loss:   13.57061627302152\n",
      "Pure loss: 14.472914345456866.....Total loss: 14.472914345456866\n",
      "Pure loss: 13.589408319423638.....Total loss: 13.589408319423638\n",
      "epoch 4172 learning rate:  0.010239693192713327   Training loss:   14.472914345456866  Valing loss:   13.589408319423638\n",
      "Pure loss: 14.443967454623099.....Total loss: 14.443967454623099\n",
      "Pure loss: 13.459745852279214.....Total loss: 13.459745852279214\n",
      "epoch 4173 learning rate:  0.010239635753654445   Training loss:   14.443967454623099  Valing loss:   13.459745852279214\n",
      "Pure loss: 14.476955329480473.....Total loss: 14.476955329480473\n",
      "Pure loss: 13.40193126110169.....Total loss: 13.40193126110169\n",
      "epoch 4174 learning rate:  0.010239578342117873   Training loss:   14.476955329480473  Valing loss:   13.40193126110169\n",
      "Pure loss: 14.665804823106432.....Total loss: 14.665804823106432\n",
      "Pure loss: 14.013801998089912.....Total loss: 14.013801998089912\n",
      "epoch 4175 learning rate:  0.010239520958083833   Training loss:   14.665804823106432  Valing loss:   14.013801998089912\n",
      "Pure loss: 14.828449829960359.....Total loss: 14.828449829960359\n",
      "Pure loss: 14.333551656326044.....Total loss: 14.333551656326044\n",
      "epoch 4176 learning rate:  0.010239463601532568   Training loss:   14.828449829960359  Valing loss:   14.333551656326044\n",
      "Pure loss: 14.91537466783561.....Total loss: 14.91537466783561\n",
      "Pure loss: 14.493050573284325.....Total loss: 14.493050573284325\n",
      "epoch 4177 learning rate:  0.010239406272444337   Training loss:   14.91537466783561  Valing loss:   14.493050573284325\n",
      "Pure loss: 14.8596646270537.....Total loss: 14.8596646270537\n",
      "Pure loss: 14.400057826625629.....Total loss: 14.400057826625629\n",
      "epoch 4178 learning rate:  0.010239348970799426   Training loss:   14.8596646270537  Valing loss:   14.400057826625629\n",
      "Pure loss: 14.825171030343094.....Total loss: 14.825171030343094\n",
      "Pure loss: 14.33484334693965.....Total loss: 14.33484334693965\n",
      "epoch 4179 learning rate:  0.01023929169657813   Training loss:   14.825171030343094  Valing loss:   14.33484334693965\n",
      "Pure loss: 15.085442989171963.....Total loss: 15.085442989171963\n",
      "Pure loss: 14.757084092807691.....Total loss: 14.757084092807691\n",
      "epoch 4180 learning rate:  0.010239234449760765   Training loss:   15.085442989171963  Valing loss:   14.757084092807691\n",
      "Pure loss: 14.562225045776364.....Total loss: 14.562225045776364\n",
      "Pure loss: 13.818194455318055.....Total loss: 13.818194455318055\n",
      "epoch 4181 learning rate:  0.010239177230327672   Training loss:   14.562225045776364  Valing loss:   13.818194455318055\n",
      "Pure loss: 14.574839713474466.....Total loss: 14.574839713474466\n",
      "Pure loss: 13.846520953086186.....Total loss: 13.846520953086186\n",
      "epoch 4182 learning rate:  0.010239120038259206   Training loss:   14.574839713474466  Valing loss:   13.846520953086186\n",
      "Pure loss: 14.805667419493922.....Total loss: 14.805667419493922\n",
      "Pure loss: 14.308408208256152.....Total loss: 14.308408208256152\n",
      "epoch 4183 learning rate:  0.01023906287353574   Training loss:   14.805667419493922  Valing loss:   14.308408208256152\n",
      "Pure loss: 14.781250305347738.....Total loss: 14.781250305347738\n",
      "Pure loss: 14.257540633949843.....Total loss: 14.257540633949843\n",
      "epoch 4184 learning rate:  0.010239005736137667   Training loss:   14.781250305347738  Valing loss:   14.257540633949843\n",
      "Pure loss: 15.104408321555914.....Total loss: 15.104408321555914\n",
      "Pure loss: 14.825134892545192.....Total loss: 14.825134892545192\n",
      "epoch 4185 learning rate:  0.0102389486260454   Training loss:   15.104408321555914  Valing loss:   14.825134892545192\n",
      "Pure loss: 14.985159802958472.....Total loss: 14.985159802958472\n",
      "Pure loss: 14.631361752528738.....Total loss: 14.631361752528738\n",
      "epoch 4186 learning rate:  0.01023889154323937   Training loss:   14.985159802958472  Valing loss:   14.631361752528738\n",
      "Pure loss: 14.90347912488754.....Total loss: 14.90347912488754\n",
      "Pure loss: 14.487698754647576.....Total loss: 14.487698754647576\n",
      "epoch 4187 learning rate:  0.010238834487700025   Training loss:   14.90347912488754  Valing loss:   14.487698754647576\n",
      "Pure loss: 14.827179525409663.....Total loss: 14.827179525409663\n",
      "Pure loss: 14.356496809281841.....Total loss: 14.356496809281841\n",
      "epoch 4188 learning rate:  0.010238777459407832   Training loss:   14.827179525409663  Valing loss:   14.356496809281841\n",
      "Pure loss: 14.75633486548637.....Total loss: 14.75633486548637\n",
      "Pure loss: 14.224030097749269.....Total loss: 14.224030097749269\n",
      "epoch 4189 learning rate:  0.01023872045834328   Training loss:   14.75633486548637  Valing loss:   14.224030097749269\n",
      "Pure loss: 14.858835581975736.....Total loss: 14.858835581975736\n",
      "Pure loss: 14.456466026980147.....Total loss: 14.456466026980147\n",
      "epoch 4190 learning rate:  0.010238663484486875   Training loss:   14.858835581975736  Valing loss:   14.456466026980147\n",
      "Pure loss: 14.872123064767731.....Total loss: 14.872123064767731\n",
      "Pure loss: 14.477712212704109.....Total loss: 14.477712212704109\n",
      "epoch 4191 learning rate:  0.010238606537819137   Training loss:   14.872123064767731  Valing loss:   14.477712212704109\n",
      "Pure loss: 14.728009613389943.....Total loss: 14.728009613389943\n",
      "Pure loss: 14.167480956453032.....Total loss: 14.167480956453032\n",
      "epoch 4192 learning rate:  0.010238549618320611   Training loss:   14.728009613389943  Valing loss:   14.167480956453032\n",
      "Pure loss: 14.718196122336737.....Total loss: 14.718196122336737\n",
      "Pure loss: 14.17125908502378.....Total loss: 14.17125908502378\n",
      "epoch 4193 learning rate:  0.010238492725971857   Training loss:   14.718196122336737  Valing loss:   14.17125908502378\n",
      "Pure loss: 14.688984162655855.....Total loss: 14.688984162655855\n",
      "Pure loss: 14.101983158288867.....Total loss: 14.101983158288867\n",
      "epoch 4194 learning rate:  0.010238435860753457   Training loss:   14.688984162655855  Valing loss:   14.101983158288867\n",
      "Pure loss: 14.692894244167551.....Total loss: 14.692894244167551\n",
      "Pure loss: 14.114623556311264.....Total loss: 14.114623556311264\n",
      "epoch 4195 learning rate:  0.010238379022646007   Training loss:   14.692894244167551  Valing loss:   14.114623556311264\n",
      "Pure loss: 14.784767943066768.....Total loss: 14.784767943066768\n",
      "Pure loss: 14.2888018981412.....Total loss: 14.2888018981412\n",
      "epoch 4196 learning rate:  0.010238322211630124   Training loss:   14.784767943066768  Valing loss:   14.2888018981412\n",
      "Pure loss: 14.775534478456517.....Total loss: 14.775534478456517\n",
      "Pure loss: 14.271385205535406.....Total loss: 14.271385205535406\n",
      "epoch 4197 learning rate:  0.010238265427686443   Training loss:   14.775534478456517  Valing loss:   14.271385205535406\n",
      "Pure loss: 14.776320319639874.....Total loss: 14.776320319639874\n",
      "Pure loss: 14.18592064062454.....Total loss: 14.18592064062454\n",
      "epoch 4198 learning rate:  0.010238208670795617   Training loss:   14.776320319639874  Valing loss:   14.18592064062454\n",
      "Pure loss: 14.641691661437719.....Total loss: 14.641691661437719\n",
      "Pure loss: 13.740955006873445.....Total loss: 13.740955006873445\n",
      "epoch 4199 learning rate:  0.01023815194093832   Training loss:   14.641691661437719  Valing loss:   13.740955006873445\n",
      "Pure loss: 14.639930222127447.....Total loss: 14.639930222127447\n",
      "Pure loss: 13.746203940739596.....Total loss: 13.746203940739596\n",
      "epoch 4200 learning rate:  0.010238095238095239   Training loss:   14.639930222127447  Valing loss:   13.746203940739596\n",
      "Pure loss: 14.68954508169892.....Total loss: 14.68954508169892\n",
      "Pure loss: 13.77130943060721.....Total loss: 13.77130943060721\n",
      "epoch 4201 learning rate:  0.010238038562247084   Training loss:   14.68954508169892  Valing loss:   13.77130943060721\n",
      "Pure loss: 14.761911860125283.....Total loss: 14.761911860125283\n",
      "Pure loss: 13.616445055424741.....Total loss: 13.616445055424741\n",
      "epoch 4202 learning rate:  0.010237981913374584   Training loss:   14.761911860125283  Valing loss:   13.616445055424741\n",
      "Pure loss: 14.753182694752443.....Total loss: 14.753182694752443\n",
      "Pure loss: 13.633072768505459.....Total loss: 13.633072768505459\n",
      "epoch 4203 learning rate:  0.010237925291458482   Training loss:   14.753182694752443  Valing loss:   13.633072768505459\n",
      "Pure loss: 14.686967624611656.....Total loss: 14.686967624611656\n",
      "Pure loss: 14.023889623583791.....Total loss: 14.023889623583791\n",
      "epoch 4204 learning rate:  0.010237868696479543   Training loss:   14.686967624611656  Valing loss:   14.023889623583791\n",
      "Pure loss: 14.60418056800246.....Total loss: 14.60418056800246\n",
      "Pure loss: 13.706902154374946.....Total loss: 13.706902154374946\n",
      "epoch 4205 learning rate:  0.010237812128418549   Training loss:   14.60418056800246  Valing loss:   13.706902154374946\n",
      "Pure loss: 14.599625912289598.....Total loss: 14.599625912289598\n",
      "Pure loss: 13.730422620041784.....Total loss: 13.730422620041784\n",
      "epoch 4206 learning rate:  0.0102377555872563   Training loss:   14.599625912289598  Valing loss:   13.730422620041784\n",
      "Pure loss: 14.60980162167906.....Total loss: 14.60980162167906\n",
      "Pure loss: 13.792315961541876.....Total loss: 13.792315961541876\n",
      "epoch 4207 learning rate:  0.010237699072973616   Training loss:   14.60980162167906  Valing loss:   13.792315961541876\n",
      "Pure loss: 14.73462374620253.....Total loss: 14.73462374620253\n",
      "Pure loss: 13.848564875523584.....Total loss: 13.848564875523584\n",
      "epoch 4208 learning rate:  0.010237642585551331   Training loss:   14.73462374620253  Valing loss:   13.848564875523584\n",
      "Pure loss: 14.667890181797533.....Total loss: 14.667890181797533\n",
      "Pure loss: 13.824829904389578.....Total loss: 13.824829904389578\n",
      "epoch 4209 learning rate:  0.010237586124970301   Training loss:   14.667890181797533  Valing loss:   13.824829904389578\n",
      "Pure loss: 14.642701197039035.....Total loss: 14.642701197039035\n",
      "Pure loss: 13.832809700791827.....Total loss: 13.832809700791827\n",
      "epoch 4210 learning rate:  0.010237529691211401   Training loss:   14.642701197039035  Valing loss:   13.832809700791827\n",
      "Pure loss: 14.6385858529827.....Total loss: 14.6385858529827\n",
      "Pure loss: 13.741512330571469.....Total loss: 13.741512330571469\n",
      "epoch 4211 learning rate:  0.010237473284255522   Training loss:   14.6385858529827  Valing loss:   13.741512330571469\n",
      "Pure loss: 14.64745739375055.....Total loss: 14.64745739375055\n",
      "Pure loss: 13.74523846319218.....Total loss: 13.74523846319218\n",
      "epoch 4212 learning rate:  0.010237416904083571   Training loss:   14.64745739375055  Valing loss:   13.74523846319218\n",
      "Pure loss: 14.584782298330433.....Total loss: 14.584782298330433\n",
      "Pure loss: 13.686889321063143.....Total loss: 13.686889321063143\n",
      "epoch 4213 learning rate:  0.010237360550676478   Training loss:   14.584782298330433  Valing loss:   13.686889321063143\n",
      "Pure loss: 14.516242541686031.....Total loss: 14.516242541686031\n",
      "Pure loss: 13.374352991885006.....Total loss: 13.374352991885006\n",
      "epoch 4214 learning rate:  0.010237304224015187   Training loss:   14.516242541686031  Valing loss:   13.374352991885006\n",
      "Pure loss: 14.634179913887783.....Total loss: 14.634179913887783\n",
      "Pure loss: 13.226142603365094.....Total loss: 13.226142603365094\n",
      "epoch 4215 learning rate:  0.010237247924080664   Training loss:   14.634179913887783  Valing loss:   13.226142603365094\n",
      "Pure loss: 14.78485280831966.....Total loss: 14.78485280831966\n",
      "Pure loss: 13.351441147101848.....Total loss: 13.351441147101848\n",
      "epoch 4216 learning rate:  0.01023719165085389   Training loss:   14.78485280831966  Valing loss:   13.351441147101848\n",
      "Pure loss: 14.847595320195884.....Total loss: 14.847595320195884\n",
      "Pure loss: 13.348335834788799.....Total loss: 13.348335834788799\n",
      "epoch 4217 learning rate:  0.010237135404315865   Training loss:   14.847595320195884  Valing loss:   13.348335834788799\n",
      "Pure loss: 14.85429379122366.....Total loss: 14.85429379122366\n",
      "Pure loss: 13.353133484302054.....Total loss: 13.353133484302054\n",
      "epoch 4218 learning rate:  0.010237079184447606   Training loss:   14.85429379122366  Valing loss:   13.353133484302054\n",
      "Pure loss: 14.317570778882873.....Total loss: 14.317570778882873\n",
      "Pure loss: 13.087220833820567.....Total loss: 13.087220833820567\n",
      "epoch 4219 learning rate:  0.01023702299123015   Training loss:   14.317570778882873  Valing loss:   13.087220833820567\n",
      "Pure loss: 14.330550359185372.....Total loss: 14.330550359185372\n",
      "Pure loss: 13.097268649790362.....Total loss: 13.097268649790362\n",
      "epoch 4220 learning rate:  0.01023696682464455   Training loss:   14.330550359185372  Valing loss:   13.097268649790362\n",
      "Pure loss: 14.41139922863179.....Total loss: 14.41139922863179\n",
      "Pure loss: 13.050183844425387.....Total loss: 13.050183844425387\n",
      "epoch 4221 learning rate:  0.010236910684671878   Training loss:   14.41139922863179  Valing loss:   13.050183844425387\n",
      "Pure loss: 14.315628222851668.....Total loss: 14.315628222851668\n",
      "Pure loss: 13.013724677084596.....Total loss: 13.013724677084596\n",
      "epoch 4222 learning rate:  0.010236854571293226   Training loss:   14.315628222851668  Valing loss:   13.013724677084596\n",
      "Pure loss: 14.266610713275112.....Total loss: 14.266610713275112\n",
      "Pure loss: 13.084786169895331.....Total loss: 13.084786169895331\n",
      "epoch 4223 learning rate:  0.0102367984844897   Training loss:   14.266610713275112  Valing loss:   13.084786169895331\n",
      "Pure loss: 14.26988145186516.....Total loss: 14.26988145186516\n",
      "Pure loss: 13.036897094327351.....Total loss: 13.036897094327351\n",
      "epoch 4224 learning rate:  0.010236742424242424   Training loss:   14.26988145186516  Valing loss:   13.036897094327351\n",
      "Pure loss: 14.276778676521868.....Total loss: 14.276778676521868\n",
      "Pure loss: 13.025377960581933.....Total loss: 13.025377960581933\n",
      "epoch 4225 learning rate:  0.010236686390532544   Training loss:   14.276778676521868  Valing loss:   13.025377960581933\n",
      "Pure loss: 14.279940820412481.....Total loss: 14.279940820412481\n",
      "Pure loss: 12.929285407081226.....Total loss: 12.929285407081226\n",
      "epoch 4226 learning rate:  0.010236630383341222   Training loss:   14.279940820412481  Valing loss:   12.929285407081226\n",
      "Pure loss: 14.398016135522912.....Total loss: 14.398016135522912\n",
      "Pure loss: 12.958956427220656.....Total loss: 12.958956427220656\n",
      "epoch 4227 learning rate:  0.010236574402649634   Training loss:   14.398016135522912  Valing loss:   12.958956427220656\n",
      "Pure loss: 14.363182660011192.....Total loss: 14.363182660011192\n",
      "Pure loss: 12.978733890844575.....Total loss: 12.978733890844575\n",
      "epoch 4228 learning rate:  0.010236518448438979   Training loss:   14.363182660011192  Valing loss:   12.978733890844575\n",
      "Pure loss: 14.372279229178494.....Total loss: 14.372279229178494\n",
      "Pure loss: 12.936249473421238.....Total loss: 12.936249473421238\n",
      "epoch 4229 learning rate:  0.010236462520690472   Training loss:   14.372279229178494  Valing loss:   12.936249473421238\n",
      "Pure loss: 14.441169668153735.....Total loss: 14.441169668153735\n",
      "Pure loss: 12.883732282114652.....Total loss: 12.883732282114652\n",
      "epoch 4230 learning rate:  0.010236406619385343   Training loss:   14.441169668153735  Valing loss:   12.883732282114652\n",
      "Pure loss: 14.412956479618686.....Total loss: 14.412956479618686\n",
      "Pure loss: 12.863364352539586.....Total loss: 12.863364352539586\n",
      "epoch 4231 learning rate:  0.010236350744504845   Training loss:   14.412956479618686  Valing loss:   12.863364352539586\n",
      "Pure loss: 14.445151671230114.....Total loss: 14.445151671230114\n",
      "Pure loss: 12.848195543139145.....Total loss: 12.848195543139145\n",
      "epoch 4232 learning rate:  0.010236294896030246   Training loss:   14.445151671230114  Valing loss:   12.848195543139145\n",
      "Pure loss: 14.345186603974643.....Total loss: 14.345186603974643\n",
      "Pure loss: 12.991428660367351.....Total loss: 12.991428660367351\n",
      "epoch 4233 learning rate:  0.01023623907394283   Training loss:   14.345186603974643  Valing loss:   12.991428660367351\n",
      "Pure loss: 14.417787045959885.....Total loss: 14.417787045959885\n",
      "Pure loss: 12.934153622453854.....Total loss: 12.934153622453854\n",
      "epoch 4234 learning rate:  0.010236183278223902   Training loss:   14.417787045959885  Valing loss:   12.934153622453854\n",
      "Pure loss: 14.373493390607429.....Total loss: 14.373493390607429\n",
      "Pure loss: 12.90796187548237.....Total loss: 12.90796187548237\n",
      "epoch 4235 learning rate:  0.010236127508854782   Training loss:   14.373493390607429  Valing loss:   12.90796187548237\n",
      "Pure loss: 14.441008323168338.....Total loss: 14.441008323168338\n",
      "Pure loss: 12.849611757914968.....Total loss: 12.849611757914968\n",
      "epoch 4236 learning rate:  0.010236071765816809   Training loss:   14.441008323168338  Valing loss:   12.849611757914968\n",
      "Pure loss: 14.39419695798526.....Total loss: 14.39419695798526\n",
      "Pure loss: 13.402159514139512.....Total loss: 13.402159514139512\n",
      "epoch 4237 learning rate:  0.01023601604909134   Training loss:   14.39419695798526  Valing loss:   13.402159514139512\n",
      "Pure loss: 14.398502742863444.....Total loss: 14.398502742863444\n",
      "Pure loss: 13.416876817274929.....Total loss: 13.416876817274929\n",
      "epoch 4238 learning rate:  0.010235960358659745   Training loss:   14.398502742863444  Valing loss:   13.416876817274929\n",
      "Pure loss: 14.415676899558749.....Total loss: 14.415676899558749\n",
      "Pure loss: 13.472779975437057.....Total loss: 13.472779975437057\n",
      "epoch 4239 learning rate:  0.01023590469450342   Training loss:   14.415676899558749  Valing loss:   13.472779975437057\n",
      "Pure loss: 14.452186324016242.....Total loss: 14.452186324016242\n",
      "Pure loss: 13.695119046313804.....Total loss: 13.695119046313804\n",
      "epoch 4240 learning rate:  0.010235849056603774   Training loss:   14.452186324016242  Valing loss:   13.695119046313804\n",
      "Pure loss: 14.414043862268416.....Total loss: 14.414043862268416\n",
      "Pure loss: 13.598432880213968.....Total loss: 13.598432880213968\n",
      "epoch 4241 learning rate:  0.01023579344494223   Training loss:   14.414043862268416  Valing loss:   13.598432880213968\n",
      "Pure loss: 14.409425007717454.....Total loss: 14.409425007717454\n",
      "Pure loss: 13.584883336467502.....Total loss: 13.584883336467502\n",
      "epoch 4242 learning rate:  0.010235737859500237   Training loss:   14.409425007717454  Valing loss:   13.584883336467502\n",
      "Pure loss: 14.365650531633364.....Total loss: 14.365650531633364\n",
      "Pure loss: 13.366697616177309.....Total loss: 13.366697616177309\n",
      "epoch 4243 learning rate:  0.010235682300259251   Training loss:   14.365650531633364  Valing loss:   13.366697616177309\n",
      "Pure loss: 14.414819843791856.....Total loss: 14.414819843791856\n",
      "Pure loss: 13.372566907470928.....Total loss: 13.372566907470928\n",
      "epoch 4244 learning rate:  0.010235626767200754   Training loss:   14.414819843791856  Valing loss:   13.372566907470928\n",
      "Pure loss: 14.39049556690277.....Total loss: 14.39049556690277\n",
      "Pure loss: 13.41969130823294.....Total loss: 13.41969130823294\n",
      "epoch 4245 learning rate:  0.010235571260306242   Training loss:   14.39049556690277  Valing loss:   13.41969130823294\n",
      "Pure loss: 14.418372221673675.....Total loss: 14.418372221673675\n",
      "Pure loss: 13.528174059475326.....Total loss: 13.528174059475326\n",
      "epoch 4246 learning rate:  0.01023551577955723   Training loss:   14.418372221673675  Valing loss:   13.528174059475326\n",
      "Pure loss: 14.395715906726751.....Total loss: 14.395715906726751\n",
      "Pure loss: 13.192639647856403.....Total loss: 13.192639647856403\n",
      "epoch 4247 learning rate:  0.010235460324935249   Training loss:   14.395715906726751  Valing loss:   13.192639647856403\n",
      "Pure loss: 14.27280886455529.....Total loss: 14.27280886455529\n",
      "Pure loss: 13.196851594449896.....Total loss: 13.196851594449896\n",
      "epoch 4248 learning rate:  0.010235404896421847   Training loss:   14.27280886455529  Valing loss:   13.196851594449896\n",
      "Pure loss: 14.276946952523387.....Total loss: 14.276946952523387\n",
      "Pure loss: 13.219834240213554.....Total loss: 13.219834240213554\n",
      "epoch 4249 learning rate:  0.010235349493998588   Training loss:   14.276946952523387  Valing loss:   13.219834240213554\n",
      "Pure loss: 14.347245935020013.....Total loss: 14.347245935020013\n",
      "Pure loss: 13.20388320698296.....Total loss: 13.20388320698296\n",
      "epoch 4250 learning rate:  0.01023529411764706   Training loss:   14.347245935020013  Valing loss:   13.20388320698296\n",
      "Pure loss: 14.43875824786417.....Total loss: 14.43875824786417\n",
      "Pure loss: 13.023298718695345.....Total loss: 13.023298718695345\n",
      "epoch 4251 learning rate:  0.010235238767348859   Training loss:   14.43875824786417  Valing loss:   13.023298718695345\n",
      "Pure loss: 14.386548172640605.....Total loss: 14.386548172640605\n",
      "Pure loss: 12.992532702317545.....Total loss: 12.992532702317545\n",
      "epoch 4252 learning rate:  0.010235183443085607   Training loss:   14.386548172640605  Valing loss:   12.992532702317545\n",
      "Pure loss: 14.454387782933894.....Total loss: 14.454387782933894\n",
      "Pure loss: 13.031742666254727.....Total loss: 13.031742666254727\n",
      "epoch 4253 learning rate:  0.010235128144838937   Training loss:   14.454387782933894  Valing loss:   13.031742666254727\n",
      "Pure loss: 14.428918383713134.....Total loss: 14.428918383713134\n",
      "Pure loss: 13.031376274324161.....Total loss: 13.031376274324161\n",
      "epoch 4254 learning rate:  0.010235072872590503   Training loss:   14.428918383713134  Valing loss:   13.031376274324161\n",
      "Pure loss: 14.63735872932089.....Total loss: 14.63735872932089\n",
      "Pure loss: 12.962577511914208.....Total loss: 12.962577511914208\n",
      "epoch 4255 learning rate:  0.010235017626321974   Training loss:   14.63735872932089  Valing loss:   12.962577511914208\n",
      "Pure loss: 14.78367508144801.....Total loss: 14.78367508144801\n",
      "Pure loss: 12.98948332250224.....Total loss: 12.98948332250224\n",
      "epoch 4256 learning rate:  0.010234962406015038   Training loss:   14.78367508144801  Valing loss:   12.98948332250224\n",
      "Pure loss: 14.75653632623687.....Total loss: 14.75653632623687\n",
      "Pure loss: 12.988896832589043.....Total loss: 12.988896832589043\n",
      "epoch 4257 learning rate:  0.010234907211651397   Training loss:   14.75653632623687  Valing loss:   12.988896832589043\n",
      "Pure loss: 15.07120275648249.....Total loss: 15.07120275648249\n",
      "Pure loss: 13.248811370400555.....Total loss: 13.248811370400555\n",
      "epoch 4258 learning rate:  0.010234852043212776   Training loss:   15.07120275648249  Valing loss:   13.248811370400555\n",
      "Pure loss: 15.05769027341708.....Total loss: 15.05769027341708\n",
      "Pure loss: 13.28209485340509.....Total loss: 13.28209485340509\n",
      "epoch 4259 learning rate:  0.010234796900680911   Training loss:   15.05769027341708  Valing loss:   13.28209485340509\n",
      "Pure loss: 14.743525853479786.....Total loss: 14.743525853479786\n",
      "Pure loss: 13.23104171059241.....Total loss: 13.23104171059241\n",
      "epoch 4260 learning rate:  0.01023474178403756   Training loss:   14.743525853479786  Valing loss:   13.23104171059241\n",
      "Pure loss: 14.820013101916187.....Total loss: 14.820013101916187\n",
      "Pure loss: 13.24202780646268.....Total loss: 13.24202780646268\n",
      "epoch 4261 learning rate:  0.010234686693264492   Training loss:   14.820013101916187  Valing loss:   13.24202780646268\n",
      "Pure loss: 14.692956249074843.....Total loss: 14.692956249074843\n",
      "Pure loss: 13.456135079559921.....Total loss: 13.456135079559921\n",
      "epoch 4262 learning rate:  0.010234631628343501   Training loss:   14.692956249074843  Valing loss:   13.456135079559921\n",
      "Pure loss: 14.684131295280311.....Total loss: 14.684131295280311\n",
      "Pure loss: 13.47501101153094.....Total loss: 13.47501101153094\n",
      "epoch 4263 learning rate:  0.010234576589256392   Training loss:   14.684131295280311  Valing loss:   13.47501101153094\n",
      "Pure loss: 14.877709700008786.....Total loss: 14.877709700008786\n",
      "Pure loss: 13.393423793825804.....Total loss: 13.393423793825804\n",
      "epoch 4264 learning rate:  0.01023452157598499   Training loss:   14.877709700008786  Valing loss:   13.393423793825804\n",
      "Pure loss: 14.89952709098551.....Total loss: 14.89952709098551\n",
      "Pure loss: 13.401721874500312.....Total loss: 13.401721874500312\n",
      "epoch 4265 learning rate:  0.010234466588511137   Training loss:   14.89952709098551  Valing loss:   13.401721874500312\n",
      "Pure loss: 14.60135585101016.....Total loss: 14.60135585101016\n",
      "Pure loss: 13.466362757597697.....Total loss: 13.466362757597697\n",
      "epoch 4266 learning rate:  0.010234411626816691   Training loss:   14.60135585101016  Valing loss:   13.466362757597697\n",
      "Pure loss: 14.391063047155253.....Total loss: 14.391063047155253\n",
      "Pure loss: 13.385319325121761.....Total loss: 13.385319325121761\n",
      "epoch 4267 learning rate:  0.010234356690883526   Training loss:   14.391063047155253  Valing loss:   13.385319325121761\n",
      "Pure loss: 14.404715110737246.....Total loss: 14.404715110737246\n",
      "Pure loss: 13.389324888721537.....Total loss: 13.389324888721537\n",
      "epoch 4268 learning rate:  0.010234301780693534   Training loss:   14.404715110737246  Valing loss:   13.389324888721537\n",
      "Pure loss: 14.408224636785976.....Total loss: 14.408224636785976\n",
      "Pure loss: 13.3550021668164.....Total loss: 13.3550021668164\n",
      "epoch 4269 learning rate:  0.010234246896228625   Training loss:   14.408224636785976  Valing loss:   13.3550021668164\n",
      "Pure loss: 14.390345046782343.....Total loss: 14.390345046782343\n",
      "Pure loss: 13.358014206352749.....Total loss: 13.358014206352749\n",
      "epoch 4270 learning rate:  0.010234192037470726   Training loss:   14.390345046782343  Valing loss:   13.358014206352749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 14.333793870646392.....Total loss: 14.333793870646392\n",
      "Pure loss: 13.326909614319723.....Total loss: 13.326909614319723\n",
      "epoch 4271 learning rate:  0.01023413720440178   Training loss:   14.333793870646392  Valing loss:   13.326909614319723\n",
      "Pure loss: 14.380306641274043.....Total loss: 14.380306641274043\n",
      "Pure loss: 13.3370006628605.....Total loss: 13.3370006628605\n",
      "epoch 4272 learning rate:  0.010234082397003746   Training loss:   14.380306641274043  Valing loss:   13.3370006628605\n",
      "Pure loss: 14.31764129002526.....Total loss: 14.31764129002526\n",
      "Pure loss: 13.27520743350933.....Total loss: 13.27520743350933\n",
      "epoch 4273 learning rate:  0.0102340276152586   Training loss:   14.31764129002526  Valing loss:   13.27520743350933\n",
      "Pure loss: 14.31041126847117.....Total loss: 14.31041126847117\n",
      "Pure loss: 13.210991049300086.....Total loss: 13.210991049300086\n",
      "epoch 4274 learning rate:  0.010233972859148338   Training loss:   14.31041126847117  Valing loss:   13.210991049300086\n",
      "Pure loss: 14.319148375411723.....Total loss: 14.319148375411723\n",
      "Pure loss: 13.15262802965252.....Total loss: 13.15262802965252\n",
      "epoch 4275 learning rate:  0.01023391812865497   Training loss:   14.319148375411723  Valing loss:   13.15262802965252\n",
      "Pure loss: 14.30433215898681.....Total loss: 14.30433215898681\n",
      "Pure loss: 13.204786894623744.....Total loss: 13.204786894623744\n",
      "epoch 4276 learning rate:  0.010233863423760524   Training loss:   14.30433215898681  Valing loss:   13.204786894623744\n",
      "Pure loss: 14.2985828795656.....Total loss: 14.2985828795656\n",
      "Pure loss: 13.317239138252729.....Total loss: 13.317239138252729\n",
      "epoch 4277 learning rate:  0.010233808744447043   Training loss:   14.2985828795656  Valing loss:   13.317239138252729\n",
      "Pure loss: 14.285252211181701.....Total loss: 14.285252211181701\n",
      "Pure loss: 13.219298901355879.....Total loss: 13.219298901355879\n",
      "epoch 4278 learning rate:  0.010233754090696587   Training loss:   14.285252211181701  Valing loss:   13.219298901355879\n",
      "Pure loss: 14.346954260147001.....Total loss: 14.346954260147001\n",
      "Pure loss: 13.226100717349746.....Total loss: 13.226100717349746\n",
      "epoch 4279 learning rate:  0.010233699462491237   Training loss:   14.346954260147001  Valing loss:   13.226100717349746\n",
      "Pure loss: 14.73874638368262.....Total loss: 14.73874638368262\n",
      "Pure loss: 14.291250486538175.....Total loss: 14.291250486538175\n",
      "epoch 4280 learning rate:  0.010233644859813084   Training loss:   14.73874638368262  Valing loss:   14.291250486538175\n",
      "Pure loss: 14.80723186311552.....Total loss: 14.80723186311552\n",
      "Pure loss: 14.39579910648172.....Total loss: 14.39579910648172\n",
      "epoch 4281 learning rate:  0.010233590282644242   Training loss:   14.80723186311552  Valing loss:   14.39579910648172\n",
      "Pure loss: 14.328567394325145.....Total loss: 14.328567394325145\n",
      "Pure loss: 13.508656337130631.....Total loss: 13.508656337130631\n",
      "epoch 4282 learning rate:  0.010233535730966838   Training loss:   14.328567394325145  Valing loss:   13.508656337130631\n",
      "Pure loss: 14.188419577066988.....Total loss: 14.188419577066988\n",
      "Pure loss: 13.148701322966527.....Total loss: 13.148701322966527\n",
      "epoch 4283 learning rate:  0.010233481204763016   Training loss:   14.188419577066988  Valing loss:   13.148701322966527\n",
      "Pure loss: 14.153102850567148.....Total loss: 14.153102850567148\n",
      "Pure loss: 12.907327448055485.....Total loss: 12.907327448055485\n",
      "epoch 4284 learning rate:  0.010233426704014939   Training loss:   14.153102850567148  Valing loss:   12.907327448055485\n",
      "Pure loss: 14.23368991670309.....Total loss: 14.23368991670309\n",
      "Pure loss: 12.634782759720164.....Total loss: 12.634782759720164\n",
      "epoch 4285 learning rate:  0.010233372228704784   Training loss:   14.23368991670309  Valing loss:   12.634782759720164\n",
      "Pure loss: 14.319768123315344.....Total loss: 14.319768123315344\n",
      "Pure loss: 12.586967317225135.....Total loss: 12.586967317225135\n",
      "epoch 4286 learning rate:  0.010233317778814746   Training loss:   14.319768123315344  Valing loss:   12.586967317225135\n",
      "Pure loss: 14.407607476035967.....Total loss: 14.407607476035967\n",
      "Pure loss: 12.595404697927327.....Total loss: 12.595404697927327\n",
      "epoch 4287 learning rate:  0.010233263354327036   Training loss:   14.407607476035967  Valing loss:   12.595404697927327\n",
      "Pure loss: 14.328891264991537.....Total loss: 14.328891264991537\n",
      "Pure loss: 12.619962361384237.....Total loss: 12.619962361384237\n",
      "epoch 4288 learning rate:  0.010233208955223881   Training loss:   14.328891264991537  Valing loss:   12.619962361384237\n",
      "Pure loss: 14.458173872036157.....Total loss: 14.458173872036157\n",
      "Pure loss: 12.577693172455515.....Total loss: 12.577693172455515\n",
      "epoch 4289 learning rate:  0.010233154581487526   Training loss:   14.458173872036157  Valing loss:   12.577693172455515\n",
      "Pure loss: 14.41953957022558.....Total loss: 14.41953957022558\n",
      "Pure loss: 12.579872049123415.....Total loss: 12.579872049123415\n",
      "epoch 4290 learning rate:  0.010233100233100234   Training loss:   14.41953957022558  Valing loss:   12.579872049123415\n",
      "Pure loss: 14.38488495270098.....Total loss: 14.38488495270098\n",
      "Pure loss: 12.57672612360681.....Total loss: 12.57672612360681\n",
      "epoch 4291 learning rate:  0.01023304591004428   Training loss:   14.38488495270098  Valing loss:   12.57672612360681\n",
      "Pure loss: 14.527390331925698.....Total loss: 14.527390331925698\n",
      "Pure loss: 12.622391523652292.....Total loss: 12.622391523652292\n",
      "epoch 4292 learning rate:  0.010232991612301958   Training loss:   14.527390331925698  Valing loss:   12.622391523652292\n",
      "Pure loss: 14.46504976263262.....Total loss: 14.46504976263262\n",
      "Pure loss: 12.605264763104268.....Total loss: 12.605264763104268\n",
      "epoch 4293 learning rate:  0.010232937339855579   Training loss:   14.46504976263262  Valing loss:   12.605264763104268\n",
      "Pure loss: 14.636215043781535.....Total loss: 14.636215043781535\n",
      "Pure loss: 12.646632892838777.....Total loss: 12.646632892838777\n",
      "epoch 4294 learning rate:  0.01023288309268747   Training loss:   14.636215043781535  Valing loss:   12.646632892838777\n",
      "Pure loss: 14.346625578991912.....Total loss: 14.346625578991912\n",
      "Pure loss: 12.594698868351115.....Total loss: 12.594698868351115\n",
      "epoch 4295 learning rate:  0.010232828870779977   Training loss:   14.346625578991912  Valing loss:   12.594698868351115\n",
      "Pure loss: 14.142367356435024.....Total loss: 14.142367356435024\n",
      "Pure loss: 12.752030226036894.....Total loss: 12.752030226036894\n",
      "epoch 4296 learning rate:  0.010232774674115456   Training loss:   14.142367356435024  Valing loss:   12.752030226036894\n",
      "Pure loss: 14.127724350720856.....Total loss: 14.127724350720856\n",
      "Pure loss: 12.807373501071504.....Total loss: 12.807373501071504\n",
      "epoch 4297 learning rate:  0.010232720502676287   Training loss:   14.127724350720856  Valing loss:   12.807373501071504\n",
      "Pure loss: 14.108206726448108.....Total loss: 14.108206726448108\n",
      "Pure loss: 12.876520219432479.....Total loss: 12.876520219432479\n",
      "epoch 4298 learning rate:  0.010232666356444858   Training loss:   14.108206726448108  Valing loss:   12.876520219432479\n",
      "Pure loss: 14.123279108668768.....Total loss: 14.123279108668768\n",
      "Pure loss: 13.042047419880506.....Total loss: 13.042047419880506\n",
      "epoch 4299 learning rate:  0.010232612235403582   Training loss:   14.123279108668768  Valing loss:   13.042047419880506\n",
      "Pure loss: 14.12220331004185.....Total loss: 14.12220331004185\n",
      "Pure loss: 13.002665433424259.....Total loss: 13.002665433424259\n",
      "epoch 4300 learning rate:  0.010232558139534885   Training loss:   14.12220331004185  Valing loss:   13.002665433424259\n",
      "Pure loss: 14.146667121509534.....Total loss: 14.146667121509534\n",
      "Pure loss: 12.881997466332539.....Total loss: 12.881997466332539\n",
      "epoch 4301 learning rate:  0.010232504068821204   Training loss:   14.146667121509534  Valing loss:   12.881997466332539\n",
      "Pure loss: 14.170446176850543.....Total loss: 14.170446176850543\n",
      "Pure loss: 12.850155036241837.....Total loss: 12.850155036241837\n",
      "epoch 4302 learning rate:  0.010232450023245002   Training loss:   14.170446176850543  Valing loss:   12.850155036241837\n",
      "Pure loss: 14.212433578058443.....Total loss: 14.212433578058443\n",
      "Pure loss: 13.399419636779422.....Total loss: 13.399419636779422\n",
      "epoch 4303 learning rate:  0.010232396002788753   Training loss:   14.212433578058443  Valing loss:   13.399419636779422\n",
      "Pure loss: 14.208287243343976.....Total loss: 14.208287243343976\n",
      "Pure loss: 13.354571533543384.....Total loss: 13.354571533543384\n",
      "epoch 4304 learning rate:  0.010232342007434945   Training loss:   14.208287243343976  Valing loss:   13.354571533543384\n",
      "Pure loss: 14.214758995066395.....Total loss: 14.214758995066395\n",
      "Pure loss: 13.374142531253534.....Total loss: 13.374142531253534\n",
      "epoch 4305 learning rate:  0.010232288037166085   Training loss:   14.214758995066395  Valing loss:   13.374142531253534\n",
      "Pure loss: 14.307577246059115.....Total loss: 14.307577246059115\n",
      "Pure loss: 13.616655705398331.....Total loss: 13.616655705398331\n",
      "epoch 4306 learning rate:  0.010232234091964701   Training loss:   14.307577246059115  Valing loss:   13.616655705398331\n",
      "Pure loss: 14.288896560445766.....Total loss: 14.288896560445766\n",
      "Pure loss: 13.529962556197951.....Total loss: 13.529962556197951\n",
      "epoch 4307 learning rate:  0.010232180171813328   Training loss:   14.288896560445766  Valing loss:   13.529962556197951\n",
      "Pure loss: 14.285218175771714.....Total loss: 14.285218175771714\n",
      "Pure loss: 13.4541071896104.....Total loss: 13.4541071896104\n",
      "epoch 4308 learning rate:  0.010232126276694523   Training loss:   14.285218175771714  Valing loss:   13.4541071896104\n",
      "Pure loss: 14.346528823254092.....Total loss: 14.346528823254092\n",
      "Pure loss: 13.641393186722775.....Total loss: 13.641393186722775\n",
      "epoch 4309 learning rate:  0.010232072406590857   Training loss:   14.346528823254092  Valing loss:   13.641393186722775\n",
      "Pure loss: 14.294942532582038.....Total loss: 14.294942532582038\n",
      "Pure loss: 13.513338176565979.....Total loss: 13.513338176565979\n",
      "epoch 4310 learning rate:  0.010232018561484918   Training loss:   14.294942532582038  Valing loss:   13.513338176565979\n",
      "Pure loss: 14.268608895756016.....Total loss: 14.268608895756016\n",
      "Pure loss: 13.412727744041915.....Total loss: 13.412727744041915\n",
      "epoch 4311 learning rate:  0.010231964741359314   Training loss:   14.268608895756016  Valing loss:   13.412727744041915\n",
      "Pure loss: 14.41485551671914.....Total loss: 14.41485551671914\n",
      "Pure loss: 13.803101108599305.....Total loss: 13.803101108599305\n",
      "epoch 4312 learning rate:  0.010231910946196661   Training loss:   14.41485551671914  Valing loss:   13.803101108599305\n",
      "Pure loss: 14.42919548840066.....Total loss: 14.42919548840066\n",
      "Pure loss: 13.80551105548644.....Total loss: 13.80551105548644\n",
      "epoch 4313 learning rate:  0.010231857175979596   Training loss:   14.42919548840066  Valing loss:   13.80551105548644\n",
      "Pure loss: 14.484999092504811.....Total loss: 14.484999092504811\n",
      "Pure loss: 13.940838633967164.....Total loss: 13.940838633967164\n",
      "epoch 4314 learning rate:  0.010231803430690774   Training loss:   14.484999092504811  Valing loss:   13.940838633967164\n",
      "Pure loss: 14.327659265849755.....Total loss: 14.327659265849755\n",
      "Pure loss: 13.487549037525865.....Total loss: 13.487549037525865\n",
      "epoch 4315 learning rate:  0.010231749710312862   Training loss:   14.327659265849755  Valing loss:   13.487549037525865\n",
      "Pure loss: 14.748945722853184.....Total loss: 14.748945722853184\n",
      "Pure loss: 14.373705723274213.....Total loss: 14.373705723274213\n",
      "epoch 4316 learning rate:  0.010231696014828545   Training loss:   14.748945722853184  Valing loss:   14.373705723274213\n",
      "Pure loss: 14.99817003073592.....Total loss: 14.99817003073592\n",
      "Pure loss: 14.819378607295029.....Total loss: 14.819378607295029\n",
      "epoch 4317 learning rate:  0.010231642344220524   Training loss:   14.99817003073592  Valing loss:   14.819378607295029\n",
      "Pure loss: 14.975947507219065.....Total loss: 14.975947507219065\n",
      "Pure loss: 14.777261301049055.....Total loss: 14.777261301049055\n",
      "epoch 4318 learning rate:  0.010231588698471515   Training loss:   14.975947507219065  Valing loss:   14.777261301049055\n",
      "Pure loss: 14.804780547466622.....Total loss: 14.804780547466622\n",
      "Pure loss: 14.488065818688819.....Total loss: 14.488065818688819\n",
      "epoch 4319 learning rate:  0.010231535077564252   Training loss:   14.804780547466622  Valing loss:   14.488065818688819\n",
      "Pure loss: 14.854856605894714.....Total loss: 14.854856605894714\n",
      "Pure loss: 14.592296272458745.....Total loss: 14.592296272458745\n",
      "epoch 4320 learning rate:  0.010231481481481482   Training loss:   14.854856605894714  Valing loss:   14.592296272458745\n",
      "Pure loss: 15.009068008147203.....Total loss: 15.009068008147203\n",
      "Pure loss: 14.85166050150176.....Total loss: 14.85166050150176\n",
      "epoch 4321 learning rate:  0.010231427910205971   Training loss:   15.009068008147203  Valing loss:   14.85166050150176\n",
      "Pure loss: 14.906538509818372.....Total loss: 14.906538509818372\n",
      "Pure loss: 14.694028070440663.....Total loss: 14.694028070440663\n",
      "epoch 4322 learning rate:  0.0102313743637205   Training loss:   14.906538509818372  Valing loss:   14.694028070440663\n",
      "Pure loss: 14.99093945841331.....Total loss: 14.99093945841331\n",
      "Pure loss: 14.8484115522134.....Total loss: 14.8484115522134\n",
      "epoch 4323 learning rate:  0.010231320842007865   Training loss:   14.99093945841331  Valing loss:   14.8484115522134\n",
      "Pure loss: 14.68869969386473.....Total loss: 14.68869969386473\n",
      "Pure loss: 14.342592904584414.....Total loss: 14.342592904584414\n",
      "epoch 4324 learning rate:  0.010231267345050879   Training loss:   14.68869969386473  Valing loss:   14.342592904584414\n",
      "Pure loss: 14.6080013507624.....Total loss: 14.6080013507624\n",
      "Pure loss: 14.193540039643423.....Total loss: 14.193540039643423\n",
      "epoch 4325 learning rate:  0.01023121387283237   Training loss:   14.6080013507624  Valing loss:   14.193540039643423\n",
      "Pure loss: 14.59542351499705.....Total loss: 14.59542351499705\n",
      "Pure loss: 14.163194253182857.....Total loss: 14.163194253182857\n",
      "epoch 4326 learning rate:  0.010231160425335183   Training loss:   14.59542351499705  Valing loss:   14.163194253182857\n",
      "Pure loss: 14.34812730625095.....Total loss: 14.34812730625095\n",
      "Pure loss: 13.658916668713546.....Total loss: 13.658916668713546\n",
      "epoch 4327 learning rate:  0.010231107002542178   Training loss:   14.34812730625095  Valing loss:   13.658916668713546\n",
      "Pure loss: 14.344761569621234.....Total loss: 14.344761569621234\n",
      "Pure loss: 13.630619827977418.....Total loss: 13.630619827977418\n",
      "epoch 4328 learning rate:  0.010231053604436229   Training loss:   14.344761569621234  Valing loss:   13.630619827977418\n",
      "Pure loss: 14.361961672813543.....Total loss: 14.361961672813543\n",
      "Pure loss: 13.623246765783211.....Total loss: 13.623246765783211\n",
      "epoch 4329 learning rate:  0.010231000231000232   Training loss:   14.361961672813543  Valing loss:   13.623246765783211\n",
      "Pure loss: 14.347846545860191.....Total loss: 14.347846545860191\n",
      "Pure loss: 13.473157410939576.....Total loss: 13.473157410939576\n",
      "epoch 4330 learning rate:  0.01023094688221709   Training loss:   14.347846545860191  Valing loss:   13.473157410939576\n",
      "Pure loss: 14.4291759617931.....Total loss: 14.4291759617931\n",
      "Pure loss: 13.794863071764068.....Total loss: 13.794863071764068\n",
      "epoch 4331 learning rate:  0.01023089355806973   Training loss:   14.4291759617931  Valing loss:   13.794863071764068\n",
      "Pure loss: 14.404382717782383.....Total loss: 14.404382717782383\n",
      "Pure loss: 13.725102266717983.....Total loss: 13.725102266717983\n",
      "epoch 4332 learning rate:  0.01023084025854109   Training loss:   14.404382717782383  Valing loss:   13.725102266717983\n",
      "Pure loss: 14.504352456026327.....Total loss: 14.504352456026327\n",
      "Pure loss: 13.99874845237872.....Total loss: 13.99874845237872\n",
      "epoch 4333 learning rate:  0.010230786983614125   Training loss:   14.504352456026327  Valing loss:   13.99874845237872\n",
      "Pure loss: 14.616193871324834.....Total loss: 14.616193871324834\n",
      "Pure loss: 14.224809841130684.....Total loss: 14.224809841130684\n",
      "epoch 4334 learning rate:  0.010230733733271805   Training loss:   14.616193871324834  Valing loss:   14.224809841130684\n",
      "Pure loss: 14.602086828433395.....Total loss: 14.602086828433395\n",
      "Pure loss: 14.170069461463612.....Total loss: 14.170069461463612\n",
      "epoch 4335 learning rate:  0.010230680507497116   Training loss:   14.602086828433395  Valing loss:   14.170069461463612\n",
      "Pure loss: 14.535880502607448.....Total loss: 14.535880502607448\n",
      "Pure loss: 14.049830152639222.....Total loss: 14.049830152639222\n",
      "epoch 4336 learning rate:  0.010230627306273063   Training loss:   14.535880502607448  Valing loss:   14.049830152639222\n",
      "Pure loss: 14.531181396999793.....Total loss: 14.531181396999793\n",
      "Pure loss: 14.09921260277203.....Total loss: 14.09921260277203\n",
      "epoch 4337 learning rate:  0.010230574129582662   Training loss:   14.531181396999793  Valing loss:   14.09921260277203\n",
      "Pure loss: 14.493766260262683.....Total loss: 14.493766260262683\n",
      "Pure loss: 13.969432091178.....Total loss: 13.969432091178\n",
      "epoch 4338 learning rate:  0.010230520977408944   Training loss:   14.493766260262683  Valing loss:   13.969432091178\n",
      "Pure loss: 14.547993018167954.....Total loss: 14.547993018167954\n",
      "Pure loss: 14.081984169357602.....Total loss: 14.081984169357602\n",
      "epoch 4339 learning rate:  0.010230467849734963   Training loss:   14.547993018167954  Valing loss:   14.081984169357602\n",
      "Pure loss: 14.437243453807055.....Total loss: 14.437243453807055\n",
      "Pure loss: 13.838417352366065.....Total loss: 13.838417352366065\n",
      "epoch 4340 learning rate:  0.010230414746543778   Training loss:   14.437243453807055  Valing loss:   13.838417352366065\n",
      "Pure loss: 14.644560537277968.....Total loss: 14.644560537277968\n",
      "Pure loss: 14.23094858976287.....Total loss: 14.23094858976287\n",
      "epoch 4341 learning rate:  0.010230361667818474   Training loss:   14.644560537277968  Valing loss:   14.23094858976287\n",
      "Pure loss: 14.53813780523672.....Total loss: 14.53813780523672\n",
      "Pure loss: 14.05328730684875.....Total loss: 14.05328730684875\n",
      "epoch 4342 learning rate:  0.010230308613542146   Training loss:   14.53813780523672  Valing loss:   14.05328730684875\n",
      "Pure loss: 14.438119449185178.....Total loss: 14.438119449185178\n",
      "Pure loss: 13.822233991144518.....Total loss: 13.822233991144518\n",
      "epoch 4343 learning rate:  0.010230255583697905   Training loss:   14.438119449185178  Valing loss:   13.822233991144518\n",
      "Pure loss: 14.394186361431357.....Total loss: 14.394186361431357\n",
      "Pure loss: 13.709934082485752.....Total loss: 13.709934082485752\n",
      "epoch 4344 learning rate:  0.010230202578268876   Training loss:   14.394186361431357  Valing loss:   13.709934082485752\n",
      "Pure loss: 14.380736950252937.....Total loss: 14.380736950252937\n",
      "Pure loss: 13.585350015738193.....Total loss: 13.585350015738193\n",
      "epoch 4345 learning rate:  0.010230149597238204   Training loss:   14.380736950252937  Valing loss:   13.585350015738193\n",
      "Pure loss: 14.310595968720499.....Total loss: 14.310595968720499\n",
      "Pure loss: 13.514218416445196.....Total loss: 13.514218416445196\n",
      "epoch 4346 learning rate:  0.010230096640589048   Training loss:   14.310595968720499  Valing loss:   13.514218416445196\n",
      "Pure loss: 14.311003399267323.....Total loss: 14.311003399267323\n",
      "Pure loss: 13.484961172701125.....Total loss: 13.484961172701125\n",
      "epoch 4347 learning rate:  0.010230043708304578   Training loss:   14.311003399267323  Valing loss:   13.484961172701125\n",
      "Pure loss: 14.354468533590639.....Total loss: 14.354468533590639\n",
      "Pure loss: 13.500484142338234.....Total loss: 13.500484142338234\n",
      "epoch 4348 learning rate:  0.010229990800367985   Training loss:   14.354468533590639  Valing loss:   13.500484142338234\n",
      "Pure loss: 14.35844109137421.....Total loss: 14.35844109137421\n",
      "Pure loss: 13.618460254979645.....Total loss: 13.618460254979645\n",
      "epoch 4349 learning rate:  0.010229937916762475   Training loss:   14.35844109137421  Valing loss:   13.618460254979645\n",
      "Pure loss: 14.425206925636557.....Total loss: 14.425206925636557\n",
      "Pure loss: 13.814473656049097.....Total loss: 13.814473656049097\n",
      "epoch 4350 learning rate:  0.010229885057471265   Training loss:   14.425206925636557  Valing loss:   13.814473656049097\n",
      "Pure loss: 15.027620454124266.....Total loss: 15.027620454124266\n",
      "Pure loss: 14.892068995459589.....Total loss: 14.892068995459589\n",
      "epoch 4351 learning rate:  0.010229832222477591   Training loss:   15.027620454124266  Valing loss:   14.892068995459589\n",
      "Pure loss: 14.88718026310875.....Total loss: 14.88718026310875\n",
      "Pure loss: 14.66514658126756.....Total loss: 14.66514658126756\n",
      "epoch 4352 learning rate:  0.010229779411764705   Training loss:   14.88718026310875  Valing loss:   14.66514658126756\n",
      "Pure loss: 15.077483188024681.....Total loss: 15.077483188024681\n",
      "Pure loss: 14.960667151486804.....Total loss: 14.960667151486804\n",
      "epoch 4353 learning rate:  0.010229726625315873   Training loss:   15.077483188024681  Valing loss:   14.960667151486804\n",
      "Pure loss: 15.216052315842434.....Total loss: 15.216052315842434\n",
      "Pure loss: 15.169732608153542.....Total loss: 15.169732608153542\n",
      "epoch 4354 learning rate:  0.010229673863114378   Training loss:   15.216052315842434  Valing loss:   15.169732608153542\n",
      "Pure loss: 15.59051362306269.....Total loss: 15.59051362306269\n",
      "Pure loss: 15.717118775421103.....Total loss: 15.717118775421103\n",
      "epoch 4355 learning rate:  0.010229621125143513   Training loss:   15.59051362306269  Valing loss:   15.717118775421103\n",
      "Pure loss: 15.257942878225432.....Total loss: 15.257942878225432\n",
      "Pure loss: 15.232532577694593.....Total loss: 15.232532577694593\n",
      "epoch 4356 learning rate:  0.010229568411386593   Training loss:   15.257942878225432  Valing loss:   15.232532577694593\n",
      "Pure loss: 14.964460424417894.....Total loss: 14.964460424417894\n",
      "Pure loss: 14.783295614721895.....Total loss: 14.783295614721895\n",
      "epoch 4357 learning rate:  0.010229515721826945   Training loss:   14.964460424417894  Valing loss:   14.783295614721895\n",
      "Pure loss: 14.919173916889141.....Total loss: 14.919173916889141\n",
      "Pure loss: 14.707836871937856.....Total loss: 14.707836871937856\n",
      "epoch 4358 learning rate:  0.010229463056447912   Training loss:   14.919173916889141  Valing loss:   14.707836871937856\n",
      "Pure loss: 14.849646767990851.....Total loss: 14.849646767990851\n",
      "Pure loss: 14.596022001807924.....Total loss: 14.596022001807924\n",
      "epoch 4359 learning rate:  0.010229410415232851   Training loss:   14.849646767990851  Valing loss:   14.596022001807924\n",
      "Pure loss: 15.073606135402661.....Total loss: 15.073606135402661\n",
      "Pure loss: 14.948823651081335.....Total loss: 14.948823651081335\n",
      "epoch 4360 learning rate:  0.010229357798165138   Training loss:   15.073606135402661  Valing loss:   14.948823651081335\n",
      "Pure loss: 15.111840691423527.....Total loss: 15.111840691423527\n",
      "Pure loss: 15.027693383634226.....Total loss: 15.027693383634226\n",
      "epoch 4361 learning rate:  0.010229305205228158   Training loss:   15.111840691423527  Valing loss:   15.027693383634226\n",
      "Pure loss: 14.902331974701319.....Total loss: 14.902331974701319\n",
      "Pure loss: 14.698545398943965.....Total loss: 14.698545398943965\n",
      "epoch 4362 learning rate:  0.010229252636405319   Training loss:   14.902331974701319  Valing loss:   14.698545398943965\n",
      "Pure loss: 14.659904254036746.....Total loss: 14.659904254036746\n",
      "Pure loss: 14.23746681439655.....Total loss: 14.23746681439655\n",
      "epoch 4363 learning rate:  0.010229200091680038   Training loss:   14.659904254036746  Valing loss:   14.23746681439655\n",
      "Pure loss: 14.942317970275997.....Total loss: 14.942317970275997\n",
      "Pure loss: 14.718835366309971.....Total loss: 14.718835366309971\n",
      "epoch 4364 learning rate:  0.010229147571035747   Training loss:   14.942317970275997  Valing loss:   14.718835366309971\n",
      "Pure loss: 14.965968608330677.....Total loss: 14.965968608330677\n",
      "Pure loss: 14.758484105756716.....Total loss: 14.758484105756716\n",
      "epoch 4365 learning rate:  0.010229095074455899   Training loss:   14.965968608330677  Valing loss:   14.758484105756716\n",
      "Pure loss: 14.95835276476167.....Total loss: 14.95835276476167\n",
      "Pure loss: 14.72867582552694.....Total loss: 14.72867582552694\n",
      "epoch 4366 learning rate:  0.010229042601923958   Training loss:   14.95835276476167  Valing loss:   14.72867582552694\n",
      "Pure loss: 15.317438616651216.....Total loss: 15.317438616651216\n",
      "Pure loss: 15.305475649761096.....Total loss: 15.305475649761096\n",
      "epoch 4367 learning rate:  0.010228990153423403   Training loss:   15.317438616651216  Valing loss:   15.305475649761096\n",
      "Pure loss: 15.152378280748113.....Total loss: 15.152378280748113\n",
      "Pure loss: 15.065463739113094.....Total loss: 15.065463739113094\n",
      "epoch 4368 learning rate:  0.010228937728937728   Training loss:   15.152378280748113  Valing loss:   15.065463739113094\n",
      "Pure loss: 14.92441483626757.....Total loss: 14.92441483626757\n",
      "Pure loss: 14.696236694191324.....Total loss: 14.696236694191324\n",
      "epoch 4369 learning rate:  0.010228885328450446   Training loss:   14.92441483626757  Valing loss:   14.696236694191324\n",
      "Pure loss: 14.701802629740694.....Total loss: 14.701802629740694\n",
      "Pure loss: 14.321109908856494.....Total loss: 14.321109908856494\n",
      "epoch 4370 learning rate:  0.01022883295194508   Training loss:   14.701802629740694  Valing loss:   14.321109908856494\n",
      "Pure loss: 14.715416288267006.....Total loss: 14.715416288267006\n",
      "Pure loss: 14.37698432899223.....Total loss: 14.37698432899223\n",
      "epoch 4371 learning rate:  0.010228780599405171   Training loss:   14.715416288267006  Valing loss:   14.37698432899223\n",
      "Pure loss: 15.112971113234662.....Total loss: 15.112971113234662\n",
      "Pure loss: 15.046343715698272.....Total loss: 15.046343715698272\n",
      "epoch 4372 learning rate:  0.010228728270814273   Training loss:   15.112971113234662  Valing loss:   15.046343715698272\n",
      "Pure loss: 15.234673697784569.....Total loss: 15.234673697784569\n",
      "Pure loss: 15.23225951224902.....Total loss: 15.23225951224902\n",
      "epoch 4373 learning rate:  0.010228675966155957   Training loss:   15.234673697784569  Valing loss:   15.23225951224902\n",
      "Pure loss: 15.224845967222763.....Total loss: 15.224845967222763\n",
      "Pure loss: 15.239051445297578.....Total loss: 15.239051445297578\n",
      "epoch 4374 learning rate:  0.010228623685413808   Training loss:   15.224845967222763  Valing loss:   15.239051445297578\n",
      "Pure loss: 14.864400033439763.....Total loss: 14.864400033439763\n",
      "Pure loss: 14.686567969301011.....Total loss: 14.686567969301011\n",
      "epoch 4375 learning rate:  0.010228571428571429   Training loss:   14.864400033439763  Valing loss:   14.686567969301011\n",
      "Pure loss: 14.947813210645007.....Total loss: 14.947813210645007\n",
      "Pure loss: 14.86608598106058.....Total loss: 14.86608598106058\n",
      "epoch 4376 learning rate:  0.010228519195612431   Training loss:   14.947813210645007  Valing loss:   14.86608598106058\n",
      "Pure loss: 15.10503490646952.....Total loss: 15.10503490646952\n",
      "Pure loss: 15.136149785785594.....Total loss: 15.136149785785594\n",
      "epoch 4377 learning rate:  0.010228466986520447   Training loss:   15.10503490646952  Valing loss:   15.136149785785594\n",
      "Pure loss: 14.744386758135189.....Total loss: 14.744386758135189\n",
      "Pure loss: 14.578793030276932.....Total loss: 14.578793030276932\n",
      "epoch 4378 learning rate:  0.010228414801279123   Training loss:   14.744386758135189  Valing loss:   14.578793030276932\n",
      "Pure loss: 14.763749325361621.....Total loss: 14.763749325361621\n",
      "Pure loss: 14.62910335182479.....Total loss: 14.62910335182479\n",
      "epoch 4379 learning rate:  0.010228362639872117   Training loss:   14.763749325361621  Valing loss:   14.62910335182479\n",
      "Pure loss: 14.482391541111548.....Total loss: 14.482391541111548\n",
      "Pure loss: 14.160286158564999.....Total loss: 14.160286158564999\n",
      "epoch 4380 learning rate:  0.010228310502283105   Training loss:   14.482391541111548  Valing loss:   14.160286158564999\n",
      "Pure loss: 14.895003725013485.....Total loss: 14.895003725013485\n",
      "Pure loss: 14.847036876585108.....Total loss: 14.847036876585108\n",
      "epoch 4381 learning rate:  0.010228258388495778   Training loss:   14.895003725013485  Valing loss:   14.847036876585108\n",
      "Pure loss: 14.692764145012392.....Total loss: 14.692764145012392\n",
      "Pure loss: 14.52702229292465.....Total loss: 14.52702229292465\n",
      "epoch 4382 learning rate:  0.010228206298493839   Training loss:   14.692764145012392  Valing loss:   14.52702229292465\n",
      "Pure loss: 14.711633063860523.....Total loss: 14.711633063860523\n",
      "Pure loss: 14.550043254325164.....Total loss: 14.550043254325164\n",
      "epoch 4383 learning rate:  0.010228154232261009   Training loss:   14.711633063860523  Valing loss:   14.550043254325164\n",
      "Pure loss: 14.810350484963623.....Total loss: 14.810350484963623\n",
      "Pure loss: 14.699388873668212.....Total loss: 14.699388873668212\n",
      "epoch 4384 learning rate:  0.010228102189781022   Training loss:   14.810350484963623  Valing loss:   14.699388873668212\n",
      "Pure loss: 14.742768308498007.....Total loss: 14.742768308498007\n",
      "Pure loss: 14.599427292022959.....Total loss: 14.599427292022959\n",
      "epoch 4385 learning rate:  0.01022805017103763   Training loss:   14.742768308498007  Valing loss:   14.599427292022959\n",
      "Pure loss: 14.743260686315075.....Total loss: 14.743260686315075\n",
      "Pure loss: 14.600099540310353.....Total loss: 14.600099540310353\n",
      "epoch 4386 learning rate:  0.010227998176014592   Training loss:   14.743260686315075  Valing loss:   14.600099540310353\n",
      "Pure loss: 14.853485030092294.....Total loss: 14.853485030092294\n",
      "Pure loss: 14.745837815673733.....Total loss: 14.745837815673733\n",
      "epoch 4387 learning rate:  0.010227946204695693   Training loss:   14.853485030092294  Valing loss:   14.745837815673733\n",
      "Pure loss: 14.462865794607348.....Total loss: 14.462865794607348\n",
      "Pure loss: 14.132506850364498.....Total loss: 14.132506850364498\n",
      "epoch 4388 learning rate:  0.010227894257064721   Training loss:   14.462865794607348  Valing loss:   14.132506850364498\n",
      "Pure loss: 14.239175470895374.....Total loss: 14.239175470895374\n",
      "Pure loss: 13.7216582083026.....Total loss: 13.7216582083026\n",
      "epoch 4389 learning rate:  0.010227842333105492   Training loss:   14.239175470895374  Valing loss:   13.7216582083026\n",
      "Pure loss: 14.275628825266983.....Total loss: 14.275628825266983\n",
      "Pure loss: 13.793056704335529.....Total loss: 13.793056704335529\n",
      "epoch 4390 learning rate:  0.010227790432801823   Training loss:   14.275628825266983  Valing loss:   13.793056704335529\n",
      "Pure loss: 14.195809845022413.....Total loss: 14.195809845022413\n",
      "Pure loss: 13.651631517806273.....Total loss: 13.651631517806273\n",
      "epoch 4391 learning rate:  0.010227738556137555   Training loss:   14.195809845022413  Valing loss:   13.651631517806273\n",
      "Pure loss: 14.181245648058988.....Total loss: 14.181245648058988\n",
      "Pure loss: 13.621014695650414.....Total loss: 13.621014695650414\n",
      "epoch 4392 learning rate:  0.010227686703096539   Training loss:   14.181245648058988  Valing loss:   13.621014695650414\n",
      "Pure loss: 14.295324845229558.....Total loss: 14.295324845229558\n",
      "Pure loss: 13.840159492922355.....Total loss: 13.840159492922355\n",
      "epoch 4393 learning rate:  0.010227634873662645   Training loss:   14.295324845229558  Valing loss:   13.840159492922355\n",
      "Pure loss: 14.356764953039903.....Total loss: 14.356764953039903\n",
      "Pure loss: 13.909689270733494.....Total loss: 13.909689270733494\n",
      "epoch 4394 learning rate:  0.010227583067819755   Training loss:   14.356764953039903  Valing loss:   13.909689270733494\n",
      "Pure loss: 14.583514291378835.....Total loss: 14.583514291378835\n",
      "Pure loss: 14.302305689597638.....Total loss: 14.302305689597638\n",
      "epoch 4395 learning rate:  0.010227531285551764   Training loss:   14.583514291378835  Valing loss:   14.302305689597638\n",
      "Pure loss: 14.63822487435986.....Total loss: 14.63822487435986\n",
      "Pure loss: 14.377806114557343.....Total loss: 14.377806114557343\n",
      "epoch 4396 learning rate:  0.010227479526842585   Training loss:   14.63822487435986  Valing loss:   14.377806114557343\n",
      "Pure loss: 14.17583600427266.....Total loss: 14.17583600427266\n",
      "Pure loss: 13.525895542267028.....Total loss: 13.525895542267028\n",
      "epoch 4397 learning rate:  0.010227427791676144   Training loss:   14.17583600427266  Valing loss:   13.525895542267028\n",
      "Pure loss: 13.99287636399082.....Total loss: 13.99287636399082\n",
      "Pure loss: 13.041360873652916.....Total loss: 13.041360873652916\n",
      "epoch 4398 learning rate:  0.01022737608003638   Training loss:   13.99287636399082  Valing loss:   13.041360873652916\n",
      "Pure loss: 13.977233948461274.....Total loss: 13.977233948461274\n",
      "Pure loss: 12.986471218156.....Total loss: 12.986471218156\n",
      "epoch 4399 learning rate:  0.010227324391907253   Training loss:   13.977233948461274  Valing loss:   12.986471218156\n",
      "Pure loss: 14.096990231924874.....Total loss: 14.096990231924874\n",
      "Pure loss: 13.373735528752281.....Total loss: 13.373735528752281\n",
      "epoch 4400 learning rate:  0.010227272727272727   Training loss:   14.096990231924874  Valing loss:   13.373735528752281\n",
      "Pure loss: 13.995438263522365.....Total loss: 13.995438263522365\n",
      "Pure loss: 13.057996870683965.....Total loss: 13.057996870683965\n",
      "epoch 4401 learning rate:  0.010227221086116792   Training loss:   13.995438263522365  Valing loss:   13.057996870683965\n",
      "Pure loss: 14.019533013472973.....Total loss: 14.019533013472973\n",
      "Pure loss: 13.15643303837854.....Total loss: 13.15643303837854\n",
      "epoch 4402 learning rate:  0.010227169468423444   Training loss:   14.019533013472973  Valing loss:   13.15643303837854\n",
      "Pure loss: 14.1068191649848.....Total loss: 14.1068191649848\n",
      "Pure loss: 13.339056894833526.....Total loss: 13.339056894833526\n",
      "epoch 4403 learning rate:  0.010227117874176697   Training loss:   14.1068191649848  Valing loss:   13.339056894833526\n",
      "Pure loss: 14.254738823557433.....Total loss: 14.254738823557433\n",
      "Pure loss: 13.661390346280431.....Total loss: 13.661390346280431\n",
      "epoch 4404 learning rate:  0.010227066303360581   Training loss:   14.254738823557433  Valing loss:   13.661390346280431\n",
      "Pure loss: 14.30184577900441.....Total loss: 14.30184577900441\n",
      "Pure loss: 13.74873491956692.....Total loss: 13.74873491956692\n",
      "epoch 4405 learning rate:  0.010227014755959137   Training loss:   14.30184577900441  Valing loss:   13.74873491956692\n",
      "Pure loss: 14.13322499397103.....Total loss: 14.13322499397103\n",
      "Pure loss: 13.471028587834411.....Total loss: 13.471028587834411\n",
      "epoch 4406 learning rate:  0.010226963231956423   Training loss:   14.13322499397103  Valing loss:   13.471028587834411\n",
      "Pure loss: 14.126101248637461.....Total loss: 14.126101248637461\n",
      "Pure loss: 13.45697664873624.....Total loss: 13.45697664873624\n",
      "epoch 4407 learning rate:  0.01022691173133651   Training loss:   14.126101248637461  Valing loss:   13.45697664873624\n",
      "Pure loss: 14.030940043368409.....Total loss: 14.030940043368409\n",
      "Pure loss: 13.251668169582494.....Total loss: 13.251668169582494\n",
      "epoch 4408 learning rate:  0.010226860254083484   Training loss:   14.030940043368409  Valing loss:   13.251668169582494\n",
      "Pure loss: 14.016112534621874.....Total loss: 14.016112534621874\n",
      "Pure loss: 13.229259222668569.....Total loss: 13.229259222668569\n",
      "epoch 4409 learning rate:  0.010226808800181448   Training loss:   14.016112534621874  Valing loss:   13.229259222668569\n",
      "Pure loss: 13.946713584905643.....Total loss: 13.946713584905643\n",
      "Pure loss: 12.937342415654259.....Total loss: 12.937342415654259\n",
      "epoch 4410 learning rate:  0.010226757369614512   Training loss:   13.946713584905643  Valing loss:   12.937342415654259\n",
      "Pure loss: 13.952173154563651.....Total loss: 13.952173154563651\n",
      "Pure loss: 12.9597129559206.....Total loss: 12.9597129559206\n",
      "epoch 4411 learning rate:  0.01022670596236681   Training loss:   13.952173154563651  Valing loss:   12.9597129559206\n",
      "Pure loss: 13.980037983610059.....Total loss: 13.980037983610059\n",
      "Pure loss: 13.053651113411592.....Total loss: 13.053651113411592\n",
      "epoch 4412 learning rate:  0.010226654578422485   Training loss:   13.980037983610059  Valing loss:   13.053651113411592\n",
      "Pure loss: 14.067532060906247.....Total loss: 14.067532060906247\n",
      "Pure loss: 13.239834054205245.....Total loss: 13.239834054205245\n",
      "epoch 4413 learning rate:  0.010226603217765692   Training loss:   14.067532060906247  Valing loss:   13.239834054205245\n",
      "Pure loss: 13.999383853507869.....Total loss: 13.999383853507869\n",
      "Pure loss: 13.013582395138135.....Total loss: 13.013582395138135\n",
      "epoch 4414 learning rate:  0.010226551880380607   Training loss:   13.999383853507869  Valing loss:   13.013582395138135\n",
      "Pure loss: 13.980566768152745.....Total loss: 13.980566768152745\n",
      "Pure loss: 12.790994269062729.....Total loss: 12.790994269062729\n",
      "epoch 4415 learning rate:  0.010226500566251415   Training loss:   13.980566768152745  Valing loss:   12.790994269062729\n",
      "Pure loss: 14.020268968729573.....Total loss: 14.020268968729573\n",
      "Pure loss: 12.605137365441058.....Total loss: 12.605137365441058\n",
      "epoch 4416 learning rate:  0.010226449275362318   Training loss:   14.020268968729573  Valing loss:   12.605137365441058\n",
      "Pure loss: 14.053199827771703.....Total loss: 14.053199827771703\n",
      "Pure loss: 12.563481438665418.....Total loss: 12.563481438665418\n",
      "epoch 4417 learning rate:  0.010226398007697533   Training loss:   14.053199827771703  Valing loss:   12.563481438665418\n",
      "Pure loss: 14.012996051017685.....Total loss: 14.012996051017685\n",
      "Pure loss: 12.613723090353304.....Total loss: 12.613723090353304\n",
      "epoch 4418 learning rate:  0.010226346763241285   Training loss:   14.012996051017685  Valing loss:   12.613723090353304\n",
      "Pure loss: 14.17485395004793.....Total loss: 14.17485395004793\n",
      "Pure loss: 13.638160392715063.....Total loss: 13.638160392715063\n",
      "epoch 4419 learning rate:  0.010226295541977824   Training loss:   14.17485395004793  Valing loss:   13.638160392715063\n",
      "Pure loss: 13.98251092993762.....Total loss: 13.98251092993762\n",
      "Pure loss: 13.162069668347472.....Total loss: 13.162069668347472\n",
      "epoch 4420 learning rate:  0.010226244343891402   Training loss:   13.98251092993762  Valing loss:   13.162069668347472\n",
      "Pure loss: 13.96030750593027.....Total loss: 13.96030750593027\n",
      "Pure loss: 13.065244569190613.....Total loss: 13.065244569190613\n",
      "epoch 4421 learning rate:  0.010226193168966297   Training loss:   13.96030750593027  Valing loss:   13.065244569190613\n",
      "Pure loss: 14.012838266097967.....Total loss: 14.012838266097967\n",
      "Pure loss: 13.252601877303146.....Total loss: 13.252601877303146\n",
      "epoch 4422 learning rate:  0.010226142017186794   Training loss:   14.012838266097967  Valing loss:   13.252601877303146\n",
      "Pure loss: 13.999258294154798.....Total loss: 13.999258294154798\n",
      "Pure loss: 13.213738683644568.....Total loss: 13.213738683644568\n",
      "epoch 4423 learning rate:  0.010226090888537192   Training loss:   13.999258294154798  Valing loss:   13.213738683644568\n",
      "Pure loss: 13.971369272045118.....Total loss: 13.971369272045118\n",
      "Pure loss: 13.09820075399231.....Total loss: 13.09820075399231\n",
      "epoch 4424 learning rate:  0.010226039783001808   Training loss:   13.971369272045118  Valing loss:   13.09820075399231\n",
      "Pure loss: 13.949657487910011.....Total loss: 13.949657487910011\n",
      "Pure loss: 12.932534703478575.....Total loss: 12.932534703478575\n",
      "epoch 4425 learning rate:  0.010225988700564972   Training loss:   13.949657487910011  Valing loss:   12.932534703478575\n",
      "Pure loss: 13.952382495457245.....Total loss: 13.952382495457245\n",
      "Pure loss: 12.886016571433522.....Total loss: 12.886016571433522\n",
      "epoch 4426 learning rate:  0.010225937641211026   Training loss:   13.952382495457245  Valing loss:   12.886016571433522\n",
      "Pure loss: 14.019883436032647.....Total loss: 14.019883436032647\n",
      "Pure loss: 12.799694982874977.....Total loss: 12.799694982874977\n",
      "epoch 4427 learning rate:  0.010225886604924328   Training loss:   14.019883436032647  Valing loss:   12.799694982874977\n",
      "Pure loss: 14.121841797912502.....Total loss: 14.121841797912502\n",
      "Pure loss: 12.7881112193189.....Total loss: 12.7881112193189\n",
      "epoch 4428 learning rate:  0.01022583559168925   Training loss:   14.121841797912502  Valing loss:   12.7881112193189\n",
      "Pure loss: 14.148019975568706.....Total loss: 14.148019975568706\n",
      "Pure loss: 12.767578290902437.....Total loss: 12.767578290902437\n",
      "epoch 4429 learning rate:  0.010225784601490179   Training loss:   14.148019975568706  Valing loss:   12.767578290902437\n",
      "Pure loss: 14.153410743791582.....Total loss: 14.153410743791582\n",
      "Pure loss: 12.765752017807213.....Total loss: 12.765752017807213\n",
      "epoch 4430 learning rate:  0.010225733634311512   Training loss:   14.153410743791582  Valing loss:   12.765752017807213\n",
      "Pure loss: 14.08832381610081.....Total loss: 14.08832381610081\n",
      "Pure loss: 12.735272043338556.....Total loss: 12.735272043338556\n",
      "epoch 4431 learning rate:  0.010225682690137667   Training loss:   14.08832381610081  Valing loss:   12.735272043338556\n",
      "Pure loss: 13.958660056068293.....Total loss: 13.958660056068293\n",
      "Pure loss: 12.835137826040588.....Total loss: 12.835137826040588\n",
      "epoch 4432 learning rate:  0.010225631768953069   Training loss:   13.958660056068293  Valing loss:   12.835137826040588\n",
      "Pure loss: 14.01066746264037.....Total loss: 14.01066746264037\n",
      "Pure loss: 12.80618557244127.....Total loss: 12.80618557244127\n",
      "epoch 4433 learning rate:  0.01022558087074216   Training loss:   14.01066746264037  Valing loss:   12.80618557244127\n",
      "Pure loss: 14.028881103633845.....Total loss: 14.028881103633845\n",
      "Pure loss: 12.824682739522183.....Total loss: 12.824682739522183\n",
      "epoch 4434 learning rate:  0.0102255299954894   Training loss:   14.028881103633845  Valing loss:   12.824682739522183\n",
      "Pure loss: 13.978931647645059.....Total loss: 13.978931647645059\n",
      "Pure loss: 13.02005704590001.....Total loss: 13.02005704590001\n",
      "epoch 4435 learning rate:  0.010225479143179256   Training loss:   13.978931647645059  Valing loss:   13.02005704590001\n",
      "Pure loss: 14.071431321228777.....Total loss: 14.071431321228777\n",
      "Pure loss: 13.452584170708299.....Total loss: 13.452584170708299\n",
      "epoch 4436 learning rate:  0.010225428313796213   Training loss:   14.071431321228777  Valing loss:   13.452584170708299\n",
      "Pure loss: 14.01581878426004.....Total loss: 14.01581878426004\n",
      "Pure loss: 13.304608758824191.....Total loss: 13.304608758824191\n",
      "epoch 4437 learning rate:  0.01022537750732477   Training loss:   14.01581878426004  Valing loss:   13.304608758824191\n",
      "Pure loss: 13.959703043241857.....Total loss: 13.959703043241857\n",
      "Pure loss: 13.103646799507967.....Total loss: 13.103646799507967\n",
      "epoch 4438 learning rate:  0.010225326723749437   Training loss:   13.959703043241857  Valing loss:   13.103646799507967\n",
      "Pure loss: 14.01543832185347.....Total loss: 14.01543832185347\n",
      "Pure loss: 13.24434581152447.....Total loss: 13.24434581152447\n",
      "epoch 4439 learning rate:  0.010225275963054743   Training loss:   14.01543832185347  Valing loss:   13.24434581152447\n",
      "Pure loss: 14.01023241443138.....Total loss: 14.01023241443138\n",
      "Pure loss: 12.96581954986083.....Total loss: 12.96581954986083\n",
      "epoch 4440 learning rate:  0.010225225225225225   Training loss:   14.01023241443138  Valing loss:   12.96581954986083\n",
      "Pure loss: 13.996997160687373.....Total loss: 13.996997160687373\n",
      "Pure loss: 12.886851128594515.....Total loss: 12.886851128594515\n",
      "epoch 4441 learning rate:  0.01022517451024544   Training loss:   13.996997160687373  Valing loss:   12.886851128594515\n",
      "Pure loss: 13.999766717789841.....Total loss: 13.999766717789841\n",
      "Pure loss: 12.914568720715092.....Total loss: 12.914568720715092\n",
      "epoch 4442 learning rate:  0.010225123818099955   Training loss:   13.999766717789841  Valing loss:   12.914568720715092\n",
      "Pure loss: 14.069982666318422.....Total loss: 14.069982666318422\n",
      "Pure loss: 12.811133672140617.....Total loss: 12.811133672140617\n",
      "epoch 4443 learning rate:  0.01022507314877335   Training loss:   14.069982666318422  Valing loss:   12.811133672140617\n",
      "Pure loss: 14.139417283694577.....Total loss: 14.139417283694577\n",
      "Pure loss: 12.769777973032854.....Total loss: 12.769777973032854\n",
      "epoch 4444 learning rate:  0.010225022502250226   Training loss:   14.139417283694577  Valing loss:   12.769777973032854\n",
      "Pure loss: 14.266985828358843.....Total loss: 14.266985828358843\n",
      "Pure loss: 12.825045346704243.....Total loss: 12.825045346704243\n",
      "epoch 4445 learning rate:  0.010224971878515185   Training loss:   14.266985828358843  Valing loss:   12.825045346704243\n",
      "Pure loss: 14.405340725108942.....Total loss: 14.405340725108942\n",
      "Pure loss: 12.922542204873913.....Total loss: 12.922542204873913\n",
      "epoch 4446 learning rate:  0.010224921277552857   Training loss:   14.405340725108942  Valing loss:   12.922542204873913\n",
      "Pure loss: 14.419367477673003.....Total loss: 14.419367477673003\n",
      "Pure loss: 12.92386020965476.....Total loss: 12.92386020965476\n",
      "epoch 4447 learning rate:  0.010224870699347875   Training loss:   14.419367477673003  Valing loss:   12.92386020965476\n",
      "Pure loss: 14.340613753430134.....Total loss: 14.340613753430134\n",
      "Pure loss: 12.917326751528085.....Total loss: 12.917326751528085\n",
      "epoch 4448 learning rate:  0.010224820143884893   Training loss:   14.340613753430134  Valing loss:   12.917326751528085\n",
      "Pure loss: 14.355418001921512.....Total loss: 14.355418001921512\n",
      "Pure loss: 12.917662903239203.....Total loss: 12.917662903239203\n",
      "epoch 4449 learning rate:  0.010224769611148573   Training loss:   14.355418001921512  Valing loss:   12.917662903239203\n",
      "Pure loss: 14.273823780440736.....Total loss: 14.273823780440736\n",
      "Pure loss: 12.914235881177625.....Total loss: 12.914235881177625\n",
      "epoch 4450 learning rate:  0.010224719101123596   Training loss:   14.273823780440736  Valing loss:   12.914235881177625\n",
      "Pure loss: 14.17526780781505.....Total loss: 14.17526780781505\n",
      "Pure loss: 12.824901269245917.....Total loss: 12.824901269245917\n",
      "epoch 4451 learning rate:  0.010224668613794653   Training loss:   14.17526780781505  Valing loss:   12.824901269245917\n",
      "Pure loss: 13.967508609524918.....Total loss: 13.967508609524918\n",
      "Pure loss: 12.854841124563432.....Total loss: 12.854841124563432\n",
      "epoch 4452 learning rate:  0.010224618149146452   Training loss:   13.967508609524918  Valing loss:   12.854841124563432\n",
      "Pure loss: 13.922898665184228.....Total loss: 13.922898665184228\n",
      "Pure loss: 12.895347093542904.....Total loss: 12.895347093542904\n",
      "epoch 4453 learning rate:  0.01022456770716371   Training loss:   13.922898665184228  Valing loss:   12.895347093542904\n",
      "Pure loss: 13.998104415166354.....Total loss: 13.998104415166354\n",
      "Pure loss: 12.854919175806831.....Total loss: 12.854919175806831\n",
      "epoch 4454 learning rate:  0.010224517287831162   Training loss:   13.998104415166354  Valing loss:   12.854919175806831\n",
      "Pure loss: 14.107372383375774.....Total loss: 14.107372383375774\n",
      "Pure loss: 12.836928460172215.....Total loss: 12.836928460172215\n",
      "epoch 4455 learning rate:  0.010224466891133557   Training loss:   14.107372383375774  Valing loss:   12.836928460172215\n",
      "Pure loss: 13.991174470965966.....Total loss: 13.991174470965966\n",
      "Pure loss: 12.949677656090753.....Total loss: 12.949677656090753\n",
      "epoch 4456 learning rate:  0.010224416517055655   Training loss:   13.991174470965966  Valing loss:   12.949677656090753\n",
      "Pure loss: 14.077076927731598.....Total loss: 14.077076927731598\n",
      "Pure loss: 12.858948203285301.....Total loss: 12.858948203285301\n",
      "epoch 4457 learning rate:  0.01022436616558223   Training loss:   14.077076927731598  Valing loss:   12.858948203285301\n",
      "Pure loss: 14.04428474037327.....Total loss: 14.04428474037327\n",
      "Pure loss: 12.875940259192948.....Total loss: 12.875940259192948\n",
      "epoch 4458 learning rate:  0.010224315836698071   Training loss:   14.04428474037327  Valing loss:   12.875940259192948\n",
      "Pure loss: 14.147845646703473.....Total loss: 14.147845646703473\n",
      "Pure loss: 12.92451514502393.....Total loss: 12.92451514502393\n",
      "epoch 4459 learning rate:  0.01022426553038798   Training loss:   14.147845646703473  Valing loss:   12.92451514502393\n",
      "Pure loss: 13.972871223903038.....Total loss: 13.972871223903038\n",
      "Pure loss: 12.991203362745907.....Total loss: 12.991203362745907\n",
      "epoch 4460 learning rate:  0.010224215246636772   Training loss:   13.972871223903038  Valing loss:   12.991203362745907\n",
      "Pure loss: 13.970660929976525.....Total loss: 13.970660929976525\n",
      "Pure loss: 12.999172980109844.....Total loss: 12.999172980109844\n",
      "epoch 4461 learning rate:  0.010224164985429276   Training loss:   13.970660929976525  Valing loss:   12.999172980109844\n",
      "Pure loss: 13.947665598033456.....Total loss: 13.947665598033456\n",
      "Pure loss: 13.02732446001844.....Total loss: 13.02732446001844\n",
      "epoch 4462 learning rate:  0.010224114746750336   Training loss:   13.947665598033456  Valing loss:   13.02732446001844\n",
      "Pure loss: 14.001259142432689.....Total loss: 14.001259142432689\n",
      "Pure loss: 13.042117659067678.....Total loss: 13.042117659067678\n",
      "epoch 4463 learning rate:  0.010224064530584808   Training loss:   14.001259142432689  Valing loss:   13.042117659067678\n",
      "Pure loss: 14.177843435619812.....Total loss: 14.177843435619812\n",
      "Pure loss: 13.147649536662719.....Total loss: 13.147649536662719\n",
      "epoch 4464 learning rate:  0.010224014336917563   Training loss:   14.177843435619812  Valing loss:   13.147649536662719\n",
      "Pure loss: 14.00452021964888.....Total loss: 14.00452021964888\n",
      "Pure loss: 13.080493930531613.....Total loss: 13.080493930531613\n",
      "epoch 4465 learning rate:  0.010223964165733483   Training loss:   14.00452021964888  Valing loss:   13.080493930531613\n",
      "Pure loss: 14.0058093796002.....Total loss: 14.0058093796002\n",
      "Pure loss: 13.079705822772102.....Total loss: 13.079705822772102\n",
      "epoch 4466 learning rate:  0.010223914017017466   Training loss:   14.0058093796002  Valing loss:   13.079705822772102\n",
      "Pure loss: 14.243846845674248.....Total loss: 14.243846845674248\n",
      "Pure loss: 13.050652578586218.....Total loss: 13.050652578586218\n",
      "epoch 4467 learning rate:  0.010223863890754421   Training loss:   14.243846845674248  Valing loss:   13.050652578586218\n",
      "Pure loss: 14.55890019420221.....Total loss: 14.55890019420221\n",
      "Pure loss: 13.017024937001633.....Total loss: 13.017024937001633\n",
      "epoch 4468 learning rate:  0.010223813786929275   Training loss:   14.55890019420221  Valing loss:   13.017024937001633\n",
      "Pure loss: 14.841620074180295.....Total loss: 14.841620074180295\n",
      "Pure loss: 13.161259667520971.....Total loss: 13.161259667520971\n",
      "epoch 4469 learning rate:  0.010223763705526964   Training loss:   14.841620074180295  Valing loss:   13.161259667520971\n",
      "Pure loss: 14.949750683050565.....Total loss: 14.949750683050565\n",
      "Pure loss: 13.248451022467057.....Total loss: 13.248451022467057\n",
      "epoch 4470 learning rate:  0.01022371364653244   Training loss:   14.949750683050565  Valing loss:   13.248451022467057\n",
      "Pure loss: 15.092099827376973.....Total loss: 15.092099827376973\n",
      "Pure loss: 13.314606089684087.....Total loss: 13.314606089684087\n",
      "epoch 4471 learning rate:  0.010223663609930664   Training loss:   15.092099827376973  Valing loss:   13.314606089684087\n",
      "Pure loss: 15.12105191423476.....Total loss: 15.12105191423476\n",
      "Pure loss: 13.33730599848264.....Total loss: 13.33730599848264\n",
      "epoch 4472 learning rate:  0.010223613595706619   Training loss:   15.12105191423476  Valing loss:   13.33730599848264\n",
      "Pure loss: 14.75232597137229.....Total loss: 14.75232597137229\n",
      "Pure loss: 13.229157434514146.....Total loss: 13.229157434514146\n",
      "epoch 4473 learning rate:  0.010223563603845295   Training loss:   14.75232597137229  Valing loss:   13.229157434514146\n",
      "Pure loss: 14.393705277436974.....Total loss: 14.393705277436974\n",
      "Pure loss: 12.937314946511497.....Total loss: 12.937314946511497\n",
      "epoch 4474 learning rate:  0.010223513634331695   Training loss:   14.393705277436974  Valing loss:   12.937314946511497\n",
      "Pure loss: 13.86908667072235.....Total loss: 13.86908667072235\n",
      "Pure loss: 12.915584392297657.....Total loss: 12.915584392297657\n",
      "epoch 4475 learning rate:  0.010223463687150838   Training loss:   13.86908667072235  Valing loss:   12.915584392297657\n",
      "Pure loss: 13.89963086835705.....Total loss: 13.89963086835705\n",
      "Pure loss: 12.873661169376971.....Total loss: 12.873661169376971\n",
      "epoch 4476 learning rate:  0.010223413762287758   Training loss:   13.89963086835705  Valing loss:   12.873661169376971\n",
      "Pure loss: 13.943088971745015.....Total loss: 13.943088971745015\n",
      "Pure loss: 12.766814658848812.....Total loss: 12.766814658848812\n",
      "epoch 4477 learning rate:  0.010223363859727497   Training loss:   13.943088971745015  Valing loss:   12.766814658848812\n",
      "Pure loss: 14.060514305626745.....Total loss: 14.060514305626745\n",
      "Pure loss: 12.707370278732563.....Total loss: 12.707370278732563\n",
      "epoch 4478 learning rate:  0.010223313979455114   Training loss:   14.060514305626745  Valing loss:   12.707370278732563\n",
      "Pure loss: 14.083107824374702.....Total loss: 14.083107824374702\n",
      "Pure loss: 12.708747081047283.....Total loss: 12.708747081047283\n",
      "epoch 4479 learning rate:  0.010223264121455682   Training loss:   14.083107824374702  Valing loss:   12.708747081047283\n",
      "Pure loss: 13.976454828907615.....Total loss: 13.976454828907615\n",
      "Pure loss: 12.699673116998282.....Total loss: 12.699673116998282\n",
      "epoch 4480 learning rate:  0.010223214285714285   Training loss:   13.976454828907615  Valing loss:   12.699673116998282\n",
      "Pure loss: 13.926610261350024.....Total loss: 13.926610261350024\n",
      "Pure loss: 12.754325038588156.....Total loss: 12.754325038588156\n",
      "epoch 4481 learning rate:  0.010223164472216023   Training loss:   13.926610261350024  Valing loss:   12.754325038588156\n",
      "Pure loss: 13.92845191689862.....Total loss: 13.92845191689862\n",
      "Pure loss: 12.758872921305453.....Total loss: 12.758872921305453\n",
      "epoch 4482 learning rate:  0.010223114680946006   Training loss:   13.92845191689862  Valing loss:   12.758872921305453\n",
      "Pure loss: 13.93416798125175.....Total loss: 13.93416798125175\n",
      "Pure loss: 12.764980951889338.....Total loss: 12.764980951889338\n",
      "epoch 4483 learning rate:  0.01022306491188936   Training loss:   13.93416798125175  Valing loss:   12.764980951889338\n",
      "Pure loss: 14.0017638832012.....Total loss: 14.0017638832012\n",
      "Pure loss: 12.692967171898264.....Total loss: 12.692967171898264\n",
      "epoch 4484 learning rate:  0.010223015165031222   Training loss:   14.0017638832012  Valing loss:   12.692967171898264\n",
      "Pure loss: 14.264164708453087.....Total loss: 14.264164708453087\n",
      "Pure loss: 12.652923424169682.....Total loss: 12.652923424169682\n",
      "epoch 4485 learning rate:  0.010222965440356745   Training loss:   14.264164708453087  Valing loss:   12.652923424169682\n",
      "Pure loss: 14.142633589297624.....Total loss: 14.142633589297624\n",
      "Pure loss: 12.663530105030878.....Total loss: 12.663530105030878\n",
      "epoch 4486 learning rate:  0.010222915737851092   Training loss:   14.142633589297624  Valing loss:   12.663530105030878\n",
      "Pure loss: 14.189929383593705.....Total loss: 14.189929383593705\n",
      "Pure loss: 12.67109904739071.....Total loss: 12.67109904739071\n",
      "epoch 4487 learning rate:  0.010222866057499443   Training loss:   14.189929383593705  Valing loss:   12.67109904739071\n",
      "Pure loss: 14.205650587316516.....Total loss: 14.205650587316516\n",
      "Pure loss: 12.679512858126296.....Total loss: 12.679512858126296\n",
      "epoch 4488 learning rate:  0.010222816399286988   Training loss:   14.205650587316516  Valing loss:   12.679512858126296\n",
      "Pure loss: 14.383411845493848.....Total loss: 14.383411845493848\n",
      "Pure loss: 12.69775820733649.....Total loss: 12.69775820733649\n",
      "epoch 4489 learning rate:  0.01022276676319893   Training loss:   14.383411845493848  Valing loss:   12.69775820733649\n",
      "Pure loss: 14.46306207303441.....Total loss: 14.46306207303441\n",
      "Pure loss: 12.737711702677426.....Total loss: 12.737711702677426\n",
      "epoch 4490 learning rate:  0.01022271714922049   Training loss:   14.46306207303441  Valing loss:   12.737711702677426\n",
      "Pure loss: 14.273648428529585.....Total loss: 14.273648428529585\n",
      "Pure loss: 12.682701084321199.....Total loss: 12.682701084321199\n",
      "epoch 4491 learning rate:  0.010222667557336897   Training loss:   14.273648428529585  Valing loss:   12.682701084321199\n",
      "Pure loss: 14.286913528509539.....Total loss: 14.286913528509539\n",
      "Pure loss: 12.685365012085956.....Total loss: 12.685365012085956\n",
      "epoch 4492 learning rate:  0.010222617987533393   Training loss:   14.286913528509539  Valing loss:   12.685365012085956\n",
      "Pure loss: 14.292398304874778.....Total loss: 14.292398304874778\n",
      "Pure loss: 12.687055275895066.....Total loss: 12.687055275895066\n",
      "epoch 4493 learning rate:  0.010222568439795238   Training loss:   14.292398304874778  Valing loss:   12.687055275895066\n",
      "Pure loss: 14.303274746674118.....Total loss: 14.303274746674118\n",
      "Pure loss: 12.68873811265241.....Total loss: 12.68873811265241\n",
      "epoch 4494 learning rate:  0.0102225189141077   Training loss:   14.303274746674118  Valing loss:   12.68873811265241\n",
      "Pure loss: 14.192899697326968.....Total loss: 14.192899697326968\n",
      "Pure loss: 12.655734589985892.....Total loss: 12.655734589985892\n",
      "epoch 4495 learning rate:  0.010222469410456063   Training loss:   14.192899697326968  Valing loss:   12.655734589985892\n",
      "Pure loss: 14.364500571154904.....Total loss: 14.364500571154904\n",
      "Pure loss: 12.668643739045766.....Total loss: 12.668643739045766\n",
      "epoch 4496 learning rate:  0.010222419928825622   Training loss:   14.364500571154904  Valing loss:   12.668643739045766\n",
      "Pure loss: 14.356739314024507.....Total loss: 14.356739314024507\n",
      "Pure loss: 12.667242174053762.....Total loss: 12.667242174053762\n",
      "epoch 4497 learning rate:  0.01022237046920169   Training loss:   14.356739314024507  Valing loss:   12.667242174053762\n",
      "Pure loss: 14.309713909016486.....Total loss: 14.309713909016486\n",
      "Pure loss: 12.662342672370624.....Total loss: 12.662342672370624\n",
      "epoch 4498 learning rate:  0.010222321031569587   Training loss:   14.309713909016486  Valing loss:   12.662342672370624\n",
      "Pure loss: 14.545619535364713.....Total loss: 14.545619535364713\n",
      "Pure loss: 12.745016594851956.....Total loss: 12.745016594851956\n",
      "epoch 4499 learning rate:  0.010222271615914647   Training loss:   14.545619535364713  Valing loss:   12.745016594851956\n",
      "Pure loss: 14.028269943778586.....Total loss: 14.028269943778586\n",
      "Pure loss: 12.672823382592084.....Total loss: 12.672823382592084\n",
      "epoch 4500 learning rate:  0.010222222222222223   Training loss:   14.028269943778586  Valing loss:   12.672823382592084\n",
      "Pure loss: 13.963990055639401.....Total loss: 13.963990055639401\n",
      "Pure loss: 12.714507840627533.....Total loss: 12.714507840627533\n",
      "epoch 4501 learning rate:  0.010222172850477673   Training loss:   13.963990055639401  Valing loss:   12.714507840627533\n",
      "Pure loss: 13.92199557346101.....Total loss: 13.92199557346101\n",
      "Pure loss: 12.765496141654323.....Total loss: 12.765496141654323\n",
      "epoch 4502 learning rate:  0.01022212350066637   Training loss:   13.92199557346101  Valing loss:   12.765496141654323\n",
      "Pure loss: 13.919148616432864.....Total loss: 13.919148616432864\n",
      "Pure loss: 12.854656736379813.....Total loss: 12.854656736379813\n",
      "epoch 4503 learning rate:  0.010222074172773706   Training loss:   13.919148616432864  Valing loss:   12.854656736379813\n",
      "Pure loss: 13.952416088273493.....Total loss: 13.952416088273493\n",
      "Pure loss: 12.901748841746839.....Total loss: 12.901748841746839\n",
      "epoch 4504 learning rate:  0.010222024866785081   Training loss:   13.952416088273493  Valing loss:   12.901748841746839\n",
      "Pure loss: 13.988967025988174.....Total loss: 13.988967025988174\n",
      "Pure loss: 12.774890565900993.....Total loss: 12.774890565900993\n",
      "epoch 4505 learning rate:  0.010221975582685904   Training loss:   13.988967025988174  Valing loss:   12.774890565900993\n",
      "Pure loss: 14.014036809095959.....Total loss: 14.014036809095959\n",
      "Pure loss: 12.88632068882005.....Total loss: 12.88632068882005\n",
      "epoch 4506 learning rate:  0.010221926320461606   Training loss:   14.014036809095959  Valing loss:   12.88632068882005\n",
      "Pure loss: 14.018257044066804.....Total loss: 14.018257044066804\n",
      "Pure loss: 12.892261968125556.....Total loss: 12.892261968125556\n",
      "epoch 4507 learning rate:  0.010221877080097625   Training loss:   14.018257044066804  Valing loss:   12.892261968125556\n",
      "Pure loss: 14.076218207808193.....Total loss: 14.076218207808193\n",
      "Pure loss: 12.778435816382268.....Total loss: 12.778435816382268\n",
      "epoch 4508 learning rate:  0.010221827861579415   Training loss:   14.076218207808193  Valing loss:   12.778435816382268\n",
      "Pure loss: 13.989021650064862.....Total loss: 13.989021650064862\n",
      "Pure loss: 12.975315705182624.....Total loss: 12.975315705182624\n",
      "epoch 4509 learning rate:  0.010221778664892437   Training loss:   13.989021650064862  Valing loss:   12.975315705182624\n",
      "Pure loss: 14.021232621905407.....Total loss: 14.021232621905407\n",
      "Pure loss: 13.312575968106996.....Total loss: 13.312575968106996\n",
      "epoch 4510 learning rate:  0.010221729490022173   Training loss:   14.021232621905407  Valing loss:   13.312575968106996\n",
      "Pure loss: 14.021729036880318.....Total loss: 14.021729036880318\n",
      "Pure loss: 13.392962798671626.....Total loss: 13.392962798671626\n",
      "epoch 4511 learning rate:  0.010221680336954113   Training loss:   14.021729036880318  Valing loss:   13.392962798671626\n",
      "Pure loss: 13.996789742832654.....Total loss: 13.996789742832654\n",
      "Pure loss: 13.241909632892208.....Total loss: 13.241909632892208\n",
      "epoch 4512 learning rate:  0.01022163120567376   Training loss:   13.996789742832654  Valing loss:   13.241909632892208\n",
      "Pure loss: 13.981153208479915.....Total loss: 13.981153208479915\n",
      "Pure loss: 13.171460850304618.....Total loss: 13.171460850304618\n",
      "epoch 4513 learning rate:  0.01022158209616663   Training loss:   13.981153208479915  Valing loss:   13.171460850304618\n",
      "Pure loss: 13.981835230482805.....Total loss: 13.981835230482805\n",
      "Pure loss: 13.015044227620313.....Total loss: 13.015044227620313\n",
      "epoch 4514 learning rate:  0.010221533008418255   Training loss:   13.981835230482805  Valing loss:   13.015044227620313\n",
      "Pure loss: 13.922625387299595.....Total loss: 13.922625387299595\n",
      "Pure loss: 12.726298007416302.....Total loss: 12.726298007416302\n",
      "epoch 4515 learning rate:  0.010221483942414174   Training loss:   13.922625387299595  Valing loss:   12.726298007416302\n",
      "Pure loss: 13.87635805279796.....Total loss: 13.87635805279796\n",
      "Pure loss: 12.783801654341513.....Total loss: 12.783801654341513\n",
      "epoch 4516 learning rate:  0.010221434898139948   Training loss:   13.87635805279796  Valing loss:   12.783801654341513\n",
      "Pure loss: 13.857175310396167.....Total loss: 13.857175310396167\n",
      "Pure loss: 12.818452258703921.....Total loss: 12.818452258703921\n",
      "epoch 4517 learning rate:  0.010221385875581138   Training loss:   13.857175310396167  Valing loss:   12.818452258703921\n",
      "Pure loss: 13.869603045166361.....Total loss: 13.869603045166361\n",
      "Pure loss: 12.789985470512853.....Total loss: 12.789985470512853\n",
      "epoch 4518 learning rate:  0.01022133687472333   Training loss:   13.869603045166361  Valing loss:   12.789985470512853\n",
      "Pure loss: 13.912815881599382.....Total loss: 13.912815881599382\n",
      "Pure loss: 12.733866706542806.....Total loss: 12.733866706542806\n",
      "epoch 4519 learning rate:  0.010221287895552114   Training loss:   13.912815881599382  Valing loss:   12.733866706542806\n",
      "Pure loss: 13.868446479347343.....Total loss: 13.868446479347343\n",
      "Pure loss: 13.253127790025667.....Total loss: 13.253127790025667\n",
      "epoch 4520 learning rate:  0.010221238938053098   Training loss:   13.868446479347343  Valing loss:   13.253127790025667\n",
      "Pure loss: 13.811658561652052.....Total loss: 13.811658561652052\n",
      "Pure loss: 12.950803590653992.....Total loss: 12.950803590653992\n",
      "epoch 4521 learning rate:  0.0102211900022119   Training loss:   13.811658561652052  Valing loss:   12.950803590653992\n",
      "Pure loss: 13.814233605674593.....Total loss: 13.814233605674593\n",
      "Pure loss: 13.004873656452043.....Total loss: 13.004873656452043\n",
      "epoch 4522 learning rate:  0.010221141088014153   Training loss:   13.814233605674593  Valing loss:   13.004873656452043\n",
      "Pure loss: 13.87020608120161.....Total loss: 13.87020608120161\n",
      "Pure loss: 13.250972572264942.....Total loss: 13.250972572264942\n",
      "epoch 4523 learning rate:  0.010221092195445501   Training loss:   13.87020608120161  Valing loss:   13.250972572264942\n",
      "Pure loss: 13.848128252996505.....Total loss: 13.848128252996505\n",
      "Pure loss: 13.176356654027618.....Total loss: 13.176356654027618\n",
      "epoch 4524 learning rate:  0.010221043324491601   Training loss:   13.848128252996505  Valing loss:   13.176356654027618\n",
      "Pure loss: 13.833790783917825.....Total loss: 13.833790783917825\n",
      "Pure loss: 13.14230073127951.....Total loss: 13.14230073127951\n",
      "epoch 4525 learning rate:  0.010220994475138122   Training loss:   13.833790783917825  Valing loss:   13.14230073127951\n",
      "Pure loss: 13.814493822084208.....Total loss: 13.814493822084208\n",
      "Pure loss: 13.036457729853886.....Total loss: 13.036457729853886\n",
      "epoch 4526 learning rate:  0.010220945647370748   Training loss:   13.814493822084208  Valing loss:   13.036457729853886\n",
      "Pure loss: 13.821478173056907.....Total loss: 13.821478173056907\n",
      "Pure loss: 12.891975901448204.....Total loss: 12.891975901448204\n",
      "epoch 4527 learning rate:  0.010220896841175171   Training loss:   13.821478173056907  Valing loss:   12.891975901448204\n",
      "Pure loss: 13.990378087180305.....Total loss: 13.990378087180305\n",
      "Pure loss: 12.764993178998743.....Total loss: 12.764993178998743\n",
      "epoch 4528 learning rate:  0.010220848056537103   Training loss:   13.990378087180305  Valing loss:   12.764993178998743\n",
      "Pure loss: 14.11557662770334.....Total loss: 14.11557662770334\n",
      "Pure loss: 12.881374303118228.....Total loss: 12.881374303118228\n",
      "epoch 4529 learning rate:  0.010220799293442261   Training loss:   14.11557662770334  Valing loss:   12.881374303118228\n",
      "Pure loss: 14.10210754670221.....Total loss: 14.10210754670221\n",
      "Pure loss: 12.875682640004705.....Total loss: 12.875682640004705\n",
      "epoch 4530 learning rate:  0.01022075055187638   Training loss:   14.10210754670221  Valing loss:   12.875682640004705\n",
      "Pure loss: 14.06547713039935.....Total loss: 14.06547713039935\n",
      "Pure loss: 12.912523562977665.....Total loss: 12.912523562977665\n",
      "epoch 4531 learning rate:  0.010220701831825205   Training loss:   14.06547713039935  Valing loss:   12.912523562977665\n",
      "Pure loss: 14.256300568702464.....Total loss: 14.256300568702464\n",
      "Pure loss: 12.878074898122033.....Total loss: 12.878074898122033\n",
      "epoch 4532 learning rate:  0.010220653133274494   Training loss:   14.256300568702464  Valing loss:   12.878074898122033\n",
      "Pure loss: 14.138419787126747.....Total loss: 14.138419787126747\n",
      "Pure loss: 12.93248744843571.....Total loss: 12.93248744843571\n",
      "epoch 4533 learning rate:  0.010220604456210015   Training loss:   14.138419787126747  Valing loss:   12.93248744843571\n",
      "Pure loss: 14.048280485558328.....Total loss: 14.048280485558328\n",
      "Pure loss: 12.946403814696803.....Total loss: 12.946403814696803\n",
      "epoch 4534 learning rate:  0.010220555800617556   Training loss:   14.048280485558328  Valing loss:   12.946403814696803\n",
      "Pure loss: 14.142743832255752.....Total loss: 14.142743832255752\n",
      "Pure loss: 12.91960477539171.....Total loss: 12.91960477539171\n",
      "epoch 4535 learning rate:  0.010220507166482911   Training loss:   14.142743832255752  Valing loss:   12.91960477539171\n",
      "Pure loss: 13.990565590531553.....Total loss: 13.990565590531553\n",
      "Pure loss: 12.802687598643375.....Total loss: 12.802687598643375\n",
      "epoch 4536 learning rate:  0.010220458553791887   Training loss:   13.990565590531553  Valing loss:   12.802687598643375\n",
      "Pure loss: 14.111911414285625.....Total loss: 14.111911414285625\n",
      "Pure loss: 12.755744158265781.....Total loss: 12.755744158265781\n",
      "epoch 4537 learning rate:  0.010220409962530307   Training loss:   14.111911414285625  Valing loss:   12.755744158265781\n",
      "Pure loss: 14.040189164386218.....Total loss: 14.040189164386218\n",
      "Pure loss: 12.760111575629226.....Total loss: 12.760111575629226\n",
      "epoch 4538 learning rate:  0.010220361392684002   Training loss:   14.040189164386218  Valing loss:   12.760111575629226\n",
      "Pure loss: 14.129033326877197.....Total loss: 14.129033326877197\n",
      "Pure loss: 12.775656780634879.....Total loss: 12.775656780634879\n",
      "epoch 4539 learning rate:  0.01022031284423882   Training loss:   14.129033326877197  Valing loss:   12.775656780634879\n",
      "Pure loss: 14.176164967661506.....Total loss: 14.176164967661506\n",
      "Pure loss: 12.780724148485994.....Total loss: 12.780724148485994\n",
      "epoch 4540 learning rate:  0.010220264317180617   Training loss:   14.176164967661506  Valing loss:   12.780724148485994\n",
      "Pure loss: 14.085340626161228.....Total loss: 14.085340626161228\n",
      "Pure loss: 12.723481100764076.....Total loss: 12.723481100764076\n",
      "epoch 4541 learning rate:  0.010220215811495266   Training loss:   14.085340626161228  Valing loss:   12.723481100764076\n",
      "Pure loss: 14.012848698187096.....Total loss: 14.012848698187096\n",
      "Pure loss: 12.718604290079883.....Total loss: 12.718604290079883\n",
      "epoch 4542 learning rate:  0.010220167327168649   Training loss:   14.012848698187096  Valing loss:   12.718604290079883\n",
      "Pure loss: 13.791627909115107.....Total loss: 13.791627909115107\n",
      "Pure loss: 12.757332368922658.....Total loss: 12.757332368922658\n",
      "epoch 4543 learning rate:  0.010220118864186662   Training loss:   13.791627909115107  Valing loss:   12.757332368922658\n",
      "Pure loss: 13.840801932786132.....Total loss: 13.840801932786132\n",
      "Pure loss: 12.683883513283869.....Total loss: 12.683883513283869\n",
      "epoch 4544 learning rate:  0.010220070422535211   Training loss:   13.840801932786132  Valing loss:   12.683883513283869\n",
      "Pure loss: 13.809797492054646.....Total loss: 13.809797492054646\n",
      "Pure loss: 12.698283818832943.....Total loss: 12.698283818832943\n",
      "epoch 4545 learning rate:  0.01022002200220022   Training loss:   13.809797492054646  Valing loss:   12.698283818832943\n",
      "Pure loss: 13.888418062714768.....Total loss: 13.888418062714768\n",
      "Pure loss: 12.631437102170192.....Total loss: 12.631437102170192\n",
      "epoch 4546 learning rate:  0.01021997360316762   Training loss:   13.888418062714768  Valing loss:   12.631437102170192\n",
      "Pure loss: 13.881633986384482.....Total loss: 13.881633986384482\n",
      "Pure loss: 12.632346854792228.....Total loss: 12.632346854792228\n",
      "epoch 4547 learning rate:  0.010219925225423357   Training loss:   13.881633986384482  Valing loss:   12.632346854792228\n",
      "Pure loss: 13.870420261745746.....Total loss: 13.870420261745746\n",
      "Pure loss: 12.632085413140713.....Total loss: 12.632085413140713\n",
      "epoch 4548 learning rate:  0.010219876868953386   Training loss:   13.870420261745746  Valing loss:   12.632085413140713\n",
      "Pure loss: 13.915507056491327.....Total loss: 13.915507056491327\n",
      "Pure loss: 12.611303786728113.....Total loss: 12.611303786728113\n",
      "epoch 4549 learning rate:  0.01021982853374368   Training loss:   13.915507056491327  Valing loss:   12.611303786728113\n",
      "Pure loss: 14.177673499677994.....Total loss: 14.177673499677994\n",
      "Pure loss: 13.424353693716984.....Total loss: 13.424353693716984\n",
      "epoch 4550 learning rate:  0.01021978021978022   Training loss:   14.177673499677994  Valing loss:   13.424353693716984\n",
      "Pure loss: 14.125667863705878.....Total loss: 14.125667863705878\n",
      "Pure loss: 13.223244878017653.....Total loss: 13.223244878017653\n",
      "epoch 4551 learning rate:  0.010219731927049001   Training loss:   14.125667863705878  Valing loss:   13.223244878017653\n",
      "Pure loss: 14.046178641563793.....Total loss: 14.046178641563793\n",
      "Pure loss: 13.000726207978392.....Total loss: 13.000726207978392\n",
      "epoch 4552 learning rate:  0.010219683655536028   Training loss:   14.046178641563793  Valing loss:   13.000726207978392\n",
      "Pure loss: 14.068906543341408.....Total loss: 14.068906543341408\n",
      "Pure loss: 13.029703547369456.....Total loss: 13.029703547369456\n",
      "epoch 4553 learning rate:  0.010219635405227323   Training loss:   14.068906543341408  Valing loss:   13.029703547369456\n",
      "Pure loss: 14.11113246550302.....Total loss: 14.11113246550302\n",
      "Pure loss: 12.803667529767223.....Total loss: 12.803667529767223\n",
      "epoch 4554 learning rate:  0.010219587176108916   Training loss:   14.11113246550302  Valing loss:   12.803667529767223\n",
      "Pure loss: 14.065623109342853.....Total loss: 14.065623109342853\n",
      "Pure loss: 12.747660746077477.....Total loss: 12.747660746077477\n",
      "epoch 4555 learning rate:  0.010219538968166849   Training loss:   14.065623109342853  Valing loss:   12.747660746077477\n",
      "Pure loss: 13.987862002432452.....Total loss: 13.987862002432452\n",
      "Pure loss: 13.021578456453666.....Total loss: 13.021578456453666\n",
      "epoch 4556 learning rate:  0.010219490781387182   Training loss:   13.987862002432452  Valing loss:   13.021578456453666\n",
      "Pure loss: 13.968548130380473.....Total loss: 13.968548130380473\n",
      "Pure loss: 12.895331307367357.....Total loss: 12.895331307367357\n",
      "epoch 4557 learning rate:  0.01021944261575598   Training loss:   13.968548130380473  Valing loss:   12.895331307367357\n",
      "Pure loss: 14.009418601854268.....Total loss: 14.009418601854268\n",
      "Pure loss: 12.82177216418391.....Total loss: 12.82177216418391\n",
      "epoch 4558 learning rate:  0.010219394471259325   Training loss:   14.009418601854268  Valing loss:   12.82177216418391\n",
      "Pure loss: 13.987567434714473.....Total loss: 13.987567434714473\n",
      "Pure loss: 12.852826819953025.....Total loss: 12.852826819953025\n",
      "epoch 4559 learning rate:  0.010219346347883308   Training loss:   13.987567434714473  Valing loss:   12.852826819953025\n",
      "Pure loss: 13.997323506909723.....Total loss: 13.997323506909723\n",
      "Pure loss: 12.83723832188106.....Total loss: 12.83723832188106\n",
      "epoch 4560 learning rate:  0.010219298245614036   Training loss:   13.997323506909723  Valing loss:   12.83723832188106\n",
      "Pure loss: 13.99143604729231.....Total loss: 13.99143604729231\n",
      "Pure loss: 12.824788057118717.....Total loss: 12.824788057118717\n",
      "epoch 4561 learning rate:  0.010219250164437624   Training loss:   13.99143604729231  Valing loss:   12.824788057118717\n",
      "Pure loss: 14.006328512657747.....Total loss: 14.006328512657747\n",
      "Pure loss: 12.756492425394239.....Total loss: 12.756492425394239\n",
      "epoch 4562 learning rate:  0.010219202104340202   Training loss:   14.006328512657747  Valing loss:   12.756492425394239\n",
      "Pure loss: 13.905093542750024.....Total loss: 13.905093542750024\n",
      "Pure loss: 12.911828911279684.....Total loss: 12.911828911279684\n",
      "epoch 4563 learning rate:  0.010219154065307911   Training loss:   13.905093542750024  Valing loss:   12.911828911279684\n",
      "Pure loss: 13.907499933769557.....Total loss: 13.907499933769557\n",
      "Pure loss: 12.790627396268146.....Total loss: 12.790627396268146\n",
      "epoch 4564 learning rate:  0.010219106047326907   Training loss:   13.907499933769557  Valing loss:   12.790627396268146\n",
      "Pure loss: 13.910269765652897.....Total loss: 13.910269765652897\n",
      "Pure loss: 12.80067943503392.....Total loss: 12.80067943503392\n",
      "epoch 4565 learning rate:  0.010219058050383352   Training loss:   13.910269765652897  Valing loss:   12.80067943503392\n",
      "Pure loss: 13.903540945152324.....Total loss: 13.903540945152324\n",
      "Pure loss: 12.741745202209442.....Total loss: 12.741745202209442\n",
      "epoch 4566 learning rate:  0.010219010074463426   Training loss:   13.903540945152324  Valing loss:   12.741745202209442\n",
      "Pure loss: 13.908934232229035.....Total loss: 13.908934232229035\n",
      "Pure loss: 12.889302163513046.....Total loss: 12.889302163513046\n",
      "epoch 4567 learning rate:  0.010218962119553317   Training loss:   13.908934232229035  Valing loss:   12.889302163513046\n",
      "Pure loss: 13.966535543668856.....Total loss: 13.966535543668856\n",
      "Pure loss: 13.363777245379312.....Total loss: 13.363777245379312\n",
      "epoch 4568 learning rate:  0.010218914185639229   Training loss:   13.966535543668856  Valing loss:   13.363777245379312\n",
      "Pure loss: 13.980838345250438.....Total loss: 13.980838345250438\n",
      "Pure loss: 13.395386796741938.....Total loss: 13.395386796741938\n",
      "epoch 4569 learning rate:  0.010218866272707376   Training loss:   13.980838345250438  Valing loss:   13.395386796741938\n",
      "Pure loss: 13.999349674158475.....Total loss: 13.999349674158475\n",
      "Pure loss: 13.440289961418058.....Total loss: 13.440289961418058\n",
      "epoch 4570 learning rate:  0.010218818380743983   Training loss:   13.999349674158475  Valing loss:   13.440289961418058\n",
      "Pure loss: 14.046291099018443.....Total loss: 14.046291099018443\n",
      "Pure loss: 13.540517192467897.....Total loss: 13.540517192467897\n",
      "epoch 4571 learning rate:  0.010218770509735287   Training loss:   14.046291099018443  Valing loss:   13.540517192467897\n",
      "Pure loss: 14.012497856203534.....Total loss: 14.012497856203534\n",
      "Pure loss: 13.470891361859556.....Total loss: 13.470891361859556\n",
      "epoch 4572 learning rate:  0.010218722659667542   Training loss:   14.012497856203534  Valing loss:   13.470891361859556\n",
      "Pure loss: 13.97071024494249.....Total loss: 13.97071024494249\n",
      "Pure loss: 13.376674901572366.....Total loss: 13.376674901572366\n",
      "epoch 4573 learning rate:  0.010218674830527007   Training loss:   13.97071024494249  Valing loss:   13.376674901572366\n",
      "Pure loss: 14.386391613743548.....Total loss: 14.386391613743548\n",
      "Pure loss: 14.163410662200196.....Total loss: 14.163410662200196\n",
      "epoch 4574 learning rate:  0.010218627022299956   Training loss:   14.386391613743548  Valing loss:   14.163410662200196\n",
      "Pure loss: 14.343810972563102.....Total loss: 14.343810972563102\n",
      "Pure loss: 14.081449374687258.....Total loss: 14.081449374687258\n",
      "epoch 4575 learning rate:  0.010218579234972679   Training loss:   14.343810972563102  Valing loss:   14.081449374687258\n",
      "Pure loss: 14.158654751230245.....Total loss: 14.158654751230245\n",
      "Pure loss: 13.743921713061377.....Total loss: 13.743921713061377\n",
      "epoch 4576 learning rate:  0.010218531468531468   Training loss:   14.158654751230245  Valing loss:   13.743921713061377\n",
      "Pure loss: 13.986717910214212.....Total loss: 13.986717910214212\n",
      "Pure loss: 13.369451034087962.....Total loss: 13.369451034087962\n",
      "epoch 4577 learning rate:  0.01021848372296264   Training loss:   13.986717910214212  Valing loss:   13.369451034087962\n",
      "Pure loss: 13.886840149319978.....Total loss: 13.886840149319978\n",
      "Pure loss: 13.113076076712282.....Total loss: 13.113076076712282\n",
      "epoch 4578 learning rate:  0.010218435998252512   Training loss:   13.886840149319978  Valing loss:   13.113076076712282\n",
      "Pure loss: 13.901693872830133.....Total loss: 13.901693872830133\n",
      "Pure loss: 12.793752708683959.....Total loss: 12.793752708683959\n",
      "epoch 4579 learning rate:  0.01021838829438742   Training loss:   13.901693872830133  Valing loss:   12.793752708683959\n",
      "Pure loss: 13.875251434858935.....Total loss: 13.875251434858935\n",
      "Pure loss: 12.866441019518883.....Total loss: 12.866441019518883\n",
      "epoch 4580 learning rate:  0.010218340611353712   Training loss:   13.875251434858935  Valing loss:   12.866441019518883\n",
      "Pure loss: 13.837183242862631.....Total loss: 13.837183242862631\n",
      "Pure loss: 12.78717746268747.....Total loss: 12.78717746268747\n",
      "epoch 4581 learning rate:  0.010218292949137743   Training loss:   13.837183242862631  Valing loss:   12.78717746268747\n",
      "Pure loss: 13.808707978051334.....Total loss: 13.808707978051334\n",
      "Pure loss: 12.80627947587723.....Total loss: 12.80627947587723\n",
      "epoch 4582 learning rate:  0.010218245307725884   Training loss:   13.808707978051334  Valing loss:   12.80627947587723\n",
      "Pure loss: 13.801773136405185.....Total loss: 13.801773136405185\n",
      "Pure loss: 12.836846474507976.....Total loss: 12.836846474507976\n",
      "epoch 4583 learning rate:  0.010218197687104517   Training loss:   13.801773136405185  Valing loss:   12.836846474507976\n",
      "Pure loss: 13.785024295657106.....Total loss: 13.785024295657106\n",
      "Pure loss: 12.906897726594993.....Total loss: 12.906897726594993\n",
      "epoch 4584 learning rate:  0.010218150087260035   Training loss:   13.785024295657106  Valing loss:   12.906897726594993\n",
      "Pure loss: 13.78894546217535.....Total loss: 13.78894546217535\n",
      "Pure loss: 12.81762235566957.....Total loss: 12.81762235566957\n",
      "epoch 4585 learning rate:  0.010218102508178844   Training loss:   13.78894546217535  Valing loss:   12.81762235566957\n",
      "Pure loss: 13.881068730100742.....Total loss: 13.881068730100742\n",
      "Pure loss: 12.643492776708525.....Total loss: 12.643492776708525\n",
      "epoch 4586 learning rate:  0.010218054949847361   Training loss:   13.881068730100742  Valing loss:   12.643492776708525\n",
      "Pure loss: 13.773679959730822.....Total loss: 13.773679959730822\n",
      "Pure loss: 12.88377929540874.....Total loss: 12.88377929540874\n",
      "epoch 4587 learning rate:  0.010218007412252017   Training loss:   13.773679959730822  Valing loss:   12.88377929540874\n",
      "Pure loss: 13.893764151374722.....Total loss: 13.893764151374722\n",
      "Pure loss: 13.27044873942386.....Total loss: 13.27044873942386\n",
      "epoch 4588 learning rate:  0.01021795989537925   Training loss:   13.893764151374722  Valing loss:   13.27044873942386\n",
      "Pure loss: 13.797511844298619.....Total loss: 13.797511844298619\n",
      "Pure loss: 13.098657024484952.....Total loss: 13.098657024484952\n",
      "epoch 4589 learning rate:  0.010217912399215516   Training loss:   13.797511844298619  Valing loss:   13.098657024484952\n",
      "Pure loss: 13.745236703485913.....Total loss: 13.745236703485913\n",
      "Pure loss: 13.003727800035215.....Total loss: 13.003727800035215\n",
      "epoch 4590 learning rate:  0.010217864923747277   Training loss:   13.745236703485913  Valing loss:   13.003727800035215\n",
      "Pure loss: 13.768417803870706.....Total loss: 13.768417803870706\n",
      "Pure loss: 13.104817322986186.....Total loss: 13.104817322986186\n",
      "epoch 4591 learning rate:  0.010217817468961011   Training loss:   13.768417803870706  Valing loss:   13.104817322986186\n",
      "Pure loss: 13.730562900514075.....Total loss: 13.730562900514075\n",
      "Pure loss: 12.775549236586158.....Total loss: 12.775549236586158\n",
      "epoch 4592 learning rate:  0.010217770034843206   Training loss:   13.730562900514075  Valing loss:   12.775549236586158\n",
      "Pure loss: 13.73067288390542.....Total loss: 13.73067288390542\n",
      "Pure loss: 12.775050583619837.....Total loss: 12.775050583619837\n",
      "epoch 4593 learning rate:  0.010217722621380362   Training loss:   13.73067288390542  Valing loss:   12.775050583619837\n",
      "Pure loss: 13.722776709490763.....Total loss: 13.722776709490763\n",
      "Pure loss: 12.81007516919433.....Total loss: 12.81007516919433\n",
      "epoch 4594 learning rate:  0.01021767522855899   Training loss:   13.722776709490763  Valing loss:   12.81007516919433\n",
      "Pure loss: 13.753713197729576.....Total loss: 13.753713197729576\n",
      "Pure loss: 12.685134510983314.....Total loss: 12.685134510983314\n",
      "epoch 4595 learning rate:  0.010217627856365616   Training loss:   13.753713197729576  Valing loss:   12.685134510983314\n",
      "Pure loss: 13.744323043496665.....Total loss: 13.744323043496665\n",
      "Pure loss: 12.968369081679317.....Total loss: 12.968369081679317\n",
      "epoch 4596 learning rate:  0.010217580504786772   Training loss:   13.744323043496665  Valing loss:   12.968369081679317\n",
      "Pure loss: 13.738722609661876.....Total loss: 13.738722609661876\n",
      "Pure loss: 12.940208130256295.....Total loss: 12.940208130256295\n",
      "epoch 4597 learning rate:  0.010217533173809006   Training loss:   13.738722609661876  Valing loss:   12.940208130256295\n",
      "Pure loss: 13.717481742387772.....Total loss: 13.717481742387772\n",
      "Pure loss: 12.777374130668388.....Total loss: 12.777374130668388\n",
      "epoch 4598 learning rate:  0.010217485863418878   Training loss:   13.717481742387772  Valing loss:   12.777374130668388\n",
      "Pure loss: 13.717097266916506.....Total loss: 13.717097266916506\n",
      "Pure loss: 12.793712785748705.....Total loss: 12.793712785748705\n",
      "epoch 4599 learning rate:  0.010217438573602957   Training loss:   13.717097266916506  Valing loss:   12.793712785748705\n",
      "Pure loss: 13.828385216330291.....Total loss: 13.828385216330291\n",
      "Pure loss: 13.120109141857114.....Total loss: 13.120109141857114\n",
      "epoch 4600 learning rate:  0.010217391304347826   Training loss:   13.828385216330291  Valing loss:   13.120109141857114\n",
      "Pure loss: 13.85199267831989.....Total loss: 13.85199267831989\n",
      "Pure loss: 13.192140787756735.....Total loss: 13.192140787756735\n",
      "epoch 4601 learning rate:  0.010217344055640079   Training loss:   13.85199267831989  Valing loss:   13.192140787756735\n",
      "Pure loss: 13.848470250226207.....Total loss: 13.848470250226207\n",
      "Pure loss: 13.185528172792965.....Total loss: 13.185528172792965\n",
      "epoch 4602 learning rate:  0.010217296827466318   Training loss:   13.848470250226207  Valing loss:   13.185528172792965\n",
      "Pure loss: 13.820731613593445.....Total loss: 13.820731613593445\n",
      "Pure loss: 13.108434390268837.....Total loss: 13.108434390268837\n",
      "epoch 4603 learning rate:  0.010217249619813165   Training loss:   13.820731613593445  Valing loss:   13.108434390268837\n",
      "Pure loss: 13.739432382441688.....Total loss: 13.739432382441688\n",
      "Pure loss: 12.873162689750385.....Total loss: 12.873162689750385\n",
      "epoch 4604 learning rate:  0.010217202432667246   Training loss:   13.739432382441688  Valing loss:   12.873162689750385\n",
      "Pure loss: 13.72622547569271.....Total loss: 13.72622547569271\n",
      "Pure loss: 12.851769884536738.....Total loss: 12.851769884536738\n",
      "epoch 4605 learning rate:  0.010217155266015202   Training loss:   13.72622547569271  Valing loss:   12.851769884536738\n",
      "Pure loss: 13.717829349613902.....Total loss: 13.717829349613902\n",
      "Pure loss: 12.777815658852075.....Total loss: 12.777815658852075\n",
      "epoch 4606 learning rate:  0.010217108119843683   Training loss:   13.717829349613902  Valing loss:   12.777815658852075\n",
      "Pure loss: 13.811691470113919.....Total loss: 13.811691470113919\n",
      "Pure loss: 12.655603584705956.....Total loss: 12.655603584705956\n",
      "epoch 4607 learning rate:  0.010217060994139354   Training loss:   13.811691470113919  Valing loss:   12.655603584705956\n",
      "Pure loss: 13.847953493636282.....Total loss: 13.847953493636282\n",
      "Pure loss: 12.62421377242036.....Total loss: 12.62421377242036\n",
      "epoch 4608 learning rate:  0.010217013888888888   Training loss:   13.847953493636282  Valing loss:   12.62421377242036\n",
      "Pure loss: 13.748863690199.....Total loss: 13.748863690199\n",
      "Pure loss: 12.632240477597037.....Total loss: 12.632240477597037\n",
      "epoch 4609 learning rate:  0.010216966804078976   Training loss:   13.748863690199  Valing loss:   12.632240477597037\n",
      "Pure loss: 13.797481225034423.....Total loss: 13.797481225034423\n",
      "Pure loss: 12.629979969012322.....Total loss: 12.629979969012322\n",
      "epoch 4610 learning rate:  0.010216919739696313   Training loss:   13.797481225034423  Valing loss:   12.629979969012322\n",
      "Pure loss: 13.775987622299226.....Total loss: 13.775987622299226\n",
      "Pure loss: 12.639014019092182.....Total loss: 12.639014019092182\n",
      "epoch 4611 learning rate:  0.010216872695727608   Training loss:   13.775987622299226  Valing loss:   12.639014019092182\n",
      "Pure loss: 13.729043426688788.....Total loss: 13.729043426688788\n",
      "Pure loss: 12.800374434413545.....Total loss: 12.800374434413545\n",
      "epoch 4612 learning rate:  0.010216825672159584   Training loss:   13.729043426688788  Valing loss:   12.800374434413545\n",
      "Pure loss: 13.724028461158118.....Total loss: 13.724028461158118\n",
      "Pure loss: 12.760080953894278.....Total loss: 12.760080953894278\n",
      "epoch 4613 learning rate:  0.010216778668978972   Training loss:   13.724028461158118  Valing loss:   12.760080953894278\n",
      "Pure loss: 13.723045873347035.....Total loss: 13.723045873347035\n",
      "Pure loss: 12.703726523944903.....Total loss: 12.703726523944903\n",
      "epoch 4614 learning rate:  0.010216731686172518   Training loss:   13.723045873347035  Valing loss:   12.703726523944903\n",
      "Pure loss: 13.725152992292584.....Total loss: 13.725152992292584\n",
      "Pure loss: 12.634276715359137.....Total loss: 12.634276715359137\n",
      "epoch 4615 learning rate:  0.010216684723726978   Training loss:   13.725152992292584  Valing loss:   12.634276715359137\n",
      "Pure loss: 13.723368557271732.....Total loss: 13.723368557271732\n",
      "Pure loss: 12.738671455962269.....Total loss: 12.738671455962269\n",
      "epoch 4616 learning rate:  0.010216637781629116   Training loss:   13.723368557271732  Valing loss:   12.738671455962269\n",
      "Pure loss: 13.718688247789201.....Total loss: 13.718688247789201\n",
      "Pure loss: 12.684210442995534.....Total loss: 12.684210442995534\n",
      "epoch 4617 learning rate:  0.010216590859865714   Training loss:   13.718688247789201  Valing loss:   12.684210442995534\n",
      "Pure loss: 13.717711953692474.....Total loss: 13.717711953692474\n",
      "Pure loss: 12.666451942945507.....Total loss: 12.666451942945507\n",
      "epoch 4618 learning rate:  0.01021654395842356   Training loss:   13.717711953692474  Valing loss:   12.666451942945507\n",
      "Pure loss: 13.737717365065764.....Total loss: 13.737717365065764\n",
      "Pure loss: 12.51501664110258.....Total loss: 12.51501664110258\n",
      "epoch 4619 learning rate:  0.010216497077289458   Training loss:   13.737717365065764  Valing loss:   12.51501664110258\n",
      "Pure loss: 13.735462403047324.....Total loss: 13.735462403047324\n",
      "Pure loss: 12.523485448488584.....Total loss: 12.523485448488584\n",
      "epoch 4620 learning rate:  0.010216450216450217   Training loss:   13.735462403047324  Valing loss:   12.523485448488584\n",
      "Pure loss: 13.720423521087914.....Total loss: 13.720423521087914\n",
      "Pure loss: 12.599495085122106.....Total loss: 12.599495085122106\n",
      "epoch 4621 learning rate:  0.010216403375892665   Training loss:   13.720423521087914  Valing loss:   12.599495085122106\n",
      "Pure loss: 13.73284831984953.....Total loss: 13.73284831984953\n",
      "Pure loss: 12.532161040918718.....Total loss: 12.532161040918718\n",
      "epoch 4622 learning rate:  0.010216356555603635   Training loss:   13.73284831984953  Valing loss:   12.532161040918718\n",
      "Pure loss: 13.746530119157097.....Total loss: 13.746530119157097\n",
      "Pure loss: 12.551347013678846.....Total loss: 12.551347013678846\n",
      "epoch 4623 learning rate:  0.010216309755569976   Training loss:   13.746530119157097  Valing loss:   12.551347013678846\n",
      "Pure loss: 13.762688285150443.....Total loss: 13.762688285150443\n",
      "Pure loss: 12.491554430341752.....Total loss: 12.491554430341752\n",
      "epoch 4624 learning rate:  0.010216262975778547   Training loss:   13.762688285150443  Valing loss:   12.491554430341752\n",
      "Pure loss: 13.917115844908226.....Total loss: 13.917115844908226\n",
      "Pure loss: 12.338683182052861.....Total loss: 12.338683182052861\n",
      "epoch 4625 learning rate:  0.010216216216216217   Training loss:   13.917115844908226  Valing loss:   12.338683182052861\n",
      "Pure loss: 13.764332830050284.....Total loss: 13.764332830050284\n",
      "Pure loss: 12.471334629552661.....Total loss: 12.471334629552661\n",
      "epoch 4626 learning rate:  0.010216169476869865   Training loss:   13.764332830050284  Valing loss:   12.471334629552661\n",
      "Pure loss: 13.778513436978837.....Total loss: 13.778513436978837\n",
      "Pure loss: 12.552011268247.....Total loss: 12.552011268247\n",
      "epoch 4627 learning rate:  0.01021612275772639   Training loss:   13.778513436978837  Valing loss:   12.552011268247\n",
      "Pure loss: 13.792618362465355.....Total loss: 13.792618362465355\n",
      "Pure loss: 12.494691744476599.....Total loss: 12.494691744476599\n",
      "epoch 4628 learning rate:  0.010216076058772687   Training loss:   13.792618362465355  Valing loss:   12.494691744476599\n",
      "Pure loss: 13.803492866215247.....Total loss: 13.803492866215247\n",
      "Pure loss: 12.466826223635108.....Total loss: 12.466826223635108\n",
      "epoch 4629 learning rate:  0.010216029379995679   Training loss:   13.803492866215247  Valing loss:   12.466826223635108\n",
      "Pure loss: 13.905947442648452.....Total loss: 13.905947442648452\n",
      "Pure loss: 12.37470812490675.....Total loss: 12.37470812490675\n",
      "epoch 4630 learning rate:  0.01021598272138229   Training loss:   13.905947442648452  Valing loss:   12.37470812490675\n",
      "Pure loss: 13.912375425294268.....Total loss: 13.912375425294268\n",
      "Pure loss: 12.35651386865462.....Total loss: 12.35651386865462\n",
      "epoch 4631 learning rate:  0.010215936082919456   Training loss:   13.912375425294268  Valing loss:   12.35651386865462\n",
      "Pure loss: 13.926478136547061.....Total loss: 13.926478136547061\n",
      "Pure loss: 12.351421053409721.....Total loss: 12.351421053409721\n",
      "epoch 4632 learning rate:  0.010215889464594128   Training loss:   13.926478136547061  Valing loss:   12.351421053409721\n",
      "Pure loss: 13.74255814674494.....Total loss: 13.74255814674494\n",
      "Pure loss: 12.542935498935577.....Total loss: 12.542935498935577\n",
      "epoch 4633 learning rate:  0.010215842866393265   Training loss:   13.74255814674494  Valing loss:   12.542935498935577\n",
      "Pure loss: 13.733521685488368.....Total loss: 13.733521685488368\n",
      "Pure loss: 12.603141460668573.....Total loss: 12.603141460668573\n",
      "epoch 4634 learning rate:  0.010215796288303842   Training loss:   13.733521685488368  Valing loss:   12.603141460668573\n",
      "Pure loss: 13.723705902719459.....Total loss: 13.723705902719459\n",
      "Pure loss: 12.560140818351886.....Total loss: 12.560140818351886\n",
      "epoch 4635 learning rate:  0.010215749730312838   Training loss:   13.723705902719459  Valing loss:   12.560140818351886\n",
      "Pure loss: 13.738264505733461.....Total loss: 13.738264505733461\n",
      "Pure loss: 12.511771723719828.....Total loss: 12.511771723719828\n",
      "epoch 4636 learning rate:  0.010215703192407248   Training loss:   13.738264505733461  Valing loss:   12.511771723719828\n",
      "Pure loss: 13.74818345310637.....Total loss: 13.74818345310637\n",
      "Pure loss: 12.489129526647858.....Total loss: 12.489129526647858\n",
      "epoch 4637 learning rate:  0.010215656674574079   Training loss:   13.74818345310637  Valing loss:   12.489129526647858\n",
      "Pure loss: 13.746981202345285.....Total loss: 13.746981202345285\n",
      "Pure loss: 12.49115581529804.....Total loss: 12.49115581529804\n",
      "epoch 4638 learning rate:  0.010215610176800345   Training loss:   13.746981202345285  Valing loss:   12.49115581529804\n",
      "Pure loss: 13.71836576423025.....Total loss: 13.71836576423025\n",
      "Pure loss: 12.560266805955175.....Total loss: 12.560266805955175\n",
      "epoch 4639 learning rate:  0.010215563699073076   Training loss:   13.71836576423025  Valing loss:   12.560266805955175\n",
      "Pure loss: 13.732438822070419.....Total loss: 13.732438822070419\n",
      "Pure loss: 12.841891422159325.....Total loss: 12.841891422159325\n",
      "epoch 4640 learning rate:  0.010215517241379311   Training loss:   13.732438822070419  Valing loss:   12.841891422159325\n",
      "Pure loss: 13.749189154006286.....Total loss: 13.749189154006286\n",
      "Pure loss: 12.445687229568135.....Total loss: 12.445687229568135\n",
      "epoch 4641 learning rate:  0.010215470803706097   Training loss:   13.749189154006286  Valing loss:   12.445687229568135\n",
      "Pure loss: 13.739005052922362.....Total loss: 13.739005052922362\n",
      "Pure loss: 12.510935515660119.....Total loss: 12.510935515660119\n",
      "epoch 4642 learning rate:  0.0102154243860405   Training loss:   13.739005052922362  Valing loss:   12.510935515660119\n",
      "Pure loss: 13.74442038483452.....Total loss: 13.74442038483452\n",
      "Pure loss: 12.518464665308928.....Total loss: 12.518464665308928\n",
      "epoch 4643 learning rate:  0.010215377988369589   Training loss:   13.74442038483452  Valing loss:   12.518464665308928\n",
      "Pure loss: 13.833186013315295.....Total loss: 13.833186013315295\n",
      "Pure loss: 12.337758082855023.....Total loss: 12.337758082855023\n",
      "epoch 4644 learning rate:  0.010215331610680448   Training loss:   13.833186013315295  Valing loss:   12.337758082855023\n",
      "Pure loss: 13.894515019545942.....Total loss: 13.894515019545942\n",
      "Pure loss: 12.307147033271672.....Total loss: 12.307147033271672\n",
      "epoch 4645 learning rate:  0.010215285252960173   Training loss:   13.894515019545942  Valing loss:   12.307147033271672\n",
      "Pure loss: 13.795522082629528.....Total loss: 13.795522082629528\n",
      "Pure loss: 12.662421777872849.....Total loss: 12.662421777872849\n",
      "epoch 4646 learning rate:  0.010215238915195868   Training loss:   13.795522082629528  Valing loss:   12.662421777872849\n",
      "Pure loss: 13.800823132266334.....Total loss: 13.800823132266334\n",
      "Pure loss: 12.72894669375534.....Total loss: 12.72894669375534\n",
      "epoch 4647 learning rate:  0.010215192597374651   Training loss:   13.800823132266334  Valing loss:   12.72894669375534\n",
      "Pure loss: 13.82085285724783.....Total loss: 13.82085285724783\n",
      "Pure loss: 12.82617816585438.....Total loss: 12.82617816585438\n",
      "epoch 4648 learning rate:  0.01021514629948365   Training loss:   13.82085285724783  Valing loss:   12.82617816585438\n",
      "Pure loss: 13.795225630441118.....Total loss: 13.795225630441118\n",
      "Pure loss: 12.761979146169086.....Total loss: 12.761979146169086\n",
      "epoch 4649 learning rate:  0.010215100021510002   Training loss:   13.795225630441118  Valing loss:   12.761979146169086\n",
      "Pure loss: 13.832591021443465.....Total loss: 13.832591021443465\n",
      "Pure loss: 12.850100509899907.....Total loss: 12.850100509899907\n",
      "epoch 4650 learning rate:  0.01021505376344086   Training loss:   13.832591021443465  Valing loss:   12.850100509899907\n",
      "Pure loss: 13.940962149689256.....Total loss: 13.940962149689256\n",
      "Pure loss: 13.164025478111434.....Total loss: 13.164025478111434\n",
      "epoch 4651 learning rate:  0.010215007525263385   Training loss:   13.940962149689256  Valing loss:   13.164025478111434\n",
      "Pure loss: 14.12305789883429.....Total loss: 14.12305789883429\n",
      "Pure loss: 13.665591184541215.....Total loss: 13.665591184541215\n",
      "epoch 4652 learning rate:  0.010214961306964747   Training loss:   14.12305789883429  Valing loss:   13.665591184541215\n",
      "Pure loss: 14.061496388199664.....Total loss: 14.061496388199664\n",
      "Pure loss: 13.553953723694681.....Total loss: 13.553953723694681\n",
      "epoch 4653 learning rate:  0.01021491510853213   Training loss:   14.061496388199664  Valing loss:   13.553953723694681\n",
      "Pure loss: 13.823852241178974.....Total loss: 13.823852241178974\n",
      "Pure loss: 13.089309998133066.....Total loss: 13.089309998133066\n",
      "epoch 4654 learning rate:  0.01021486892995273   Training loss:   13.823852241178974  Valing loss:   13.089309998133066\n",
      "Pure loss: 13.958963276400658.....Total loss: 13.958963276400658\n",
      "Pure loss: 13.270856480095782.....Total loss: 13.270856480095782\n",
      "epoch 4655 learning rate:  0.01021482277121375   Training loss:   13.958963276400658  Valing loss:   13.270856480095782\n",
      "Pure loss: 13.823928778843001.....Total loss: 13.823928778843001\n",
      "Pure loss: 12.981090728847409.....Total loss: 12.981090728847409\n",
      "epoch 4656 learning rate:  0.010214776632302405   Training loss:   13.823928778843001  Valing loss:   12.981090728847409\n",
      "Pure loss: 13.70113418076584.....Total loss: 13.70113418076584\n",
      "Pure loss: 12.762162699240944.....Total loss: 12.762162699240944\n",
      "epoch 4657 learning rate:  0.010214730513205927   Training loss:   13.70113418076584  Valing loss:   12.762162699240944\n",
      "Pure loss: 13.857938924674007.....Total loss: 13.857938924674007\n",
      "Pure loss: 13.189971020003087.....Total loss: 13.189971020003087\n",
      "epoch 4658 learning rate:  0.01021468441391155   Training loss:   13.857938924674007  Valing loss:   13.189971020003087\n",
      "Pure loss: 14.781512026842542.....Total loss: 14.781512026842542\n",
      "Pure loss: 14.839993496426013.....Total loss: 14.839993496426013\n",
      "epoch 4659 learning rate:  0.010214638334406525   Training loss:   14.781512026842542  Valing loss:   14.839993496426013\n",
      "Pure loss: 14.904396617973152.....Total loss: 14.904396617973152\n",
      "Pure loss: 14.996258879817411.....Total loss: 14.996258879817411\n",
      "epoch 4660 learning rate:  0.010214592274678112   Training loss:   14.904396617973152  Valing loss:   14.996258879817411\n",
      "Pure loss: 14.831349510741212.....Total loss: 14.831349510741212\n",
      "Pure loss: 14.898636118179793.....Total loss: 14.898636118179793\n",
      "epoch 4661 learning rate:  0.010214546234713582   Training loss:   14.831349510741212  Valing loss:   14.898636118179793\n",
      "Pure loss: 14.576990767839055.....Total loss: 14.576990767839055\n",
      "Pure loss: 14.55526648408773.....Total loss: 14.55526648408773\n",
      "epoch 4662 learning rate:  0.010214500214500214   Training loss:   14.576990767839055  Valing loss:   14.55526648408773\n",
      "Pure loss: 14.514070720940484.....Total loss: 14.514070720940484\n",
      "Pure loss: 14.458031681610395.....Total loss: 14.458031681610395\n",
      "epoch 4663 learning rate:  0.010214454214025306   Training loss:   14.514070720940484  Valing loss:   14.458031681610395\n",
      "Pure loss: 14.687298502006367.....Total loss: 14.687298502006367\n",
      "Pure loss: 14.695967766933098.....Total loss: 14.695967766933098\n",
      "epoch 4664 learning rate:  0.010214408233276158   Training loss:   14.687298502006367  Valing loss:   14.695967766933098\n",
      "Pure loss: 14.721441772645354.....Total loss: 14.721441772645354\n",
      "Pure loss: 14.74564873481324.....Total loss: 14.74564873481324\n",
      "epoch 4665 learning rate:  0.010214362272240086   Training loss:   14.721441772645354  Valing loss:   14.74564873481324\n",
      "Pure loss: 15.698997149584395.....Total loss: 15.698997149584395\n",
      "Pure loss: 16.100297419124725.....Total loss: 16.100297419124725\n",
      "epoch 4666 learning rate:  0.010214316330904414   Training loss:   15.698997149584395  Valing loss:   16.100297419124725\n",
      "Pure loss: 15.734135978073601.....Total loss: 15.734135978073601\n",
      "Pure loss: 16.146675655492654.....Total loss: 16.146675655492654\n",
      "epoch 4667 learning rate:  0.010214270409256482   Training loss:   15.734135978073601  Valing loss:   16.146675655492654\n",
      "Pure loss: 15.39102195668887.....Total loss: 15.39102195668887\n",
      "Pure loss: 15.707805591612361.....Total loss: 15.707805591612361\n",
      "epoch 4668 learning rate:  0.010214224507283633   Training loss:   15.39102195668887  Valing loss:   15.707805591612361\n",
      "Pure loss: 14.742494264007107.....Total loss: 14.742494264007107\n",
      "Pure loss: 14.802827294808065.....Total loss: 14.802827294808065\n",
      "epoch 4669 learning rate:  0.010214178624973229   Training loss:   14.742494264007107  Valing loss:   14.802827294808065\n",
      "Pure loss: 14.418946580410418.....Total loss: 14.418946580410418\n",
      "Pure loss: 14.312602381754756.....Total loss: 14.312602381754756\n",
      "epoch 4670 learning rate:  0.010214132762312634   Training loss:   14.418946580410418  Valing loss:   14.312602381754756\n",
      "Pure loss: 14.57295422986545.....Total loss: 14.57295422986545\n",
      "Pure loss: 14.528144084304694.....Total loss: 14.528144084304694\n",
      "epoch 4671 learning rate:  0.010214086919289232   Training loss:   14.57295422986545  Valing loss:   14.528144084304694\n",
      "Pure loss: 16.701041927036933.....Total loss: 16.701041927036933\n",
      "Pure loss: 17.25239834645891.....Total loss: 17.25239834645891\n",
      "epoch 4672 learning rate:  0.01021404109589041   Training loss:   16.701041927036933  Valing loss:   17.25239834645891\n",
      "Pure loss: 16.663674872338483.....Total loss: 16.663674872338483\n",
      "Pure loss: 17.20574334255258.....Total loss: 17.20574334255258\n",
      "epoch 4673 learning rate:  0.010213995292103574   Training loss:   16.663674872338483  Valing loss:   17.20574334255258\n",
      "Pure loss: 16.248412370918874.....Total loss: 16.248412370918874\n",
      "Pure loss: 16.663994193267285.....Total loss: 16.663994193267285\n",
      "epoch 4674 learning rate:  0.010213949507916133   Training loss:   16.248412370918874  Valing loss:   16.663994193267285\n",
      "Pure loss: 15.400391511582717.....Total loss: 15.400391511582717\n",
      "Pure loss: 15.531376896193638.....Total loss: 15.531376896193638\n",
      "epoch 4675 learning rate:  0.010213903743315508   Training loss:   15.400391511582717  Valing loss:   15.531376896193638\n",
      "Pure loss: 14.819429459904034.....Total loss: 14.819429459904034\n",
      "Pure loss: 14.700643195814804.....Total loss: 14.700643195814804\n",
      "epoch 4676 learning rate:  0.010213857998289137   Training loss:   14.819429459904034  Valing loss:   14.700643195814804\n",
      "Pure loss: 14.39433796833378.....Total loss: 14.39433796833378\n",
      "Pure loss: 14.043767063398029.....Total loss: 14.043767063398029\n",
      "epoch 4677 learning rate:  0.01021381227282446   Training loss:   14.39433796833378  Valing loss:   14.043767063398029\n",
      "Pure loss: 14.361152523907496.....Total loss: 14.361152523907496\n",
      "Pure loss: 13.994671161374491.....Total loss: 13.994671161374491\n",
      "epoch 4678 learning rate:  0.010213766566908936   Training loss:   14.361152523907496  Valing loss:   13.994671161374491\n",
      "Pure loss: 14.250013008448368.....Total loss: 14.250013008448368\n",
      "Pure loss: 13.783564571334601.....Total loss: 13.783564571334601\n",
      "epoch 4679 learning rate:  0.010213720880530028   Training loss:   14.250013008448368  Valing loss:   13.783564571334601\n",
      "Pure loss: 14.23360227198495.....Total loss: 14.23360227198495\n",
      "Pure loss: 13.761968717645203.....Total loss: 13.761968717645203\n",
      "epoch 4680 learning rate:  0.010213675213675213   Training loss:   14.23360227198495  Valing loss:   13.761968717645203\n",
      "Pure loss: 14.282961876933687.....Total loss: 14.282961876933687\n",
      "Pure loss: 13.857840025414857.....Total loss: 13.857840025414857\n",
      "epoch 4681 learning rate:  0.01021362956633198   Training loss:   14.282961876933687  Valing loss:   13.857840025414857\n",
      "Pure loss: 14.494166518170225.....Total loss: 14.494166518170225\n",
      "Pure loss: 14.243573429033846.....Total loss: 14.243573429033846\n",
      "epoch 4682 learning rate:  0.010213583938487826   Training loss:   14.494166518170225  Valing loss:   14.243573429033846\n",
      "Pure loss: 14.271376880126873.....Total loss: 14.271376880126873\n",
      "Pure loss: 13.865978251505194.....Total loss: 13.865978251505194\n",
      "epoch 4683 learning rate:  0.01021353833013026   Training loss:   14.271376880126873  Valing loss:   13.865978251505194\n",
      "Pure loss: 14.261465392466087.....Total loss: 14.261465392466087\n",
      "Pure loss: 13.847944266952576.....Total loss: 13.847944266952576\n",
      "epoch 4684 learning rate:  0.010213492741246798   Training loss:   14.261465392466087  Valing loss:   13.847944266952576\n",
      "Pure loss: 14.250664329356438.....Total loss: 14.250664329356438\n",
      "Pure loss: 13.82811444217862.....Total loss: 13.82811444217862\n",
      "epoch 4685 learning rate:  0.010213447171824973   Training loss:   14.250664329356438  Valing loss:   13.82811444217862\n",
      "Pure loss: 15.21437663805465.....Total loss: 15.21437663805465\n",
      "Pure loss: 15.228336410600754.....Total loss: 15.228336410600754\n",
      "epoch 4686 learning rate:  0.010213401621852327   Training loss:   15.21437663805465  Valing loss:   15.228336410600754\n",
      "Pure loss: 14.7663527323173.....Total loss: 14.7663527323173\n",
      "Pure loss: 14.51272305266606.....Total loss: 14.51272305266606\n",
      "epoch 4687 learning rate:  0.010213356091316407   Training loss:   14.7663527323173  Valing loss:   14.51272305266606\n",
      "Pure loss: 14.519452025703323.....Total loss: 14.519452025703323\n",
      "Pure loss: 14.107862975873564.....Total loss: 14.107862975873564\n",
      "epoch 4688 learning rate:  0.010213310580204778   Training loss:   14.519452025703323  Valing loss:   14.107862975873564\n",
      "Pure loss: 14.085509606130119.....Total loss: 14.085509606130119\n",
      "Pure loss: 13.525853791775372.....Total loss: 13.525853791775372\n",
      "epoch 4689 learning rate:  0.010213265088505011   Training loss:   14.085509606130119  Valing loss:   13.525853791775372\n",
      "Pure loss: 13.853364162406676.....Total loss: 13.853364162406676\n",
      "Pure loss: 13.231950587151696.....Total loss: 13.231950587151696\n",
      "epoch 4690 learning rate:  0.01021321961620469   Training loss:   13.853364162406676  Valing loss:   13.231950587151696\n",
      "Pure loss: 13.80006424937985.....Total loss: 13.80006424937985\n",
      "Pure loss: 13.139385505635655.....Total loss: 13.139385505635655\n",
      "epoch 4691 learning rate:  0.01021317416329141   Training loss:   13.80006424937985  Valing loss:   13.139385505635655\n",
      "Pure loss: 13.733632089690486.....Total loss: 13.733632089690486\n",
      "Pure loss: 12.969136677736408.....Total loss: 12.969136677736408\n",
      "epoch 4692 learning rate:  0.01021312872975277   Training loss:   13.733632089690486  Valing loss:   12.969136677736408\n",
      "Pure loss: 13.680067474252722.....Total loss: 13.680067474252722\n",
      "Pure loss: 12.828640766891578.....Total loss: 12.828640766891578\n",
      "epoch 4693 learning rate:  0.010213083315576391   Training loss:   13.680067474252722  Valing loss:   12.828640766891578\n",
      "Pure loss: 13.679611431654898.....Total loss: 13.679611431654898\n",
      "Pure loss: 12.827849472819397.....Total loss: 12.827849472819397\n",
      "epoch 4694 learning rate:  0.010213037920749894   Training loss:   13.679611431654898  Valing loss:   12.827849472819397\n",
      "Pure loss: 13.612455657252918.....Total loss: 13.612455657252918\n",
      "Pure loss: 12.638066539440246.....Total loss: 12.638066539440246\n",
      "epoch 4695 learning rate:  0.010212992545260917   Training loss:   13.612455657252918  Valing loss:   12.638066539440246\n",
      "Pure loss: 13.608998736352188.....Total loss: 13.608998736352188\n",
      "Pure loss: 12.60809898146529.....Total loss: 12.60809898146529\n",
      "epoch 4696 learning rate:  0.010212947189097104   Training loss:   13.608998736352188  Valing loss:   12.60809898146529\n",
      "Pure loss: 13.642621589407291.....Total loss: 13.642621589407291\n",
      "Pure loss: 12.701543683841928.....Total loss: 12.701543683841928\n",
      "epoch 4697 learning rate:  0.010212901852246114   Training loss:   13.642621589407291  Valing loss:   12.701543683841928\n",
      "Pure loss: 13.633897113755657.....Total loss: 13.633897113755657\n",
      "Pure loss: 12.473374592616949.....Total loss: 12.473374592616949\n",
      "epoch 4698 learning rate:  0.010212856534695615   Training loss:   13.633897113755657  Valing loss:   12.473374592616949\n",
      "Pure loss: 13.65675408290678.....Total loss: 13.65675408290678\n",
      "Pure loss: 12.555836906951985.....Total loss: 12.555836906951985\n",
      "epoch 4699 learning rate:  0.010212811236433283   Training loss:   13.65675408290678  Valing loss:   12.555836906951985\n",
      "Pure loss: 13.69605967528858.....Total loss: 13.69605967528858\n",
      "Pure loss: 12.377112031363655.....Total loss: 12.377112031363655\n",
      "epoch 4700 learning rate:  0.010212765957446808   Training loss:   13.69605967528858  Valing loss:   12.377112031363655\n",
      "Pure loss: 13.696504748017441.....Total loss: 13.696504748017441\n",
      "Pure loss: 12.290912717017497.....Total loss: 12.290912717017497\n",
      "epoch 4701 learning rate:  0.010212720697723889   Training loss:   13.696504748017441  Valing loss:   12.290912717017497\n",
      "Pure loss: 13.705270204340701.....Total loss: 13.705270204340701\n",
      "Pure loss: 12.279444150383302.....Total loss: 12.279444150383302\n",
      "epoch 4702 learning rate:  0.010212675457252234   Training loss:   13.705270204340701  Valing loss:   12.279444150383302\n",
      "Pure loss: 13.693223082113448.....Total loss: 13.693223082113448\n",
      "Pure loss: 12.291494362811305.....Total loss: 12.291494362811305\n",
      "epoch 4703 learning rate:  0.010212630236019562   Training loss:   13.693223082113448  Valing loss:   12.291494362811305\n",
      "Pure loss: 13.764712014761615.....Total loss: 13.764712014761615\n",
      "Pure loss: 12.259314850668048.....Total loss: 12.259314850668048\n",
      "epoch 4704 learning rate:  0.010212585034013606   Training loss:   13.764712014761615  Valing loss:   12.259314850668048\n",
      "Pure loss: 13.821694172986248.....Total loss: 13.821694172986248\n",
      "Pure loss: 12.2601382669165.....Total loss: 12.2601382669165\n",
      "epoch 4705 learning rate:  0.010212539851222105   Training loss:   13.821694172986248  Valing loss:   12.2601382669165\n",
      "Pure loss: 13.984684743089677.....Total loss: 13.984684743089677\n",
      "Pure loss: 12.291946520690033.....Total loss: 12.291946520690033\n",
      "epoch 4706 learning rate:  0.010212494687632809   Training loss:   13.984684743089677  Valing loss:   12.291946520690033\n",
      "Pure loss: 14.004195009503803.....Total loss: 14.004195009503803\n",
      "Pure loss: 12.964154445962162.....Total loss: 12.964154445962162\n",
      "epoch 4707 learning rate:  0.010212449543233483   Training loss:   14.004195009503803  Valing loss:   12.964154445962162\n",
      "Pure loss: 14.05291783261672.....Total loss: 14.05291783261672\n",
      "Pure loss: 13.100840502066665.....Total loss: 13.100840502066665\n",
      "epoch 4708 learning rate:  0.010212404418011896   Training loss:   14.05291783261672  Valing loss:   13.100840502066665\n",
      "Pure loss: 14.032223621343174.....Total loss: 14.032223621343174\n",
      "Pure loss: 13.039037210435293.....Total loss: 13.039037210435293\n",
      "epoch 4709 learning rate:  0.01021235931195583   Training loss:   14.032223621343174  Valing loss:   13.039037210435293\n",
      "Pure loss: 14.125157385043495.....Total loss: 14.125157385043495\n",
      "Pure loss: 13.14167123707343.....Total loss: 13.14167123707343\n",
      "epoch 4710 learning rate:  0.010212314225053079   Training loss:   14.125157385043495  Valing loss:   13.14167123707343\n",
      "Pure loss: 14.138865846403828.....Total loss: 14.138865846403828\n",
      "Pure loss: 13.209043267765328.....Total loss: 13.209043267765328\n",
      "epoch 4711 learning rate:  0.010212269157291445   Training loss:   14.138865846403828  Valing loss:   13.209043267765328\n",
      "Pure loss: 14.05920165135273.....Total loss: 14.05920165135273\n",
      "Pure loss: 12.953050728857574.....Total loss: 12.953050728857574\n",
      "epoch 4712 learning rate:  0.010212224108658744   Training loss:   14.05920165135273  Valing loss:   12.953050728857574\n",
      "Pure loss: 14.05566618658888.....Total loss: 14.05566618658888\n",
      "Pure loss: 12.913426559832196.....Total loss: 12.913426559832196\n",
      "epoch 4713 learning rate:  0.010212179079142797   Training loss:   14.05566618658888  Valing loss:   12.913426559832196\n",
      "Pure loss: 14.294897541396503.....Total loss: 14.294897541396503\n",
      "Pure loss: 13.720333655828059.....Total loss: 13.720333655828059\n",
      "epoch 4714 learning rate:  0.010212134068731438   Training loss:   14.294897541396503  Valing loss:   13.720333655828059\n",
      "Pure loss: 14.004006850080712.....Total loss: 14.004006850080712\n",
      "Pure loss: 13.15495803618963.....Total loss: 13.15495803618963\n",
      "epoch 4715 learning rate:  0.010212089077412513   Training loss:   14.004006850080712  Valing loss:   13.15495803618963\n",
      "Pure loss: 13.875437526802143.....Total loss: 13.875437526802143\n",
      "Pure loss: 12.941629037760919.....Total loss: 12.941629037760919\n",
      "epoch 4716 learning rate:  0.010212044105173877   Training loss:   13.875437526802143  Valing loss:   12.941629037760919\n",
      "Pure loss: 13.827891587464077.....Total loss: 13.827891587464077\n",
      "Pure loss: 12.819707828330088.....Total loss: 12.819707828330088\n",
      "epoch 4717 learning rate:  0.010211999152003392   Training loss:   13.827891587464077  Valing loss:   12.819707828330088\n",
      "Pure loss: 13.820722956004007.....Total loss: 13.820722956004007\n",
      "Pure loss: 12.790700501721759.....Total loss: 12.790700501721759\n",
      "epoch 4718 learning rate:  0.010211954217888937   Training loss:   13.820722956004007  Valing loss:   12.790700501721759\n",
      "Pure loss: 13.840697288369244.....Total loss: 13.840697288369244\n",
      "Pure loss: 12.716686382681454.....Total loss: 12.716686382681454\n",
      "epoch 4719 learning rate:  0.010211909302818394   Training loss:   13.840697288369244  Valing loss:   12.716686382681454\n",
      "Pure loss: 13.830423825212229.....Total loss: 13.830423825212229\n",
      "Pure loss: 13.005904919421118.....Total loss: 13.005904919421118\n",
      "epoch 4720 learning rate:  0.010211864406779661   Training loss:   13.830423825212229  Valing loss:   13.005904919421118\n",
      "Pure loss: 13.76963046783486.....Total loss: 13.76963046783486\n",
      "Pure loss: 12.780288400056254.....Total loss: 12.780288400056254\n",
      "epoch 4721 learning rate:  0.010211819529760644   Training loss:   13.76963046783486  Valing loss:   12.780288400056254\n",
      "Pure loss: 13.806482689477281.....Total loss: 13.806482689477281\n",
      "Pure loss: 12.94184900058158.....Total loss: 12.94184900058158\n",
      "epoch 4722 learning rate:  0.010211774671749259   Training loss:   13.806482689477281  Valing loss:   12.94184900058158\n",
      "Pure loss: 13.844267178126502.....Total loss: 13.844267178126502\n",
      "Pure loss: 13.063459570363205.....Total loss: 13.063459570363205\n",
      "epoch 4723 learning rate:  0.010211729832733432   Training loss:   13.844267178126502  Valing loss:   13.063459570363205\n",
      "Pure loss: 13.793282742547936.....Total loss: 13.793282742547936\n",
      "Pure loss: 12.979922170861235.....Total loss: 12.979922170861235\n",
      "epoch 4724 learning rate:  0.0102116850127011   Training loss:   13.793282742547936  Valing loss:   12.979922170861235\n",
      "Pure loss: 13.894208677462027.....Total loss: 13.894208677462027\n",
      "Pure loss: 13.242255426184746.....Total loss: 13.242255426184746\n",
      "epoch 4725 learning rate:  0.010211640211640212   Training loss:   13.894208677462027  Valing loss:   13.242255426184746\n",
      "Pure loss: 13.851218123967431.....Total loss: 13.851218123967431\n",
      "Pure loss: 13.132251132129886.....Total loss: 13.132251132129886\n",
      "epoch 4726 learning rate:  0.010211595429538723   Training loss:   13.851218123967431  Valing loss:   13.132251132129886\n",
      "Pure loss: 14.363652283919242.....Total loss: 14.363652283919242\n",
      "Pure loss: 14.092616071714115.....Total loss: 14.092616071714115\n",
      "epoch 4727 learning rate:  0.0102115506663846   Training loss:   14.363652283919242  Valing loss:   14.092616071714115\n",
      "Pure loss: 13.961292206787117.....Total loss: 13.961292206787117\n",
      "Pure loss: 13.41826365787371.....Total loss: 13.41826365787371\n",
      "epoch 4728 learning rate:  0.01021150592216582   Training loss:   13.961292206787117  Valing loss:   13.41826365787371\n",
      "Pure loss: 14.564906750150161.....Total loss: 14.564906750150161\n",
      "Pure loss: 14.501533467418277.....Total loss: 14.501533467418277\n",
      "epoch 4729 learning rate:  0.010211461196870374   Training loss:   14.564906750150161  Valing loss:   14.501533467418277\n",
      "Pure loss: 14.296738417857956.....Total loss: 14.296738417857956\n",
      "Pure loss: 14.115818272215716.....Total loss: 14.115818272215716\n",
      "epoch 4730 learning rate:  0.010211416490486258   Training loss:   14.296738417857956  Valing loss:   14.115818272215716\n",
      "Pure loss: 14.232507877305308.....Total loss: 14.232507877305308\n",
      "Pure loss: 14.003851299507508.....Total loss: 14.003851299507508\n",
      "epoch 4731 learning rate:  0.01021137180300148   Training loss:   14.232507877305308  Valing loss:   14.003851299507508\n",
      "Pure loss: 14.679353368727302.....Total loss: 14.679353368727302\n",
      "Pure loss: 14.669210767018605.....Total loss: 14.669210767018605\n",
      "epoch 4732 learning rate:  0.010211327134404058   Training loss:   14.679353368727302  Valing loss:   14.669210767018605\n",
      "Pure loss: 14.294171241862813.....Total loss: 14.294171241862813\n",
      "Pure loss: 14.059518820671904.....Total loss: 14.059518820671904\n",
      "epoch 4733 learning rate:  0.01021128248468202   Training loss:   14.294171241862813  Valing loss:   14.059518820671904\n",
      "Pure loss: 14.055351530402149.....Total loss: 14.055351530402149\n",
      "Pure loss: 13.72415793705979.....Total loss: 13.72415793705979\n",
      "epoch 4734 learning rate:  0.010211237853823406   Training loss:   14.055351530402149  Valing loss:   13.72415793705979\n",
      "Pure loss: 13.959049226350189.....Total loss: 13.959049226350189\n",
      "Pure loss: 13.551192725083588.....Total loss: 13.551192725083588\n",
      "epoch 4735 learning rate:  0.010211193241816262   Training loss:   13.959049226350189  Valing loss:   13.551192725083588\n",
      "Pure loss: 13.97662106836784.....Total loss: 13.97662106836784\n",
      "Pure loss: 13.573507180026514.....Total loss: 13.573507180026514\n",
      "epoch 4736 learning rate:  0.010211148648648648   Training loss:   13.97662106836784  Valing loss:   13.573507180026514\n",
      "Pure loss: 14.069661274381291.....Total loss: 14.069661274381291\n",
      "Pure loss: 13.705178992260773.....Total loss: 13.705178992260773\n",
      "epoch 4737 learning rate:  0.010211104074308634   Training loss:   14.069661274381291  Valing loss:   13.705178992260773\n",
      "Pure loss: 13.858044849987335.....Total loss: 13.858044849987335\n",
      "Pure loss: 13.41790417967399.....Total loss: 13.41790417967399\n",
      "epoch 4738 learning rate:  0.010211059518784297   Training loss:   13.858044849987335  Valing loss:   13.41790417967399\n",
      "Pure loss: 13.731153471023005.....Total loss: 13.731153471023005\n",
      "Pure loss: 13.181924493658043.....Total loss: 13.181924493658043\n",
      "epoch 4739 learning rate:  0.010211014982063726   Training loss:   13.731153471023005  Valing loss:   13.181924493658043\n",
      "Pure loss: 13.709284740500967.....Total loss: 13.709284740500967\n",
      "Pure loss: 13.142178293122768.....Total loss: 13.142178293122768\n",
      "epoch 4740 learning rate:  0.010210970464135022   Training loss:   13.709284740500967  Valing loss:   13.142178293122768\n",
      "Pure loss: 13.684181298810012.....Total loss: 13.684181298810012\n",
      "Pure loss: 13.108882815632436.....Total loss: 13.108882815632436\n",
      "epoch 4741 learning rate:  0.01021092596498629   Training loss:   13.684181298810012  Valing loss:   13.108882815632436\n",
      "Pure loss: 13.618630489398416.....Total loss: 13.618630489398416\n",
      "Pure loss: 12.942561181326298.....Total loss: 12.942561181326298\n",
      "epoch 4742 learning rate:  0.010210881484605652   Training loss:   13.618630489398416  Valing loss:   12.942561181326298\n",
      "Pure loss: 13.750907156121846.....Total loss: 13.750907156121846\n",
      "Pure loss: 13.232764311222256.....Total loss: 13.232764311222256\n",
      "epoch 4743 learning rate:  0.010210837022981235   Training loss:   13.750907156121846  Valing loss:   13.232764311222256\n",
      "Pure loss: 13.562361396162785.....Total loss: 13.562361396162785\n",
      "Pure loss: 12.783734108586485.....Total loss: 12.783734108586485\n",
      "epoch 4744 learning rate:  0.01021079258010118   Training loss:   13.562361396162785  Valing loss:   12.783734108586485\n",
      "Pure loss: 13.79676865207765.....Total loss: 13.79676865207765\n",
      "Pure loss: 13.347543110954847.....Total loss: 13.347543110954847\n",
      "epoch 4745 learning rate:  0.010210748155953636   Training loss:   13.79676865207765  Valing loss:   13.347543110954847\n",
      "Pure loss: 13.851501067377368.....Total loss: 13.851501067377368\n",
      "Pure loss: 13.447604891626538.....Total loss: 13.447604891626538\n",
      "epoch 4746 learning rate:  0.01021070375052676   Training loss:   13.851501067377368  Valing loss:   13.447604891626538\n",
      "Pure loss: 13.808039297200779.....Total loss: 13.808039297200779\n",
      "Pure loss: 13.372595454654805.....Total loss: 13.372595454654805\n",
      "epoch 4747 learning rate:  0.010210659363808721   Training loss:   13.808039297200779  Valing loss:   13.372595454654805\n",
      "Pure loss: 13.774710272599496.....Total loss: 13.774710272599496\n",
      "Pure loss: 13.333721333730939.....Total loss: 13.333721333730939\n",
      "epoch 4748 learning rate:  0.0102106149957877   Training loss:   13.774710272599496  Valing loss:   13.333721333730939\n",
      "Pure loss: 13.741204123803795.....Total loss: 13.741204123803795\n",
      "Pure loss: 13.262631970344135.....Total loss: 13.262631970344135\n",
      "epoch 4749 learning rate:  0.010210570646451885   Training loss:   13.741204123803795  Valing loss:   13.262631970344135\n",
      "Pure loss: 13.773102383472498.....Total loss: 13.773102383472498\n",
      "Pure loss: 13.327125677697625.....Total loss: 13.327125677697625\n",
      "epoch 4750 learning rate:  0.010210526315789474   Training loss:   13.773102383472498  Valing loss:   13.327125677697625\n",
      "Pure loss: 13.748273914772994.....Total loss: 13.748273914772994\n",
      "Pure loss: 13.28474354923838.....Total loss: 13.28474354923838\n",
      "epoch 4751 learning rate:  0.010210482003788676   Training loss:   13.748273914772994  Valing loss:   13.28474354923838\n",
      "Pure loss: 13.590616114677434.....Total loss: 13.590616114677434\n",
      "Pure loss: 12.914986534625857.....Total loss: 12.914986534625857\n",
      "epoch 4752 learning rate:  0.010210437710437711   Training loss:   13.590616114677434  Valing loss:   12.914986534625857\n",
      "Pure loss: 13.570911351826522.....Total loss: 13.570911351826522\n",
      "Pure loss: 12.818138038111135.....Total loss: 12.818138038111135\n",
      "epoch 4753 learning rate:  0.010210393435724805   Training loss:   13.570911351826522  Valing loss:   12.818138038111135\n",
      "Pure loss: 13.685773390896271.....Total loss: 13.685773390896271\n",
      "Pure loss: 12.80248534766576.....Total loss: 12.80248534766576\n",
      "epoch 4754 learning rate:  0.010210349179638199   Training loss:   13.685773390896271  Valing loss:   12.80248534766576\n",
      "Pure loss: 13.746193002994811.....Total loss: 13.746193002994811\n",
      "Pure loss: 12.820506529391208.....Total loss: 12.820506529391208\n",
      "epoch 4755 learning rate:  0.010210304942166142   Training loss:   13.746193002994811  Valing loss:   12.820506529391208\n",
      "Pure loss: 13.748706076658673.....Total loss: 13.748706076658673\n",
      "Pure loss: 12.848402929301937.....Total loss: 12.848402929301937\n",
      "epoch 4756 learning rate:  0.010210260723296888   Training loss:   13.748706076658673  Valing loss:   12.848402929301937\n",
      "Pure loss: 13.632013844519854.....Total loss: 13.632013844519854\n",
      "Pure loss: 12.872245342196647.....Total loss: 12.872245342196647\n",
      "epoch 4757 learning rate:  0.01021021652301871   Training loss:   13.632013844519854  Valing loss:   12.872245342196647\n",
      "Pure loss: 13.767687067858578.....Total loss: 13.767687067858578\n",
      "Pure loss: 13.226092936876972.....Total loss: 13.226092936876972\n",
      "epoch 4758 learning rate:  0.010210172341319882   Training loss:   13.767687067858578  Valing loss:   13.226092936876972\n",
      "Pure loss: 13.823007832359123.....Total loss: 13.823007832359123\n",
      "Pure loss: 13.341688672613271.....Total loss: 13.341688672613271\n",
      "epoch 4759 learning rate:  0.010210128178188695   Training loss:   13.823007832359123  Valing loss:   13.341688672613271\n",
      "Pure loss: 13.727932870181014.....Total loss: 13.727932870181014\n",
      "Pure loss: 13.141130986373495.....Total loss: 13.141130986373495\n",
      "epoch 4760 learning rate:  0.010210084033613445   Training loss:   13.727932870181014  Valing loss:   13.141130986373495\n",
      "Pure loss: 13.848008731472182.....Total loss: 13.848008731472182\n",
      "Pure loss: 13.385422749447843.....Total loss: 13.385422749447843\n",
      "epoch 4761 learning rate:  0.01021003990758244   Training loss:   13.848008731472182  Valing loss:   13.385422749447843\n",
      "Pure loss: 13.837619456744948.....Total loss: 13.837619456744948\n",
      "Pure loss: 13.36629744508188.....Total loss: 13.36629744508188\n",
      "epoch 4762 learning rate:  0.010209995800083999   Training loss:   13.837619456744948  Valing loss:   13.36629744508188\n",
      "Pure loss: 13.802348758818024.....Total loss: 13.802348758818024\n",
      "Pure loss: 13.30377458063159.....Total loss: 13.30377458063159\n",
      "epoch 4763 learning rate:  0.010209951711106446   Training loss:   13.802348758818024  Valing loss:   13.30377458063159\n",
      "Pure loss: 14.030301261266024.....Total loss: 14.030301261266024\n",
      "Pure loss: 13.69832502010013.....Total loss: 13.69832502010013\n",
      "epoch 4764 learning rate:  0.01020990764063812   Training loss:   14.030301261266024  Valing loss:   13.69832502010013\n",
      "Pure loss: 13.971769506961166.....Total loss: 13.971769506961166\n",
      "Pure loss: 13.594945646108865.....Total loss: 13.594945646108865\n",
      "epoch 4765 learning rate:  0.010209863588667366   Training loss:   13.971769506961166  Valing loss:   13.594945646108865\n",
      "Pure loss: 13.825691058484784.....Total loss: 13.825691058484784\n",
      "Pure loss: 13.342161360132796.....Total loss: 13.342161360132796\n",
      "epoch 4766 learning rate:  0.010209819555182542   Training loss:   13.825691058484784  Valing loss:   13.342161360132796\n",
      "Pure loss: 13.839364891221054.....Total loss: 13.839364891221054\n",
      "Pure loss: 13.363646784607765.....Total loss: 13.363646784607765\n",
      "epoch 4767 learning rate:  0.010209775540172016   Training loss:   13.839364891221054  Valing loss:   13.363646784607765\n",
      "Pure loss: 13.856356572438632.....Total loss: 13.856356572438632\n",
      "Pure loss: 13.394587433882856.....Total loss: 13.394587433882856\n",
      "epoch 4768 learning rate:  0.010209731543624161   Training loss:   13.856356572438632  Valing loss:   13.394587433882856\n",
      "Pure loss: 13.739385412487099.....Total loss: 13.739385412487099\n",
      "Pure loss: 13.175535761937851.....Total loss: 13.175535761937851\n",
      "epoch 4769 learning rate:  0.010209687565527365   Training loss:   13.739385412487099  Valing loss:   13.175535761937851\n",
      "Pure loss: 14.133283236335032.....Total loss: 14.133283236335032\n",
      "Pure loss: 13.86748587066156.....Total loss: 13.86748587066156\n",
      "epoch 4770 learning rate:  0.010209643605870022   Training loss:   14.133283236335032  Valing loss:   13.86748587066156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure loss: 13.663671329979383.....Total loss: 13.663671329979383\n",
      "Pure loss: 12.995024279157084.....Total loss: 12.995024279157084\n",
      "epoch 4771 learning rate:  0.010209599664640537   Training loss:   13.663671329979383  Valing loss:   12.995024279157084\n",
      "Pure loss: 13.639923027752.....Total loss: 13.639923027752\n",
      "Pure loss: 12.949819397870588.....Total loss: 12.949819397870588\n",
      "epoch 4772 learning rate:  0.010209555741827326   Training loss:   13.639923027752  Valing loss:   12.949819397870588\n",
      "Pure loss: 13.688716755776522.....Total loss: 13.688716755776522\n",
      "Pure loss: 13.038776968447074.....Total loss: 13.038776968447074\n",
      "epoch 4773 learning rate:  0.010209511837418814   Training loss:   13.688716755776522  Valing loss:   13.038776968447074\n",
      "Pure loss: 13.931783395783684.....Total loss: 13.931783395783684\n",
      "Pure loss: 13.503113025422646.....Total loss: 13.503113025422646\n",
      "epoch 4774 learning rate:  0.010209467951403435   Training loss:   13.931783395783684  Valing loss:   13.503113025422646\n",
      "Pure loss: 13.718304049933975.....Total loss: 13.718304049933975\n",
      "Pure loss: 13.16092229760772.....Total loss: 13.16092229760772\n",
      "epoch 4775 learning rate:  0.010209424083769634   Training loss:   13.718304049933975  Valing loss:   13.16092229760772\n",
      "Pure loss: 13.683024657306655.....Total loss: 13.683024657306655\n",
      "Pure loss: 13.094286181196669.....Total loss: 13.094286181196669\n",
      "epoch 4776 learning rate:  0.010209380234505863   Training loss:   13.683024657306655  Valing loss:   13.094286181196669\n",
      "Pure loss: 13.748139498294742.....Total loss: 13.748139498294742\n",
      "Pure loss: 13.225332005175902.....Total loss: 13.225332005175902\n",
      "epoch 4777 learning rate:  0.010209336403600586   Training loss:   13.748139498294742  Valing loss:   13.225332005175902\n",
      "Pure loss: 13.671658832498347.....Total loss: 13.671658832498347\n",
      "Pure loss: 13.095530347777194.....Total loss: 13.095530347777194\n",
      "epoch 4778 learning rate:  0.010209292591042278   Training loss:   13.671658832498347  Valing loss:   13.095530347777194\n",
      "Pure loss: 13.640848562562898.....Total loss: 13.640848562562898\n",
      "Pure loss: 13.019234825710681.....Total loss: 13.019234825710681\n",
      "epoch 4779 learning rate:  0.010209248796819418   Training loss:   13.640848562562898  Valing loss:   13.019234825710681\n",
      "Pure loss: 13.621497448316028.....Total loss: 13.621497448316028\n",
      "Pure loss: 12.954490706047219.....Total loss: 12.954490706047219\n",
      "epoch 4780 learning rate:  0.010209205020920502   Training loss:   13.621497448316028  Valing loss:   12.954490706047219\n",
      "Pure loss: 13.570925810949419.....Total loss: 13.570925810949419\n",
      "Pure loss: 12.827285767861962.....Total loss: 12.827285767861962\n",
      "epoch 4781 learning rate:  0.010209161263334031   Training loss:   13.570925810949419  Valing loss:   12.827285767861962\n",
      "Pure loss: 13.583465417547597.....Total loss: 13.583465417547597\n",
      "Pure loss: 12.870782382562595.....Total loss: 12.870782382562595\n",
      "epoch 4782 learning rate:  0.010209117524048515   Training loss:   13.583465417547597  Valing loss:   12.870782382562595\n",
      "Pure loss: 13.603871207928705.....Total loss: 13.603871207928705\n",
      "Pure loss: 12.933638306363001.....Total loss: 12.933638306363001\n",
      "epoch 4783 learning rate:  0.010209073803052478   Training loss:   13.603871207928705  Valing loss:   12.933638306363001\n",
      "Pure loss: 13.56285117569661.....Total loss: 13.56285117569661\n",
      "Pure loss: 12.80589934009078.....Total loss: 12.80589934009078\n",
      "epoch 4784 learning rate:  0.010209030100334448   Training loss:   13.56285117569661  Valing loss:   12.80589934009078\n",
      "Pure loss: 13.524287983477551.....Total loss: 13.524287983477551\n",
      "Pure loss: 12.627703472759922.....Total loss: 12.627703472759922\n",
      "epoch 4785 learning rate:  0.010208986415882968   Training loss:   13.524287983477551  Valing loss:   12.627703472759922\n",
      "Pure loss: 13.528725899938259.....Total loss: 13.528725899938259\n",
      "Pure loss: 12.660151946226168.....Total loss: 12.660151946226168\n",
      "epoch 4786 learning rate:  0.010208942749686586   Training loss:   13.528725899938259  Valing loss:   12.660151946226168\n",
      "Pure loss: 13.538453975954441.....Total loss: 13.538453975954441\n",
      "Pure loss: 12.577495663259329.....Total loss: 12.577495663259329\n",
      "epoch 4787 learning rate:  0.010208899101733863   Training loss:   13.538453975954441  Valing loss:   12.577495663259329\n",
      "Pure loss: 13.61754307107843.....Total loss: 13.61754307107843\n",
      "Pure loss: 12.62024783077414.....Total loss: 12.62024783077414\n",
      "epoch 4788 learning rate:  0.010208855472013367   Training loss:   13.61754307107843  Valing loss:   12.62024783077414\n",
      "Pure loss: 13.62242314640331.....Total loss: 13.62242314640331\n",
      "Pure loss: 12.6065227598931.....Total loss: 12.6065227598931\n",
      "epoch 4789 learning rate:  0.010208811860513678   Training loss:   13.62242314640331  Valing loss:   12.6065227598931\n",
      "Pure loss: 13.585451921829575.....Total loss: 13.585451921829575\n",
      "Pure loss: 12.67836097923214.....Total loss: 12.67836097923214\n",
      "epoch 4790 learning rate:  0.010208768267223382   Training loss:   13.585451921829575  Valing loss:   12.67836097923214\n",
      "Pure loss: 13.536627317341503.....Total loss: 13.536627317341503\n",
      "Pure loss: 12.64613624478967.....Total loss: 12.64613624478967\n",
      "epoch 4791 learning rate:  0.010208724692131079   Training loss:   13.536627317341503  Valing loss:   12.64613624478967\n",
      "Pure loss: 13.519246488972582.....Total loss: 13.519246488972582\n",
      "Pure loss: 12.638287239176243.....Total loss: 12.638287239176243\n",
      "epoch 4792 learning rate:  0.010208681135225375   Training loss:   13.519246488972582  Valing loss:   12.638287239176243\n",
      "Pure loss: 13.537896769453804.....Total loss: 13.537896769453804\n",
      "Pure loss: 12.744941955963673.....Total loss: 12.744941955963673\n",
      "epoch 4793 learning rate:  0.010208637596494888   Training loss:   13.537896769453804  Valing loss:   12.744941955963673\n",
      "Pure loss: 13.534574542114468.....Total loss: 13.534574542114468\n",
      "Pure loss: 12.566648913072964.....Total loss: 12.566648913072964\n",
      "epoch 4794 learning rate:  0.010208594075928244   Training loss:   13.534574542114468  Valing loss:   12.566648913072964\n",
      "Pure loss: 13.54631874101572.....Total loss: 13.54631874101572\n",
      "Pure loss: 12.63288320836209.....Total loss: 12.63288320836209\n",
      "epoch 4795 learning rate:  0.010208550573514077   Training loss:   13.54631874101572  Valing loss:   12.63288320836209\n",
      "Pure loss: 13.528872304457519.....Total loss: 13.528872304457519\n",
      "Pure loss: 12.695791807465334.....Total loss: 12.695791807465334\n",
      "epoch 4796 learning rate:  0.010208507089241035   Training loss:   13.528872304457519  Valing loss:   12.695791807465334\n",
      "Pure loss: 13.533551880839301.....Total loss: 13.533551880839301\n",
      "Pure loss: 12.589497378814992.....Total loss: 12.589497378814992\n",
      "epoch 4797 learning rate:  0.010208463623097769   Training loss:   13.533551880839301  Valing loss:   12.589497378814992\n",
      "Pure loss: 13.692980961811653.....Total loss: 13.692980961811653\n",
      "Pure loss: 13.173571571713728.....Total loss: 13.173571571713728\n",
      "epoch 4798 learning rate:  0.010208420175072947   Training loss:   13.692980961811653  Valing loss:   13.173571571713728\n",
      "Pure loss: 13.677556648041532.....Total loss: 13.677556648041532\n",
      "Pure loss: 13.140624715944671.....Total loss: 13.140624715944671\n",
      "epoch 4799 learning rate:  0.010208376745155241   Training loss:   13.677556648041532  Valing loss:   13.140624715944671\n",
      "Pure loss: 13.770602172429374.....Total loss: 13.770602172429374\n",
      "Pure loss: 13.344927262386394.....Total loss: 13.344927262386394\n",
      "epoch 4800 learning rate:  0.010208333333333333   Training loss:   13.770602172429374  Valing loss:   13.344927262386394\n",
      "Pure loss: 13.68183618340012.....Total loss: 13.68183618340012\n",
      "Pure loss: 13.16162254099105.....Total loss: 13.16162254099105\n",
      "epoch 4801 learning rate:  0.010208289939595917   Training loss:   13.68183618340012  Valing loss:   13.16162254099105\n",
      "Pure loss: 13.720750908001062.....Total loss: 13.720750908001062\n",
      "Pure loss: 13.239299148863221.....Total loss: 13.239299148863221\n",
      "epoch 4802 learning rate:  0.010208246563931695   Training loss:   13.720750908001062  Valing loss:   13.239299148863221\n",
      "Pure loss: 13.554447843287944.....Total loss: 13.554447843287944\n",
      "Pure loss: 12.831358941703629.....Total loss: 12.831358941703629\n",
      "epoch 4803 learning rate:  0.010208203206329378   Training loss:   13.554447843287944  Valing loss:   12.831358941703629\n",
      "Pure loss: 13.696859720565374.....Total loss: 13.696859720565374\n",
      "Pure loss: 13.154265161797895.....Total loss: 13.154265161797895\n",
      "epoch 4804 learning rate:  0.010208159866777686   Training loss:   13.696859720565374  Valing loss:   13.154265161797895\n",
      "Pure loss: 13.855070518067523.....Total loss: 13.855070518067523\n",
      "Pure loss: 13.433237659533866.....Total loss: 13.433237659533866\n",
      "epoch 4805 learning rate:  0.010208116545265348   Training loss:   13.855070518067523  Valing loss:   13.433237659533866\n",
      "Pure loss: 13.73836678644023.....Total loss: 13.73836678644023\n",
      "Pure loss: 13.240009291292647.....Total loss: 13.240009291292647\n",
      "epoch 4806 learning rate:  0.010208073241781107   Training loss:   13.73836678644023  Valing loss:   13.240009291292647\n",
      "Pure loss: 13.769360984327392.....Total loss: 13.769360984327392\n",
      "Pure loss: 13.298932054956307.....Total loss: 13.298932054956307\n",
      "epoch 4807 learning rate:  0.01020802995631371   Training loss:   13.769360984327392  Valing loss:   13.298932054956307\n",
      "Pure loss: 13.848233954425817.....Total loss: 13.848233954425817\n",
      "Pure loss: 13.443295118406942.....Total loss: 13.443295118406942\n",
      "epoch 4808 learning rate:  0.010207986688851914   Training loss:   13.848233954425817  Valing loss:   13.443295118406942\n",
      "Pure loss: 13.81672776193125.....Total loss: 13.81672776193125\n",
      "Pure loss: 13.391062422073174.....Total loss: 13.391062422073174\n",
      "epoch 4809 learning rate:  0.010207943439384488   Training loss:   13.81672776193125  Valing loss:   13.391062422073174\n",
      "Pure loss: 13.826354499803262.....Total loss: 13.826354499803262\n",
      "Pure loss: 13.408115063703889.....Total loss: 13.408115063703889\n",
      "epoch 4810 learning rate:  0.010207900207900208   Training loss:   13.826354499803262  Valing loss:   13.408115063703889\n",
      "Pure loss: 13.778571331438386.....Total loss: 13.778571331438386\n",
      "Pure loss: 13.319955800855405.....Total loss: 13.319955800855405\n",
      "epoch 4811 learning rate:  0.010207856994387862   Training loss:   13.778571331438386  Valing loss:   13.319955800855405\n",
      "Pure loss: 13.591114782939696.....Total loss: 13.591114782939696\n",
      "Pure loss: 12.961783466520412.....Total loss: 12.961783466520412\n",
      "epoch 4812 learning rate:  0.010207813798836243   Training loss:   13.591114782939696  Valing loss:   12.961783466520412\n",
      "Pure loss: 13.771619644703215.....Total loss: 13.771619644703215\n",
      "Pure loss: 13.36949225056503.....Total loss: 13.36949225056503\n",
      "epoch 4813 learning rate:  0.010207770621234158   Training loss:   13.771619644703215  Valing loss:   13.36949225056503\n",
      "Pure loss: 13.847085356164797.....Total loss: 13.847085356164797\n",
      "Pure loss: 13.506998248682311.....Total loss: 13.506998248682311\n",
      "epoch 4814 learning rate:  0.01020772746157042   Training loss:   13.847085356164797  Valing loss:   13.506998248682311\n",
      "Pure loss: 13.80383077232736.....Total loss: 13.80383077232736\n",
      "Pure loss: 13.428590967544942.....Total loss: 13.428590967544942\n",
      "epoch 4815 learning rate:  0.010207684319833852   Training loss:   13.80383077232736  Valing loss:   13.428590967544942\n",
      "Pure loss: 13.733435943476527.....Total loss: 13.733435943476527\n",
      "Pure loss: 13.2903812158124.....Total loss: 13.2903812158124\n",
      "epoch 4816 learning rate:  0.010207641196013289   Training loss:   13.733435943476527  Valing loss:   13.2903812158124\n",
      "Pure loss: 13.588600317327112.....Total loss: 13.588600317327112\n",
      "Pure loss: 12.979315324555925.....Total loss: 12.979315324555925\n",
      "epoch 4817 learning rate:  0.010207598090097572   Training loss:   13.588600317327112  Valing loss:   12.979315324555925\n",
      "Pure loss: 13.637630263882967.....Total loss: 13.637630263882967\n",
      "Pure loss: 13.09375354648814.....Total loss: 13.09375354648814\n",
      "epoch 4818 learning rate:  0.01020755500207555   Training loss:   13.637630263882967  Valing loss:   13.09375354648814\n",
      "Pure loss: 13.567521189475409.....Total loss: 13.567521189475409\n",
      "Pure loss: 12.93110350851089.....Total loss: 12.93110350851089\n",
      "epoch 4819 learning rate:  0.010207511931936086   Training loss:   13.567521189475409  Valing loss:   12.93110350851089\n",
      "Pure loss: 13.490456596756234.....Total loss: 13.490456596756234\n",
      "Pure loss: 12.713094558222458.....Total loss: 12.713094558222458\n",
      "epoch 4820 learning rate:  0.01020746887966805   Training loss:   13.490456596756234  Valing loss:   12.713094558222458\n",
      "Pure loss: 13.483729315761034.....Total loss: 13.483729315761034\n",
      "Pure loss: 12.689717388551951.....Total loss: 12.689717388551951\n",
      "epoch 4821 learning rate:  0.01020742584526032   Training loss:   13.483729315761034  Valing loss:   12.689717388551951\n",
      "Pure loss: 13.547949668492361.....Total loss: 13.547949668492361\n",
      "Pure loss: 12.876832986050744.....Total loss: 12.876832986050744\n",
      "epoch 4822 learning rate:  0.010207382828701783   Training loss:   13.547949668492361  Valing loss:   12.876832986050744\n",
      "Pure loss: 13.657611587171589.....Total loss: 13.657611587171589\n",
      "Pure loss: 13.154026318276673.....Total loss: 13.154026318276673\n",
      "epoch 4823 learning rate:  0.01020733982998134   Training loss:   13.657611587171589  Valing loss:   13.154026318276673\n",
      "Pure loss: 13.668301961593096.....Total loss: 13.668301961593096\n",
      "Pure loss: 13.16651499809419.....Total loss: 13.16651499809419\n",
      "epoch 4824 learning rate:  0.010207296849087895   Training loss:   13.668301961593096  Valing loss:   13.16651499809419\n",
      "Pure loss: 13.741440344319845.....Total loss: 13.741440344319845\n",
      "Pure loss: 13.310986954372435.....Total loss: 13.310986954372435\n",
      "epoch 4825 learning rate:  0.010207253886010364   Training loss:   13.741440344319845  Valing loss:   13.310986954372435\n",
      "Pure loss: 13.529970238473028.....Total loss: 13.529970238473028\n",
      "Pure loss: 12.840258819747351.....Total loss: 12.840258819747351\n",
      "epoch 4826 learning rate:  0.010207210940737671   Training loss:   13.529970238473028  Valing loss:   12.840258819747351\n",
      "Pure loss: 13.507517898077575.....Total loss: 13.507517898077575\n",
      "Pure loss: 12.770523366116297.....Total loss: 12.770523366116297\n",
      "epoch 4827 learning rate:  0.010207168013258753   Training loss:   13.507517898077575  Valing loss:   12.770523366116297\n",
      "Pure loss: 13.548741631523372.....Total loss: 13.548741631523372\n",
      "Pure loss: 12.883431406646446.....Total loss: 12.883431406646446\n",
      "epoch 4828 learning rate:  0.010207125103562552   Training loss:   13.548741631523372  Valing loss:   12.883431406646446\n",
      "Pure loss: 13.49620517369543.....Total loss: 13.49620517369543\n",
      "Pure loss: 12.70508517137664.....Total loss: 12.70508517137664\n",
      "epoch 4829 learning rate:  0.01020708221163802   Training loss:   13.49620517369543  Valing loss:   12.70508517137664\n",
      "Pure loss: 13.468357642587073.....Total loss: 13.468357642587073\n",
      "Pure loss: 12.454182313725985.....Total loss: 12.454182313725985\n",
      "epoch 4830 learning rate:  0.010207039337474121   Training loss:   13.468357642587073  Valing loss:   12.454182313725985\n",
      "Pure loss: 13.625153516634592.....Total loss: 13.625153516634592\n",
      "Pure loss: 12.332214204878946.....Total loss: 12.332214204878946\n",
      "epoch 4831 learning rate:  0.010206996481059822   Training loss:   13.625153516634592  Valing loss:   12.332214204878946\n",
      "Pure loss: 13.849772654645184.....Total loss: 13.849772654645184\n",
      "Pure loss: 12.272852396613292.....Total loss: 12.272852396613292\n",
      "epoch 4832 learning rate:  0.010206953642384105   Training loss:   13.849772654645184  Valing loss:   12.272852396613292\n",
      "Pure loss: 13.800855723385977.....Total loss: 13.800855723385977\n",
      "Pure loss: 12.254602051692517.....Total loss: 12.254602051692517\n",
      "epoch 4833 learning rate:  0.01020691082143596   Training loss:   13.800855723385977  Valing loss:   12.254602051692517\n",
      "Pure loss: 13.601436929679721.....Total loss: 13.601436929679721\n",
      "Pure loss: 12.209359261435784.....Total loss: 12.209359261435784\n",
      "epoch 4834 learning rate:  0.010206868018204385   Training loss:   13.601436929679721  Valing loss:   12.209359261435784\n",
      "Pure loss: 13.706781707286707.....Total loss: 13.706781707286707\n",
      "Pure loss: 12.205181193104998.....Total loss: 12.205181193104998\n",
      "epoch 4835 learning rate:  0.010206825232678386   Training loss:   13.706781707286707  Valing loss:   12.205181193104998\n",
      "Pure loss: 13.710348592087161.....Total loss: 13.710348592087161\n",
      "Pure loss: 12.206134746209413.....Total loss: 12.206134746209413\n",
      "epoch 4836 learning rate:  0.010206782464846981   Training loss:   13.710348592087161  Valing loss:   12.206134746209413\n",
      "Pure loss: 13.76933428143038.....Total loss: 13.76933428143038\n",
      "Pure loss: 12.227455551987328.....Total loss: 12.227455551987328\n",
      "epoch 4837 learning rate:  0.010206739714699193   Training loss:   13.76933428143038  Valing loss:   12.227455551987328\n",
      "Pure loss: 13.454400942464796.....Total loss: 13.454400942464796\n",
      "Pure loss: 12.275562045251174.....Total loss: 12.275562045251174\n",
      "epoch 4838 learning rate:  0.01020669698222406   Training loss:   13.454400942464796  Valing loss:   12.275562045251174\n",
      "Pure loss: 13.43852320857875.....Total loss: 13.43852320857875\n",
      "Pure loss: 12.306364988988777.....Total loss: 12.306364988988777\n",
      "epoch 4839 learning rate:  0.010206654267410622   Training loss:   13.43852320857875  Valing loss:   12.306364988988777\n",
      "Pure loss: 13.47400342920692.....Total loss: 13.47400342920692\n",
      "Pure loss: 12.303261018695375.....Total loss: 12.303261018695375\n",
      "epoch 4840 learning rate:  0.010206611570247935   Training loss:   13.47400342920692  Valing loss:   12.303261018695375\n",
      "Pure loss: 13.496369998150806.....Total loss: 13.496369998150806\n",
      "Pure loss: 12.28956062829133.....Total loss: 12.28956062829133\n",
      "epoch 4841 learning rate:  0.010206568890725057   Training loss:   13.496369998150806  Valing loss:   12.28956062829133\n",
      "Pure loss: 13.494152969048315.....Total loss: 13.494152969048315\n",
      "Pure loss: 12.294633512933832.....Total loss: 12.294633512933832\n",
      "epoch 4842 learning rate:  0.010206526228831062   Training loss:   13.494152969048315  Valing loss:   12.294633512933832\n",
      "Pure loss: 13.41726175861672.....Total loss: 13.41726175861672\n",
      "Pure loss: 12.325954552690515.....Total loss: 12.325954552690515\n",
      "epoch 4843 learning rate:  0.010206483584555028   Training loss:   13.41726175861672  Valing loss:   12.325954552690515\n",
      "Pure loss: 13.41120339209305.....Total loss: 13.41120339209305\n",
      "Pure loss: 12.362251930877083.....Total loss: 12.362251930877083\n",
      "epoch 4844 learning rate:  0.010206440957886045   Training loss:   13.41120339209305  Valing loss:   12.362251930877083\n",
      "Pure loss: 13.436444522582407.....Total loss: 13.436444522582407\n",
      "Pure loss: 12.307484493370817.....Total loss: 12.307484493370817\n",
      "epoch 4845 learning rate:  0.01020639834881321   Training loss:   13.436444522582407  Valing loss:   12.307484493370817\n",
      "Pure loss: 13.488362743292976.....Total loss: 13.488362743292976\n",
      "Pure loss: 12.224507459681337.....Total loss: 12.224507459681337\n",
      "epoch 4846 learning rate:  0.01020635575732563   Training loss:   13.488362743292976  Valing loss:   12.224507459681337\n",
      "Pure loss: 13.520897180201471.....Total loss: 13.520897180201471\n",
      "Pure loss: 12.228047022783949.....Total loss: 12.228047022783949\n",
      "epoch 4847 learning rate:  0.01020631318341242   Training loss:   13.520897180201471  Valing loss:   12.228047022783949\n",
      "Pure loss: 13.529861955920026.....Total loss: 13.529861955920026\n",
      "Pure loss: 12.226290357122819.....Total loss: 12.226290357122819\n",
      "epoch 4848 learning rate:  0.010206270627062707   Training loss:   13.529861955920026  Valing loss:   12.226290357122819\n",
      "Pure loss: 13.598839973249392.....Total loss: 13.598839973249392\n",
      "Pure loss: 12.270124376983246.....Total loss: 12.270124376983246\n",
      "epoch 4849 learning rate:  0.010206228088265622   Training loss:   13.598839973249392  Valing loss:   12.270124376983246\n",
      "Pure loss: 13.546462347065834.....Total loss: 13.546462347065834\n",
      "Pure loss: 12.302250759646183.....Total loss: 12.302250759646183\n",
      "epoch 4850 learning rate:  0.01020618556701031   Training loss:   13.546462347065834  Valing loss:   12.302250759646183\n",
      "Pure loss: 13.442271988396515.....Total loss: 13.442271988396515\n",
      "Pure loss: 12.264734505996934.....Total loss: 12.264734505996934\n",
      "epoch 4851 learning rate:  0.01020614306328592   Training loss:   13.442271988396515  Valing loss:   12.264734505996934\n",
      "Pure loss: 13.483340076838608.....Total loss: 13.483340076838608\n",
      "Pure loss: 12.806633238637568.....Total loss: 12.806633238637568\n",
      "epoch 4852 learning rate:  0.010206100577081616   Training loss:   13.483340076838608  Valing loss:   12.806633238637568\n",
      "Pure loss: 13.399409071332435.....Total loss: 13.399409071332435\n",
      "Pure loss: 12.501412716242283.....Total loss: 12.501412716242283\n",
      "epoch 4853 learning rate:  0.010206058108386566   Training loss:   13.399409071332435  Valing loss:   12.501412716242283\n",
      "Pure loss: 13.420769678519404.....Total loss: 13.420769678519404\n",
      "Pure loss: 12.619732494216578.....Total loss: 12.619732494216578\n",
      "epoch 4854 learning rate:  0.010206015657189947   Training loss:   13.420769678519404  Valing loss:   12.619732494216578\n",
      "Pure loss: 13.384217362633578.....Total loss: 13.384217362633578\n",
      "Pure loss: 12.368379933862304.....Total loss: 12.368379933862304\n",
      "epoch 4855 learning rate:  0.010205973223480947   Training loss:   13.384217362633578  Valing loss:   12.368379933862304\n",
      "Pure loss: 13.391478060449604.....Total loss: 13.391478060449604\n",
      "Pure loss: 12.454550867299249.....Total loss: 12.454550867299249\n",
      "epoch 4856 learning rate:  0.010205930807248765   Training loss:   13.391478060449604  Valing loss:   12.454550867299249\n",
      "Pure loss: 13.39952318398389.....Total loss: 13.39952318398389\n",
      "Pure loss: 12.473862127942214.....Total loss: 12.473862127942214\n",
      "epoch 4857 learning rate:  0.010205888408482603   Training loss:   13.39952318398389  Valing loss:   12.473862127942214\n",
      "Pure loss: 13.793213931339093.....Total loss: 13.793213931339093\n",
      "Pure loss: 13.308393727135348.....Total loss: 13.308393727135348\n",
      "epoch 4858 learning rate:  0.010205846027171676   Training loss:   13.793213931339093  Valing loss:   13.308393727135348\n",
      "Pure loss: 13.847642163848947.....Total loss: 13.847642163848947\n",
      "Pure loss: 13.412182042319936.....Total loss: 13.412182042319936\n",
      "epoch 4859 learning rate:  0.010205803663305207   Training loss:   13.847642163848947  Valing loss:   13.412182042319936\n",
      "Pure loss: 13.763693889612433.....Total loss: 13.763693889612433\n",
      "Pure loss: 13.266606684933.....Total loss: 13.266606684933\n",
      "epoch 4860 learning rate:  0.010205761316872428   Training loss:   13.763693889612433  Valing loss:   13.266606684933\n",
      "Pure loss: 13.692133611055262.....Total loss: 13.692133611055262\n",
      "Pure loss: 13.141130524643327.....Total loss: 13.141130524643327\n",
      "epoch 4861 learning rate:  0.01020571898786258   Training loss:   13.692133611055262  Valing loss:   13.141130524643327\n",
      "Pure loss: 13.735285025066293.....Total loss: 13.735285025066293\n",
      "Pure loss: 13.231961734241475.....Total loss: 13.231961734241475\n",
      "epoch 4862 learning rate:  0.010205676676264912   Training loss:   13.735285025066293  Valing loss:   13.231961734241475\n",
      "Pure loss: 13.67436815742915.....Total loss: 13.67436815742915\n",
      "Pure loss: 13.131986863841364.....Total loss: 13.131986863841364\n",
      "epoch 4863 learning rate:  0.010205634382068682   Training loss:   13.67436815742915  Valing loss:   13.131986863841364\n",
      "Pure loss: 13.648918649864138.....Total loss: 13.648918649864138\n",
      "Pure loss: 13.088359072946965.....Total loss: 13.088359072946965\n",
      "epoch 4864 learning rate:  0.010205592105263159   Training loss:   13.648918649864138  Valing loss:   13.088359072946965\n",
      "Pure loss: 13.639300161474411.....Total loss: 13.639300161474411\n",
      "Pure loss: 13.071435611504917.....Total loss: 13.071435611504917\n",
      "epoch 4865 learning rate:  0.010205549845837616   Training loss:   13.639300161474411  Valing loss:   13.071435611504917\n",
      "Pure loss: 13.685129657457052.....Total loss: 13.685129657457052\n",
      "Pure loss: 13.174397934116348.....Total loss: 13.174397934116348\n",
      "epoch 4866 learning rate:  0.01020550760378134   Training loss:   13.685129657457052  Valing loss:   13.174397934116348\n",
      "Pure loss: 13.581374525637097.....Total loss: 13.581374525637097\n",
      "Pure loss: 12.98501338191918.....Total loss: 12.98501338191918\n",
      "epoch 4867 learning rate:  0.010205465379083624   Training loss:   13.581374525637097  Valing loss:   12.98501338191918\n",
      "Pure loss: 13.434826886779875.....Total loss: 13.434826886779875\n",
      "Pure loss: 12.671267990109165.....Total loss: 12.671267990109165\n",
      "epoch 4868 learning rate:  0.010205423171733771   Training loss:   13.434826886779875  Valing loss:   12.671267990109165\n",
      "Pure loss: 13.398479045463695.....Total loss: 13.398479045463695\n",
      "Pure loss: 12.304669970639152.....Total loss: 12.304669970639152\n",
      "epoch 4869 learning rate:  0.010205380981721093   Training loss:   13.398479045463695  Valing loss:   12.304669970639152\n",
      "Pure loss: 13.449887821504584.....Total loss: 13.449887821504584\n",
      "Pure loss: 12.200977040703657.....Total loss: 12.200977040703657\n",
      "epoch 4870 learning rate:  0.010205338809034908   Training loss:   13.449887821504584  Valing loss:   12.200977040703657\n",
      "Pure loss: 13.448299496369625.....Total loss: 13.448299496369625\n",
      "Pure loss: 12.202837816051671.....Total loss: 12.202837816051671\n",
      "epoch 4871 learning rate:  0.010205296653664545   Training loss:   13.448299496369625  Valing loss:   12.202837816051671\n",
      "Pure loss: 13.524452326639404.....Total loss: 13.524452326639404\n",
      "Pure loss: 12.203664374634853.....Total loss: 12.203664374634853\n",
      "epoch 4872 learning rate:  0.010205254515599343   Training loss:   13.524452326639404  Valing loss:   12.203664374634853\n",
      "Pure loss: 13.423842477751952.....Total loss: 13.423842477751952\n",
      "Pure loss: 12.273004468746013.....Total loss: 12.273004468746013\n",
      "epoch 4873 learning rate:  0.010205212394828647   Training loss:   13.423842477751952  Valing loss:   12.273004468746013\n",
      "Pure loss: 13.516235489734276.....Total loss: 13.516235489734276\n",
      "Pure loss: 12.22228315717971.....Total loss: 12.22228315717971\n",
      "epoch 4874 learning rate:  0.010205170291341815   Training loss:   13.516235489734276  Valing loss:   12.22228315717971\n",
      "Pure loss: 13.53704327285006.....Total loss: 13.53704327285006\n",
      "Pure loss: 12.209817434789219.....Total loss: 12.209817434789219\n",
      "epoch 4875 learning rate:  0.010205128205128205   Training loss:   13.53704327285006  Valing loss:   12.209817434789219\n",
      "Pure loss: 13.573569052102295.....Total loss: 13.573569052102295\n",
      "Pure loss: 12.214389287948345.....Total loss: 12.214389287948345\n",
      "epoch 4876 learning rate:  0.010205086136177195   Training loss:   13.573569052102295  Valing loss:   12.214389287948345\n",
      "Pure loss: 13.53811265152526.....Total loss: 13.53811265152526\n",
      "Pure loss: 12.093243860887057.....Total loss: 12.093243860887057\n",
      "epoch 4877 learning rate:  0.010205044084478162   Training loss:   13.53811265152526  Valing loss:   12.093243860887057\n",
      "Pure loss: 13.490369527768028.....Total loss: 13.490369527768028\n",
      "Pure loss: 12.18240257065981.....Total loss: 12.18240257065981\n",
      "epoch 4878 learning rate:  0.010205002050020501   Training loss:   13.490369527768028  Valing loss:   12.18240257065981\n",
      "Pure loss: 13.47406505047553.....Total loss: 13.47406505047553\n",
      "Pure loss: 12.268465845257321.....Total loss: 12.268465845257321\n",
      "epoch 4879 learning rate:  0.010204960032793605   Training loss:   13.47406505047553  Valing loss:   12.268465845257321\n",
      "Pure loss: 13.486136282878057.....Total loss: 13.486136282878057\n",
      "Pure loss: 12.205651796601408.....Total loss: 12.205651796601408\n",
      "epoch 4880 learning rate:  0.010204918032786885   Training loss:   13.486136282878057  Valing loss:   12.205651796601408\n",
      "Pure loss: 13.479120463920076.....Total loss: 13.479120463920076\n",
      "Pure loss: 12.251348003077828.....Total loss: 12.251348003077828\n",
      "epoch 4881 learning rate:  0.010204876049989757   Training loss:   13.479120463920076  Valing loss:   12.251348003077828\n",
      "Pure loss: 13.574473175900303.....Total loss: 13.574473175900303\n",
      "Pure loss: 12.08873495582679.....Total loss: 12.08873495582679\n",
      "epoch 4882 learning rate:  0.010204834084391642   Training loss:   13.574473175900303  Valing loss:   12.08873495582679\n",
      "Pure loss: 13.573986320265705.....Total loss: 13.573986320265705\n",
      "Pure loss: 12.089102455102887.....Total loss: 12.089102455102887\n",
      "epoch 4883 learning rate:  0.010204792135981978   Training loss:   13.573986320265705  Valing loss:   12.089102455102887\n",
      "Pure loss: 13.495047152465812.....Total loss: 13.495047152465812\n",
      "Pure loss: 12.197783929458154.....Total loss: 12.197783929458154\n",
      "epoch 4884 learning rate:  0.010204750204750205   Training loss:   13.495047152465812  Valing loss:   12.197783929458154\n",
      "Pure loss: 13.52756256105783.....Total loss: 13.52756256105783\n",
      "Pure loss: 12.136700661366854.....Total loss: 12.136700661366854\n",
      "epoch 4885 learning rate:  0.010204708290685772   Training loss:   13.52756256105783  Valing loss:   12.136700661366854\n",
      "Pure loss: 13.695573809005037.....Total loss: 13.695573809005037\n",
      "Pure loss: 12.059883428274434.....Total loss: 12.059883428274434\n",
      "epoch 4886 learning rate:  0.010204666393778141   Training loss:   13.695573809005037  Valing loss:   12.059883428274434\n",
      "Pure loss: 13.66937918806281.....Total loss: 13.66937918806281\n",
      "Pure loss: 12.066114788737941.....Total loss: 12.066114788737941\n",
      "epoch 4887 learning rate:  0.01020462451401678   Training loss:   13.66937918806281  Valing loss:   12.066114788737941\n",
      "Pure loss: 13.845116496987199.....Total loss: 13.845116496987199\n",
      "Pure loss: 12.075282921597825.....Total loss: 12.075282921597825\n",
      "epoch 4888 learning rate:  0.010204582651391162   Training loss:   13.845116496987199  Valing loss:   12.075282921597825\n",
      "Pure loss: 13.422837809149538.....Total loss: 13.422837809149538\n",
      "Pure loss: 12.470147268925242.....Total loss: 12.470147268925242\n",
      "epoch 4889 learning rate:  0.010204540805890776   Training loss:   13.422837809149538  Valing loss:   12.470147268925242\n",
      "Pure loss: 13.394895691611248.....Total loss: 13.394895691611248\n",
      "Pure loss: 12.381793753542858.....Total loss: 12.381793753542858\n",
      "epoch 4890 learning rate:  0.010204498977505113   Training loss:   13.394895691611248  Valing loss:   12.381793753542858\n",
      "Pure loss: 13.377560318085013.....Total loss: 13.377560318085013\n",
      "Pure loss: 12.311024361341305.....Total loss: 12.311024361341305\n",
      "epoch 4891 learning rate:  0.010204457166223676   Training loss:   13.377560318085013  Valing loss:   12.311024361341305\n",
      "Pure loss: 13.407414699604656.....Total loss: 13.407414699604656\n",
      "Pure loss: 12.175682046359084.....Total loss: 12.175682046359084\n",
      "epoch 4892 learning rate:  0.010204415372035978   Training loss:   13.407414699604656  Valing loss:   12.175682046359084\n",
      "Pure loss: 13.397290972122509.....Total loss: 13.397290972122509\n",
      "Pure loss: 12.208363919075161.....Total loss: 12.208363919075161\n",
      "epoch 4893 learning rate:  0.010204373594931536   Training loss:   13.397290972122509  Valing loss:   12.208363919075161\n",
      "Pure loss: 13.385944582244958.....Total loss: 13.385944582244958\n",
      "Pure loss: 12.257856709094595.....Total loss: 12.257856709094595\n",
      "epoch 4894 learning rate:  0.010204331834899877   Training loss:   13.385944582244958  Valing loss:   12.257856709094595\n",
      "Pure loss: 13.3868229420088.....Total loss: 13.3868229420088\n",
      "Pure loss: 12.251087315701731.....Total loss: 12.251087315701731\n",
      "epoch 4895 learning rate:  0.01020429009193054   Training loss:   13.3868229420088  Valing loss:   12.251087315701731\n",
      "Pure loss: 13.388146212301525.....Total loss: 13.388146212301525\n",
      "Pure loss: 12.231750550570215.....Total loss: 12.231750550570215\n",
      "epoch 4896 learning rate:  0.010204248366013071   Training loss:   13.388146212301525  Valing loss:   12.231750550570215\n",
      "Pure loss: 13.546029491189124.....Total loss: 13.546029491189124\n",
      "Pure loss: 12.012605491142999.....Total loss: 12.012605491142999\n",
      "epoch 4897 learning rate:  0.010204206657137022   Training loss:   13.546029491189124  Valing loss:   12.012605491142999\n",
      "Pure loss: 13.614896995491701.....Total loss: 13.614896995491701\n",
      "Pure loss: 12.012563212934108.....Total loss: 12.012563212934108\n",
      "epoch 4898 learning rate:  0.010204164965291956   Training loss:   13.614896995491701  Valing loss:   12.012563212934108\n",
      "Pure loss: 13.545628007599356.....Total loss: 13.545628007599356\n",
      "Pure loss: 12.017877982752037.....Total loss: 12.017877982752037\n",
      "epoch 4899 learning rate:  0.010204123290467442   Training loss:   13.545628007599356  Valing loss:   12.017877982752037\n",
      "Pure loss: 13.539223851129034.....Total loss: 13.539223851129034\n",
      "Pure loss: 12.018791905285152.....Total loss: 12.018791905285152\n",
      "epoch 4900 learning rate:  0.010204081632653062   Training loss:   13.539223851129034  Valing loss:   12.018791905285152\n",
      "Pure loss: 13.425076461596529.....Total loss: 13.425076461596529\n",
      "Pure loss: 12.085490377157322.....Total loss: 12.085490377157322\n",
      "epoch 4901 learning rate:  0.010204039991838401   Training loss:   13.425076461596529  Valing loss:   12.085490377157322\n",
      "Pure loss: 13.462189683468903.....Total loss: 13.462189683468903\n",
      "Pure loss: 12.063473572039477.....Total loss: 12.063473572039477\n",
      "epoch 4902 learning rate:  0.010203998368013056   Training loss:   13.462189683468903  Valing loss:   12.063473572039477\n",
      "Pure loss: 13.453588128147395.....Total loss: 13.453588128147395\n",
      "Pure loss: 12.06729784618043.....Total loss: 12.06729784618043\n",
      "epoch 4903 learning rate:  0.010203956761166633   Training loss:   13.453588128147395  Valing loss:   12.06729784618043\n",
      "Pure loss: 13.53721265664923.....Total loss: 13.53721265664923\n",
      "Pure loss: 12.033731159514968.....Total loss: 12.033731159514968\n",
      "epoch 4904 learning rate:  0.010203915171288745   Training loss:   13.53721265664923  Valing loss:   12.033731159514968\n",
      "Pure loss: 13.336434450057565.....Total loss: 13.336434450057565\n",
      "Pure loss: 12.233448034021864.....Total loss: 12.233448034021864\n",
      "epoch 4905 learning rate:  0.010203873598369011   Training loss:   13.336434450057565  Valing loss:   12.233448034021864\n",
      "Pure loss: 13.319467976248061.....Total loss: 13.319467976248061\n",
      "Pure loss: 12.261520928424543.....Total loss: 12.261520928424543\n",
      "epoch 4906 learning rate:  0.010203832042397065   Training loss:   13.319467976248061  Valing loss:   12.261520928424543\n",
      "Pure loss: 13.313476165897187.....Total loss: 13.313476165897187\n",
      "Pure loss: 12.254436527589215.....Total loss: 12.254436527589215\n",
      "epoch 4907 learning rate:  0.010203790503362543   Training loss:   13.313476165897187  Valing loss:   12.254436527589215\n",
      "Pure loss: 13.425416614160401.....Total loss: 13.425416614160401\n",
      "Pure loss: 12.061013628397363.....Total loss: 12.061013628397363\n",
      "epoch 4908 learning rate:  0.010203748981255093   Training loss:   13.425416614160401  Valing loss:   12.061013628397363\n",
      "Pure loss: 13.750775678222869.....Total loss: 13.750775678222869\n",
      "Pure loss: 12.066629937793708.....Total loss: 12.066629937793708\n",
      "epoch 4909 learning rate:  0.010203707476064371   Training loss:   13.750775678222869  Valing loss:   12.066629937793708\n",
      "Pure loss: 13.61673117483126.....Total loss: 13.61673117483126\n",
      "Pure loss: 12.069304014594186.....Total loss: 12.069304014594186\n",
      "epoch 4910 learning rate:  0.01020366598778004   Training loss:   13.61673117483126  Valing loss:   12.069304014594186\n",
      "Pure loss: 13.507729412898218.....Total loss: 13.507729412898218\n",
      "Pure loss: 12.057061595649857.....Total loss: 12.057061595649857\n",
      "epoch 4911 learning rate:  0.010203624516391775   Training loss:   13.507729412898218  Valing loss:   12.057061595649857\n",
      "Pure loss: 13.491446763790114.....Total loss: 13.491446763790114\n",
      "Pure loss: 12.061891271617153.....Total loss: 12.061891271617153\n",
      "epoch 4912 learning rate:  0.010203583061889251   Training loss:   13.491446763790114  Valing loss:   12.061891271617153\n",
      "Pure loss: 13.54455584958996.....Total loss: 13.54455584958996\n",
      "Pure loss: 12.054203366440884.....Total loss: 12.054203366440884\n",
      "epoch 4913 learning rate:  0.010203541624262161   Training loss:   13.54455584958996  Valing loss:   12.054203366440884\n",
      "Pure loss: 13.499927474931031.....Total loss: 13.499927474931031\n",
      "Pure loss: 12.058903724270676.....Total loss: 12.058903724270676\n",
      "epoch 4914 learning rate:  0.010203500203500204   Training loss:   13.499927474931031  Valing loss:   12.058903724270676\n",
      "Pure loss: 13.727402157894565.....Total loss: 13.727402157894565\n",
      "Pure loss: 12.0528122734913.....Total loss: 12.0528122734913\n",
      "epoch 4915 learning rate:  0.010203458799593083   Training loss:   13.727402157894565  Valing loss:   12.0528122734913\n",
      "Pure loss: 13.605378605226186.....Total loss: 13.605378605226186\n",
      "Pure loss: 12.029958387008794.....Total loss: 12.029958387008794\n",
      "epoch 4916 learning rate:  0.010203417412530513   Training loss:   13.605378605226186  Valing loss:   12.029958387008794\n",
      "Pure loss: 13.89045746611211.....Total loss: 13.89045746611211\n",
      "Pure loss: 12.09499004717897.....Total loss: 12.09499004717897\n",
      "epoch 4917 learning rate:  0.010203376042302217   Training loss:   13.89045746611211  Valing loss:   12.09499004717897\n",
      "Pure loss: 13.862240223955366.....Total loss: 13.862240223955366\n",
      "Pure loss: 12.062398995291916.....Total loss: 12.062398995291916\n",
      "epoch 4918 learning rate:  0.010203334688897926   Training loss:   13.862240223955366  Valing loss:   12.062398995291916\n",
      "Pure loss: 13.801832270151525.....Total loss: 13.801832270151525\n",
      "Pure loss: 12.048174851331746.....Total loss: 12.048174851331746\n",
      "epoch 4919 learning rate:  0.01020329335230738   Training loss:   13.801832270151525  Valing loss:   12.048174851331746\n",
      "Pure loss: 14.250002358867988.....Total loss: 14.250002358867988\n",
      "Pure loss: 12.190136264431798.....Total loss: 12.190136264431798\n",
      "epoch 4920 learning rate:  0.010203252032520326   Training loss:   14.250002358867988  Valing loss:   12.190136264431798\n",
      "Pure loss: 13.851047722265362.....Total loss: 13.851047722265362\n",
      "Pure loss: 12.019200680026248.....Total loss: 12.019200680026248\n",
      "epoch 4921 learning rate:  0.01020321072952652   Training loss:   13.851047722265362  Valing loss:   12.019200680026248\n",
      "Pure loss: 13.449813435659554.....Total loss: 13.449813435659554\n",
      "Pure loss: 12.02962086465133.....Total loss: 12.02962086465133\n",
      "epoch 4922 learning rate:  0.010203169443315726   Training loss:   13.449813435659554  Valing loss:   12.02962086465133\n",
      "Pure loss: 13.448311985058341.....Total loss: 13.448311985058341\n",
      "Pure loss: 12.03324854322784.....Total loss: 12.03324854322784\n",
      "epoch 4923 learning rate:  0.010203128173877717   Training loss:   13.448311985058341  Valing loss:   12.03324854322784\n",
      "Pure loss: 13.559810154290336.....Total loss: 13.559810154290336\n",
      "Pure loss: 11.994012530894205.....Total loss: 11.994012530894205\n",
      "epoch 4924 learning rate:  0.010203086921202274   Training loss:   13.559810154290336  Valing loss:   11.994012530894205\n",
      "Pure loss: 13.506178436370682.....Total loss: 13.506178436370682\n",
      "Pure loss: 12.003816590897838.....Total loss: 12.003816590897838\n",
      "epoch 4925 learning rate:  0.010203045685279188   Training loss:   13.506178436370682  Valing loss:   12.003816590897838\n",
      "Pure loss: 13.336165557237301.....Total loss: 13.336165557237301\n",
      "Pure loss: 12.602096614607863.....Total loss: 12.602096614607863\n",
      "epoch 4926 learning rate:  0.010203004466098254   Training loss:   13.336165557237301  Valing loss:   12.602096614607863\n",
      "Pure loss: 14.012692078899853.....Total loss: 14.012692078899853\n",
      "Pure loss: 13.565488368563145.....Total loss: 13.565488368563145\n",
      "epoch 4927 learning rate:  0.01020296326364928   Training loss:   14.012692078899853  Valing loss:   13.565488368563145\n",
      "Pure loss: 13.922277746701393.....Total loss: 13.922277746701393\n",
      "Pure loss: 13.410832836328034.....Total loss: 13.410832836328034\n",
      "epoch 4928 learning rate:  0.010202922077922078   Training loss:   13.922277746701393  Valing loss:   13.410832836328034\n",
      "Pure loss: 13.816313704181336.....Total loss: 13.816313704181336\n",
      "Pure loss: 13.263818213095911.....Total loss: 13.263818213095911\n",
      "epoch 4929 learning rate:  0.010202880908906472   Training loss:   13.816313704181336  Valing loss:   13.263818213095911\n",
      "Pure loss: 13.867259540991505.....Total loss: 13.867259540991505\n",
      "Pure loss: 13.371654921904428.....Total loss: 13.371654921904428\n",
      "epoch 4930 learning rate:  0.010202839756592292   Training loss:   13.867259540991505  Valing loss:   13.371654921904428\n",
      "Pure loss: 14.072503431718422.....Total loss: 14.072503431718422\n",
      "Pure loss: 13.734254753984994.....Total loss: 13.734254753984994\n",
      "epoch 4931 learning rate:  0.010202798620969377   Training loss:   14.072503431718422  Valing loss:   13.734254753984994\n",
      "Pure loss: 14.62884730032279.....Total loss: 14.62884730032279\n",
      "Pure loss: 14.543688298717136.....Total loss: 14.543688298717136\n",
      "epoch 4932 learning rate:  0.010202757502027576   Training loss:   14.62884730032279  Valing loss:   14.543688298717136\n",
      "Pure loss: 14.928800750588508.....Total loss: 14.928800750588508\n",
      "Pure loss: 14.968514208345953.....Total loss: 14.968514208345953\n",
      "epoch 4933 learning rate:  0.01020271639975674   Training loss:   14.928800750588508  Valing loss:   14.968514208345953\n",
      "Pure loss: 15.023969513166712.....Total loss: 15.023969513166712\n",
      "Pure loss: 15.105387414285827.....Total loss: 15.105387414285827\n",
      "epoch 4934 learning rate:  0.010202675314146738   Training loss:   15.023969513166712  Valing loss:   15.105387414285827\n",
      "Pure loss: 15.15110395170619.....Total loss: 15.15110395170619\n",
      "Pure loss: 15.266211096772214.....Total loss: 15.266211096772214\n",
      "epoch 4935 learning rate:  0.010202634245187437   Training loss:   15.15110395170619  Valing loss:   15.266211096772214\n",
      "Pure loss: 15.61404487390472.....Total loss: 15.61404487390472\n",
      "Pure loss: 15.97779835582979.....Total loss: 15.97779835582979\n",
      "epoch 4936 learning rate:  0.01020259319286872   Training loss:   15.61404487390472  Valing loss:   15.97779835582979\n",
      "Pure loss: 15.609476807214085.....Total loss: 15.609476807214085\n",
      "Pure loss: 15.97150008346477.....Total loss: 15.97150008346477\n",
      "epoch 4937 learning rate:  0.010202552157180474   Training loss:   15.609476807214085  Valing loss:   15.97150008346477\n",
      "Pure loss: 15.657347638377761.....Total loss: 15.657347638377761\n",
      "Pure loss: 16.032096150229965.....Total loss: 16.032096150229965\n",
      "epoch 4938 learning rate:  0.010202511138112597   Training loss:   15.657347638377761  Valing loss:   16.032096150229965\n",
      "Pure loss: 15.264451917633725.....Total loss: 15.264451917633725\n",
      "Pure loss: 15.526458993285273.....Total loss: 15.526458993285273\n",
      "epoch 4939 learning rate:  0.01020247013565499   Training loss:   15.264451917633725  Valing loss:   15.526458993285273\n",
      "Pure loss: 15.131266456705655.....Total loss: 15.131266456705655\n",
      "Pure loss: 15.346802679658921.....Total loss: 15.346802679658921\n",
      "epoch 4940 learning rate:  0.010202429149797571   Training loss:   15.131266456705655  Valing loss:   15.346802679658921\n",
      "Pure loss: 14.527255925674117.....Total loss: 14.527255925674117\n",
      "Pure loss: 14.537565182293546.....Total loss: 14.537565182293546\n",
      "epoch 4941 learning rate:  0.010202388180530257   Training loss:   14.527255925674117  Valing loss:   14.537565182293546\n",
      "Pure loss: 14.464868122935604.....Total loss: 14.464868122935604\n",
      "Pure loss: 14.462310432607591.....Total loss: 14.462310432607591\n",
      "epoch 4942 learning rate:  0.01020234722784298   Training loss:   14.464868122935604  Valing loss:   14.462310432607591\n",
      "Pure loss: 14.154849386892607.....Total loss: 14.154849386892607\n",
      "Pure loss: 13.986566951059203.....Total loss: 13.986566951059203\n",
      "epoch 4943 learning rate:  0.010202306291725673   Training loss:   14.154849386892607  Valing loss:   13.986566951059203\n",
      "Pure loss: 13.664893314222963.....Total loss: 13.664893314222963\n",
      "Pure loss: 13.226511726711946.....Total loss: 13.226511726711946\n",
      "epoch 4944 learning rate:  0.010202265372168284   Training loss:   13.664893314222963  Valing loss:   13.226511726711946\n",
      "Pure loss: 13.644678534255766.....Total loss: 13.644678534255766\n",
      "Pure loss: 13.191148123343863.....Total loss: 13.191148123343863\n",
      "epoch 4945 learning rate:  0.010202224469160769   Training loss:   13.644678534255766  Valing loss:   13.191148123343863\n",
      "Pure loss: 13.735146293902869.....Total loss: 13.735146293902869\n",
      "Pure loss: 13.314508159131185.....Total loss: 13.314508159131185\n",
      "epoch 4946 learning rate:  0.010202183582693085   Training loss:   13.735146293902869  Valing loss:   13.314508159131185\n",
      "Pure loss: 13.624132678864372.....Total loss: 13.624132678864372\n",
      "Pure loss: 13.087729754534548.....Total loss: 13.087729754534548\n",
      "epoch 4947 learning rate:  0.010202142712755205   Training loss:   13.624132678864372  Valing loss:   13.087729754534548\n",
      "Pure loss: 13.432357778727592.....Total loss: 13.432357778727592\n",
      "Pure loss: 12.74159704249777.....Total loss: 12.74159704249777\n",
      "epoch 4948 learning rate:  0.010202101859337107   Training loss:   13.432357778727592  Valing loss:   12.74159704249777\n",
      "Pure loss: 13.451481695428601.....Total loss: 13.451481695428601\n",
      "Pure loss: 12.857945856177048.....Total loss: 12.857945856177048\n",
      "epoch 4949 learning rate:  0.010202061022428774   Training loss:   13.451481695428601  Valing loss:   12.857945856177048\n",
      "Pure loss: 13.606096932030132.....Total loss: 13.606096932030132\n",
      "Pure loss: 13.204761668125991.....Total loss: 13.204761668125991\n",
      "epoch 4950 learning rate:  0.010202020202020202   Training loss:   13.606096932030132  Valing loss:   13.204761668125991\n",
      "Pure loss: 13.288952242802779.....Total loss: 13.288952242802779\n",
      "Pure loss: 12.706947054983342.....Total loss: 12.706947054983342\n",
      "epoch 4951 learning rate:  0.010201979398101394   Training loss:   13.288952242802779  Valing loss:   12.706947054983342\n",
      "Pure loss: 13.341404070598278.....Total loss: 13.341404070598278\n",
      "Pure loss: 12.835830047884258.....Total loss: 12.835830047884258\n",
      "epoch 4952 learning rate:  0.010201938610662358   Training loss:   13.341404070598278  Valing loss:   12.835830047884258\n",
      "Pure loss: 13.26071407862361.....Total loss: 13.26071407862361\n",
      "Pure loss: 12.646655754549462.....Total loss: 12.646655754549462\n",
      "epoch 4953 learning rate:  0.010201897839693115   Training loss:   13.26071407862361  Valing loss:   12.646655754549462\n",
      "Pure loss: 13.232344368646052.....Total loss: 13.232344368646052\n",
      "Pure loss: 12.527397882239697.....Total loss: 12.527397882239697\n",
      "epoch 4954 learning rate:  0.01020185708518369   Training loss:   13.232344368646052  Valing loss:   12.527397882239697\n",
      "Pure loss: 13.217727199071982.....Total loss: 13.217727199071982\n",
      "Pure loss: 12.408047093974863.....Total loss: 12.408047093974863\n",
      "epoch 4955 learning rate:  0.010201816347124117   Training loss:   13.217727199071982  Valing loss:   12.408047093974863\n",
      "Pure loss: 13.2188256529707.....Total loss: 13.2188256529707\n",
      "Pure loss: 12.447456467677966.....Total loss: 12.447456467677966\n",
      "epoch 4956 learning rate:  0.01020177562550444   Training loss:   13.2188256529707  Valing loss:   12.447456467677966\n",
      "Pure loss: 13.27742565787276.....Total loss: 13.27742565787276\n",
      "Pure loss: 12.696266118266129.....Total loss: 12.696266118266129\n",
      "epoch 4957 learning rate:  0.010201734920314707   Training loss:   13.27742565787276  Valing loss:   12.696266118266129\n",
      "Pure loss: 13.282897525872492.....Total loss: 13.282897525872492\n",
      "Pure loss: 12.709998564606146.....Total loss: 12.709998564606146\n",
      "epoch 4958 learning rate:  0.010201694231544978   Training loss:   13.282897525872492  Valing loss:   12.709998564606146\n",
      "Pure loss: 13.23419765622724.....Total loss: 13.23419765622724\n",
      "Pure loss: 12.56285318789771.....Total loss: 12.56285318789771\n",
      "epoch 4959 learning rate:  0.01020165355918532   Training loss:   13.23419765622724  Valing loss:   12.56285318789771\n",
      "Pure loss: 13.223632843072762.....Total loss: 13.223632843072762\n",
      "Pure loss: 12.52261178548634.....Total loss: 12.52261178548634\n",
      "epoch 4960 learning rate:  0.010201612903225807   Training loss:   13.223632843072762  Valing loss:   12.52261178548634\n",
      "Pure loss: 13.238493743536331.....Total loss: 13.238493743536331\n",
      "Pure loss: 12.526098542703693.....Total loss: 12.526098542703693\n",
      "epoch 4961 learning rate:  0.010201572263656521   Training loss:   13.238493743536331  Valing loss:   12.526098542703693\n",
      "Pure loss: 13.255753678914763.....Total loss: 13.255753678914763\n",
      "Pure loss: 12.600312674842895.....Total loss: 12.600312674842895\n",
      "epoch 4962 learning rate:  0.010201531640467554   Training loss:   13.255753678914763  Valing loss:   12.600312674842895\n",
      "Pure loss: 13.438685994642338.....Total loss: 13.438685994642338\n",
      "Pure loss: 13.062327349359665.....Total loss: 13.062327349359665\n",
      "epoch 4963 learning rate:  0.010201491033649002   Training loss:   13.438685994642338  Valing loss:   13.062327349359665\n",
      "Pure loss: 13.512728664123163.....Total loss: 13.512728664123163\n",
      "Pure loss: 13.202416036106317.....Total loss: 13.202416036106317\n",
      "epoch 4964 learning rate:  0.010201450443190976   Training loss:   13.512728664123163  Valing loss:   13.202416036106317\n",
      "Pure loss: 13.42839720352294.....Total loss: 13.42839720352294\n",
      "Pure loss: 13.04056454267289.....Total loss: 13.04056454267289\n",
      "epoch 4965 learning rate:  0.010201409869083584   Training loss:   13.42839720352294  Valing loss:   13.04056454267289\n",
      "Pure loss: 13.438859087464085.....Total loss: 13.438859087464085\n",
      "Pure loss: 13.05846982367989.....Total loss: 13.05846982367989\n",
      "epoch 4966 learning rate:  0.010201369311316955   Training loss:   13.438859087464085  Valing loss:   13.05846982367989\n",
      "Pure loss: 13.42383241152743.....Total loss: 13.42383241152743\n",
      "Pure loss: 13.028098583883551.....Total loss: 13.028098583883551\n",
      "epoch 4967 learning rate:  0.010201328769881217   Training loss:   13.42383241152743  Valing loss:   13.028098583883551\n",
      "Pure loss: 13.639839169104999.....Total loss: 13.639839169104999\n",
      "Pure loss: 13.440702257738694.....Total loss: 13.440702257738694\n",
      "epoch 4968 learning rate:  0.010201288244766505   Training loss:   13.639839169104999  Valing loss:   13.440702257738694\n",
      "Pure loss: 13.594931047062243.....Total loss: 13.594931047062243\n",
      "Pure loss: 13.362270850018207.....Total loss: 13.362270850018207\n",
      "epoch 4969 learning rate:  0.010201247735962971   Training loss:   13.594931047062243  Valing loss:   13.362270850018207\n",
      "Pure loss: 13.44432436051127.....Total loss: 13.44432436051127\n",
      "Pure loss: 13.091748731853851.....Total loss: 13.091748731853851\n",
      "epoch 4970 learning rate:  0.010201207243460764   Training loss:   13.44432436051127  Valing loss:   13.091748731853851\n",
      "Pure loss: 13.50391788424693.....Total loss: 13.50391788424693\n",
      "Pure loss: 13.201211523179154.....Total loss: 13.201211523179154\n",
      "epoch 4971 learning rate:  0.010201166767250051   Training loss:   13.50391788424693  Valing loss:   13.201211523179154\n",
      "Pure loss: 13.635049575224734.....Total loss: 13.635049575224734\n",
      "Pure loss: 13.40175028169491.....Total loss: 13.40175028169491\n",
      "epoch 4972 learning rate:  0.010201126307320998   Training loss:   13.635049575224734  Valing loss:   13.40175028169491\n",
      "Pure loss: 13.40891501997167.....Total loss: 13.40891501997167\n",
      "Pure loss: 13.026123841561537.....Total loss: 13.026123841561537\n",
      "epoch 4973 learning rate:  0.010201085863663785   Training loss:   13.40891501997167  Valing loss:   13.026123841561537\n",
      "Pure loss: 14.345177436051618.....Total loss: 14.345177436051618\n",
      "Pure loss: 14.61706421907477.....Total loss: 14.61706421907477\n",
      "epoch 4974 learning rate:  0.010201045436268597   Training loss:   14.345177436051618  Valing loss:   14.61706421907477\n",
      "Pure loss: 14.864947512884335.....Total loss: 14.864947512884335\n",
      "Pure loss: 15.252275577171526.....Total loss: 15.252275577171526\n",
      "epoch 4975 learning rate:  0.010201005025125628   Training loss:   14.864947512884335  Valing loss:   15.252275577171526\n",
      "Pure loss: 14.899718953428387.....Total loss: 14.899718953428387\n",
      "Pure loss: 15.29885780921444.....Total loss: 15.29885780921444\n",
      "epoch 4976 learning rate:  0.01020096463022508   Training loss:   14.899718953428387  Valing loss:   15.29885780921444\n",
      "Pure loss: 14.81594997865993.....Total loss: 14.81594997865993\n",
      "Pure loss: 15.184272656703566.....Total loss: 15.184272656703566\n",
      "epoch 4977 learning rate:  0.010200924251557162   Training loss:   14.81594997865993  Valing loss:   15.184272656703566\n",
      "Pure loss: 14.573610584447918.....Total loss: 14.573610584447918\n",
      "Pure loss: 14.8472986018363.....Total loss: 14.8472986018363\n",
      "epoch 4978 learning rate:  0.010200883889112094   Training loss:   14.573610584447918  Valing loss:   14.8472986018363\n",
      "Pure loss: 14.523508023346027.....Total loss: 14.523508023346027\n",
      "Pure loss: 14.776735276664384.....Total loss: 14.776735276664384\n",
      "epoch 4979 learning rate:  0.010200843542880096   Training loss:   14.523508023346027  Valing loss:   14.776735276664384\n",
      "Pure loss: 14.11636804554936.....Total loss: 14.11636804554936\n",
      "Pure loss: 14.21501902545426.....Total loss: 14.21501902545426\n",
      "epoch 4980 learning rate:  0.010200803212851406   Training loss:   14.11636804554936  Valing loss:   14.21501902545426\n",
      "Pure loss: 13.945965901861971.....Total loss: 13.945965901861971\n",
      "Pure loss: 13.981571112544454.....Total loss: 13.981571112544454\n",
      "epoch 4981 learning rate:  0.010200762899016263   Training loss:   13.945965901861971  Valing loss:   13.981571112544454\n",
      "Pure loss: 13.604124254305107.....Total loss: 13.604124254305107\n",
      "Pure loss: 13.445725201402398.....Total loss: 13.445725201402398\n",
      "epoch 4982 learning rate:  0.010200722601364913   Training loss:   13.604124254305107  Valing loss:   13.445725201402398\n",
      "Pure loss: 14.005434376590536.....Total loss: 14.005434376590536\n",
      "Pure loss: 14.02421900719846.....Total loss: 14.02421900719846\n",
      "epoch 4983 learning rate:  0.010200682319887619   Training loss:   14.005434376590536  Valing loss:   14.02421900719846\n",
      "Pure loss: 13.808486944996758.....Total loss: 13.808486944996758\n",
      "Pure loss: 13.771383419855082.....Total loss: 13.771383419855082\n",
      "epoch 4984 learning rate:  0.01020064205457464   Training loss:   13.808486944996758  Valing loss:   13.771383419855082\n",
      "Pure loss: 13.62508110056741.....Total loss: 13.62508110056741\n",
      "Pure loss: 13.479368696298692.....Total loss: 13.479368696298692\n",
      "epoch 4985 learning rate:  0.01020060180541625   Training loss:   13.62508110056741  Valing loss:   13.479368696298692\n",
      "Pure loss: 13.310084620041456.....Total loss: 13.310084620041456\n",
      "Pure loss: 12.885909715346527.....Total loss: 12.885909715346527\n",
      "epoch 4986 learning rate:  0.010200561572402728   Training loss:   13.310084620041456  Valing loss:   12.885909715346527\n",
      "Pure loss: 13.160151263215145.....Total loss: 13.160151263215145\n",
      "Pure loss: 12.457614533141038.....Total loss: 12.457614533141038\n",
      "epoch 4987 learning rate:  0.010200521355524364   Training loss:   13.160151263215145  Valing loss:   12.457614533141038\n",
      "Pure loss: 13.173215716096019.....Total loss: 13.173215716096019\n",
      "Pure loss: 12.525784594539275.....Total loss: 12.525784594539275\n",
      "epoch 4988 learning rate:  0.010200481154771452   Training loss:   13.173215716096019  Valing loss:   12.525784594539275\n",
      "Pure loss: 13.167177621943097.....Total loss: 13.167177621943097\n",
      "Pure loss: 12.495756591322616.....Total loss: 12.495756591322616\n",
      "epoch 4989 learning rate:  0.010200440970134296   Training loss:   13.167177621943097  Valing loss:   12.495756591322616\n",
      "Pure loss: 13.169534648921822.....Total loss: 13.169534648921822\n",
      "Pure loss: 12.508972546314142.....Total loss: 12.508972546314142\n",
      "epoch 4990 learning rate:  0.010200400801603206   Training loss:   13.169534648921822  Valing loss:   12.508972546314142\n",
      "Pure loss: 13.15515676830032.....Total loss: 13.15515676830032\n",
      "Pure loss: 12.423169245636634.....Total loss: 12.423169245636634\n",
      "epoch 4991 learning rate:  0.010200360649168504   Training loss:   13.15515676830032  Valing loss:   12.423169245636634\n",
      "Pure loss: 13.270174332646851.....Total loss: 13.270174332646851\n",
      "Pure loss: 12.84026976136692.....Total loss: 12.84026976136692\n",
      "epoch 4992 learning rate:  0.010200320512820512   Training loss:   13.270174332646851  Valing loss:   12.84026976136692\n",
      "Pure loss: 13.28858895642405.....Total loss: 13.28858895642405\n",
      "Pure loss: 12.885653668857442.....Total loss: 12.885653668857442\n",
      "epoch 4993 learning rate:  0.01020028039254957   Training loss:   13.28858895642405  Valing loss:   12.885653668857442\n",
      "Pure loss: 13.253698967244116.....Total loss: 13.253698967244116\n",
      "Pure loss: 12.804920248125626.....Total loss: 12.804920248125626\n",
      "epoch 4994 learning rate:  0.010200240288346015   Training loss:   13.253698967244116  Valing loss:   12.804920248125626\n",
      "Pure loss: 13.229999518011335.....Total loss: 13.229999518011335\n",
      "Pure loss: 12.738795730418834.....Total loss: 12.738795730418834\n",
      "epoch 4995 learning rate:  0.0102002002002002   Training loss:   13.229999518011335  Valing loss:   12.738795730418834\n",
      "Pure loss: 13.211867946117609.....Total loss: 13.211867946117609\n",
      "Pure loss: 12.681467451697248.....Total loss: 12.681467451697248\n",
      "epoch 4996 learning rate:  0.010200160128102482   Training loss:   13.211867946117609  Valing loss:   12.681467451697248\n",
      "Pure loss: 13.17153736617521.....Total loss: 13.17153736617521\n",
      "Pure loss: 12.528161113931764.....Total loss: 12.528161113931764\n",
      "epoch 4997 learning rate:  0.010200120072043226   Training loss:   13.17153736617521  Valing loss:   12.528161113931764\n",
      "Pure loss: 13.188551451302075.....Total loss: 13.188551451302075\n",
      "Pure loss: 12.615985171731277.....Total loss: 12.615985171731277\n",
      "epoch 4998 learning rate:  0.010200080032012806   Training loss:   13.188551451302075  Valing loss:   12.615985171731277\n",
      "Pure loss: 13.17871376942722.....Total loss: 13.17871376942722\n",
      "Pure loss: 12.571095922606242.....Total loss: 12.571095922606242\n",
      "epoch 4999 learning rate:  0.0102000400080016   Training loss:   13.17871376942722  Valing loss:   12.571095922606242\n",
      "Pure loss: 13.189525298341534.....Total loss: 13.189525298341534\n",
      "Pure loss: 12.567815518134724.....Total loss: 12.567815518134724\n",
      "epoch 5000 learning rate:  0.0102   Training loss:   13.189525298341534  Valing loss:   12.567815518134724\n",
      "Training...2.2098958492279053s...Successful!!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#initial weight with normal distribution \n",
    "gg1_train,gg1_val = gg1.train(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg2 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg2.initialPar(X_train.shape[1],'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial weight randomly \n",
    "gg2_train,gg2_val = gg2.train(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg3 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial weight with 0\n",
    "gg3.initialPar(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg3_train,gg3_val = gg3.train(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg11 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg11.initialPar(X_train.shape[1],'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg11_train,gg11_val = gg11.train(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg22 = GradientDescentLinearRegression()\n",
    "gg22.initialPar(X_train.shape[1],'random')\n",
    "gg22_train,gg22_val = gg22.train(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg33 = GradientDescentLinearRegression()\n",
    "gg33.initialPar(X_train.shape[1])\n",
    "gg33_train,gg33_val = gg33.train(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(range(1,len(gg1_train)+1), gg1_train, 'r--')\n",
    "plt.plot(range(1,len(gg2_train)+1), gg2_train, 'bp-.')\n",
    "label = ['gg1_train', 'gg2_train'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(range(1,len(gg2_train)+1), gg2_train, 'b--')\n",
    "plt.plot(range(1,len(gg3_train)+1), gg3_train, 'gp-.')\n",
    "plt.plot(range(1,len(gg22_train)+1), gg22_train, 'c--')\n",
    "plt.plot(range(1,len(gg33_train)+1), gg33_train, 'kp-.')\n",
    "label = ['gg2_train','gg3_train', 'gg22_train','gg33_train'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(range(1,len(gg2_train)+1), gg2_train, 'b--')\n",
    "plt.plot(range(1,len(gg3_train)+1), gg3_train, 'gp-.')\n",
    "label = ['gg2_train','gg3_train'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.tight_layout()\n",
    "#plt.subplots_adjust(wspace =2, hspace =2)\n",
    "plt.savefig('gg3.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg4 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg4.initialPar(X_train.shape[1],'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gg4_train,gg4_val = gg4.train(X_train,y_train,X_val,y_val,rate0=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg5 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg5.initialPar(X_train.shape[1],'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg5_train,gg5_val = gg5.train(X_train,y_train,X_val,y_val,rate0=0.1,maxLoop=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg6 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg6.initialPar(X_train.shape[1],'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg6_train,gg6_val = gg6.train(X_train,y_train,X_val,y_val,rate0=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg7 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg7.initialPar(X_train.shape[1],'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg7_train,gg7_val = gg7.train(X_train,y_train,X_val,y_val,rate0=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg8 = GradientDescentLinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg8.initialPar(X_train.shape[1],'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg8_train,gg8_val = gg8.train(X_train,y_train,X_val,y_val,rate0=0.001,maxLoop=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(range(1,len(gg4_train)+1), gg4_train, 'r--')\n",
    "plt.plot(range(1,len(gg4_val)+1), gg4_val, 'b--')\n",
    "label = ['gg4_train', 'gg4_val'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.title('learning rate = 1.0')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(range(1,len(gg5_train)+1), gg5_train, 'r--')\n",
    "plt.plot(range(1,len(gg5_val)+1), gg5_val, 'b-.')\n",
    "label = ['gg5_train','gg5_val'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.title('learning rate = 0.1')\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(range(1,len(gg6_train)+1), gg6_train, 'r--')\n",
    "plt.plot(range(1,len(gg6_val)+1), gg6_val, 'b--')\n",
    "label = ['gg6_train', 'gg6_val'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.title('learning rate = 0.01')\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(range(1,len(gg8_train)+1), gg8_train, 'r--')\n",
    "plt.plot(range(1,len(gg8_val)+1), gg8_val, 'b--')\n",
    "label = ['gg8_train', 'gg8_val'] \n",
    "plt.legend(label, loc='upper right')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.title('learning rate = 0.001')\n",
    "plt.tight_layout()\n",
    "#plt.subplots_adjust(wspace =2, hspace =2)\n",
    "plt.savefig('gg4.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
